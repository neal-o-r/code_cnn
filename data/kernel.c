int acct_parm[3] = {4, 2, 30};





struct bsd_acct_struct {
 struct fs_pin pin;
 atomic_long_t count;
 struct rcu_head rcu;
 struct mutex lock;
 int active;
 unsigned long needcheck;
 struct file *file;
 struct pid_namespace *ns;
 struct work_struct work;
 struct completion done;
};

static void do_acct_process(struct bsd_acct_struct *acct);




static int check_free_space(struct bsd_acct_struct *acct)
{
 struct kstatfs sbuf;

 if (time_is_before_jiffies(acct->needcheck))
  goto out;


 if (vfs_statfs(&acct->file->f_path, &sbuf))
  goto out;

 if (acct->active) {
  u64 suspend = sbuf.f_blocks * SUSPEND;
  do_div(suspend, 100);
  if (sbuf.f_bavail <= suspend) {
   acct->active = 0;
   pr_info("Process accounting paused\n");
  }
 } else {
  u64 resume = sbuf.f_blocks * RESUME;
  do_div(resume, 100);
  if (sbuf.f_bavail >= resume) {
   acct->active = 1;
   pr_info("Process accounting resumed\n");
  }
 }

 acct->needcheck = jiffies + ACCT_TIMEOUT*HZ;
 out:
 return acct->active;
}

static void acct_put(struct bsd_acct_struct *p)
{
 if (atomic_long_dec_and_test(&p->count))
  kfree_rcu(p, rcu);
}

static inline struct bsd_acct_struct *to_acct(struct fs_pin *p)
{
 return p ? container_of(p, struct bsd_acct_struct, pin) : NULL;
}

static struct bsd_acct_struct *acct_get(struct pid_namespace *ns)
{
 struct bsd_acct_struct *res;
 again:
 smp_rmb();
 rcu_read_lock();
 res = to_acct(ACCESS_ONCE(ns->bacct));
 if (!res) {
  rcu_read_unlock();
  return NULL;
 }
 if (!atomic_long_inc_not_zero(&res->count)) {
  rcu_read_unlock();
  cpu_relax();
  goto again;
 }
 rcu_read_unlock();
 mutex_lock(&res->lock);
 if (res != to_acct(ACCESS_ONCE(ns->bacct))) {
  mutex_unlock(&res->lock);
  acct_put(res);
  goto again;
 }
 return res;
}

static void acct_pin_kill(struct fs_pin *pin)
{
 struct bsd_acct_struct *acct = to_acct(pin);
 mutex_lock(&acct->lock);
 do_acct_process(acct);
 schedule_work(&acct->work);
 wait_for_completion(&acct->done);
 cmpxchg(&acct->ns->bacct, pin, NULL);
 mutex_unlock(&acct->lock);
 pin_remove(pin);
 acct_put(acct);
}

static void close_work(struct work_struct *work)
{
 struct bsd_acct_struct *acct = container_of(work, struct bsd_acct_struct, work);
 struct file *file = acct->file;
 if (file->f_op->flush)
  file->f_op->flush(file, NULL);
 __fput_sync(file);
 complete(&acct->done);
}

static int acct_on(struct filename *pathname)
{
 struct file *file;
 struct vfsmount *mnt, *internal;
 struct pid_namespace *ns = task_active_pid_ns(current);
 struct bsd_acct_struct *acct;
 struct fs_pin *old;
 int err;

 acct = kzalloc(sizeof(struct bsd_acct_struct), GFP_KERNEL);
 if (!acct)
  return -ENOMEM;


 file = file_open_name(pathname, O_WRONLY|O_APPEND|O_LARGEFILE, 0);
 if (IS_ERR(file)) {
  kfree(acct);
  return PTR_ERR(file);
 }

 if (!S_ISREG(file_inode(file)->i_mode)) {
  kfree(acct);
  filp_close(file, NULL);
  return -EACCES;
 }

 if (!(file->f_mode & FMODE_CAN_WRITE)) {
  kfree(acct);
  filp_close(file, NULL);
  return -EIO;
 }
 internal = mnt_clone_internal(&file->f_path);
 if (IS_ERR(internal)) {
  kfree(acct);
  filp_close(file, NULL);
  return PTR_ERR(internal);
 }
 err = mnt_want_write(internal);
 if (err) {
  mntput(internal);
  kfree(acct);
  filp_close(file, NULL);
  return err;
 }
 mnt = file->f_path.mnt;
 file->f_path.mnt = internal;

 atomic_long_set(&acct->count, 1);
 init_fs_pin(&acct->pin, acct_pin_kill);
 acct->file = file;
 acct->needcheck = jiffies;
 acct->ns = ns;
 mutex_init(&acct->lock);
 INIT_WORK(&acct->work, close_work);
 init_completion(&acct->done);
 mutex_lock_nested(&acct->lock, 1);
 pin_insert(&acct->pin, mnt);

 rcu_read_lock();
 old = xchg(&ns->bacct, &acct->pin);
 mutex_unlock(&acct->lock);
 pin_kill(old);
 mnt_drop_write(mnt);
 mntput(mnt);
 return 0;
}

static DEFINE_MUTEX(acct_on_mutex);
SYSCALL_DEFINE1(acct, const char __user *, name)
{
 int error = 0;

 if (!capable(CAP_SYS_PACCT))
  return -EPERM;

 if (name) {
  struct filename *tmp = getname(name);

  if (IS_ERR(tmp))
   return PTR_ERR(tmp);
  mutex_lock(&acct_on_mutex);
  error = acct_on(tmp);
  mutex_unlock(&acct_on_mutex);
  putname(tmp);
 } else {
  rcu_read_lock();
  pin_kill(task_active_pid_ns(current)->bacct);
 }

 return error;
}

void acct_exit_ns(struct pid_namespace *ns)
{
 rcu_read_lock();
 pin_kill(ns->bacct);
}

static comp_t encode_comp_t(unsigned long value)
{
 int exp, rnd;

 exp = rnd = 0;
 while (value > MAXFRACT) {
  rnd = value & (1 << (EXPSIZE - 1));
  value >>= EXPSIZE;
  exp++;
 }




 if (rnd && (++value > MAXFRACT)) {
  value >>= EXPSIZE;
  exp++;
 }




 exp <<= MANTSIZE;
 exp += value;
 return exp;
}


static comp2_t encode_comp2_t(u64 value)
{
 int exp, rnd;

 exp = (value > (MAXFRACT2>>1));
 rnd = 0;
 while (value > MAXFRACT2) {
  rnd = value & 1;
  value >>= 1;
  exp++;
 }




 if (rnd && (++value > MAXFRACT2)) {
  value >>= 1;
  exp++;
 }

 if (exp > MAXEXP2) {

  return (1ul << (MANTSIZE2+EXPSIZE2-1)) - 1;
 } else {
  return (value & (MAXFRACT2>>1)) | (exp << (MANTSIZE2-1));
 }
}




static u32 encode_float(u64 value)
{
 unsigned exp = 190;
 unsigned u;

 if (value == 0)
  return 0;
 while ((s64)value > 0) {
  value <<= 1;
  exp--;
 }
 u = (u32)(value >> 40) & 0x7fffffu;
 return u | (exp << 23);
}
static void fill_ac(acct_t *ac)
{
 struct pacct_struct *pacct = &current->signal->pacct;
 u64 elapsed, run_time;
 struct tty_struct *tty;





 memset(ac, 0, sizeof(acct_t));

 ac->ac_version = ACCT_VERSION | ACCT_BYTEORDER;
 strlcpy(ac->ac_comm, current->comm, sizeof(ac->ac_comm));


 run_time = ktime_get_ns();
 run_time -= current->group_leader->start_time;

 elapsed = nsec_to_AHZ(run_time);
 ac->ac_etime = encode_float(elapsed);
 ac->ac_etime = encode_comp_t(elapsed < (unsigned long) -1l ?
    (unsigned long) elapsed : (unsigned long) -1l);
 {

  comp2_t etime = encode_comp2_t(elapsed);

  ac->ac_etime_hi = etime >> 16;
  ac->ac_etime_lo = (u16) etime;
 }
 do_div(elapsed, AHZ);
 ac->ac_btime = get_seconds() - elapsed;
 ac->ac_ahz = AHZ;

 spin_lock_irq(&current->sighand->siglock);
 tty = current->signal->tty;
 ac->ac_tty = tty ? old_encode_dev(tty_devnum(tty)) : 0;
 ac->ac_utime = encode_comp_t(jiffies_to_AHZ(cputime_to_jiffies(pacct->ac_utime)));
 ac->ac_stime = encode_comp_t(jiffies_to_AHZ(cputime_to_jiffies(pacct->ac_stime)));
 ac->ac_flag = pacct->ac_flag;
 ac->ac_mem = encode_comp_t(pacct->ac_mem);
 ac->ac_minflt = encode_comp_t(pacct->ac_minflt);
 ac->ac_majflt = encode_comp_t(pacct->ac_majflt);
 ac->ac_exitcode = pacct->ac_exitcode;
 spin_unlock_irq(&current->sighand->siglock);
}



static void do_acct_process(struct bsd_acct_struct *acct)
{
 acct_t ac;
 unsigned long flim;
 const struct cred *orig_cred;
 struct file *file = acct->file;




 flim = current->signal->rlim[RLIMIT_FSIZE].rlim_cur;
 current->signal->rlim[RLIMIT_FSIZE].rlim_cur = RLIM_INFINITY;

 orig_cred = override_creds(file->f_cred);





 if (!check_free_space(acct))
  goto out;

 fill_ac(&ac);

 ac.ac_uid = from_kuid_munged(file->f_cred->user_ns, orig_cred->uid);
 ac.ac_gid = from_kgid_munged(file->f_cred->user_ns, orig_cred->gid);

 ac.ac_uid16 = ac.ac_uid;
 ac.ac_gid16 = ac.ac_gid;
 {
  struct pid_namespace *ns = acct->ns;

  ac.ac_pid = task_tgid_nr_ns(current, ns);
  rcu_read_lock();
  ac.ac_ppid = task_tgid_nr_ns(rcu_dereference(current->real_parent),
          ns);
  rcu_read_unlock();
 }




 if (file_start_write_trylock(file)) {

  loff_t pos = 0;
  __kernel_write(file, (char *)&ac, sizeof(acct_t), &pos);
  file_end_write(file);
 }
out:
 current->signal->rlim[RLIMIT_FSIZE].rlim_cur = flim;
 revert_creds(orig_cred);
}






void acct_collect(long exitcode, int group_dead)
{
 struct pacct_struct *pacct = &current->signal->pacct;
 cputime_t utime, stime;
 unsigned long vsize = 0;

 if (group_dead && current->mm) {
  struct vm_area_struct *vma;

  down_read(&current->mm->mmap_sem);
  vma = current->mm->mmap;
  while (vma) {
   vsize += vma->vm_end - vma->vm_start;
   vma = vma->vm_next;
  }
  up_read(&current->mm->mmap_sem);
 }

 spin_lock_irq(&current->sighand->siglock);
 if (group_dead)
  pacct->ac_mem = vsize / 1024;
 if (thread_group_leader(current)) {
  pacct->ac_exitcode = exitcode;
  if (current->flags & PF_FORKNOEXEC)
   pacct->ac_flag |= AFORK;
 }
 if (current->flags & PF_SUPERPRIV)
  pacct->ac_flag |= ASU;
 if (current->flags & PF_DUMPCORE)
  pacct->ac_flag |= ACORE;
 if (current->flags & PF_SIGNALED)
  pacct->ac_flag |= AXSIG;
 task_cputime(current, &utime, &stime);
 pacct->ac_utime += utime;
 pacct->ac_stime += stime;
 pacct->ac_minflt += current->min_flt;
 pacct->ac_majflt += current->maj_flt;
 spin_unlock_irq(&current->sighand->siglock);
}

static void slow_acct_process(struct pid_namespace *ns)
{
 for ( ; ns; ns = ns->parent) {
  struct bsd_acct_struct *acct = acct_get(ns);
  if (acct) {
   do_acct_process(acct);
   mutex_unlock(&acct->lock);
   acct_put(acct);
  }
 }
}






void acct_process(void)
{
 struct pid_namespace *ns;






 for (ns = task_active_pid_ns(current); ns != NULL; ns = ns->parent) {
  if (ns->bacct)
   break;
 }
 if (unlikely(ns))
  slow_acct_process(ns);
}
static struct alarm_base {
 spinlock_t lock;
 struct timerqueue_head timerqueue;
 ktime_t (*gettime)(void);
 clockid_t base_clockid;
} alarm_bases[ALARM_NUMTYPE];


static ktime_t freezer_delta;
static DEFINE_SPINLOCK(freezer_delta_lock);

static struct wakeup_source *ws;


static struct rtc_timer rtctimer;
static struct rtc_device *rtcdev;
static DEFINE_SPINLOCK(rtcdev_lock);
struct rtc_device *alarmtimer_get_rtcdev(void)
{
 unsigned long flags;
 struct rtc_device *ret;

 spin_lock_irqsave(&rtcdev_lock, flags);
 ret = rtcdev;
 spin_unlock_irqrestore(&rtcdev_lock, flags);

 return ret;
}
EXPORT_SYMBOL_GPL(alarmtimer_get_rtcdev);

static int alarmtimer_rtc_add_device(struct device *dev,
    struct class_interface *class_intf)
{
 unsigned long flags;
 struct rtc_device *rtc = to_rtc_device(dev);

 if (rtcdev)
  return -EBUSY;

 if (!rtc->ops->set_alarm)
  return -1;
 if (!device_may_wakeup(rtc->dev.parent))
  return -1;

 spin_lock_irqsave(&rtcdev_lock, flags);
 if (!rtcdev) {
  rtcdev = rtc;

  get_device(dev);
 }
 spin_unlock_irqrestore(&rtcdev_lock, flags);
 return 0;
}

static inline void alarmtimer_rtc_timer_init(void)
{
 rtc_timer_init(&rtctimer, NULL, NULL);
}

static struct class_interface alarmtimer_rtc_interface = {
 .add_dev = &alarmtimer_rtc_add_device,
};

static int alarmtimer_rtc_interface_setup(void)
{
 alarmtimer_rtc_interface.class = rtc_class;
 return class_interface_register(&alarmtimer_rtc_interface);
}
static void alarmtimer_rtc_interface_remove(void)
{
 class_interface_unregister(&alarmtimer_rtc_interface);
}
struct rtc_device *alarmtimer_get_rtcdev(void)
{
 return NULL;
}
static inline int alarmtimer_rtc_interface_setup(void) { return 0; }
static inline void alarmtimer_rtc_interface_remove(void) { }
static inline void alarmtimer_rtc_timer_init(void) { }
static void alarmtimer_enqueue(struct alarm_base *base, struct alarm *alarm)
{
 if (alarm->state & ALARMTIMER_STATE_ENQUEUED)
  timerqueue_del(&base->timerqueue, &alarm->node);

 timerqueue_add(&base->timerqueue, &alarm->node);
 alarm->state |= ALARMTIMER_STATE_ENQUEUED;
}
static void alarmtimer_dequeue(struct alarm_base *base, struct alarm *alarm)
{
 if (!(alarm->state & ALARMTIMER_STATE_ENQUEUED))
  return;

 timerqueue_del(&base->timerqueue, &alarm->node);
 alarm->state &= ~ALARMTIMER_STATE_ENQUEUED;
}
static enum hrtimer_restart alarmtimer_fired(struct hrtimer *timer)
{
 struct alarm *alarm = container_of(timer, struct alarm, timer);
 struct alarm_base *base = &alarm_bases[alarm->type];
 unsigned long flags;
 int ret = HRTIMER_NORESTART;
 int restart = ALARMTIMER_NORESTART;

 spin_lock_irqsave(&base->lock, flags);
 alarmtimer_dequeue(base, alarm);
 spin_unlock_irqrestore(&base->lock, flags);

 if (alarm->function)
  restart = alarm->function(alarm, base->gettime());

 spin_lock_irqsave(&base->lock, flags);
 if (restart != ALARMTIMER_NORESTART) {
  hrtimer_set_expires(&alarm->timer, alarm->node.expires);
  alarmtimer_enqueue(base, alarm);
  ret = HRTIMER_RESTART;
 }
 spin_unlock_irqrestore(&base->lock, flags);

 return ret;

}

ktime_t alarm_expires_remaining(const struct alarm *alarm)
{
 struct alarm_base *base = &alarm_bases[alarm->type];
 return ktime_sub(alarm->node.expires, base->gettime());
}
EXPORT_SYMBOL_GPL(alarm_expires_remaining);

static int alarmtimer_suspend(struct device *dev)
{
 struct rtc_time tm;
 ktime_t min, now;
 unsigned long flags;
 struct rtc_device *rtc;
 int i;
 int ret;

 spin_lock_irqsave(&freezer_delta_lock, flags);
 min = freezer_delta;
 freezer_delta = ktime_set(0, 0);
 spin_unlock_irqrestore(&freezer_delta_lock, flags);

 rtc = alarmtimer_get_rtcdev();

 if (!rtc)
  return 0;


 for (i = 0; i < ALARM_NUMTYPE; i++) {
  struct alarm_base *base = &alarm_bases[i];
  struct timerqueue_node *next;
  ktime_t delta;

  spin_lock_irqsave(&base->lock, flags);
  next = timerqueue_getnext(&base->timerqueue);
  spin_unlock_irqrestore(&base->lock, flags);
  if (!next)
   continue;
  delta = ktime_sub(next->expires, base->gettime());
  if (!min.tv64 || (delta.tv64 < min.tv64))
   min = delta;
 }
 if (min.tv64 == 0)
  return 0;

 if (ktime_to_ns(min) < 2 * NSEC_PER_SEC) {
  __pm_wakeup_event(ws, 2 * MSEC_PER_SEC);
  return -EBUSY;
 }


 rtc_timer_cancel(rtc, &rtctimer);
 rtc_read_time(rtc, &tm);
 now = rtc_tm_to_ktime(tm);
 now = ktime_add(now, min);


 ret = rtc_timer_start(rtc, &rtctimer, now, ktime_set(0, 0));
 if (ret < 0)
  __pm_wakeup_event(ws, MSEC_PER_SEC);
 return ret;
}

static int alarmtimer_resume(struct device *dev)
{
 struct rtc_device *rtc;

 rtc = alarmtimer_get_rtcdev();
 if (rtc)
  rtc_timer_cancel(rtc, &rtctimer);
 return 0;
}

static int alarmtimer_suspend(struct device *dev)
{
 return 0;
}

static int alarmtimer_resume(struct device *dev)
{
 return 0;
}

static void alarmtimer_freezerset(ktime_t absexp, enum alarmtimer_type type)
{
 ktime_t delta;
 unsigned long flags;
 struct alarm_base *base = &alarm_bases[type];

 delta = ktime_sub(absexp, base->gettime());

 spin_lock_irqsave(&freezer_delta_lock, flags);
 if (!freezer_delta.tv64 || (delta.tv64 < freezer_delta.tv64))
  freezer_delta = delta;
 spin_unlock_irqrestore(&freezer_delta_lock, flags);
}
void alarm_init(struct alarm *alarm, enum alarmtimer_type type,
  enum alarmtimer_restart (*function)(struct alarm *, ktime_t))
{
 timerqueue_init(&alarm->node);
 hrtimer_init(&alarm->timer, alarm_bases[type].base_clockid,
   HRTIMER_MODE_ABS);
 alarm->timer.function = alarmtimer_fired;
 alarm->function = function;
 alarm->type = type;
 alarm->state = ALARMTIMER_STATE_INACTIVE;
}
EXPORT_SYMBOL_GPL(alarm_init);






void alarm_start(struct alarm *alarm, ktime_t start)
{
 struct alarm_base *base = &alarm_bases[alarm->type];
 unsigned long flags;

 spin_lock_irqsave(&base->lock, flags);
 alarm->node.expires = start;
 alarmtimer_enqueue(base, alarm);
 hrtimer_start(&alarm->timer, alarm->node.expires, HRTIMER_MODE_ABS);
 spin_unlock_irqrestore(&base->lock, flags);
}
EXPORT_SYMBOL_GPL(alarm_start);






void alarm_start_relative(struct alarm *alarm, ktime_t start)
{
 struct alarm_base *base = &alarm_bases[alarm->type];

 start = ktime_add(start, base->gettime());
 alarm_start(alarm, start);
}
EXPORT_SYMBOL_GPL(alarm_start_relative);

void alarm_restart(struct alarm *alarm)
{
 struct alarm_base *base = &alarm_bases[alarm->type];
 unsigned long flags;

 spin_lock_irqsave(&base->lock, flags);
 hrtimer_set_expires(&alarm->timer, alarm->node.expires);
 hrtimer_restart(&alarm->timer);
 alarmtimer_enqueue(base, alarm);
 spin_unlock_irqrestore(&base->lock, flags);
}
EXPORT_SYMBOL_GPL(alarm_restart);
int alarm_try_to_cancel(struct alarm *alarm)
{
 struct alarm_base *base = &alarm_bases[alarm->type];
 unsigned long flags;
 int ret;

 spin_lock_irqsave(&base->lock, flags);
 ret = hrtimer_try_to_cancel(&alarm->timer);
 if (ret >= 0)
  alarmtimer_dequeue(base, alarm);
 spin_unlock_irqrestore(&base->lock, flags);
 return ret;
}
EXPORT_SYMBOL_GPL(alarm_try_to_cancel);
int alarm_cancel(struct alarm *alarm)
{
 for (;;) {
  int ret = alarm_try_to_cancel(alarm);
  if (ret >= 0)
   return ret;
  cpu_relax();
 }
}
EXPORT_SYMBOL_GPL(alarm_cancel);


u64 alarm_forward(struct alarm *alarm, ktime_t now, ktime_t interval)
{
 u64 overrun = 1;
 ktime_t delta;

 delta = ktime_sub(now, alarm->node.expires);

 if (delta.tv64 < 0)
  return 0;

 if (unlikely(delta.tv64 >= interval.tv64)) {
  s64 incr = ktime_to_ns(interval);

  overrun = ktime_divns(delta, incr);

  alarm->node.expires = ktime_add_ns(alarm->node.expires,
       incr*overrun);

  if (alarm->node.expires.tv64 > now.tv64)
   return overrun;




  overrun++;
 }

 alarm->node.expires = ktime_add(alarm->node.expires, interval);
 return overrun;
}
EXPORT_SYMBOL_GPL(alarm_forward);

u64 alarm_forward_now(struct alarm *alarm, ktime_t interval)
{
 struct alarm_base *base = &alarm_bases[alarm->type];

 return alarm_forward(alarm, base->gettime(), interval);
}
EXPORT_SYMBOL_GPL(alarm_forward_now);






static enum alarmtimer_type clock2alarm(clockid_t clockid)
{
 if (clockid == CLOCK_REALTIME_ALARM)
  return ALARM_REALTIME;
 if (clockid == CLOCK_BOOTTIME_ALARM)
  return ALARM_BOOTTIME;
 return -1;
}







static enum alarmtimer_restart alarm_handle_timer(struct alarm *alarm,
       ktime_t now)
{
 unsigned long flags;
 struct k_itimer *ptr = container_of(alarm, struct k_itimer,
      it.alarm.alarmtimer);
 enum alarmtimer_restart result = ALARMTIMER_NORESTART;

 spin_lock_irqsave(&ptr->it_lock, flags);
 if ((ptr->it_sigev_notify & ~SIGEV_THREAD_ID) != SIGEV_NONE) {
  if (posix_timer_event(ptr, 0) != 0)
   ptr->it_overrun++;
 }


 if (ptr->it.alarm.interval.tv64) {
  ptr->it_overrun += alarm_forward(alarm, now,
      ptr->it.alarm.interval);
  result = ALARMTIMER_RESTART;
 }
 spin_unlock_irqrestore(&ptr->it_lock, flags);

 return result;
}
static int alarm_clock_getres(const clockid_t which_clock, struct timespec *tp)
{
 if (!alarmtimer_get_rtcdev())
  return -EINVAL;

 tp->tv_sec = 0;
 tp->tv_nsec = hrtimer_resolution;
 return 0;
}
static int alarm_clock_get(clockid_t which_clock, struct timespec *tp)
{
 struct alarm_base *base = &alarm_bases[clock2alarm(which_clock)];

 if (!alarmtimer_get_rtcdev())
  return -EINVAL;

 *tp = ktime_to_timespec(base->gettime());
 return 0;
}







static int alarm_timer_create(struct k_itimer *new_timer)
{
 enum alarmtimer_type type;
 struct alarm_base *base;

 if (!alarmtimer_get_rtcdev())
  return -ENOTSUPP;

 if (!capable(CAP_WAKE_ALARM))
  return -EPERM;

 type = clock2alarm(new_timer->it_clock);
 base = &alarm_bases[type];
 alarm_init(&new_timer->it.alarm.alarmtimer, type, alarm_handle_timer);
 return 0;
}
static void alarm_timer_get(struct k_itimer *timr,
    struct itimerspec *cur_setting)
{
 ktime_t relative_expiry_time =
  alarm_expires_remaining(&(timr->it.alarm.alarmtimer));

 if (ktime_to_ns(relative_expiry_time) > 0) {
  cur_setting->it_value = ktime_to_timespec(relative_expiry_time);
 } else {
  cur_setting->it_value.tv_sec = 0;
  cur_setting->it_value.tv_nsec = 0;
 }

 cur_setting->it_interval = ktime_to_timespec(timr->it.alarm.interval);
}







static int alarm_timer_del(struct k_itimer *timr)
{
 if (!rtcdev)
  return -ENOTSUPP;

 if (alarm_try_to_cancel(&timr->it.alarm.alarmtimer) < 0)
  return TIMER_RETRY;

 return 0;
}
static int alarm_timer_set(struct k_itimer *timr, int flags,
    struct itimerspec *new_setting,
    struct itimerspec *old_setting)
{
 ktime_t exp;

 if (!rtcdev)
  return -ENOTSUPP;

 if (flags & ~TIMER_ABSTIME)
  return -EINVAL;

 if (old_setting)
  alarm_timer_get(timr, old_setting);


 if (alarm_try_to_cancel(&timr->it.alarm.alarmtimer) < 0)
  return TIMER_RETRY;


 timr->it.alarm.interval = timespec_to_ktime(new_setting->it_interval);
 exp = timespec_to_ktime(new_setting->it_value);

 if (flags != TIMER_ABSTIME) {
  ktime_t now;

  now = alarm_bases[timr->it.alarm.alarmtimer.type].gettime();
  exp = ktime_add(now, exp);
 }

 alarm_start(&timr->it.alarm.alarmtimer, exp);
 return 0;
}







static enum alarmtimer_restart alarmtimer_nsleep_wakeup(struct alarm *alarm,
        ktime_t now)
{
 struct task_struct *task = (struct task_struct *)alarm->data;

 alarm->data = NULL;
 if (task)
  wake_up_process(task);
 return ALARMTIMER_NORESTART;
}
static int alarmtimer_do_nsleep(struct alarm *alarm, ktime_t absexp)
{
 alarm->data = (void *)current;
 do {
  set_current_state(TASK_INTERRUPTIBLE);
  alarm_start(alarm, absexp);
  if (likely(alarm->data))
   schedule();

  alarm_cancel(alarm);
 } while (alarm->data && !signal_pending(current));

 __set_current_state(TASK_RUNNING);

 return (alarm->data == NULL);
}
static int update_rmtp(ktime_t exp, enum alarmtimer_type type,
   struct timespec __user *rmtp)
{
 struct timespec rmt;
 ktime_t rem;

 rem = ktime_sub(exp, alarm_bases[type].gettime());

 if (rem.tv64 <= 0)
  return 0;
 rmt = ktime_to_timespec(rem);

 if (copy_to_user(rmtp, &rmt, sizeof(*rmtp)))
  return -EFAULT;

 return 1;

}







static long __sched alarm_timer_nsleep_restart(struct restart_block *restart)
{
 enum alarmtimer_type type = restart->nanosleep.clockid;
 ktime_t exp;
 struct timespec __user *rmtp;
 struct alarm alarm;
 int ret = 0;

 exp.tv64 = restart->nanosleep.expires;
 alarm_init(&alarm, type, alarmtimer_nsleep_wakeup);

 if (alarmtimer_do_nsleep(&alarm, exp))
  goto out;

 if (freezing(current))
  alarmtimer_freezerset(exp, type);

 rmtp = restart->nanosleep.rmtp;
 if (rmtp) {
  ret = update_rmtp(exp, type, rmtp);
  if (ret <= 0)
   goto out;
 }



 ret = -ERESTART_RESTARTBLOCK;
out:
 return ret;
}
static int alarm_timer_nsleep(const clockid_t which_clock, int flags,
       struct timespec *tsreq, struct timespec __user *rmtp)
{
 enum alarmtimer_type type = clock2alarm(which_clock);
 struct alarm alarm;
 ktime_t exp;
 int ret = 0;
 struct restart_block *restart;

 if (!alarmtimer_get_rtcdev())
  return -ENOTSUPP;

 if (flags & ~TIMER_ABSTIME)
  return -EINVAL;

 if (!capable(CAP_WAKE_ALARM))
  return -EPERM;

 alarm_init(&alarm, type, alarmtimer_nsleep_wakeup);

 exp = timespec_to_ktime(*tsreq);

 if (flags != TIMER_ABSTIME) {
  ktime_t now = alarm_bases[type].gettime();
  exp = ktime_add(now, exp);
 }

 if (alarmtimer_do_nsleep(&alarm, exp))
  goto out;

 if (freezing(current))
  alarmtimer_freezerset(exp, type);


 if (flags == TIMER_ABSTIME) {
  ret = -ERESTARTNOHAND;
  goto out;
 }

 if (rmtp) {
  ret = update_rmtp(exp, type, rmtp);
  if (ret <= 0)
   goto out;
 }

 restart = &current->restart_block;
 restart->fn = alarm_timer_nsleep_restart;
 restart->nanosleep.clockid = type;
 restart->nanosleep.expires = exp.tv64;
 restart->nanosleep.rmtp = rmtp;
 ret = -ERESTART_RESTARTBLOCK;

out:
 return ret;
}



static const struct dev_pm_ops alarmtimer_pm_ops = {
 .suspend = alarmtimer_suspend,
 .resume = alarmtimer_resume,
};

static struct platform_driver alarmtimer_driver = {
 .driver = {
  .name = "alarmtimer",
  .pm = &alarmtimer_pm_ops,
 }
};







static int __init alarmtimer_init(void)
{
 struct platform_device *pdev;
 int error = 0;
 int i;
 struct k_clock alarm_clock = {
  .clock_getres = alarm_clock_getres,
  .clock_get = alarm_clock_get,
  .timer_create = alarm_timer_create,
  .timer_set = alarm_timer_set,
  .timer_del = alarm_timer_del,
  .timer_get = alarm_timer_get,
  .nsleep = alarm_timer_nsleep,
 };

 alarmtimer_rtc_timer_init();

 posix_timers_register_clock(CLOCK_REALTIME_ALARM, &alarm_clock);
 posix_timers_register_clock(CLOCK_BOOTTIME_ALARM, &alarm_clock);


 alarm_bases[ALARM_REALTIME].base_clockid = CLOCK_REALTIME;
 alarm_bases[ALARM_REALTIME].gettime = &ktime_get_real;
 alarm_bases[ALARM_BOOTTIME].base_clockid = CLOCK_BOOTTIME;
 alarm_bases[ALARM_BOOTTIME].gettime = &ktime_get_boottime;
 for (i = 0; i < ALARM_NUMTYPE; i++) {
  timerqueue_init_head(&alarm_bases[i].timerqueue);
  spin_lock_init(&alarm_bases[i].lock);
 }

 error = alarmtimer_rtc_interface_setup();
 if (error)
  return error;

 error = platform_driver_register(&alarmtimer_driver);
 if (error)
  goto out_if;

 pdev = platform_device_register_simple("alarmtimer", -1, NULL, 0);
 if (IS_ERR(pdev)) {
  error = PTR_ERR(pdev);
  goto out_drv;
 }
 ws = wakeup_source_register("alarmtimer");
 return 0;

out_drv:
 platform_driver_unregister(&alarmtimer_driver);
out_if:
 alarmtimer_rtc_interface_remove();
 return error;
}
device_initcall(alarmtimer_init);

static void bpf_array_free_percpu(struct bpf_array *array)
{
 int i;

 for (i = 0; i < array->map.max_entries; i++)
  free_percpu(array->pptrs[i]);
}

static int bpf_array_alloc_percpu(struct bpf_array *array)
{
 void __percpu *ptr;
 int i;

 for (i = 0; i < array->map.max_entries; i++) {
  ptr = __alloc_percpu_gfp(array->elem_size, 8,
      GFP_USER | __GFP_NOWARN);
  if (!ptr) {
   bpf_array_free_percpu(array);
   return -ENOMEM;
  }
  array->pptrs[i] = ptr;
 }

 return 0;
}


static struct bpf_map *array_map_alloc(union bpf_attr *attr)
{
 bool percpu = attr->map_type == BPF_MAP_TYPE_PERCPU_ARRAY;
 struct bpf_array *array;
 u64 array_size;
 u32 elem_size;


 if (attr->max_entries == 0 || attr->key_size != 4 ||
     attr->value_size == 0 || attr->map_flags)
  return ERR_PTR(-EINVAL);

 if (attr->value_size >= 1 << (KMALLOC_SHIFT_MAX - 1))



  return ERR_PTR(-E2BIG);

 elem_size = round_up(attr->value_size, 8);

 array_size = sizeof(*array);
 if (percpu)
  array_size += (u64) attr->max_entries * sizeof(void *);
 else
  array_size += (u64) attr->max_entries * elem_size;


 if (array_size >= U32_MAX - PAGE_SIZE)
  return ERR_PTR(-ENOMEM);



 array = kzalloc(array_size, GFP_USER | __GFP_NOWARN);
 if (!array) {
  array = vzalloc(array_size);
  if (!array)
   return ERR_PTR(-ENOMEM);
 }


 array->map.map_type = attr->map_type;
 array->map.key_size = attr->key_size;
 array->map.value_size = attr->value_size;
 array->map.max_entries = attr->max_entries;
 array->elem_size = elem_size;

 if (!percpu)
  goto out;

 array_size += (u64) attr->max_entries * elem_size * num_possible_cpus();

 if (array_size >= U32_MAX - PAGE_SIZE ||
     elem_size > PCPU_MIN_UNIT_SIZE || bpf_array_alloc_percpu(array)) {
  kvfree(array);
  return ERR_PTR(-ENOMEM);
 }
out:
 array->map.pages = round_up(array_size, PAGE_SIZE) >> PAGE_SHIFT;

 return &array->map;
}


static void *array_map_lookup_elem(struct bpf_map *map, void *key)
{
 struct bpf_array *array = container_of(map, struct bpf_array, map);
 u32 index = *(u32 *)key;

 if (unlikely(index >= array->map.max_entries))
  return NULL;

 return array->value + array->elem_size * index;
}


static void *percpu_array_map_lookup_elem(struct bpf_map *map, void *key)
{
 struct bpf_array *array = container_of(map, struct bpf_array, map);
 u32 index = *(u32 *)key;

 if (unlikely(index >= array->map.max_entries))
  return NULL;

 return this_cpu_ptr(array->pptrs[index]);
}

int bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value)
{
 struct bpf_array *array = container_of(map, struct bpf_array, map);
 u32 index = *(u32 *)key;
 void __percpu *pptr;
 int cpu, off = 0;
 u32 size;

 if (unlikely(index >= array->map.max_entries))
  return -ENOENT;





 size = round_up(map->value_size, 8);
 rcu_read_lock();
 pptr = array->pptrs[index];
 for_each_possible_cpu(cpu) {
  bpf_long_memcpy(value + off, per_cpu_ptr(pptr, cpu), size);
  off += size;
 }
 rcu_read_unlock();
 return 0;
}


static int array_map_get_next_key(struct bpf_map *map, void *key, void *next_key)
{
 struct bpf_array *array = container_of(map, struct bpf_array, map);
 u32 index = *(u32 *)key;
 u32 *next = (u32 *)next_key;

 if (index >= array->map.max_entries) {
  *next = 0;
  return 0;
 }

 if (index == array->map.max_entries - 1)
  return -ENOENT;

 *next = index + 1;
 return 0;
}


static int array_map_update_elem(struct bpf_map *map, void *key, void *value,
     u64 map_flags)
{
 struct bpf_array *array = container_of(map, struct bpf_array, map);
 u32 index = *(u32 *)key;

 if (unlikely(map_flags > BPF_EXIST))

  return -EINVAL;

 if (unlikely(index >= array->map.max_entries))

  return -E2BIG;

 if (unlikely(map_flags == BPF_NOEXIST))

  return -EEXIST;

 if (array->map.map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
  memcpy(this_cpu_ptr(array->pptrs[index]),
         value, map->value_size);
 else
  memcpy(array->value + array->elem_size * index,
         value, map->value_size);
 return 0;
}

int bpf_percpu_array_update(struct bpf_map *map, void *key, void *value,
       u64 map_flags)
{
 struct bpf_array *array = container_of(map, struct bpf_array, map);
 u32 index = *(u32 *)key;
 void __percpu *pptr;
 int cpu, off = 0;
 u32 size;

 if (unlikely(map_flags > BPF_EXIST))

  return -EINVAL;

 if (unlikely(index >= array->map.max_entries))

  return -E2BIG;

 if (unlikely(map_flags == BPF_NOEXIST))

  return -EEXIST;







 size = round_up(map->value_size, 8);
 rcu_read_lock();
 pptr = array->pptrs[index];
 for_each_possible_cpu(cpu) {
  bpf_long_memcpy(per_cpu_ptr(pptr, cpu), value + off, size);
  off += size;
 }
 rcu_read_unlock();
 return 0;
}


static int array_map_delete_elem(struct bpf_map *map, void *key)
{
 return -EINVAL;
}


static void array_map_free(struct bpf_map *map)
{
 struct bpf_array *array = container_of(map, struct bpf_array, map);






 synchronize_rcu();

 if (array->map.map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
  bpf_array_free_percpu(array);

 kvfree(array);
}

static const struct bpf_map_ops array_ops = {
 .map_alloc = array_map_alloc,
 .map_free = array_map_free,
 .map_get_next_key = array_map_get_next_key,
 .map_lookup_elem = array_map_lookup_elem,
 .map_update_elem = array_map_update_elem,
 .map_delete_elem = array_map_delete_elem,
};

static struct bpf_map_type_list array_type __read_mostly = {
 .ops = &array_ops,
 .type = BPF_MAP_TYPE_ARRAY,
};

static const struct bpf_map_ops percpu_array_ops = {
 .map_alloc = array_map_alloc,
 .map_free = array_map_free,
 .map_get_next_key = array_map_get_next_key,
 .map_lookup_elem = percpu_array_map_lookup_elem,
 .map_update_elem = array_map_update_elem,
 .map_delete_elem = array_map_delete_elem,
};

static struct bpf_map_type_list percpu_array_type __read_mostly = {
 .ops = &percpu_array_ops,
 .type = BPF_MAP_TYPE_PERCPU_ARRAY,
};

static int __init register_array_map(void)
{
 bpf_register_map_type(&array_type);
 bpf_register_map_type(&percpu_array_type);
 return 0;
}
late_initcall(register_array_map);

static struct bpf_map *fd_array_map_alloc(union bpf_attr *attr)
{

 if (attr->value_size != sizeof(u32))
  return ERR_PTR(-EINVAL);
 return array_map_alloc(attr);
}

static void fd_array_map_free(struct bpf_map *map)
{
 struct bpf_array *array = container_of(map, struct bpf_array, map);
 int i;

 synchronize_rcu();


 for (i = 0; i < array->map.max_entries; i++)
  BUG_ON(array->ptrs[i] != NULL);
 kvfree(array);
}

static void *fd_array_map_lookup_elem(struct bpf_map *map, void *key)
{
 return NULL;
}


static int fd_array_map_update_elem(struct bpf_map *map, void *key,
        void *value, u64 map_flags)
{
 struct bpf_array *array = container_of(map, struct bpf_array, map);
 void *new_ptr, *old_ptr;
 u32 index = *(u32 *)key, ufd;

 if (map_flags != BPF_ANY)
  return -EINVAL;

 if (index >= array->map.max_entries)
  return -E2BIG;

 ufd = *(u32 *)value;
 new_ptr = map->ops->map_fd_get_ptr(map, ufd);
 if (IS_ERR(new_ptr))
  return PTR_ERR(new_ptr);

 old_ptr = xchg(array->ptrs + index, new_ptr);
 if (old_ptr)
  map->ops->map_fd_put_ptr(old_ptr);

 return 0;
}

static int fd_array_map_delete_elem(struct bpf_map *map, void *key)
{
 struct bpf_array *array = container_of(map, struct bpf_array, map);
 void *old_ptr;
 u32 index = *(u32 *)key;

 if (index >= array->map.max_entries)
  return -E2BIG;

 old_ptr = xchg(array->ptrs + index, NULL);
 if (old_ptr) {
  map->ops->map_fd_put_ptr(old_ptr);
  return 0;
 } else {
  return -ENOENT;
 }
}

static void *prog_fd_array_get_ptr(struct bpf_map *map, int fd)
{
 struct bpf_array *array = container_of(map, struct bpf_array, map);
 struct bpf_prog *prog = bpf_prog_get(fd);
 if (IS_ERR(prog))
  return prog;

 if (!bpf_prog_array_compatible(array, prog)) {
  bpf_prog_put(prog);
  return ERR_PTR(-EINVAL);
 }
 return prog;
}

static void prog_fd_array_put_ptr(void *ptr)
{
 struct bpf_prog *prog = ptr;

 bpf_prog_put_rcu(prog);
}


void bpf_fd_array_map_clear(struct bpf_map *map)
{
 struct bpf_array *array = container_of(map, struct bpf_array, map);
 int i;

 for (i = 0; i < array->map.max_entries; i++)
  fd_array_map_delete_elem(map, &i);
}

static const struct bpf_map_ops prog_array_ops = {
 .map_alloc = fd_array_map_alloc,
 .map_free = fd_array_map_free,
 .map_get_next_key = array_map_get_next_key,
 .map_lookup_elem = fd_array_map_lookup_elem,
 .map_update_elem = fd_array_map_update_elem,
 .map_delete_elem = fd_array_map_delete_elem,
 .map_fd_get_ptr = prog_fd_array_get_ptr,
 .map_fd_put_ptr = prog_fd_array_put_ptr,
};

static struct bpf_map_type_list prog_array_type __read_mostly = {
 .ops = &prog_array_ops,
 .type = BPF_MAP_TYPE_PROG_ARRAY,
};

static int __init register_prog_array_map(void)
{
 bpf_register_map_type(&prog_array_type);
 return 0;
}
late_initcall(register_prog_array_map);

static void perf_event_array_map_free(struct bpf_map *map)
{
 bpf_fd_array_map_clear(map);
 fd_array_map_free(map);
}

static void *perf_event_fd_array_get_ptr(struct bpf_map *map, int fd)
{
 struct perf_event *event;
 const struct perf_event_attr *attr;
 struct file *file;

 file = perf_event_get(fd);
 if (IS_ERR(file))
  return file;

 event = file->private_data;

 attr = perf_event_attrs(event);
 if (IS_ERR(attr))
  goto err;

 if (attr->inherit)
  goto err;

 if (attr->type == PERF_TYPE_RAW)
  return file;

 if (attr->type == PERF_TYPE_HARDWARE)
  return file;

 if (attr->type == PERF_TYPE_SOFTWARE &&
     attr->config == PERF_COUNT_SW_BPF_OUTPUT)
  return file;
err:
 fput(file);
 return ERR_PTR(-EINVAL);
}

static void perf_event_fd_array_put_ptr(void *ptr)
{
 fput((struct file *)ptr);
}

static const struct bpf_map_ops perf_event_array_ops = {
 .map_alloc = fd_array_map_alloc,
 .map_free = perf_event_array_map_free,
 .map_get_next_key = array_map_get_next_key,
 .map_lookup_elem = fd_array_map_lookup_elem,
 .map_update_elem = fd_array_map_update_elem,
 .map_delete_elem = fd_array_map_delete_elem,
 .map_fd_get_ptr = perf_event_fd_array_get_ptr,
 .map_fd_put_ptr = perf_event_fd_array_put_ptr,
};

static struct bpf_map_type_list perf_event_array_type __read_mostly = {
 .ops = &perf_event_array_ops,
 .type = BPF_MAP_TYPE_PERF_EVENT_ARRAY,
};

static int __init register_perf_event_array_map(void)
{
 bpf_register_map_type(&perf_event_array_type);
 return 0;
}
late_initcall(register_perf_event_array_map);


static async_cookie_t next_cookie = 1;


static LIST_HEAD(async_global_pending);
static ASYNC_DOMAIN(async_dfl_domain);
static DEFINE_SPINLOCK(async_lock);

struct async_entry {
 struct list_head domain_list;
 struct list_head global_list;
 struct work_struct work;
 async_cookie_t cookie;
 async_func_t func;
 void *data;
 struct async_domain *domain;
};

static DECLARE_WAIT_QUEUE_HEAD(async_done);

static atomic_t entry_count;

static async_cookie_t lowest_in_progress(struct async_domain *domain)
{
 struct list_head *pending;
 async_cookie_t ret = ASYNC_COOKIE_MAX;
 unsigned long flags;

 spin_lock_irqsave(&async_lock, flags);

 if (domain)
  pending = &domain->pending;
 else
  pending = &async_global_pending;

 if (!list_empty(pending))
  ret = list_first_entry(pending, struct async_entry,
           domain_list)->cookie;

 spin_unlock_irqrestore(&async_lock, flags);
 return ret;
}




static void async_run_entry_fn(struct work_struct *work)
{
 struct async_entry *entry =
  container_of(work, struct async_entry, work);
 unsigned long flags;
 ktime_t uninitialized_var(calltime), delta, rettime;


 if (initcall_debug && system_state == SYSTEM_BOOTING) {
  pr_debug("calling  %lli_%pF @ %i\n",
   (long long)entry->cookie,
   entry->func, task_pid_nr(current));
  calltime = ktime_get();
 }
 entry->func(entry->data, entry->cookie);
 if (initcall_debug && system_state == SYSTEM_BOOTING) {
  rettime = ktime_get();
  delta = ktime_sub(rettime, calltime);
  pr_debug("initcall %lli_%pF returned 0 after %lld usecs\n",
   (long long)entry->cookie,
   entry->func,
   (long long)ktime_to_ns(delta) >> 10);
 }


 spin_lock_irqsave(&async_lock, flags);
 list_del_init(&entry->domain_list);
 list_del_init(&entry->global_list);


 kfree(entry);
 atomic_dec(&entry_count);

 spin_unlock_irqrestore(&async_lock, flags);


 wake_up(&async_done);
}

static async_cookie_t __async_schedule(async_func_t func, void *data, struct async_domain *domain)
{
 struct async_entry *entry;
 unsigned long flags;
 async_cookie_t newcookie;


 entry = kzalloc(sizeof(struct async_entry), GFP_ATOMIC);





 if (!entry || atomic_read(&entry_count) > MAX_WORK) {
  kfree(entry);
  spin_lock_irqsave(&async_lock, flags);
  newcookie = next_cookie++;
  spin_unlock_irqrestore(&async_lock, flags);


  func(data, newcookie);
  return newcookie;
 }
 INIT_LIST_HEAD(&entry->domain_list);
 INIT_LIST_HEAD(&entry->global_list);
 INIT_WORK(&entry->work, async_run_entry_fn);
 entry->func = func;
 entry->data = data;
 entry->domain = domain;

 spin_lock_irqsave(&async_lock, flags);


 newcookie = entry->cookie = next_cookie++;

 list_add_tail(&entry->domain_list, &domain->pending);
 if (domain->registered)
  list_add_tail(&entry->global_list, &async_global_pending);

 atomic_inc(&entry_count);
 spin_unlock_irqrestore(&async_lock, flags);


 current->flags |= PF_USED_ASYNC;


 queue_work(system_unbound_wq, &entry->work);

 return newcookie;
}
async_cookie_t async_schedule(async_func_t func, void *data)
{
 return __async_schedule(func, data, &async_dfl_domain);
}
EXPORT_SYMBOL_GPL(async_schedule);
async_cookie_t async_schedule_domain(async_func_t func, void *data,
         struct async_domain *domain)
{
 return __async_schedule(func, data, domain);
}
EXPORT_SYMBOL_GPL(async_schedule_domain);






void async_synchronize_full(void)
{
 async_synchronize_full_domain(NULL);
}
EXPORT_SYMBOL_GPL(async_synchronize_full);
void async_unregister_domain(struct async_domain *domain)
{
 spin_lock_irq(&async_lock);
 WARN_ON(!domain->registered || !list_empty(&domain->pending));
 domain->registered = 0;
 spin_unlock_irq(&async_lock);
}
EXPORT_SYMBOL_GPL(async_unregister_domain);
void async_synchronize_full_domain(struct async_domain *domain)
{
 async_synchronize_cookie_domain(ASYNC_COOKIE_MAX, domain);
}
EXPORT_SYMBOL_GPL(async_synchronize_full_domain);
void async_synchronize_cookie_domain(async_cookie_t cookie, struct async_domain *domain)
{
 ktime_t uninitialized_var(starttime), delta, endtime;

 if (initcall_debug && system_state == SYSTEM_BOOTING) {
  pr_debug("async_waiting @ %i\n", task_pid_nr(current));
  starttime = ktime_get();
 }

 wait_event(async_done, lowest_in_progress(domain) >= cookie);

 if (initcall_debug && system_state == SYSTEM_BOOTING) {
  endtime = ktime_get();
  delta = ktime_sub(endtime, starttime);

  pr_debug("async_continuing @ %i after %lli usec\n",
   task_pid_nr(current),
   (long long)ktime_to_ns(delta) >> 10);
 }
}
EXPORT_SYMBOL_GPL(async_synchronize_cookie_domain);
void async_synchronize_cookie(async_cookie_t cookie)
{
 async_synchronize_cookie_domain(cookie, &async_dfl_domain);
}
EXPORT_SYMBOL_GPL(async_synchronize_cookie);






bool current_is_async(void)
{
 struct worker *worker = current_wq_worker();

 return worker && worker->current_func == async_run_entry_fn;
}
EXPORT_SYMBOL_GPL(current_is_async);







static int audit_initialized;

u32 audit_enabled;
u32 audit_ever_enabled;

EXPORT_SYMBOL_GPL(audit_enabled);


static u32 audit_default;


static u32 audit_failure = AUDIT_FAIL_PRINTK;






int audit_pid;
static __u32 audit_nlk_portid;




static u32 audit_rate_limit;



static u32 audit_backlog_limit = 64;
static u32 audit_backlog_wait_time_master = AUDIT_BACKLOG_WAIT_TIME;
static u32 audit_backlog_wait_time = AUDIT_BACKLOG_WAIT_TIME;


kuid_t audit_sig_uid = INVALID_UID;
pid_t audit_sig_pid = -1;
u32 audit_sig_sid = 0;
static atomic_t audit_lost = ATOMIC_INIT(0);


static struct sock *audit_sock;
static int audit_net_id;


struct list_head audit_inode_hash[AUDIT_INODE_BUCKETS];




static DEFINE_SPINLOCK(audit_freelist_lock);
static int audit_freelist_count;
static LIST_HEAD(audit_freelist);

static struct sk_buff_head audit_skb_queue;

static struct sk_buff_head audit_skb_hold_queue;
static struct task_struct *kauditd_task;
static DECLARE_WAIT_QUEUE_HEAD(kauditd_wait);
static DECLARE_WAIT_QUEUE_HEAD(audit_backlog_wait);

static struct audit_features af = {.vers = AUDIT_FEATURE_VERSION,
       .mask = -1,
       .features = 0,
       .lock = 0,};

static char *audit_feature_names[2] = {
 "only_unset_loginuid",
 "loginuid_immutable",
};



DEFINE_MUTEX(audit_cmd_mutex);













struct audit_buffer {
 struct list_head list;
 struct sk_buff *skb;
 struct audit_context *ctx;
 gfp_t gfp_mask;
};

struct audit_reply {
 __u32 portid;
 struct net *net;
 struct sk_buff *skb;
};

static void audit_set_portid(struct audit_buffer *ab, __u32 portid)
{
 if (ab) {
  struct nlmsghdr *nlh = nlmsg_hdr(ab->skb);
  nlh->nlmsg_pid = portid;
 }
}

void audit_panic(const char *message)
{
 switch (audit_failure) {
 case AUDIT_FAIL_SILENT:
  break;
 case AUDIT_FAIL_PRINTK:
  if (printk_ratelimit())
   pr_err("%s\n", message);
  break;
 case AUDIT_FAIL_PANIC:

  if (audit_pid)
   panic("audit: %s\n", message);
  break;
 }
}

static inline int audit_rate_check(void)
{
 static unsigned long last_check = 0;
 static int messages = 0;
 static DEFINE_SPINLOCK(lock);
 unsigned long flags;
 unsigned long now;
 unsigned long elapsed;
 int retval = 0;

 if (!audit_rate_limit) return 1;

 spin_lock_irqsave(&lock, flags);
 if (++messages < audit_rate_limit) {
  retval = 1;
 } else {
  now = jiffies;
  elapsed = now - last_check;
  if (elapsed > HZ) {
   last_check = now;
   messages = 0;
   retval = 1;
  }
 }
 spin_unlock_irqrestore(&lock, flags);

 return retval;
}
void audit_log_lost(const char *message)
{
 static unsigned long last_msg = 0;
 static DEFINE_SPINLOCK(lock);
 unsigned long flags;
 unsigned long now;
 int print;

 atomic_inc(&audit_lost);

 print = (audit_failure == AUDIT_FAIL_PANIC || !audit_rate_limit);

 if (!print) {
  spin_lock_irqsave(&lock, flags);
  now = jiffies;
  if (now - last_msg > HZ) {
   print = 1;
   last_msg = now;
  }
  spin_unlock_irqrestore(&lock, flags);
 }

 if (print) {
  if (printk_ratelimit())
   pr_warn("audit_lost=%u audit_rate_limit=%u audit_backlog_limit=%u\n",
    atomic_read(&audit_lost),
    audit_rate_limit,
    audit_backlog_limit);
  audit_panic(message);
 }
}

static int audit_log_config_change(char *function_name, u32 new, u32 old,
       int allow_changes)
{
 struct audit_buffer *ab;
 int rc = 0;

 ab = audit_log_start(NULL, GFP_KERNEL, AUDIT_CONFIG_CHANGE);
 if (unlikely(!ab))
  return rc;
 audit_log_format(ab, "%s=%u old=%u", function_name, new, old);
 audit_log_session_info(ab);
 rc = audit_log_task_context(ab);
 if (rc)
  allow_changes = 0;
 audit_log_format(ab, " res=%d", allow_changes);
 audit_log_end(ab);
 return rc;
}

static int audit_do_config_change(char *function_name, u32 *to_change, u32 new)
{
 int allow_changes, rc = 0;
 u32 old = *to_change;


 if (audit_enabled == AUDIT_LOCKED)
  allow_changes = 0;
 else
  allow_changes = 1;

 if (audit_enabled != AUDIT_OFF) {
  rc = audit_log_config_change(function_name, new, old, allow_changes);
  if (rc)
   allow_changes = 0;
 }


 if (allow_changes == 1)
  *to_change = new;

 else if (rc == 0)
  rc = -EPERM;
 return rc;
}

static int audit_set_rate_limit(u32 limit)
{
 return audit_do_config_change("audit_rate_limit", &audit_rate_limit, limit);
}

static int audit_set_backlog_limit(u32 limit)
{
 return audit_do_config_change("audit_backlog_limit", &audit_backlog_limit, limit);
}

static int audit_set_backlog_wait_time(u32 timeout)
{
 return audit_do_config_change("audit_backlog_wait_time",
          &audit_backlog_wait_time_master, timeout);
}

static int audit_set_enabled(u32 state)
{
 int rc;
 if (state > AUDIT_LOCKED)
  return -EINVAL;

 rc = audit_do_config_change("audit_enabled", &audit_enabled, state);
 if (!rc)
  audit_ever_enabled |= !!state;

 return rc;
}

static int audit_set_failure(u32 state)
{
 if (state != AUDIT_FAIL_SILENT
     && state != AUDIT_FAIL_PRINTK
     && state != AUDIT_FAIL_PANIC)
  return -EINVAL;

 return audit_do_config_change("audit_failure", &audit_failure, state);
}
static void audit_hold_skb(struct sk_buff *skb)
{
 if (audit_default &&
     (!audit_backlog_limit ||
      skb_queue_len(&audit_skb_hold_queue) < audit_backlog_limit))
  skb_queue_tail(&audit_skb_hold_queue, skb);
 else
  kfree_skb(skb);
}





static void audit_printk_skb(struct sk_buff *skb)
{
 struct nlmsghdr *nlh = nlmsg_hdr(skb);
 char *data = nlmsg_data(nlh);

 if (nlh->nlmsg_type != AUDIT_EOE) {
  if (printk_ratelimit())
   pr_notice("type=%d %s\n", nlh->nlmsg_type, data);
  else
   audit_log_lost("printk limit exceeded");
 }

 audit_hold_skb(skb);
}

static void kauditd_send_skb(struct sk_buff *skb)
{
 int err;
 int attempts = 0;

restart:

 skb_get(skb);
 err = netlink_unicast(audit_sock, skb, audit_nlk_portid, 0);
 if (err < 0) {
  pr_err("netlink_unicast sending to audit_pid=%d returned error: %d\n",
         audit_pid, err);
  if (audit_pid) {
   if (err == -ECONNREFUSED || err == -EPERM
       || ++attempts >= AUDITD_RETRIES) {
    char s[32];

    snprintf(s, sizeof(s), "audit_pid=%d reset", audit_pid);
    audit_log_lost(s);
    audit_pid = 0;
    audit_sock = NULL;
   } else {
    pr_warn("re-scheduling(#%d) write to audit_pid=%d\n",
     attempts, audit_pid);
    set_current_state(TASK_INTERRUPTIBLE);
    schedule();
    goto restart;
   }
  }

  audit_hold_skb(skb);
 } else

  consume_skb(skb);
}







static void kauditd_send_multicast_skb(struct sk_buff *skb, gfp_t gfp_mask)
{
 struct sk_buff *copy;
 struct audit_net *aunet = net_generic(&init_net, audit_net_id);
 struct sock *sock = aunet->nlsk;

 if (!netlink_has_listeners(sock, AUDIT_NLGRP_READLOG))
  return;
 copy = skb_copy(skb, gfp_mask);
 if (!copy)
  return;

 nlmsg_multicast(sock, copy, 0, AUDIT_NLGRP_READLOG, gfp_mask);
}
static void flush_hold_queue(void)
{
 struct sk_buff *skb;

 if (!audit_default || !audit_pid)
  return;

 skb = skb_dequeue(&audit_skb_hold_queue);
 if (likely(!skb))
  return;

 while (skb && audit_pid) {
  kauditd_send_skb(skb);
  skb = skb_dequeue(&audit_skb_hold_queue);
 }





 consume_skb(skb);
}

static int kauditd_thread(void *dummy)
{
 set_freezable();
 while (!kthread_should_stop()) {
  struct sk_buff *skb;

  flush_hold_queue();

  skb = skb_dequeue(&audit_skb_queue);

  if (skb) {
   if (!audit_backlog_limit ||
       (skb_queue_len(&audit_skb_queue) <= audit_backlog_limit))
    wake_up(&audit_backlog_wait);
   if (audit_pid)
    kauditd_send_skb(skb);
   else
    audit_printk_skb(skb);
   continue;
  }

  wait_event_freezable(kauditd_wait, skb_queue_len(&audit_skb_queue));
 }
 return 0;
}

int audit_send_list(void *_dest)
{
 struct audit_netlink_list *dest = _dest;
 struct sk_buff *skb;
 struct net *net = dest->net;
 struct audit_net *aunet = net_generic(net, audit_net_id);


 mutex_lock(&audit_cmd_mutex);
 mutex_unlock(&audit_cmd_mutex);

 while ((skb = __skb_dequeue(&dest->q)) != NULL)
  netlink_unicast(aunet->nlsk, skb, dest->portid, 0);

 put_net(net);
 kfree(dest);

 return 0;
}

struct sk_buff *audit_make_reply(__u32 portid, int seq, int type, int done,
     int multi, const void *payload, int size)
{
 struct sk_buff *skb;
 struct nlmsghdr *nlh;
 void *data;
 int flags = multi ? NLM_F_MULTI : 0;
 int t = done ? NLMSG_DONE : type;

 skb = nlmsg_new(size, GFP_KERNEL);
 if (!skb)
  return NULL;

 nlh = nlmsg_put(skb, portid, seq, t, size, flags);
 if (!nlh)
  goto out_kfree_skb;
 data = nlmsg_data(nlh);
 memcpy(data, payload, size);
 return skb;

out_kfree_skb:
 kfree_skb(skb);
 return NULL;
}

static int audit_send_reply_thread(void *arg)
{
 struct audit_reply *reply = (struct audit_reply *)arg;
 struct net *net = reply->net;
 struct audit_net *aunet = net_generic(net, audit_net_id);

 mutex_lock(&audit_cmd_mutex);
 mutex_unlock(&audit_cmd_mutex);



 netlink_unicast(aunet->nlsk , reply->skb, reply->portid, 0);
 put_net(net);
 kfree(reply);
 return 0;
}
static void audit_send_reply(struct sk_buff *request_skb, int seq, int type, int done,
        int multi, const void *payload, int size)
{
 u32 portid = NETLINK_CB(request_skb).portid;
 struct net *net = sock_net(NETLINK_CB(request_skb).sk);
 struct sk_buff *skb;
 struct task_struct *tsk;
 struct audit_reply *reply = kmalloc(sizeof(struct audit_reply),
         GFP_KERNEL);

 if (!reply)
  return;

 skb = audit_make_reply(portid, seq, type, done, multi, payload, size);
 if (!skb)
  goto out;

 reply->net = get_net(net);
 reply->portid = portid;
 reply->skb = skb;

 tsk = kthread_run(audit_send_reply_thread, reply, "audit_send_reply");
 if (!IS_ERR(tsk))
  return;
 kfree_skb(skb);
out:
 kfree(reply);
}





static int audit_netlink_ok(struct sk_buff *skb, u16 msg_type)
{
 int err = 0;
 if (current_user_ns() != &init_user_ns)
  return -ECONNREFUSED;

 switch (msg_type) {
 case AUDIT_LIST:
 case AUDIT_ADD:
 case AUDIT_DEL:
  return -EOPNOTSUPP;
 case AUDIT_GET:
 case AUDIT_SET:
 case AUDIT_GET_FEATURE:
 case AUDIT_SET_FEATURE:
 case AUDIT_LIST_RULES:
 case AUDIT_ADD_RULE:
 case AUDIT_DEL_RULE:
 case AUDIT_SIGNAL_INFO:
 case AUDIT_TTY_GET:
 case AUDIT_TTY_SET:
 case AUDIT_TRIM:
 case AUDIT_MAKE_EQUIV:


  if (task_active_pid_ns(current) != &init_pid_ns)
   return -EPERM;

  if (!netlink_capable(skb, CAP_AUDIT_CONTROL))
   err = -EPERM;
  break;
 case AUDIT_USER:
 case AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:
 case AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:
  if (!netlink_capable(skb, CAP_AUDIT_WRITE))
   err = -EPERM;
  break;
 default:
  err = -EINVAL;
 }

 return err;
}

static void audit_log_common_recv_msg(struct audit_buffer **ab, u16 msg_type)
{
 uid_t uid = from_kuid(&init_user_ns, current_uid());
 pid_t pid = task_tgid_nr(current);

 if (!audit_enabled && msg_type != AUDIT_USER_AVC) {
  *ab = NULL;
  return;
 }

 *ab = audit_log_start(NULL, GFP_KERNEL, msg_type);
 if (unlikely(!*ab))
  return;
 audit_log_format(*ab, "pid=%d uid=%u", pid, uid);
 audit_log_session_info(*ab);
 audit_log_task_context(*ab);
}

int is_audit_feature_set(int i)
{
 return af.features & AUDIT_FEATURE_TO_MASK(i);
}


static int audit_get_feature(struct sk_buff *skb)
{
 u32 seq;

 seq = nlmsg_hdr(skb)->nlmsg_seq;

 audit_send_reply(skb, seq, AUDIT_GET_FEATURE, 0, 0, &af, sizeof(af));

 return 0;
}

static void audit_log_feature_change(int which, u32 old_feature, u32 new_feature,
         u32 old_lock, u32 new_lock, int res)
{
 struct audit_buffer *ab;

 if (audit_enabled == AUDIT_OFF)
  return;

 ab = audit_log_start(NULL, GFP_KERNEL, AUDIT_FEATURE_CHANGE);
 audit_log_task_info(ab, current);
 audit_log_format(ab, " feature=%s old=%u new=%u old_lock=%u new_lock=%u res=%d",
    audit_feature_names[which], !!old_feature, !!new_feature,
    !!old_lock, !!new_lock, res);
 audit_log_end(ab);
}

static int audit_set_feature(struct sk_buff *skb)
{
 struct audit_features *uaf;
 int i;

 BUILD_BUG_ON(AUDIT_LAST_FEATURE + 1 > ARRAY_SIZE(audit_feature_names));
 uaf = nlmsg_data(nlmsg_hdr(skb));



 for (i = 0; i <= AUDIT_LAST_FEATURE; i++) {
  u32 feature = AUDIT_FEATURE_TO_MASK(i);
  u32 old_feature, new_feature, old_lock, new_lock;


  if (!(feature & uaf->mask))
   continue;

  old_feature = af.features & feature;
  new_feature = uaf->features & feature;
  new_lock = (uaf->lock | af.lock) & feature;
  old_lock = af.lock & feature;


  if (old_lock && (new_feature != old_feature)) {
   audit_log_feature_change(i, old_feature, new_feature,
       old_lock, new_lock, 0);
   return -EPERM;
  }
 }

 for (i = 0; i <= AUDIT_LAST_FEATURE; i++) {
  u32 feature = AUDIT_FEATURE_TO_MASK(i);
  u32 old_feature, new_feature, old_lock, new_lock;


  if (!(feature & uaf->mask))
   continue;

  old_feature = af.features & feature;
  new_feature = uaf->features & feature;
  old_lock = af.lock & feature;
  new_lock = (uaf->lock | af.lock) & feature;

  if (new_feature != old_feature)
   audit_log_feature_change(i, old_feature, new_feature,
       old_lock, new_lock, 1);

  if (new_feature)
   af.features |= feature;
  else
   af.features &= ~feature;
  af.lock |= new_lock;
 }

 return 0;
}

static int audit_replace(pid_t pid)
{
 struct sk_buff *skb = audit_make_reply(0, 0, AUDIT_REPLACE, 0, 0,
            &pid, sizeof(pid));

 if (!skb)
  return -ENOMEM;
 return netlink_unicast(audit_sock, skb, audit_nlk_portid, 0);
}

static int audit_receive_msg(struct sk_buff *skb, struct nlmsghdr *nlh)
{
 u32 seq;
 void *data;
 int err;
 struct audit_buffer *ab;
 u16 msg_type = nlh->nlmsg_type;
 struct audit_sig_info *sig_data;
 char *ctx = NULL;
 u32 len;

 err = audit_netlink_ok(skb, msg_type);
 if (err)
  return err;



 if (!kauditd_task) {
  kauditd_task = kthread_run(kauditd_thread, NULL, "kauditd");
  if (IS_ERR(kauditd_task)) {
   err = PTR_ERR(kauditd_task);
   kauditd_task = NULL;
   return err;
  }
 }
 seq = nlh->nlmsg_seq;
 data = nlmsg_data(nlh);

 switch (msg_type) {
 case AUDIT_GET: {
  struct audit_status s;
  memset(&s, 0, sizeof(s));
  s.enabled = audit_enabled;
  s.failure = audit_failure;
  s.pid = audit_pid;
  s.rate_limit = audit_rate_limit;
  s.backlog_limit = audit_backlog_limit;
  s.lost = atomic_read(&audit_lost);
  s.backlog = skb_queue_len(&audit_skb_queue);
  s.feature_bitmap = AUDIT_FEATURE_BITMAP_ALL;
  s.backlog_wait_time = audit_backlog_wait_time_master;
  audit_send_reply(skb, seq, AUDIT_GET, 0, 0, &s, sizeof(s));
  break;
 }
 case AUDIT_SET: {
  struct audit_status s;
  memset(&s, 0, sizeof(s));

  memcpy(&s, data, min_t(size_t, sizeof(s), nlmsg_len(nlh)));
  if (s.mask & AUDIT_STATUS_ENABLED) {
   err = audit_set_enabled(s.enabled);
   if (err < 0)
    return err;
  }
  if (s.mask & AUDIT_STATUS_FAILURE) {
   err = audit_set_failure(s.failure);
   if (err < 0)
    return err;
  }
  if (s.mask & AUDIT_STATUS_PID) {
   int new_pid = s.pid;
   pid_t requesting_pid = task_tgid_vnr(current);

   if ((!new_pid) && (requesting_pid != audit_pid)) {
    audit_log_config_change("audit_pid", new_pid, audit_pid, 0);
    return -EACCES;
   }
   if (audit_pid && new_pid &&
       audit_replace(requesting_pid) != -ECONNREFUSED) {
    audit_log_config_change("audit_pid", new_pid, audit_pid, 0);
    return -EEXIST;
   }
   if (audit_enabled != AUDIT_OFF)
    audit_log_config_change("audit_pid", new_pid, audit_pid, 1);
   audit_pid = new_pid;
   audit_nlk_portid = NETLINK_CB(skb).portid;
   audit_sock = skb->sk;
  }
  if (s.mask & AUDIT_STATUS_RATE_LIMIT) {
   err = audit_set_rate_limit(s.rate_limit);
   if (err < 0)
    return err;
  }
  if (s.mask & AUDIT_STATUS_BACKLOG_LIMIT) {
   err = audit_set_backlog_limit(s.backlog_limit);
   if (err < 0)
    return err;
  }
  if (s.mask & AUDIT_STATUS_BACKLOG_WAIT_TIME) {
   if (sizeof(s) > (size_t)nlh->nlmsg_len)
    return -EINVAL;
   if (s.backlog_wait_time > 10*AUDIT_BACKLOG_WAIT_TIME)
    return -EINVAL;
   err = audit_set_backlog_wait_time(s.backlog_wait_time);
   if (err < 0)
    return err;
  }
  break;
 }
 case AUDIT_GET_FEATURE:
  err = audit_get_feature(skb);
  if (err)
   return err;
  break;
 case AUDIT_SET_FEATURE:
  err = audit_set_feature(skb);
  if (err)
   return err;
  break;
 case AUDIT_USER:
 case AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:
 case AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:
  if (!audit_enabled && msg_type != AUDIT_USER_AVC)
   return 0;

  err = audit_filter_user(msg_type);
  if (err == 1) {
   err = 0;
   if (msg_type == AUDIT_USER_TTY) {
    err = tty_audit_push();
    if (err)
     break;
   }
   mutex_unlock(&audit_cmd_mutex);
   audit_log_common_recv_msg(&ab, msg_type);
   if (msg_type != AUDIT_USER_TTY)
    audit_log_format(ab, " msg='%.*s'",
       AUDIT_MESSAGE_TEXT_MAX,
       (char *)data);
   else {
    int size;

    audit_log_format(ab, " data=");
    size = nlmsg_len(nlh);
    if (size > 0 &&
        ((unsigned char *)data)[size - 1] == '\0')
     size--;
    audit_log_n_untrustedstring(ab, data, size);
   }
   audit_set_portid(ab, NETLINK_CB(skb).portid);
   audit_log_end(ab);
   mutex_lock(&audit_cmd_mutex);
  }
  break;
 case AUDIT_ADD_RULE:
 case AUDIT_DEL_RULE:
  if (nlmsg_len(nlh) < sizeof(struct audit_rule_data))
   return -EINVAL;
  if (audit_enabled == AUDIT_LOCKED) {
   audit_log_common_recv_msg(&ab, AUDIT_CONFIG_CHANGE);
   audit_log_format(ab, " audit_enabled=%d res=0", audit_enabled);
   audit_log_end(ab);
   return -EPERM;
  }
  err = audit_rule_change(msg_type, NETLINK_CB(skb).portid,
        seq, data, nlmsg_len(nlh));
  break;
 case AUDIT_LIST_RULES:
  err = audit_list_rules_send(skb, seq);
  break;
 case AUDIT_TRIM:
  audit_trim_trees();
  audit_log_common_recv_msg(&ab, AUDIT_CONFIG_CHANGE);
  audit_log_format(ab, " op=trim res=1");
  audit_log_end(ab);
  break;
 case AUDIT_MAKE_EQUIV: {
  void *bufp = data;
  u32 sizes[2];
  size_t msglen = nlmsg_len(nlh);
  char *old, *new;

  err = -EINVAL;
  if (msglen < 2 * sizeof(u32))
   break;
  memcpy(sizes, bufp, 2 * sizeof(u32));
  bufp += 2 * sizeof(u32);
  msglen -= 2 * sizeof(u32);
  old = audit_unpack_string(&bufp, &msglen, sizes[0]);
  if (IS_ERR(old)) {
   err = PTR_ERR(old);
   break;
  }
  new = audit_unpack_string(&bufp, &msglen, sizes[1]);
  if (IS_ERR(new)) {
   err = PTR_ERR(new);
   kfree(old);
   break;
  }

  err = audit_tag_tree(old, new);

  audit_log_common_recv_msg(&ab, AUDIT_CONFIG_CHANGE);

  audit_log_format(ab, " op=make_equiv old=");
  audit_log_untrustedstring(ab, old);
  audit_log_format(ab, " new=");
  audit_log_untrustedstring(ab, new);
  audit_log_format(ab, " res=%d", !err);
  audit_log_end(ab);
  kfree(old);
  kfree(new);
  break;
 }
 case AUDIT_SIGNAL_INFO:
  len = 0;
  if (audit_sig_sid) {
   err = security_secid_to_secctx(audit_sig_sid, &ctx, &len);
   if (err)
    return err;
  }
  sig_data = kmalloc(sizeof(*sig_data) + len, GFP_KERNEL);
  if (!sig_data) {
   if (audit_sig_sid)
    security_release_secctx(ctx, len);
   return -ENOMEM;
  }
  sig_data->uid = from_kuid(&init_user_ns, audit_sig_uid);
  sig_data->pid = audit_sig_pid;
  if (audit_sig_sid) {
   memcpy(sig_data->ctx, ctx, len);
   security_release_secctx(ctx, len);
  }
  audit_send_reply(skb, seq, AUDIT_SIGNAL_INFO, 0, 0,
     sig_data, sizeof(*sig_data) + len);
  kfree(sig_data);
  break;
 case AUDIT_TTY_GET: {
  struct audit_tty_status s;
  unsigned int t;

  t = READ_ONCE(current->signal->audit_tty);
  s.enabled = t & AUDIT_TTY_ENABLE;
  s.log_passwd = !!(t & AUDIT_TTY_LOG_PASSWD);

  audit_send_reply(skb, seq, AUDIT_TTY_GET, 0, 0, &s, sizeof(s));
  break;
 }
 case AUDIT_TTY_SET: {
  struct audit_tty_status s, old;
  struct audit_buffer *ab;
  unsigned int t;

  memset(&s, 0, sizeof(s));

  memcpy(&s, data, min_t(size_t, sizeof(s), nlmsg_len(nlh)));

  if ((s.enabled != 0 && s.enabled != 1) ||
      (s.log_passwd != 0 && s.log_passwd != 1))
   err = -EINVAL;

  if (err)
   t = READ_ONCE(current->signal->audit_tty);
  else {
   t = s.enabled | (-s.log_passwd & AUDIT_TTY_LOG_PASSWD);
   t = xchg(&current->signal->audit_tty, t);
  }
  old.enabled = t & AUDIT_TTY_ENABLE;
  old.log_passwd = !!(t & AUDIT_TTY_LOG_PASSWD);

  audit_log_common_recv_msg(&ab, AUDIT_CONFIG_CHANGE);
  audit_log_format(ab, " op=tty_set old-enabled=%d new-enabled=%d"
     " old-log_passwd=%d new-log_passwd=%d res=%d",
     old.enabled, s.enabled, old.log_passwd,
     s.log_passwd, !err);
  audit_log_end(ab);
  break;
 }
 default:
  err = -EINVAL;
  break;
 }

 return err < 0 ? err : 0;
}





static void audit_receive_skb(struct sk_buff *skb)
{
 struct nlmsghdr *nlh;




 int len;
 int err;

 nlh = nlmsg_hdr(skb);
 len = skb->len;

 while (nlmsg_ok(nlh, len)) {
  err = audit_receive_msg(skb, nlh);

  if (err || (nlh->nlmsg_flags & NLM_F_ACK))
   netlink_ack(skb, nlh, err);

  nlh = nlmsg_next(nlh, &len);
 }
}


static void audit_receive(struct sk_buff *skb)
{
 mutex_lock(&audit_cmd_mutex);
 audit_receive_skb(skb);
 mutex_unlock(&audit_cmd_mutex);
}


static int audit_bind(struct net *net, int group)
{
 if (!capable(CAP_AUDIT_READ))
  return -EPERM;

 return 0;
}

static int __net_init audit_net_init(struct net *net)
{
 struct netlink_kernel_cfg cfg = {
  .input = audit_receive,
  .bind = audit_bind,
  .flags = NL_CFG_F_NONROOT_RECV,
  .groups = AUDIT_NLGRP_MAX,
 };

 struct audit_net *aunet = net_generic(net, audit_net_id);

 aunet->nlsk = netlink_kernel_create(net, NETLINK_AUDIT, &cfg);
 if (aunet->nlsk == NULL) {
  audit_panic("cannot initialize netlink socket in namespace");
  return -ENOMEM;
 }
 aunet->nlsk->sk_sndtimeo = MAX_SCHEDULE_TIMEOUT;
 return 0;
}

static void __net_exit audit_net_exit(struct net *net)
{
 struct audit_net *aunet = net_generic(net, audit_net_id);
 struct sock *sock = aunet->nlsk;
 if (sock == audit_sock) {
  audit_pid = 0;
  audit_sock = NULL;
 }

 RCU_INIT_POINTER(aunet->nlsk, NULL);
 synchronize_net();
 netlink_kernel_release(sock);
}

static struct pernet_operations audit_net_ops __net_initdata = {
 .init = audit_net_init,
 .exit = audit_net_exit,
 .id = &audit_net_id,
 .size = sizeof(struct audit_net),
};


static int __init audit_init(void)
{
 int i;

 if (audit_initialized == AUDIT_DISABLED)
  return 0;

 pr_info("initializing netlink subsys (%s)\n",
  audit_default ? "enabled" : "disabled");
 register_pernet_subsys(&audit_net_ops);

 skb_queue_head_init(&audit_skb_queue);
 skb_queue_head_init(&audit_skb_hold_queue);
 audit_initialized = AUDIT_INITIALIZED;
 audit_enabled = audit_default;
 audit_ever_enabled |= !!audit_default;

 audit_log(NULL, GFP_KERNEL, AUDIT_KERNEL, "initialized");

 for (i = 0; i < AUDIT_INODE_BUCKETS; i++)
  INIT_LIST_HEAD(&audit_inode_hash[i]);

 return 0;
}
__initcall(audit_init);


static int __init audit_enable(char *str)
{
 audit_default = !!simple_strtol(str, NULL, 0);
 if (!audit_default)
  audit_initialized = AUDIT_DISABLED;

 pr_info("%s\n", audit_default ?
  "enabled (after initialization)" : "disabled (until reboot)");

 return 1;
}
__setup("audit=", audit_enable);



static int __init audit_backlog_limit_set(char *str)
{
 u32 audit_backlog_limit_arg;

 pr_info("audit_backlog_limit: ");
 if (kstrtouint(str, 0, &audit_backlog_limit_arg)) {
  pr_cont("using default of %u, unable to parse %s\n",
   audit_backlog_limit, str);
  return 1;
 }

 audit_backlog_limit = audit_backlog_limit_arg;
 pr_cont("%d\n", audit_backlog_limit);

 return 1;
}
__setup("audit_backlog_limit=", audit_backlog_limit_set);

static void audit_buffer_free(struct audit_buffer *ab)
{
 unsigned long flags;

 if (!ab)
  return;

 kfree_skb(ab->skb);
 spin_lock_irqsave(&audit_freelist_lock, flags);
 if (audit_freelist_count > AUDIT_MAXFREE)
  kfree(ab);
 else {
  audit_freelist_count++;
  list_add(&ab->list, &audit_freelist);
 }
 spin_unlock_irqrestore(&audit_freelist_lock, flags);
}

static struct audit_buffer * audit_buffer_alloc(struct audit_context *ctx,
      gfp_t gfp_mask, int type)
{
 unsigned long flags;
 struct audit_buffer *ab = NULL;
 struct nlmsghdr *nlh;

 spin_lock_irqsave(&audit_freelist_lock, flags);
 if (!list_empty(&audit_freelist)) {
  ab = list_entry(audit_freelist.next,
    struct audit_buffer, list);
  list_del(&ab->list);
  --audit_freelist_count;
 }
 spin_unlock_irqrestore(&audit_freelist_lock, flags);

 if (!ab) {
  ab = kmalloc(sizeof(*ab), gfp_mask);
  if (!ab)
   goto err;
 }

 ab->ctx = ctx;
 ab->gfp_mask = gfp_mask;

 ab->skb = nlmsg_new(AUDIT_BUFSIZ, gfp_mask);
 if (!ab->skb)
  goto err;

 nlh = nlmsg_put(ab->skb, 0, 0, type, 0, 0);
 if (!nlh)
  goto out_kfree_skb;

 return ab;

out_kfree_skb:
 kfree_skb(ab->skb);
 ab->skb = NULL;
err:
 audit_buffer_free(ab);
 return NULL;
}
unsigned int audit_serial(void)
{
 static atomic_t serial = ATOMIC_INIT(0);

 return atomic_add_return(1, &serial);
}

static inline void audit_get_stamp(struct audit_context *ctx,
       struct timespec *t, unsigned int *serial)
{
 if (!ctx || !auditsc_get_stamp(ctx, t, serial)) {
  *t = CURRENT_TIME;
  *serial = audit_serial();
 }
}




static long wait_for_auditd(long sleep_time)
{
 DECLARE_WAITQUEUE(wait, current);

 if (audit_backlog_limit &&
     skb_queue_len(&audit_skb_queue) > audit_backlog_limit) {
  add_wait_queue_exclusive(&audit_backlog_wait, &wait);
  set_current_state(TASK_UNINTERRUPTIBLE);
  sleep_time = schedule_timeout(sleep_time);
  remove_wait_queue(&audit_backlog_wait, &wait);
 }

 return sleep_time;
}
struct audit_buffer *audit_log_start(struct audit_context *ctx, gfp_t gfp_mask,
         int type)
{
 struct audit_buffer *ab = NULL;
 struct timespec t;
 unsigned int uninitialized_var(serial);
 int reserve = 5;

 unsigned long timeout_start = jiffies;

 if (audit_initialized != AUDIT_INITIALIZED)
  return NULL;

 if (unlikely(audit_filter_type(type)))
  return NULL;

 if (gfp_mask & __GFP_DIRECT_RECLAIM) {
  if (audit_pid && audit_pid == current->tgid)
   gfp_mask &= ~__GFP_DIRECT_RECLAIM;
  else
   reserve = 0;
 }

 while (audit_backlog_limit
        && skb_queue_len(&audit_skb_queue) > audit_backlog_limit + reserve) {
  if (gfp_mask & __GFP_DIRECT_RECLAIM && audit_backlog_wait_time) {
   long sleep_time;

   sleep_time = timeout_start + audit_backlog_wait_time - jiffies;
   if (sleep_time > 0) {
    sleep_time = wait_for_auditd(sleep_time);
    if (sleep_time > 0)
     continue;
   }
  }
  if (audit_rate_check() && printk_ratelimit())
   pr_warn("audit_backlog=%d > audit_backlog_limit=%d\n",
    skb_queue_len(&audit_skb_queue),
    audit_backlog_limit);
  audit_log_lost("backlog limit exceeded");
  audit_backlog_wait_time = 0;
  wake_up(&audit_backlog_wait);
  return NULL;
 }

 if (!reserve && !audit_backlog_wait_time)
  audit_backlog_wait_time = audit_backlog_wait_time_master;

 ab = audit_buffer_alloc(ctx, gfp_mask, type);
 if (!ab) {
  audit_log_lost("out of memory in audit_log_start");
  return NULL;
 }

 audit_get_stamp(ab->ctx, &t, &serial);

 audit_log_format(ab, "audit(%lu.%03lu:%u): ",
    t.tv_sec, t.tv_nsec/1000000, serial);
 return ab;
}
static inline int audit_expand(struct audit_buffer *ab, int extra)
{
 struct sk_buff *skb = ab->skb;
 int oldtail = skb_tailroom(skb);
 int ret = pskb_expand_head(skb, 0, extra, ab->gfp_mask);
 int newtail = skb_tailroom(skb);

 if (ret < 0) {
  audit_log_lost("out of memory in audit_expand");
  return 0;
 }

 skb->truesize += newtail - oldtail;
 return newtail;
}







static void audit_log_vformat(struct audit_buffer *ab, const char *fmt,
         va_list args)
{
 int len, avail;
 struct sk_buff *skb;
 va_list args2;

 if (!ab)
  return;

 BUG_ON(!ab->skb);
 skb = ab->skb;
 avail = skb_tailroom(skb);
 if (avail == 0) {
  avail = audit_expand(ab, AUDIT_BUFSIZ);
  if (!avail)
   goto out;
 }
 va_copy(args2, args);
 len = vsnprintf(skb_tail_pointer(skb), avail, fmt, args);
 if (len >= avail) {



  avail = audit_expand(ab,
   max_t(unsigned, AUDIT_BUFSIZ, 1+len-avail));
  if (!avail)
   goto out_va_end;
  len = vsnprintf(skb_tail_pointer(skb), avail, fmt, args2);
 }
 if (len > 0)
  skb_put(skb, len);
out_va_end:
 va_end(args2);
out:
 return;
}
void audit_log_format(struct audit_buffer *ab, const char *fmt, ...)
{
 va_list args;

 if (!ab)
  return;
 va_start(args, fmt);
 audit_log_vformat(ab, fmt, args);
 va_end(args);
}
void audit_log_n_hex(struct audit_buffer *ab, const unsigned char *buf,
  size_t len)
{
 int i, avail, new_len;
 unsigned char *ptr;
 struct sk_buff *skb;

 if (!ab)
  return;

 BUG_ON(!ab->skb);
 skb = ab->skb;
 avail = skb_tailroom(skb);
 new_len = len<<1;
 if (new_len >= avail) {

  new_len = AUDIT_BUFSIZ*(((new_len-avail)/AUDIT_BUFSIZ) + 1);
  avail = audit_expand(ab, new_len);
  if (!avail)
   return;
 }

 ptr = skb_tail_pointer(skb);
 for (i = 0; i < len; i++)
  ptr = hex_byte_pack_upper(ptr, buf[i]);
 *ptr = 0;
 skb_put(skb, len << 1);
}





void audit_log_n_string(struct audit_buffer *ab, const char *string,
   size_t slen)
{
 int avail, new_len;
 unsigned char *ptr;
 struct sk_buff *skb;

 if (!ab)
  return;

 BUG_ON(!ab->skb);
 skb = ab->skb;
 avail = skb_tailroom(skb);
 new_len = slen + 3;
 if (new_len > avail) {
  avail = audit_expand(ab, new_len);
  if (!avail)
   return;
 }
 ptr = skb_tail_pointer(skb);
 *ptr++ = '"';
 memcpy(ptr, string, slen);
 ptr += slen;
 *ptr++ = '"';
 *ptr = 0;
 skb_put(skb, slen + 2);
}






bool audit_string_contains_control(const char *string, size_t len)
{
 const unsigned char *p;
 for (p = string; p < (const unsigned char *)string + len; p++) {
  if (*p == '"' || *p < 0x21 || *p > 0x7e)
   return true;
 }
 return false;
}
void audit_log_n_untrustedstring(struct audit_buffer *ab, const char *string,
     size_t len)
{
 if (audit_string_contains_control(string, len))
  audit_log_n_hex(ab, string, len);
 else
  audit_log_n_string(ab, string, len);
}
void audit_log_untrustedstring(struct audit_buffer *ab, const char *string)
{
 audit_log_n_untrustedstring(ab, string, strlen(string));
}


void audit_log_d_path(struct audit_buffer *ab, const char *prefix,
        const struct path *path)
{
 char *p, *pathname;

 if (prefix)
  audit_log_format(ab, "%s", prefix);


 pathname = kmalloc(PATH_MAX+11, ab->gfp_mask);
 if (!pathname) {
  audit_log_string(ab, "<no_memory>");
  return;
 }
 p = d_path(path, pathname, PATH_MAX+11);
 if (IS_ERR(p)) {

  audit_log_string(ab, "<too_long>");
 } else
  audit_log_untrustedstring(ab, p);
 kfree(pathname);
}

void audit_log_session_info(struct audit_buffer *ab)
{
 unsigned int sessionid = audit_get_sessionid(current);
 uid_t auid = from_kuid(&init_user_ns, audit_get_loginuid(current));

 audit_log_format(ab, " auid=%u ses=%u", auid, sessionid);
}

void audit_log_key(struct audit_buffer *ab, char *key)
{
 audit_log_format(ab, " key=");
 if (key)
  audit_log_untrustedstring(ab, key);
 else
  audit_log_format(ab, "(null)");
}

void audit_log_cap(struct audit_buffer *ab, char *prefix, kernel_cap_t *cap)
{
 int i;

 audit_log_format(ab, " %s=", prefix);
 CAP_FOR_EACH_U32(i) {
  audit_log_format(ab, "%08x",
     cap->cap[CAP_LAST_U32 - i]);
 }
}

static void audit_log_fcaps(struct audit_buffer *ab, struct audit_names *name)
{
 kernel_cap_t *perm = &name->fcap.permitted;
 kernel_cap_t *inh = &name->fcap.inheritable;
 int log = 0;

 if (!cap_isclear(*perm)) {
  audit_log_cap(ab, "cap_fp", perm);
  log = 1;
 }
 if (!cap_isclear(*inh)) {
  audit_log_cap(ab, "cap_fi", inh);
  log = 1;
 }

 if (log)
  audit_log_format(ab, " cap_fe=%d cap_fver=%x",
     name->fcap.fE, name->fcap_ver);
}

static inline int audit_copy_fcaps(struct audit_names *name,
       const struct dentry *dentry)
{
 struct cpu_vfs_cap_data caps;
 int rc;

 if (!dentry)
  return 0;

 rc = get_vfs_caps_from_disk(dentry, &caps);
 if (rc)
  return rc;

 name->fcap.permitted = caps.permitted;
 name->fcap.inheritable = caps.inheritable;
 name->fcap.fE = !!(caps.magic_etc & VFS_CAP_FLAGS_EFFECTIVE);
 name->fcap_ver = (caps.magic_etc & VFS_CAP_REVISION_MASK) >>
    VFS_CAP_REVISION_SHIFT;

 return 0;
}


void audit_copy_inode(struct audit_names *name, const struct dentry *dentry,
        struct inode *inode)
{
 name->ino = inode->i_ino;
 name->dev = inode->i_sb->s_dev;
 name->mode = inode->i_mode;
 name->uid = inode->i_uid;
 name->gid = inode->i_gid;
 name->rdev = inode->i_rdev;
 security_inode_getsecid(inode, &name->osid);
 audit_copy_fcaps(name, dentry);
}
void audit_log_name(struct audit_context *context, struct audit_names *n,
      struct path *path, int record_num, int *call_panic)
{
 struct audit_buffer *ab;
 ab = audit_log_start(context, GFP_KERNEL, AUDIT_PATH);
 if (!ab)
  return;

 audit_log_format(ab, "item=%d", record_num);

 if (path)
  audit_log_d_path(ab, " name=", path);
 else if (n->name) {
  switch (n->name_len) {
  case AUDIT_NAME_FULL:

   audit_log_format(ab, " name=");
   audit_log_untrustedstring(ab, n->name->name);
   break;
  case 0:


   audit_log_d_path(ab, " name=", &context->pwd);
   break;
  default:

   audit_log_format(ab, " name=");
   audit_log_n_untrustedstring(ab, n->name->name,
          n->name_len);
  }
 } else
  audit_log_format(ab, " name=(null)");

 if (n->ino != AUDIT_INO_UNSET)
  audit_log_format(ab, " inode=%lu"
     " dev=%02x:%02x mode=%#ho"
     " ouid=%u ogid=%u rdev=%02x:%02x",
     n->ino,
     MAJOR(n->dev),
     MINOR(n->dev),
     n->mode,
     from_kuid(&init_user_ns, n->uid),
     from_kgid(&init_user_ns, n->gid),
     MAJOR(n->rdev),
     MINOR(n->rdev));
 if (n->osid != 0) {
  char *ctx = NULL;
  u32 len;
  if (security_secid_to_secctx(
   n->osid, &ctx, &len)) {
   audit_log_format(ab, " osid=%u", n->osid);
   if (call_panic)
    *call_panic = 2;
  } else {
   audit_log_format(ab, " obj=%s", ctx);
   security_release_secctx(ctx, len);
  }
 }


 audit_log_format(ab, " nametype=");
 switch(n->type) {
 case AUDIT_TYPE_NORMAL:
  audit_log_format(ab, "NORMAL");
  break;
 case AUDIT_TYPE_PARENT:
  audit_log_format(ab, "PARENT");
  break;
 case AUDIT_TYPE_CHILD_DELETE:
  audit_log_format(ab, "DELETE");
  break;
 case AUDIT_TYPE_CHILD_CREATE:
  audit_log_format(ab, "CREATE");
  break;
 default:
  audit_log_format(ab, "UNKNOWN");
  break;
 }

 audit_log_fcaps(ab, n);
 audit_log_end(ab);
}

int audit_log_task_context(struct audit_buffer *ab)
{
 char *ctx = NULL;
 unsigned len;
 int error;
 u32 sid;

 security_task_getsecid(current, &sid);
 if (!sid)
  return 0;

 error = security_secid_to_secctx(sid, &ctx, &len);
 if (error) {
  if (error != -EINVAL)
   goto error_path;
  return 0;
 }

 audit_log_format(ab, " subj=%s", ctx);
 security_release_secctx(ctx, len);
 return 0;

error_path:
 audit_panic("error in audit_log_task_context");
 return error;
}
EXPORT_SYMBOL(audit_log_task_context);

void audit_log_d_path_exe(struct audit_buffer *ab,
     struct mm_struct *mm)
{
 struct file *exe_file;

 if (!mm)
  goto out_null;

 exe_file = get_mm_exe_file(mm);
 if (!exe_file)
  goto out_null;

 audit_log_d_path(ab, " exe=", &exe_file->f_path);
 fput(exe_file);
 return;
out_null:
 audit_log_format(ab, " exe=(null)");
}

struct tty_struct *audit_get_tty(struct task_struct *tsk)
{
 struct tty_struct *tty = NULL;
 unsigned long flags;

 spin_lock_irqsave(&tsk->sighand->siglock, flags);
 if (tsk->signal)
  tty = tty_kref_get(tsk->signal->tty);
 spin_unlock_irqrestore(&tsk->sighand->siglock, flags);
 return tty;
}

void audit_put_tty(struct tty_struct *tty)
{
 tty_kref_put(tty);
}

void audit_log_task_info(struct audit_buffer *ab, struct task_struct *tsk)
{
 const struct cred *cred;
 char comm[sizeof(tsk->comm)];
 struct tty_struct *tty;

 if (!ab)
  return;


 cred = current_cred();
 tty = audit_get_tty(tsk);
 audit_log_format(ab,
    " ppid=%d pid=%d auid=%u uid=%u gid=%u"
    " euid=%u suid=%u fsuid=%u"
    " egid=%u sgid=%u fsgid=%u tty=%s ses=%u",
    task_ppid_nr(tsk),
    task_pid_nr(tsk),
    from_kuid(&init_user_ns, audit_get_loginuid(tsk)),
    from_kuid(&init_user_ns, cred->uid),
    from_kgid(&init_user_ns, cred->gid),
    from_kuid(&init_user_ns, cred->euid),
    from_kuid(&init_user_ns, cred->suid),
    from_kuid(&init_user_ns, cred->fsuid),
    from_kgid(&init_user_ns, cred->egid),
    from_kgid(&init_user_ns, cred->sgid),
    from_kgid(&init_user_ns, cred->fsgid),
    tty ? tty_name(tty) : "(none)",
    audit_get_sessionid(tsk));
 audit_put_tty(tty);
 audit_log_format(ab, " comm=");
 audit_log_untrustedstring(ab, get_task_comm(comm, tsk));
 audit_log_d_path_exe(ab, tsk->mm);
 audit_log_task_context(ab);
}
EXPORT_SYMBOL(audit_log_task_info);






void audit_log_link_denied(const char *operation, struct path *link)
{
 struct audit_buffer *ab;
 struct audit_names *name;

 name = kzalloc(sizeof(*name), GFP_NOFS);
 if (!name)
  return;


 ab = audit_log_start(current->audit_context, GFP_KERNEL,
        AUDIT_ANOM_LINK);
 if (!ab)
  goto out;
 audit_log_format(ab, "op=%s", operation);
 audit_log_task_info(ab, current);
 audit_log_format(ab, " res=0");
 audit_log_end(ab);


 name->type = AUDIT_TYPE_NORMAL;
 audit_copy_inode(name, link->dentry, d_backing_inode(link->dentry));
 audit_log_name(current->audit_context, name, link, 0, NULL);
out:
 kfree(name);
}
void audit_log_end(struct audit_buffer *ab)
{
 if (!ab)
  return;
 if (!audit_rate_check()) {
  audit_log_lost("rate limit exceeded");
 } else {
  struct nlmsghdr *nlh = nlmsg_hdr(ab->skb);

  nlh->nlmsg_len = ab->skb->len;
  kauditd_send_multicast_skb(ab->skb, ab->gfp_mask);
  nlh->nlmsg_len -= NLMSG_HDRLEN;

  if (audit_pid) {
   skb_queue_tail(&audit_skb_queue, ab->skb);
   wake_up_interruptible(&kauditd_wait);
  } else {
   audit_printk_skb(ab->skb);
  }
  ab->skb = NULL;
 }
 audit_buffer_free(ab);
}
void audit_log(struct audit_context *ctx, gfp_t gfp_mask, int type,
        const char *fmt, ...)
{
 struct audit_buffer *ab;
 va_list args;

 ab = audit_log_start(ctx, gfp_mask, type);
 if (ab) {
  va_start(args, fmt);
  audit_log_vformat(ab, fmt, args);
  va_end(args);
  audit_log_end(ab);
 }
}

void audit_log_secctx(struct audit_buffer *ab, u32 secid)
{
 u32 len;
 char *secctx;

 if (security_secid_to_secctx(secid, &secctx, &len)) {
  audit_panic("Cannot convert secid to context");
 } else {
  audit_log_format(ab, " obj=%s", secctx);
  security_release_secctx(secctx, len);
 }
}
EXPORT_SYMBOL(audit_log_secctx);

EXPORT_SYMBOL(audit_log_start);
EXPORT_SYMBOL(audit_log_end);
EXPORT_SYMBOL(audit_log_format);
EXPORT_SYMBOL(audit_log);

struct list_head audit_filter_list[AUDIT_NR_FILTERS] = {
 LIST_HEAD_INIT(audit_filter_list[0]),
 LIST_HEAD_INIT(audit_filter_list[1]),
 LIST_HEAD_INIT(audit_filter_list[2]),
 LIST_HEAD_INIT(audit_filter_list[3]),
 LIST_HEAD_INIT(audit_filter_list[4]),
 LIST_HEAD_INIT(audit_filter_list[5]),
};
static struct list_head audit_rules_list[AUDIT_NR_FILTERS] = {
 LIST_HEAD_INIT(audit_rules_list[0]),
 LIST_HEAD_INIT(audit_rules_list[1]),
 LIST_HEAD_INIT(audit_rules_list[2]),
 LIST_HEAD_INIT(audit_rules_list[3]),
 LIST_HEAD_INIT(audit_rules_list[4]),
 LIST_HEAD_INIT(audit_rules_list[5]),
};

DEFINE_MUTEX(audit_filter_mutex);

static void audit_free_lsm_field(struct audit_field *f)
{
 switch (f->type) {
 case AUDIT_SUBJ_USER:
 case AUDIT_SUBJ_ROLE:
 case AUDIT_SUBJ_TYPE:
 case AUDIT_SUBJ_SEN:
 case AUDIT_SUBJ_CLR:
 case AUDIT_OBJ_USER:
 case AUDIT_OBJ_ROLE:
 case AUDIT_OBJ_TYPE:
 case AUDIT_OBJ_LEV_LOW:
 case AUDIT_OBJ_LEV_HIGH:
  kfree(f->lsm_str);
  security_audit_rule_free(f->lsm_rule);
 }
}

static inline void audit_free_rule(struct audit_entry *e)
{
 int i;
 struct audit_krule *erule = &e->rule;


 if (erule->watch)
  audit_put_watch(erule->watch);
 if (erule->fields)
  for (i = 0; i < erule->field_count; i++)
   audit_free_lsm_field(&erule->fields[i]);
 kfree(erule->fields);
 kfree(erule->filterkey);
 kfree(e);
}

void audit_free_rule_rcu(struct rcu_head *head)
{
 struct audit_entry *e = container_of(head, struct audit_entry, rcu);
 audit_free_rule(e);
}


static inline struct audit_entry *audit_init_entry(u32 field_count)
{
 struct audit_entry *entry;
 struct audit_field *fields;

 entry = kzalloc(sizeof(*entry), GFP_KERNEL);
 if (unlikely(!entry))
  return NULL;

 fields = kcalloc(field_count, sizeof(*fields), GFP_KERNEL);
 if (unlikely(!fields)) {
  kfree(entry);
  return NULL;
 }
 entry->rule.fields = fields;

 return entry;
}



char *audit_unpack_string(void **bufp, size_t *remain, size_t len)
{
 char *str;

 if (!*bufp || (len == 0) || (len > *remain))
  return ERR_PTR(-EINVAL);




 if (len > PATH_MAX)
  return ERR_PTR(-ENAMETOOLONG);

 str = kmalloc(len + 1, GFP_KERNEL);
 if (unlikely(!str))
  return ERR_PTR(-ENOMEM);

 memcpy(str, *bufp, len);
 str[len] = 0;
 *bufp += len;
 *remain -= len;

 return str;
}


static inline int audit_to_inode(struct audit_krule *krule,
     struct audit_field *f)
{
 if (krule->listnr != AUDIT_FILTER_EXIT ||
     krule->inode_f || krule->watch || krule->tree ||
     (f->op != Audit_equal && f->op != Audit_not_equal))
  return -EINVAL;

 krule->inode_f = f;
 return 0;
}

static __u32 *classes[AUDIT_SYSCALL_CLASSES];

int __init audit_register_class(int class, unsigned *list)
{
 __u32 *p = kcalloc(AUDIT_BITMASK_SIZE, sizeof(__u32), GFP_KERNEL);
 if (!p)
  return -ENOMEM;
 while (*list != ~0U) {
  unsigned n = *list++;
  if (n >= AUDIT_BITMASK_SIZE * 32 - AUDIT_SYSCALL_CLASSES) {
   kfree(p);
   return -EINVAL;
  }
  p[AUDIT_WORD(n)] |= AUDIT_BIT(n);
 }
 if (class >= AUDIT_SYSCALL_CLASSES || classes[class]) {
  kfree(p);
  return -EINVAL;
 }
 classes[class] = p;
 return 0;
}

int audit_match_class(int class, unsigned syscall)
{
 if (unlikely(syscall >= AUDIT_BITMASK_SIZE * 32))
  return 0;
 if (unlikely(class >= AUDIT_SYSCALL_CLASSES || !classes[class]))
  return 0;
 return classes[class][AUDIT_WORD(syscall)] & AUDIT_BIT(syscall);
}

static inline int audit_match_class_bits(int class, u32 *mask)
{
 int i;

 if (classes[class]) {
  for (i = 0; i < AUDIT_BITMASK_SIZE; i++)
   if (mask[i] & classes[class][i])
    return 0;
 }
 return 1;
}

static int audit_match_signal(struct audit_entry *entry)
{
 struct audit_field *arch = entry->rule.arch_f;

 if (!arch) {


  return (audit_match_class_bits(AUDIT_CLASS_SIGNAL,
            entry->rule.mask) &&
   audit_match_class_bits(AUDIT_CLASS_SIGNAL_32,
            entry->rule.mask));
 }

 switch(audit_classify_arch(arch->val)) {
 case 0:
  return (audit_match_class_bits(AUDIT_CLASS_SIGNAL,
            entry->rule.mask));
 case 1:
  return (audit_match_class_bits(AUDIT_CLASS_SIGNAL_32,
            entry->rule.mask));
 default:
  return 1;
 }
}


static inline struct audit_entry *audit_to_entry_common(struct audit_rule_data *rule)
{
 unsigned listnr;
 struct audit_entry *entry;
 int i, err;

 err = -EINVAL;
 listnr = rule->flags & ~AUDIT_FILTER_PREPEND;
 switch(listnr) {
 default:
  goto exit_err;
 case AUDIT_FILTER_ENTRY:
  if (rule->action == AUDIT_ALWAYS)
   goto exit_err;
 case AUDIT_FILTER_EXIT:
 case AUDIT_FILTER_TASK:
 case AUDIT_FILTER_USER:
 case AUDIT_FILTER_TYPE:
  ;
 }
 if (unlikely(rule->action == AUDIT_POSSIBLE)) {
  pr_err("AUDIT_POSSIBLE is deprecated\n");
  goto exit_err;
 }
 if (rule->action != AUDIT_NEVER && rule->action != AUDIT_ALWAYS)
  goto exit_err;
 if (rule->field_count > AUDIT_MAX_FIELDS)
  goto exit_err;

 err = -ENOMEM;
 entry = audit_init_entry(rule->field_count);
 if (!entry)
  goto exit_err;

 entry->rule.flags = rule->flags & AUDIT_FILTER_PREPEND;
 entry->rule.listnr = listnr;
 entry->rule.action = rule->action;
 entry->rule.field_count = rule->field_count;

 for (i = 0; i < AUDIT_BITMASK_SIZE; i++)
  entry->rule.mask[i] = rule->mask[i];

 for (i = 0; i < AUDIT_SYSCALL_CLASSES; i++) {
  int bit = AUDIT_BITMASK_SIZE * 32 - i - 1;
  __u32 *p = &entry->rule.mask[AUDIT_WORD(bit)];
  __u32 *class;

  if (!(*p & AUDIT_BIT(bit)))
   continue;
  *p &= ~AUDIT_BIT(bit);
  class = classes[i];
  if (class) {
   int j;
   for (j = 0; j < AUDIT_BITMASK_SIZE; j++)
    entry->rule.mask[j] |= class[j];
  }
 }

 return entry;

exit_err:
 return ERR_PTR(err);
}

static u32 audit_ops[] =
{
 [Audit_equal] = AUDIT_EQUAL,
 [Audit_not_equal] = AUDIT_NOT_EQUAL,
 [Audit_bitmask] = AUDIT_BIT_MASK,
 [Audit_bittest] = AUDIT_BIT_TEST,
 [Audit_lt] = AUDIT_LESS_THAN,
 [Audit_gt] = AUDIT_GREATER_THAN,
 [Audit_le] = AUDIT_LESS_THAN_OR_EQUAL,
 [Audit_ge] = AUDIT_GREATER_THAN_OR_EQUAL,
};

static u32 audit_to_op(u32 op)
{
 u32 n;
 for (n = Audit_equal; n < Audit_bad && audit_ops[n] != op; n++)
  ;
 return n;
}


static int audit_field_valid(struct audit_entry *entry, struct audit_field *f)
{
 switch(f->type) {
 case AUDIT_MSGTYPE:
  if (entry->rule.listnr != AUDIT_FILTER_TYPE &&
      entry->rule.listnr != AUDIT_FILTER_USER)
   return -EINVAL;
  break;
 };

 switch(f->type) {
 default:
  return -EINVAL;
 case AUDIT_UID:
 case AUDIT_EUID:
 case AUDIT_SUID:
 case AUDIT_FSUID:
 case AUDIT_LOGINUID:
 case AUDIT_OBJ_UID:
 case AUDIT_GID:
 case AUDIT_EGID:
 case AUDIT_SGID:
 case AUDIT_FSGID:
 case AUDIT_OBJ_GID:
 case AUDIT_PID:
 case AUDIT_PERS:
 case AUDIT_MSGTYPE:
 case AUDIT_PPID:
 case AUDIT_DEVMAJOR:
 case AUDIT_DEVMINOR:
 case AUDIT_EXIT:
 case AUDIT_SUCCESS:
 case AUDIT_INODE:

  if (f->op == Audit_bitmask || f->op == Audit_bittest)
   return -EINVAL;
  break;
 case AUDIT_ARG0:
 case AUDIT_ARG1:
 case AUDIT_ARG2:
 case AUDIT_ARG3:
 case AUDIT_SUBJ_USER:
 case AUDIT_SUBJ_ROLE:
 case AUDIT_SUBJ_TYPE:
 case AUDIT_SUBJ_SEN:
 case AUDIT_SUBJ_CLR:
 case AUDIT_OBJ_USER:
 case AUDIT_OBJ_ROLE:
 case AUDIT_OBJ_TYPE:
 case AUDIT_OBJ_LEV_LOW:
 case AUDIT_OBJ_LEV_HIGH:
 case AUDIT_WATCH:
 case AUDIT_DIR:
 case AUDIT_FILTERKEY:
  break;
 case AUDIT_LOGINUID_SET:
  if ((f->val != 0) && (f->val != 1))
   return -EINVAL;

 case AUDIT_ARCH:
  if (f->op != Audit_not_equal && f->op != Audit_equal)
   return -EINVAL;
  break;
 case AUDIT_PERM:
  if (f->val & ~15)
   return -EINVAL;
  break;
 case AUDIT_FILETYPE:
  if (f->val & ~S_IFMT)
   return -EINVAL;
  break;
 case AUDIT_FIELD_COMPARE:
  if (f->val > AUDIT_MAX_FIELD_COMPARE)
   return -EINVAL;
  break;
 case AUDIT_EXE:
  if (f->op != Audit_equal)
   return -EINVAL;
  if (entry->rule.listnr != AUDIT_FILTER_EXIT)
   return -EINVAL;
  break;
 };
 return 0;
}


static struct audit_entry *audit_data_to_entry(struct audit_rule_data *data,
            size_t datasz)
{
 int err = 0;
 struct audit_entry *entry;
 void *bufp;
 size_t remain = datasz - sizeof(struct audit_rule_data);
 int i;
 char *str;
 struct audit_fsnotify_mark *audit_mark;

 entry = audit_to_entry_common(data);
 if (IS_ERR(entry))
  goto exit_nofree;

 bufp = data->buf;
 for (i = 0; i < data->field_count; i++) {
  struct audit_field *f = &entry->rule.fields[i];

  err = -EINVAL;

  f->op = audit_to_op(data->fieldflags[i]);
  if (f->op == Audit_bad)
   goto exit_free;

  f->type = data->fields[i];
  f->val = data->values[i];


  if ((f->type == AUDIT_LOGINUID) && (f->val == AUDIT_UID_UNSET)) {
   f->type = AUDIT_LOGINUID_SET;
   f->val = 0;
   entry->rule.pflags |= AUDIT_LOGINUID_LEGACY;
  }

  err = audit_field_valid(entry, f);
  if (err)
   goto exit_free;

  err = -EINVAL;
  switch (f->type) {
  case AUDIT_LOGINUID:
  case AUDIT_UID:
  case AUDIT_EUID:
  case AUDIT_SUID:
  case AUDIT_FSUID:
  case AUDIT_OBJ_UID:
   f->uid = make_kuid(current_user_ns(), f->val);
   if (!uid_valid(f->uid))
    goto exit_free;
   break;
  case AUDIT_GID:
  case AUDIT_EGID:
  case AUDIT_SGID:
  case AUDIT_FSGID:
  case AUDIT_OBJ_GID:
   f->gid = make_kgid(current_user_ns(), f->val);
   if (!gid_valid(f->gid))
    goto exit_free;
   break;
  case AUDIT_ARCH:
   entry->rule.arch_f = f;
   break;
  case AUDIT_SUBJ_USER:
  case AUDIT_SUBJ_ROLE:
  case AUDIT_SUBJ_TYPE:
  case AUDIT_SUBJ_SEN:
  case AUDIT_SUBJ_CLR:
  case AUDIT_OBJ_USER:
  case AUDIT_OBJ_ROLE:
  case AUDIT_OBJ_TYPE:
  case AUDIT_OBJ_LEV_LOW:
  case AUDIT_OBJ_LEV_HIGH:
   str = audit_unpack_string(&bufp, &remain, f->val);
   if (IS_ERR(str))
    goto exit_free;
   entry->rule.buflen += f->val;

   err = security_audit_rule_init(f->type, f->op, str,
             (void **)&f->lsm_rule);


   if (err == -EINVAL) {
    pr_warn("audit rule for LSM \'%s\' is invalid\n",
     str);
    err = 0;
   }
   if (err) {
    kfree(str);
    goto exit_free;
   } else
    f->lsm_str = str;
   break;
  case AUDIT_WATCH:
   str = audit_unpack_string(&bufp, &remain, f->val);
   if (IS_ERR(str))
    goto exit_free;
   entry->rule.buflen += f->val;

   err = audit_to_watch(&entry->rule, str, f->val, f->op);
   if (err) {
    kfree(str);
    goto exit_free;
   }
   break;
  case AUDIT_DIR:
   str = audit_unpack_string(&bufp, &remain, f->val);
   if (IS_ERR(str))
    goto exit_free;
   entry->rule.buflen += f->val;

   err = audit_make_tree(&entry->rule, str, f->op);
   kfree(str);
   if (err)
    goto exit_free;
   break;
  case AUDIT_INODE:
   err = audit_to_inode(&entry->rule, f);
   if (err)
    goto exit_free;
   break;
  case AUDIT_FILTERKEY:
   if (entry->rule.filterkey || f->val > AUDIT_MAX_KEY_LEN)
    goto exit_free;
   str = audit_unpack_string(&bufp, &remain, f->val);
   if (IS_ERR(str))
    goto exit_free;
   entry->rule.buflen += f->val;
   entry->rule.filterkey = str;
   break;
  case AUDIT_EXE:
   if (entry->rule.exe || f->val > PATH_MAX)
    goto exit_free;
   str = audit_unpack_string(&bufp, &remain, f->val);
   if (IS_ERR(str)) {
    err = PTR_ERR(str);
    goto exit_free;
   }
   entry->rule.buflen += f->val;

   audit_mark = audit_alloc_mark(&entry->rule, str, f->val);
   if (IS_ERR(audit_mark)) {
    kfree(str);
    err = PTR_ERR(audit_mark);
    goto exit_free;
   }
   entry->rule.exe = audit_mark;
   break;
  }
 }

 if (entry->rule.inode_f && entry->rule.inode_f->op == Audit_not_equal)
  entry->rule.inode_f = NULL;

exit_nofree:
 return entry;

exit_free:
 if (entry->rule.tree)
  audit_put_tree(entry->rule.tree);
 if (entry->rule.exe)
  audit_remove_mark(entry->rule.exe);
 audit_free_rule(entry);
 return ERR_PTR(err);
}


static inline size_t audit_pack_string(void **bufp, const char *str)
{
 size_t len = strlen(str);

 memcpy(*bufp, str, len);
 *bufp += len;

 return len;
}


static struct audit_rule_data *audit_krule_to_data(struct audit_krule *krule)
{
 struct audit_rule_data *data;
 void *bufp;
 int i;

 data = kmalloc(sizeof(*data) + krule->buflen, GFP_KERNEL);
 if (unlikely(!data))
  return NULL;
 memset(data, 0, sizeof(*data));

 data->flags = krule->flags | krule->listnr;
 data->action = krule->action;
 data->field_count = krule->field_count;
 bufp = data->buf;
 for (i = 0; i < data->field_count; i++) {
  struct audit_field *f = &krule->fields[i];

  data->fields[i] = f->type;
  data->fieldflags[i] = audit_ops[f->op];
  switch(f->type) {
  case AUDIT_SUBJ_USER:
  case AUDIT_SUBJ_ROLE:
  case AUDIT_SUBJ_TYPE:
  case AUDIT_SUBJ_SEN:
  case AUDIT_SUBJ_CLR:
  case AUDIT_OBJ_USER:
  case AUDIT_OBJ_ROLE:
  case AUDIT_OBJ_TYPE:
  case AUDIT_OBJ_LEV_LOW:
  case AUDIT_OBJ_LEV_HIGH:
   data->buflen += data->values[i] =
    audit_pack_string(&bufp, f->lsm_str);
   break;
  case AUDIT_WATCH:
   data->buflen += data->values[i] =
    audit_pack_string(&bufp,
        audit_watch_path(krule->watch));
   break;
  case AUDIT_DIR:
   data->buflen += data->values[i] =
    audit_pack_string(&bufp,
        audit_tree_path(krule->tree));
   break;
  case AUDIT_FILTERKEY:
   data->buflen += data->values[i] =
    audit_pack_string(&bufp, krule->filterkey);
   break;
  case AUDIT_EXE:
   data->buflen += data->values[i] =
    audit_pack_string(&bufp, audit_mark_path(krule->exe));
   break;
  case AUDIT_LOGINUID_SET:
   if (krule->pflags & AUDIT_LOGINUID_LEGACY && !f->val) {
    data->fields[i] = AUDIT_LOGINUID;
    data->values[i] = AUDIT_UID_UNSET;
    break;
   }

  default:
   data->values[i] = f->val;
  }
 }
 for (i = 0; i < AUDIT_BITMASK_SIZE; i++) data->mask[i] = krule->mask[i];

 return data;
}



static int audit_compare_rule(struct audit_krule *a, struct audit_krule *b)
{
 int i;

 if (a->flags != b->flags ||
     a->pflags != b->pflags ||
     a->listnr != b->listnr ||
     a->action != b->action ||
     a->field_count != b->field_count)
  return 1;

 for (i = 0; i < a->field_count; i++) {
  if (a->fields[i].type != b->fields[i].type ||
      a->fields[i].op != b->fields[i].op)
   return 1;

  switch(a->fields[i].type) {
  case AUDIT_SUBJ_USER:
  case AUDIT_SUBJ_ROLE:
  case AUDIT_SUBJ_TYPE:
  case AUDIT_SUBJ_SEN:
  case AUDIT_SUBJ_CLR:
  case AUDIT_OBJ_USER:
  case AUDIT_OBJ_ROLE:
  case AUDIT_OBJ_TYPE:
  case AUDIT_OBJ_LEV_LOW:
  case AUDIT_OBJ_LEV_HIGH:
   if (strcmp(a->fields[i].lsm_str, b->fields[i].lsm_str))
    return 1;
   break;
  case AUDIT_WATCH:
   if (strcmp(audit_watch_path(a->watch),
       audit_watch_path(b->watch)))
    return 1;
   break;
  case AUDIT_DIR:
   if (strcmp(audit_tree_path(a->tree),
       audit_tree_path(b->tree)))
    return 1;
   break;
  case AUDIT_FILTERKEY:

   if (strcmp(a->filterkey, b->filterkey))
    return 1;
   break;
  case AUDIT_EXE:

   if (strcmp(audit_mark_path(a->exe),
       audit_mark_path(b->exe)))
    return 1;
   break;
  case AUDIT_UID:
  case AUDIT_EUID:
  case AUDIT_SUID:
  case AUDIT_FSUID:
  case AUDIT_LOGINUID:
  case AUDIT_OBJ_UID:
   if (!uid_eq(a->fields[i].uid, b->fields[i].uid))
    return 1;
   break;
  case AUDIT_GID:
  case AUDIT_EGID:
  case AUDIT_SGID:
  case AUDIT_FSGID:
  case AUDIT_OBJ_GID:
   if (!gid_eq(a->fields[i].gid, b->fields[i].gid))
    return 1;
   break;
  default:
   if (a->fields[i].val != b->fields[i].val)
    return 1;
  }
 }

 for (i = 0; i < AUDIT_BITMASK_SIZE; i++)
  if (a->mask[i] != b->mask[i])
   return 1;

 return 0;
}



static inline int audit_dupe_lsm_field(struct audit_field *df,
        struct audit_field *sf)
{
 int ret = 0;
 char *lsm_str;


 lsm_str = kstrdup(sf->lsm_str, GFP_KERNEL);
 if (unlikely(!lsm_str))
  return -ENOMEM;
 df->lsm_str = lsm_str;


 ret = security_audit_rule_init(df->type, df->op, df->lsm_str,
           (void **)&df->lsm_rule);


 if (ret == -EINVAL) {
  pr_warn("audit rule for LSM \'%s\' is invalid\n",
   df->lsm_str);
  ret = 0;
 }

 return ret;
}







struct audit_entry *audit_dupe_rule(struct audit_krule *old)
{
 u32 fcount = old->field_count;
 struct audit_entry *entry;
 struct audit_krule *new;
 char *fk;
 int i, err = 0;

 entry = audit_init_entry(fcount);
 if (unlikely(!entry))
  return ERR_PTR(-ENOMEM);

 new = &entry->rule;
 new->flags = old->flags;
 new->pflags = old->pflags;
 new->listnr = old->listnr;
 new->action = old->action;
 for (i = 0; i < AUDIT_BITMASK_SIZE; i++)
  new->mask[i] = old->mask[i];
 new->prio = old->prio;
 new->buflen = old->buflen;
 new->inode_f = old->inode_f;
 new->field_count = old->field_count;
 new->tree = old->tree;
 memcpy(new->fields, old->fields, sizeof(struct audit_field) * fcount);



 for (i = 0; i < fcount; i++) {
  switch (new->fields[i].type) {
  case AUDIT_SUBJ_USER:
  case AUDIT_SUBJ_ROLE:
  case AUDIT_SUBJ_TYPE:
  case AUDIT_SUBJ_SEN:
  case AUDIT_SUBJ_CLR:
  case AUDIT_OBJ_USER:
  case AUDIT_OBJ_ROLE:
  case AUDIT_OBJ_TYPE:
  case AUDIT_OBJ_LEV_LOW:
  case AUDIT_OBJ_LEV_HIGH:
   err = audit_dupe_lsm_field(&new->fields[i],
             &old->fields[i]);
   break;
  case AUDIT_FILTERKEY:
   fk = kstrdup(old->filterkey, GFP_KERNEL);
   if (unlikely(!fk))
    err = -ENOMEM;
   else
    new->filterkey = fk;
   break;
  case AUDIT_EXE:
   err = audit_dupe_exe(new, old);
   break;
  }
  if (err) {
   if (new->exe)
    audit_remove_mark(new->exe);
   audit_free_rule(entry);
   return ERR_PTR(err);
  }
 }

 if (old->watch) {
  audit_get_watch(old->watch);
  new->watch = old->watch;
 }

 return entry;
}



static struct audit_entry *audit_find_rule(struct audit_entry *entry,
        struct list_head **p)
{
 struct audit_entry *e, *found = NULL;
 struct list_head *list;
 int h;

 if (entry->rule.inode_f) {
  h = audit_hash_ino(entry->rule.inode_f->val);
  *p = list = &audit_inode_hash[h];
 } else if (entry->rule.watch) {

  for (h = 0; h < AUDIT_INODE_BUCKETS; h++) {
   list = &audit_inode_hash[h];
   list_for_each_entry(e, list, list)
    if (!audit_compare_rule(&entry->rule, &e->rule)) {
     found = e;
     goto out;
    }
  }
  goto out;
 } else {
  *p = list = &audit_filter_list[entry->rule.listnr];
 }

 list_for_each_entry(e, list, list)
  if (!audit_compare_rule(&entry->rule, &e->rule)) {
   found = e;
   goto out;
  }

out:
 return found;
}

static u64 prio_low = ~0ULL/2;
static u64 prio_high = ~0ULL/2 - 1;


static inline int audit_add_rule(struct audit_entry *entry)
{
 struct audit_entry *e;
 struct audit_watch *watch = entry->rule.watch;
 struct audit_tree *tree = entry->rule.tree;
 struct list_head *list;
 int err = 0;
 int dont_count = 0;


 if (entry->rule.listnr == AUDIT_FILTER_USER ||
  entry->rule.listnr == AUDIT_FILTER_TYPE)
  dont_count = 1;

 mutex_lock(&audit_filter_mutex);
 e = audit_find_rule(entry, &list);
 if (e) {
  mutex_unlock(&audit_filter_mutex);
  err = -EEXIST;

  if (tree)
   audit_put_tree(tree);
  return err;
 }

 if (watch) {

  err = audit_add_watch(&entry->rule, &list);
  if (err) {
   mutex_unlock(&audit_filter_mutex);




   if (tree)
    audit_put_tree(tree);
   return err;
  }
 }
 if (tree) {
  err = audit_add_tree_rule(&entry->rule);
  if (err) {
   mutex_unlock(&audit_filter_mutex);
   return err;
  }
 }

 entry->rule.prio = ~0ULL;
 if (entry->rule.listnr == AUDIT_FILTER_EXIT) {
  if (entry->rule.flags & AUDIT_FILTER_PREPEND)
   entry->rule.prio = ++prio_high;
  else
   entry->rule.prio = --prio_low;
 }

 if (entry->rule.flags & AUDIT_FILTER_PREPEND) {
  list_add(&entry->rule.list,
    &audit_rules_list[entry->rule.listnr]);
  list_add_rcu(&entry->list, list);
  entry->rule.flags &= ~AUDIT_FILTER_PREPEND;
 } else {
  list_add_tail(&entry->rule.list,
         &audit_rules_list[entry->rule.listnr]);
  list_add_tail_rcu(&entry->list, list);
 }
 if (!dont_count)
  audit_n_rules++;

 if (!audit_match_signal(entry))
  audit_signals++;
 mutex_unlock(&audit_filter_mutex);

 return err;
}


int audit_del_rule(struct audit_entry *entry)
{
 struct audit_entry *e;
 struct audit_tree *tree = entry->rule.tree;
 struct list_head *list;
 int ret = 0;
 int dont_count = 0;


 if (entry->rule.listnr == AUDIT_FILTER_USER ||
  entry->rule.listnr == AUDIT_FILTER_TYPE)
  dont_count = 1;

 mutex_lock(&audit_filter_mutex);
 e = audit_find_rule(entry, &list);
 if (!e) {
  ret = -ENOENT;
  goto out;
 }

 if (e->rule.watch)
  audit_remove_watch_rule(&e->rule);

 if (e->rule.tree)
  audit_remove_tree_rule(&e->rule);

 if (e->rule.exe)
  audit_remove_mark_rule(&e->rule);

 if (!dont_count)
  audit_n_rules--;

 if (!audit_match_signal(entry))
  audit_signals--;

 list_del_rcu(&e->list);
 list_del(&e->rule.list);
 call_rcu(&e->rcu, audit_free_rule_rcu);

out:
 mutex_unlock(&audit_filter_mutex);

 if (tree)
  audit_put_tree(tree);

 return ret;
}


static void audit_list_rules(__u32 portid, int seq, struct sk_buff_head *q)
{
 struct sk_buff *skb;
 struct audit_krule *r;
 int i;



 for (i=0; i<AUDIT_NR_FILTERS; i++) {
  list_for_each_entry(r, &audit_rules_list[i], list) {
   struct audit_rule_data *data;

   data = audit_krule_to_data(r);
   if (unlikely(!data))
    break;
   skb = audit_make_reply(portid, seq, AUDIT_LIST_RULES,
            0, 1, data,
            sizeof(*data) + data->buflen);
   if (skb)
    skb_queue_tail(q, skb);
   kfree(data);
  }
 }
 skb = audit_make_reply(portid, seq, AUDIT_LIST_RULES, 1, 1, NULL, 0);
 if (skb)
  skb_queue_tail(q, skb);
}


static void audit_log_rule_change(char *action, struct audit_krule *rule, int res)
{
 struct audit_buffer *ab;
 uid_t loginuid = from_kuid(&init_user_ns, audit_get_loginuid(current));
 unsigned int sessionid = audit_get_sessionid(current);

 if (!audit_enabled)
  return;

 ab = audit_log_start(NULL, GFP_KERNEL, AUDIT_CONFIG_CHANGE);
 if (!ab)
  return;
 audit_log_format(ab, "auid=%u ses=%u" ,loginuid, sessionid);
 audit_log_task_context(ab);
 audit_log_format(ab, " op=");
 audit_log_string(ab, action);
 audit_log_key(ab, rule->filterkey);
 audit_log_format(ab, " list=%d res=%d", rule->listnr, res);
 audit_log_end(ab);
}
int audit_rule_change(int type, __u32 portid, int seq, void *data,
   size_t datasz)
{
 int err = 0;
 struct audit_entry *entry;

 entry = audit_data_to_entry(data, datasz);
 if (IS_ERR(entry))
  return PTR_ERR(entry);

 switch (type) {
 case AUDIT_ADD_RULE:
  err = audit_add_rule(entry);
  audit_log_rule_change("add_rule", &entry->rule, !err);
  break;
 case AUDIT_DEL_RULE:
  err = audit_del_rule(entry);
  audit_log_rule_change("remove_rule", &entry->rule, !err);
  break;
 default:
  err = -EINVAL;
  WARN_ON(1);
 }

 if (err || type == AUDIT_DEL_RULE) {
  if (entry->rule.exe)
   audit_remove_mark(entry->rule.exe);
  audit_free_rule(entry);
 }

 return err;
}






int audit_list_rules_send(struct sk_buff *request_skb, int seq)
{
 u32 portid = NETLINK_CB(request_skb).portid;
 struct net *net = sock_net(NETLINK_CB(request_skb).sk);
 struct task_struct *tsk;
 struct audit_netlink_list *dest;
 int err = 0;







 dest = kmalloc(sizeof(struct audit_netlink_list), GFP_KERNEL);
 if (!dest)
  return -ENOMEM;
 dest->net = get_net(net);
 dest->portid = portid;
 skb_queue_head_init(&dest->q);

 mutex_lock(&audit_filter_mutex);
 audit_list_rules(portid, seq, &dest->q);
 mutex_unlock(&audit_filter_mutex);

 tsk = kthread_run(audit_send_list, dest, "audit_send_list");
 if (IS_ERR(tsk)) {
  skb_queue_purge(&dest->q);
  kfree(dest);
  err = PTR_ERR(tsk);
 }

 return err;
}

int audit_comparator(u32 left, u32 op, u32 right)
{
 switch (op) {
 case Audit_equal:
  return (left == right);
 case Audit_not_equal:
  return (left != right);
 case Audit_lt:
  return (left < right);
 case Audit_le:
  return (left <= right);
 case Audit_gt:
  return (left > right);
 case Audit_ge:
  return (left >= right);
 case Audit_bitmask:
  return (left & right);
 case Audit_bittest:
  return ((left & right) == right);
 default:
  BUG();
  return 0;
 }
}

int audit_uid_comparator(kuid_t left, u32 op, kuid_t right)
{
 switch (op) {
 case Audit_equal:
  return uid_eq(left, right);
 case Audit_not_equal:
  return !uid_eq(left, right);
 case Audit_lt:
  return uid_lt(left, right);
 case Audit_le:
  return uid_lte(left, right);
 case Audit_gt:
  return uid_gt(left, right);
 case Audit_ge:
  return uid_gte(left, right);
 case Audit_bitmask:
 case Audit_bittest:
 default:
  BUG();
  return 0;
 }
}

int audit_gid_comparator(kgid_t left, u32 op, kgid_t right)
{
 switch (op) {
 case Audit_equal:
  return gid_eq(left, right);
 case Audit_not_equal:
  return !gid_eq(left, right);
 case Audit_lt:
  return gid_lt(left, right);
 case Audit_le:
  return gid_lte(left, right);
 case Audit_gt:
  return gid_gt(left, right);
 case Audit_ge:
  return gid_gte(left, right);
 case Audit_bitmask:
 case Audit_bittest:
 default:
  BUG();
  return 0;
 }
}





int parent_len(const char *path)
{
 int plen;
 const char *p;

 plen = strlen(path);

 if (plen == 0)
  return plen;


 p = path + plen - 1;
 while ((*p == '/') && (p > path))
  p--;


 while ((*p != '/') && (p > path))
  p--;


 if (*p == '/')
  p++;

 return p - path;
}
int audit_compare_dname_path(const char *dname, const char *path, int parentlen)
{
 int dlen, pathlen;
 const char *p;

 dlen = strlen(dname);
 pathlen = strlen(path);
 if (pathlen < dlen)
  return 1;

 parentlen = parentlen == AUDIT_NAME_FULL ? parent_len(path) : parentlen;
 if (pathlen - parentlen != dlen)
  return 1;

 p = path + parentlen;

 return strncmp(p, dname, dlen);
}

static int audit_filter_user_rules(struct audit_krule *rule, int type,
       enum audit_state *state)
{
 int i;

 for (i = 0; i < rule->field_count; i++) {
  struct audit_field *f = &rule->fields[i];
  pid_t pid;
  int result = 0;
  u32 sid;

  switch (f->type) {
  case AUDIT_PID:
   pid = task_pid_nr(current);
   result = audit_comparator(pid, f->op, f->val);
   break;
  case AUDIT_UID:
   result = audit_uid_comparator(current_uid(), f->op, f->uid);
   break;
  case AUDIT_GID:
   result = audit_gid_comparator(current_gid(), f->op, f->gid);
   break;
  case AUDIT_LOGINUID:
   result = audit_uid_comparator(audit_get_loginuid(current),
        f->op, f->uid);
   break;
  case AUDIT_LOGINUID_SET:
   result = audit_comparator(audit_loginuid_set(current),
        f->op, f->val);
   break;
  case AUDIT_MSGTYPE:
   result = audit_comparator(type, f->op, f->val);
   break;
  case AUDIT_SUBJ_USER:
  case AUDIT_SUBJ_ROLE:
  case AUDIT_SUBJ_TYPE:
  case AUDIT_SUBJ_SEN:
  case AUDIT_SUBJ_CLR:
   if (f->lsm_rule) {
    security_task_getsecid(current, &sid);
    result = security_audit_rule_match(sid,
           f->type,
           f->op,
           f->lsm_rule,
           NULL);
   }
   break;
  }

  if (!result)
   return 0;
 }
 switch (rule->action) {
 case AUDIT_NEVER: *state = AUDIT_DISABLED; break;
 case AUDIT_ALWAYS: *state = AUDIT_RECORD_CONTEXT; break;
 }
 return 1;
}

int audit_filter_user(int type)
{
 enum audit_state state = AUDIT_DISABLED;
 struct audit_entry *e;
 int rc, ret;

 ret = 1;

 rcu_read_lock();
 list_for_each_entry_rcu(e, &audit_filter_list[AUDIT_FILTER_USER], list) {
  rc = audit_filter_user_rules(&e->rule, type, &state);
  if (rc) {
   if (rc > 0 && state == AUDIT_DISABLED)
    ret = 0;
   break;
  }
 }
 rcu_read_unlock();

 return ret;
}

int audit_filter_type(int type)
{
 struct audit_entry *e;
 int result = 0;

 rcu_read_lock();
 if (list_empty(&audit_filter_list[AUDIT_FILTER_TYPE]))
  goto unlock_and_return;

 list_for_each_entry_rcu(e, &audit_filter_list[AUDIT_FILTER_TYPE],
    list) {
  int i;
  for (i = 0; i < e->rule.field_count; i++) {
   struct audit_field *f = &e->rule.fields[i];
   if (f->type == AUDIT_MSGTYPE) {
    result = audit_comparator(type, f->op, f->val);
    if (!result)
     break;
   }
  }
  if (result)
   goto unlock_and_return;
 }
unlock_and_return:
 rcu_read_unlock();
 return result;
}

static int update_lsm_rule(struct audit_krule *r)
{
 struct audit_entry *entry = container_of(r, struct audit_entry, rule);
 struct audit_entry *nentry;
 int err = 0;

 if (!security_audit_rule_known(r))
  return 0;

 nentry = audit_dupe_rule(r);
 if (entry->rule.exe)
  audit_remove_mark(entry->rule.exe);
 if (IS_ERR(nentry)) {


  err = PTR_ERR(nentry);
  audit_panic("error updating LSM filters");
  if (r->watch)
   list_del(&r->rlist);
  list_del_rcu(&entry->list);
  list_del(&r->list);
 } else {
  if (r->watch || r->tree)
   list_replace_init(&r->rlist, &nentry->rule.rlist);
  list_replace_rcu(&entry->list, &nentry->list);
  list_replace(&r->list, &nentry->rule.list);
 }
 call_rcu(&entry->rcu, audit_free_rule_rcu);

 return err;
}






int audit_update_lsm_rules(void)
{
 struct audit_krule *r, *n;
 int i, err = 0;


 mutex_lock(&audit_filter_mutex);

 for (i = 0; i < AUDIT_NR_FILTERS; i++) {
  list_for_each_entry_safe(r, n, &audit_rules_list[i], list) {
   int res = update_lsm_rule(r);
   if (!err)
    err = res;
  }
 }
 mutex_unlock(&audit_filter_mutex);

 return err;
}





struct audit_fsnotify_mark {
 dev_t dev;
 unsigned long ino;
 char *path;
 struct fsnotify_mark mark;
 struct audit_krule *rule;
};


static struct fsnotify_group *audit_fsnotify_group;


    FS_MOVE_SELF | FS_EVENT_ON_CHILD)

static void audit_fsnotify_mark_free(struct audit_fsnotify_mark *audit_mark)
{
 kfree(audit_mark->path);
 kfree(audit_mark);
}

static void audit_fsnotify_free_mark(struct fsnotify_mark *mark)
{
 struct audit_fsnotify_mark *audit_mark;

 audit_mark = container_of(mark, struct audit_fsnotify_mark, mark);
 audit_fsnotify_mark_free(audit_mark);
}

char *audit_mark_path(struct audit_fsnotify_mark *mark)
{
 return mark->path;
}

int audit_mark_compare(struct audit_fsnotify_mark *mark, unsigned long ino, dev_t dev)
{
 if (mark->ino == AUDIT_INO_UNSET)
  return 0;
 return (mark->ino == ino) && (mark->dev == dev);
}

static void audit_update_mark(struct audit_fsnotify_mark *audit_mark,
        struct inode *inode)
{
 audit_mark->dev = inode ? inode->i_sb->s_dev : AUDIT_DEV_UNSET;
 audit_mark->ino = inode ? inode->i_ino : AUDIT_INO_UNSET;
}

struct audit_fsnotify_mark *audit_alloc_mark(struct audit_krule *krule, char *pathname, int len)
{
 struct audit_fsnotify_mark *audit_mark;
 struct path path;
 struct dentry *dentry;
 struct inode *inode;
 int ret;

 if (pathname[0] != '/' || pathname[len-1] == '/')
  return ERR_PTR(-EINVAL);

 dentry = kern_path_locked(pathname, &path);
 if (IS_ERR(dentry))
  return (void *)dentry;
 inode = path.dentry->d_inode;
 inode_unlock(inode);

 audit_mark = kzalloc(sizeof(*audit_mark), GFP_KERNEL);
 if (unlikely(!audit_mark)) {
  audit_mark = ERR_PTR(-ENOMEM);
  goto out;
 }

 fsnotify_init_mark(&audit_mark->mark, audit_fsnotify_free_mark);
 audit_mark->mark.mask = AUDIT_FS_EVENTS;
 audit_mark->path = pathname;
 audit_update_mark(audit_mark, dentry->d_inode);
 audit_mark->rule = krule;

 ret = fsnotify_add_mark(&audit_mark->mark, audit_fsnotify_group, inode, NULL, true);
 if (ret < 0) {
  audit_fsnotify_mark_free(audit_mark);
  audit_mark = ERR_PTR(ret);
 }
out:
 dput(dentry);
 path_put(&path);
 return audit_mark;
}

static void audit_mark_log_rule_change(struct audit_fsnotify_mark *audit_mark, char *op)
{
 struct audit_buffer *ab;
 struct audit_krule *rule = audit_mark->rule;

 if (!audit_enabled)
  return;
 ab = audit_log_start(NULL, GFP_NOFS, AUDIT_CONFIG_CHANGE);
 if (unlikely(!ab))
  return;
 audit_log_format(ab, "auid=%u ses=%u op=",
    from_kuid(&init_user_ns, audit_get_loginuid(current)),
    audit_get_sessionid(current));
 audit_log_string(ab, op);
 audit_log_format(ab, " path=");
 audit_log_untrustedstring(ab, audit_mark->path);
 audit_log_key(ab, rule->filterkey);
 audit_log_format(ab, " list=%d res=1", rule->listnr);
 audit_log_end(ab);
}

void audit_remove_mark(struct audit_fsnotify_mark *audit_mark)
{
 fsnotify_destroy_mark(&audit_mark->mark, audit_fsnotify_group);
 fsnotify_put_mark(&audit_mark->mark);
}

void audit_remove_mark_rule(struct audit_krule *krule)
{
 struct audit_fsnotify_mark *mark = krule->exe;

 audit_remove_mark(mark);
}

static void audit_autoremove_mark_rule(struct audit_fsnotify_mark *audit_mark)
{
 struct audit_krule *rule = audit_mark->rule;
 struct audit_entry *entry = container_of(rule, struct audit_entry, rule);

 audit_mark_log_rule_change(audit_mark, "autoremove_rule");
 audit_del_rule(entry);
}


static int audit_mark_handle_event(struct fsnotify_group *group,
        struct inode *to_tell,
        struct fsnotify_mark *inode_mark,
        struct fsnotify_mark *vfsmount_mark,
        u32 mask, void *data, int data_type,
        const unsigned char *dname, u32 cookie)
{
 struct audit_fsnotify_mark *audit_mark;
 struct inode *inode = NULL;

 audit_mark = container_of(inode_mark, struct audit_fsnotify_mark, mark);

 BUG_ON(group != audit_fsnotify_group);

 switch (data_type) {
 case (FSNOTIFY_EVENT_PATH):
  inode = ((struct path *)data)->dentry->d_inode;
  break;
 case (FSNOTIFY_EVENT_INODE):
  inode = (struct inode *)data;
  break;
 default:
  BUG();
  return 0;
 };

 if (mask & (FS_CREATE|FS_MOVED_TO|FS_DELETE|FS_MOVED_FROM)) {
  if (audit_compare_dname_path(dname, audit_mark->path, AUDIT_NAME_FULL))
   return 0;
  audit_update_mark(audit_mark, inode);
 } else if (mask & (FS_DELETE_SELF|FS_UNMOUNT|FS_MOVE_SELF))
  audit_autoremove_mark_rule(audit_mark);

 return 0;
}

static const struct fsnotify_ops audit_mark_fsnotify_ops = {
 .handle_event = audit_mark_handle_event,
};

static int __init audit_fsnotify_init(void)
{
 audit_fsnotify_group = fsnotify_alloc_group(&audit_mark_fsnotify_ops);
 if (IS_ERR(audit_fsnotify_group)) {
  audit_fsnotify_group = NULL;
  audit_panic("cannot create audit fsnotify group");
 }
 return 0;
}
device_initcall(audit_fsnotify_init);










int audit_n_rules;


int audit_signals;

struct audit_aux_data {
 struct audit_aux_data *next;
 int type;
};




struct audit_aux_data_pids {
 struct audit_aux_data d;
 pid_t target_pid[AUDIT_AUX_PIDS];
 kuid_t target_auid[AUDIT_AUX_PIDS];
 kuid_t target_uid[AUDIT_AUX_PIDS];
 unsigned int target_sessionid[AUDIT_AUX_PIDS];
 u32 target_sid[AUDIT_AUX_PIDS];
 char target_comm[AUDIT_AUX_PIDS][TASK_COMM_LEN];
 int pid_count;
};

struct audit_aux_data_bprm_fcaps {
 struct audit_aux_data d;
 struct audit_cap_data fcap;
 unsigned int fcap_ver;
 struct audit_cap_data old_pcap;
 struct audit_cap_data new_pcap;
};

struct audit_tree_refs {
 struct audit_tree_refs *next;
 struct audit_chunk *c[31];
};

static int audit_match_perm(struct audit_context *ctx, int mask)
{
 unsigned n;
 if (unlikely(!ctx))
  return 0;
 n = ctx->major;

 switch (audit_classify_syscall(ctx->arch, n)) {
 case 0:
  if ((mask & AUDIT_PERM_WRITE) &&
       audit_match_class(AUDIT_CLASS_WRITE, n))
   return 1;
  if ((mask & AUDIT_PERM_READ) &&
       audit_match_class(AUDIT_CLASS_READ, n))
   return 1;
  if ((mask & AUDIT_PERM_ATTR) &&
       audit_match_class(AUDIT_CLASS_CHATTR, n))
   return 1;
  return 0;
 case 1:
  if ((mask & AUDIT_PERM_WRITE) &&
       audit_match_class(AUDIT_CLASS_WRITE_32, n))
   return 1;
  if ((mask & AUDIT_PERM_READ) &&
       audit_match_class(AUDIT_CLASS_READ_32, n))
   return 1;
  if ((mask & AUDIT_PERM_ATTR) &&
       audit_match_class(AUDIT_CLASS_CHATTR_32, n))
   return 1;
  return 0;
 case 2:
  return mask & ACC_MODE(ctx->argv[1]);
 case 3:
  return mask & ACC_MODE(ctx->argv[2]);
 case 4:
  return ((mask & AUDIT_PERM_WRITE) && ctx->argv[0] == SYS_BIND);
 case 5:
  return mask & AUDIT_PERM_EXEC;
 default:
  return 0;
 }
}

static int audit_match_filetype(struct audit_context *ctx, int val)
{
 struct audit_names *n;
 umode_t mode = (umode_t)val;

 if (unlikely(!ctx))
  return 0;

 list_for_each_entry(n, &ctx->names_list, list) {
  if ((n->ino != AUDIT_INO_UNSET) &&
      ((n->mode & S_IFMT) == mode))
   return 1;
 }

 return 0;
}
static void audit_set_auditable(struct audit_context *ctx)
{
 if (!ctx->prio) {
  ctx->prio = 1;
  ctx->current_state = AUDIT_RECORD_CONTEXT;
 }
}

static int put_tree_ref(struct audit_context *ctx, struct audit_chunk *chunk)
{
 struct audit_tree_refs *p = ctx->trees;
 int left = ctx->tree_count;
 if (likely(left)) {
  p->c[--left] = chunk;
  ctx->tree_count = left;
  return 1;
 }
 if (!p)
  return 0;
 p = p->next;
 if (p) {
  p->c[30] = chunk;
  ctx->trees = p;
  ctx->tree_count = 30;
  return 1;
 }
 return 0;
}

static int grow_tree_refs(struct audit_context *ctx)
{
 struct audit_tree_refs *p = ctx->trees;
 ctx->trees = kzalloc(sizeof(struct audit_tree_refs), GFP_KERNEL);
 if (!ctx->trees) {
  ctx->trees = p;
  return 0;
 }
 if (p)
  p->next = ctx->trees;
 else
  ctx->first_trees = ctx->trees;
 ctx->tree_count = 31;
 return 1;
}

static void unroll_tree_refs(struct audit_context *ctx,
        struct audit_tree_refs *p, int count)
{
 struct audit_tree_refs *q;
 int n;
 if (!p) {

  p = ctx->first_trees;
  count = 31;

  if (!p)
   return;
 }
 n = count;
 for (q = p; q != ctx->trees; q = q->next, n = 31) {
  while (n--) {
   audit_put_chunk(q->c[n]);
   q->c[n] = NULL;
  }
 }
 while (n-- > ctx->tree_count) {
  audit_put_chunk(q->c[n]);
  q->c[n] = NULL;
 }
 ctx->trees = p;
 ctx->tree_count = count;
}

static void free_tree_refs(struct audit_context *ctx)
{
 struct audit_tree_refs *p, *q;
 for (p = ctx->first_trees; p; p = q) {
  q = p->next;
  kfree(p);
 }
}

static int match_tree_refs(struct audit_context *ctx, struct audit_tree *tree)
{
 struct audit_tree_refs *p;
 int n;
 if (!tree)
  return 0;

 for (p = ctx->first_trees; p != ctx->trees; p = p->next) {
  for (n = 0; n < 31; n++)
   if (audit_tree_match(p->c[n], tree))
    return 1;
 }

 if (p) {
  for (n = ctx->tree_count; n < 31; n++)
   if (audit_tree_match(p->c[n], tree))
    return 1;
 }
 return 0;
}

static int audit_compare_uid(kuid_t uid,
        struct audit_names *name,
        struct audit_field *f,
        struct audit_context *ctx)
{
 struct audit_names *n;
 int rc;

 if (name) {
  rc = audit_uid_comparator(uid, f->op, name->uid);
  if (rc)
   return rc;
 }

 if (ctx) {
  list_for_each_entry(n, &ctx->names_list, list) {
   rc = audit_uid_comparator(uid, f->op, n->uid);
   if (rc)
    return rc;
  }
 }
 return 0;
}

static int audit_compare_gid(kgid_t gid,
        struct audit_names *name,
        struct audit_field *f,
        struct audit_context *ctx)
{
 struct audit_names *n;
 int rc;

 if (name) {
  rc = audit_gid_comparator(gid, f->op, name->gid);
  if (rc)
   return rc;
 }

 if (ctx) {
  list_for_each_entry(n, &ctx->names_list, list) {
   rc = audit_gid_comparator(gid, f->op, n->gid);
   if (rc)
    return rc;
  }
 }
 return 0;
}

static int audit_field_compare(struct task_struct *tsk,
          const struct cred *cred,
          struct audit_field *f,
          struct audit_context *ctx,
          struct audit_names *name)
{
 switch (f->val) {

 case AUDIT_COMPARE_UID_TO_OBJ_UID:
  return audit_compare_uid(cred->uid, name, f, ctx);
 case AUDIT_COMPARE_GID_TO_OBJ_GID:
  return audit_compare_gid(cred->gid, name, f, ctx);
 case AUDIT_COMPARE_EUID_TO_OBJ_UID:
  return audit_compare_uid(cred->euid, name, f, ctx);
 case AUDIT_COMPARE_EGID_TO_OBJ_GID:
  return audit_compare_gid(cred->egid, name, f, ctx);
 case AUDIT_COMPARE_AUID_TO_OBJ_UID:
  return audit_compare_uid(tsk->loginuid, name, f, ctx);
 case AUDIT_COMPARE_SUID_TO_OBJ_UID:
  return audit_compare_uid(cred->suid, name, f, ctx);
 case AUDIT_COMPARE_SGID_TO_OBJ_GID:
  return audit_compare_gid(cred->sgid, name, f, ctx);
 case AUDIT_COMPARE_FSUID_TO_OBJ_UID:
  return audit_compare_uid(cred->fsuid, name, f, ctx);
 case AUDIT_COMPARE_FSGID_TO_OBJ_GID:
  return audit_compare_gid(cred->fsgid, name, f, ctx);

 case AUDIT_COMPARE_UID_TO_AUID:
  return audit_uid_comparator(cred->uid, f->op, tsk->loginuid);
 case AUDIT_COMPARE_UID_TO_EUID:
  return audit_uid_comparator(cred->uid, f->op, cred->euid);
 case AUDIT_COMPARE_UID_TO_SUID:
  return audit_uid_comparator(cred->uid, f->op, cred->suid);
 case AUDIT_COMPARE_UID_TO_FSUID:
  return audit_uid_comparator(cred->uid, f->op, cred->fsuid);

 case AUDIT_COMPARE_AUID_TO_EUID:
  return audit_uid_comparator(tsk->loginuid, f->op, cred->euid);
 case AUDIT_COMPARE_AUID_TO_SUID:
  return audit_uid_comparator(tsk->loginuid, f->op, cred->suid);
 case AUDIT_COMPARE_AUID_TO_FSUID:
  return audit_uid_comparator(tsk->loginuid, f->op, cred->fsuid);

 case AUDIT_COMPARE_EUID_TO_SUID:
  return audit_uid_comparator(cred->euid, f->op, cred->suid);
 case AUDIT_COMPARE_EUID_TO_FSUID:
  return audit_uid_comparator(cred->euid, f->op, cred->fsuid);

 case AUDIT_COMPARE_SUID_TO_FSUID:
  return audit_uid_comparator(cred->suid, f->op, cred->fsuid);

 case AUDIT_COMPARE_GID_TO_EGID:
  return audit_gid_comparator(cred->gid, f->op, cred->egid);
 case AUDIT_COMPARE_GID_TO_SGID:
  return audit_gid_comparator(cred->gid, f->op, cred->sgid);
 case AUDIT_COMPARE_GID_TO_FSGID:
  return audit_gid_comparator(cred->gid, f->op, cred->fsgid);

 case AUDIT_COMPARE_EGID_TO_SGID:
  return audit_gid_comparator(cred->egid, f->op, cred->sgid);
 case AUDIT_COMPARE_EGID_TO_FSGID:
  return audit_gid_comparator(cred->egid, f->op, cred->fsgid);

 case AUDIT_COMPARE_SGID_TO_FSGID:
  return audit_gid_comparator(cred->sgid, f->op, cred->fsgid);
 default:
  WARN(1, "Missing AUDIT_COMPARE define.  Report as a bug\n");
  return 0;
 }
 return 0;
}
static int audit_filter_rules(struct task_struct *tsk,
         struct audit_krule *rule,
         struct audit_context *ctx,
         struct audit_names *name,
         enum audit_state *state,
         bool task_creation)
{
 const struct cred *cred;
 int i, need_sid = 1;
 u32 sid;

 cred = rcu_dereference_check(tsk->cred, tsk == current || task_creation);

 for (i = 0; i < rule->field_count; i++) {
  struct audit_field *f = &rule->fields[i];
  struct audit_names *n;
  int result = 0;
  pid_t pid;

  switch (f->type) {
  case AUDIT_PID:
   pid = task_pid_nr(tsk);
   result = audit_comparator(pid, f->op, f->val);
   break;
  case AUDIT_PPID:
   if (ctx) {
    if (!ctx->ppid)
     ctx->ppid = task_ppid_nr(tsk);
    result = audit_comparator(ctx->ppid, f->op, f->val);
   }
   break;
  case AUDIT_EXE:
   result = audit_exe_compare(tsk, rule->exe);
   break;
  case AUDIT_UID:
   result = audit_uid_comparator(cred->uid, f->op, f->uid);
   break;
  case AUDIT_EUID:
   result = audit_uid_comparator(cred->euid, f->op, f->uid);
   break;
  case AUDIT_SUID:
   result = audit_uid_comparator(cred->suid, f->op, f->uid);
   break;
  case AUDIT_FSUID:
   result = audit_uid_comparator(cred->fsuid, f->op, f->uid);
   break;
  case AUDIT_GID:
   result = audit_gid_comparator(cred->gid, f->op, f->gid);
   if (f->op == Audit_equal) {
    if (!result)
     result = in_group_p(f->gid);
   } else if (f->op == Audit_not_equal) {
    if (result)
     result = !in_group_p(f->gid);
   }
   break;
  case AUDIT_EGID:
   result = audit_gid_comparator(cred->egid, f->op, f->gid);
   if (f->op == Audit_equal) {
    if (!result)
     result = in_egroup_p(f->gid);
   } else if (f->op == Audit_not_equal) {
    if (result)
     result = !in_egroup_p(f->gid);
   }
   break;
  case AUDIT_SGID:
   result = audit_gid_comparator(cred->sgid, f->op, f->gid);
   break;
  case AUDIT_FSGID:
   result = audit_gid_comparator(cred->fsgid, f->op, f->gid);
   break;
  case AUDIT_PERS:
   result = audit_comparator(tsk->personality, f->op, f->val);
   break;
  case AUDIT_ARCH:
   if (ctx)
    result = audit_comparator(ctx->arch, f->op, f->val);
   break;

  case AUDIT_EXIT:
   if (ctx && ctx->return_valid)
    result = audit_comparator(ctx->return_code, f->op, f->val);
   break;
  case AUDIT_SUCCESS:
   if (ctx && ctx->return_valid) {
    if (f->val)
     result = audit_comparator(ctx->return_valid, f->op, AUDITSC_SUCCESS);
    else
     result = audit_comparator(ctx->return_valid, f->op, AUDITSC_FAILURE);
   }
   break;
  case AUDIT_DEVMAJOR:
   if (name) {
    if (audit_comparator(MAJOR(name->dev), f->op, f->val) ||
        audit_comparator(MAJOR(name->rdev), f->op, f->val))
     ++result;
   } else if (ctx) {
    list_for_each_entry(n, &ctx->names_list, list) {
     if (audit_comparator(MAJOR(n->dev), f->op, f->val) ||
         audit_comparator(MAJOR(n->rdev), f->op, f->val)) {
      ++result;
      break;
     }
    }
   }
   break;
  case AUDIT_DEVMINOR:
   if (name) {
    if (audit_comparator(MINOR(name->dev), f->op, f->val) ||
        audit_comparator(MINOR(name->rdev), f->op, f->val))
     ++result;
   } else if (ctx) {
    list_for_each_entry(n, &ctx->names_list, list) {
     if (audit_comparator(MINOR(n->dev), f->op, f->val) ||
         audit_comparator(MINOR(n->rdev), f->op, f->val)) {
      ++result;
      break;
     }
    }
   }
   break;
  case AUDIT_INODE:
   if (name)
    result = audit_comparator(name->ino, f->op, f->val);
   else if (ctx) {
    list_for_each_entry(n, &ctx->names_list, list) {
     if (audit_comparator(n->ino, f->op, f->val)) {
      ++result;
      break;
     }
    }
   }
   break;
  case AUDIT_OBJ_UID:
   if (name) {
    result = audit_uid_comparator(name->uid, f->op, f->uid);
   } else if (ctx) {
    list_for_each_entry(n, &ctx->names_list, list) {
     if (audit_uid_comparator(n->uid, f->op, f->uid)) {
      ++result;
      break;
     }
    }
   }
   break;
  case AUDIT_OBJ_GID:
   if (name) {
    result = audit_gid_comparator(name->gid, f->op, f->gid);
   } else if (ctx) {
    list_for_each_entry(n, &ctx->names_list, list) {
     if (audit_gid_comparator(n->gid, f->op, f->gid)) {
      ++result;
      break;
     }
    }
   }
   break;
  case AUDIT_WATCH:
   if (name)
    result = audit_watch_compare(rule->watch, name->ino, name->dev);
   break;
  case AUDIT_DIR:
   if (ctx)
    result = match_tree_refs(ctx, rule->tree);
   break;
  case AUDIT_LOGINUID:
   result = audit_uid_comparator(tsk->loginuid, f->op, f->uid);
   break;
  case AUDIT_LOGINUID_SET:
   result = audit_comparator(audit_loginuid_set(tsk), f->op, f->val);
   break;
  case AUDIT_SUBJ_USER:
  case AUDIT_SUBJ_ROLE:
  case AUDIT_SUBJ_TYPE:
  case AUDIT_SUBJ_SEN:
  case AUDIT_SUBJ_CLR:





   if (f->lsm_rule) {
    if (need_sid) {
     security_task_getsecid(tsk, &sid);
     need_sid = 0;
    }
    result = security_audit_rule_match(sid, f->type,
                                      f->op,
                                      f->lsm_rule,
                                      ctx);
   }
   break;
  case AUDIT_OBJ_USER:
  case AUDIT_OBJ_ROLE:
  case AUDIT_OBJ_TYPE:
  case AUDIT_OBJ_LEV_LOW:
  case AUDIT_OBJ_LEV_HIGH:


   if (f->lsm_rule) {

    if (name) {
     result = security_audit_rule_match(
                name->osid, f->type, f->op,
                f->lsm_rule, ctx);
    } else if (ctx) {
     list_for_each_entry(n, &ctx->names_list, list) {
      if (security_audit_rule_match(n->osid, f->type,
               f->op, f->lsm_rule,
               ctx)) {
       ++result;
       break;
      }
     }
    }

    if (!ctx || ctx->type != AUDIT_IPC)
     break;
    if (security_audit_rule_match(ctx->ipc.osid,
             f->type, f->op,
             f->lsm_rule, ctx))
     ++result;
   }
   break;
  case AUDIT_ARG0:
  case AUDIT_ARG1:
  case AUDIT_ARG2:
  case AUDIT_ARG3:
   if (ctx)
    result = audit_comparator(ctx->argv[f->type-AUDIT_ARG0], f->op, f->val);
   break;
  case AUDIT_FILTERKEY:

   result = 1;
   break;
  case AUDIT_PERM:
   result = audit_match_perm(ctx, f->val);
   break;
  case AUDIT_FILETYPE:
   result = audit_match_filetype(ctx, f->val);
   break;
  case AUDIT_FIELD_COMPARE:
   result = audit_field_compare(tsk, cred, f, ctx, name);
   break;
  }
  if (!result)
   return 0;
 }

 if (ctx) {
  if (rule->prio <= ctx->prio)
   return 0;
  if (rule->filterkey) {
   kfree(ctx->filterkey);
   ctx->filterkey = kstrdup(rule->filterkey, GFP_ATOMIC);
  }
  ctx->prio = rule->prio;
 }
 switch (rule->action) {
 case AUDIT_NEVER: *state = AUDIT_DISABLED; break;
 case AUDIT_ALWAYS: *state = AUDIT_RECORD_CONTEXT; break;
 }
 return 1;
}





static enum audit_state audit_filter_task(struct task_struct *tsk, char **key)
{
 struct audit_entry *e;
 enum audit_state state;

 rcu_read_lock();
 list_for_each_entry_rcu(e, &audit_filter_list[AUDIT_FILTER_TASK], list) {
  if (audit_filter_rules(tsk, &e->rule, NULL, NULL,
           &state, true)) {
   if (state == AUDIT_RECORD_CONTEXT)
    *key = kstrdup(e->rule.filterkey, GFP_ATOMIC);
   rcu_read_unlock();
   return state;
  }
 }
 rcu_read_unlock();
 return AUDIT_BUILD_CONTEXT;
}

static int audit_in_mask(const struct audit_krule *rule, unsigned long val)
{
 int word, bit;

 if (val > 0xffffffff)
  return false;

 word = AUDIT_WORD(val);
 if (word >= AUDIT_BITMASK_SIZE)
  return false;

 bit = AUDIT_BIT(val);

 return rule->mask[word] & bit;
}






static enum audit_state audit_filter_syscall(struct task_struct *tsk,
          struct audit_context *ctx,
          struct list_head *list)
{
 struct audit_entry *e;
 enum audit_state state;

 if (audit_pid && tsk->tgid == audit_pid)
  return AUDIT_DISABLED;

 rcu_read_lock();
 if (!list_empty(list)) {
  list_for_each_entry_rcu(e, list, list) {
   if (audit_in_mask(&e->rule, ctx->major) &&
       audit_filter_rules(tsk, &e->rule, ctx, NULL,
            &state, false)) {
    rcu_read_unlock();
    ctx->current_state = state;
    return state;
   }
  }
 }
 rcu_read_unlock();
 return AUDIT_BUILD_CONTEXT;
}





static int audit_filter_inode_name(struct task_struct *tsk,
       struct audit_names *n,
       struct audit_context *ctx) {
 int h = audit_hash_ino((u32)n->ino);
 struct list_head *list = &audit_inode_hash[h];
 struct audit_entry *e;
 enum audit_state state;

 if (list_empty(list))
  return 0;

 list_for_each_entry_rcu(e, list, list) {
  if (audit_in_mask(&e->rule, ctx->major) &&
      audit_filter_rules(tsk, &e->rule, ctx, n, &state, false)) {
   ctx->current_state = state;
   return 1;
  }
 }

 return 0;
}






void audit_filter_inodes(struct task_struct *tsk, struct audit_context *ctx)
{
 struct audit_names *n;

 if (audit_pid && tsk->tgid == audit_pid)
  return;

 rcu_read_lock();

 list_for_each_entry(n, &ctx->names_list, list) {
  if (audit_filter_inode_name(tsk, n, ctx))
   break;
 }
 rcu_read_unlock();
}


static inline struct audit_context *audit_take_context(struct task_struct *tsk,
            int return_valid,
            long return_code)
{
 struct audit_context *context = tsk->audit_context;

 if (!context)
  return NULL;
 context->return_valid = return_valid;
 if (unlikely(return_code <= -ERESTARTSYS) &&
     (return_code >= -ERESTART_RESTARTBLOCK) &&
     (return_code != -ENOIOCTLCMD))
  context->return_code = -EINTR;
 else
  context->return_code = return_code;

 if (context->in_syscall && !context->dummy) {
  audit_filter_syscall(tsk, context, &audit_filter_list[AUDIT_FILTER_EXIT]);
  audit_filter_inodes(tsk, context);
 }

 tsk->audit_context = NULL;
 return context;
}

static inline void audit_proctitle_free(struct audit_context *context)
{
 kfree(context->proctitle.value);
 context->proctitle.value = NULL;
 context->proctitle.len = 0;
}

static inline void audit_free_names(struct audit_context *context)
{
 struct audit_names *n, *next;

 list_for_each_entry_safe(n, next, &context->names_list, list) {
  list_del(&n->list);
  if (n->name)
   putname(n->name);
  if (n->should_free)
   kfree(n);
 }
 context->name_count = 0;
 path_put(&context->pwd);
 context->pwd.dentry = NULL;
 context->pwd.mnt = NULL;
}

static inline void audit_free_aux(struct audit_context *context)
{
 struct audit_aux_data *aux;

 while ((aux = context->aux)) {
  context->aux = aux->next;
  kfree(aux);
 }
 while ((aux = context->aux_pids)) {
  context->aux_pids = aux->next;
  kfree(aux);
 }
}

static inline struct audit_context *audit_alloc_context(enum audit_state state)
{
 struct audit_context *context;

 context = kzalloc(sizeof(*context), GFP_KERNEL);
 if (!context)
  return NULL;
 context->state = state;
 context->prio = state == AUDIT_RECORD_CONTEXT ? ~0ULL : 0;
 INIT_LIST_HEAD(&context->killed_trees);
 INIT_LIST_HEAD(&context->names_list);
 return context;
}
int audit_alloc(struct task_struct *tsk)
{
 struct audit_context *context;
 enum audit_state state;
 char *key = NULL;

 if (likely(!audit_ever_enabled))
  return 0;

 state = audit_filter_task(tsk, &key);
 if (state == AUDIT_DISABLED) {
  clear_tsk_thread_flag(tsk, TIF_SYSCALL_AUDIT);
  return 0;
 }

 if (!(context = audit_alloc_context(state))) {
  kfree(key);
  audit_log_lost("out of memory in audit_alloc");
  return -ENOMEM;
 }
 context->filterkey = key;

 tsk->audit_context = context;
 set_tsk_thread_flag(tsk, TIF_SYSCALL_AUDIT);
 return 0;
}

static inline void audit_free_context(struct audit_context *context)
{
 audit_free_names(context);
 unroll_tree_refs(context, NULL, 0);
 free_tree_refs(context);
 audit_free_aux(context);
 kfree(context->filterkey);
 kfree(context->sockaddr);
 audit_proctitle_free(context);
 kfree(context);
}

static int audit_log_pid_context(struct audit_context *context, pid_t pid,
     kuid_t auid, kuid_t uid, unsigned int sessionid,
     u32 sid, char *comm)
{
 struct audit_buffer *ab;
 char *ctx = NULL;
 u32 len;
 int rc = 0;

 ab = audit_log_start(context, GFP_KERNEL, AUDIT_OBJ_PID);
 if (!ab)
  return rc;

 audit_log_format(ab, "opid=%d oauid=%d ouid=%d oses=%d", pid,
    from_kuid(&init_user_ns, auid),
    from_kuid(&init_user_ns, uid), sessionid);
 if (sid) {
  if (security_secid_to_secctx(sid, &ctx, &len)) {
   audit_log_format(ab, " obj=(none)");
   rc = 1;
  } else {
   audit_log_format(ab, " obj=%s", ctx);
   security_release_secctx(ctx, len);
  }
 }
 audit_log_format(ab, " ocomm=");
 audit_log_untrustedstring(ab, comm);
 audit_log_end(ab);

 return rc;
}
static int audit_log_single_execve_arg(struct audit_context *context,
     struct audit_buffer **ab,
     int arg_num,
     size_t *len_sent,
     const char __user *p,
     char *buf)
{
 char arg_num_len_buf[12];
 const char __user *tmp_p = p;

 size_t arg_num_len = snprintf(arg_num_len_buf, 12, "%d", arg_num) + 5;
 size_t len, len_left, to_send;
 size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
 unsigned int i, has_cntl = 0, too_long = 0;
 int ret;


 len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;







 if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
  send_sig(SIGKILL, current, 0);
  return -1;
 }


 do {
  if (len_left > MAX_EXECVE_AUDIT_LEN)
   to_send = MAX_EXECVE_AUDIT_LEN;
  else
   to_send = len_left;
  ret = copy_from_user(buf, tmp_p, to_send);





  if (ret) {
   WARN_ON(1);
   send_sig(SIGKILL, current, 0);
   return -1;
  }
  buf[to_send] = '\0';
  has_cntl = audit_string_contains_control(buf, to_send);
  if (has_cntl) {




   max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
   break;
  }
  len_left -= to_send;
  tmp_p += to_send;
 } while (len_left > 0);

 len_left = len;

 if (len > max_execve_audit_len)
  too_long = 1;


 for (i = 0; len_left > 0; i++) {
  int room_left;

  if (len_left > max_execve_audit_len)
   to_send = max_execve_audit_len;
  else
   to_send = len_left;


  room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
  if (has_cntl)
   room_left -= (to_send * 2);
  else
   room_left -= to_send;
  if (room_left < 0) {
   *len_sent = 0;
   audit_log_end(*ab);
   *ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
   if (!*ab)
    return 0;
  }





  if ((i == 0) && (too_long))
   audit_log_format(*ab, " a%d_len=%zu", arg_num,
      has_cntl ? 2*len : len);






  if (len >= max_execve_audit_len)
   ret = copy_from_user(buf, p, to_send);
  else
   ret = 0;
  if (ret) {
   WARN_ON(1);
   send_sig(SIGKILL, current, 0);
   return -1;
  }
  buf[to_send] = '\0';


  audit_log_format(*ab, " a%d", arg_num);
  if (too_long)
   audit_log_format(*ab, "[%d]", i);
  audit_log_format(*ab, "=");
  if (has_cntl)
   audit_log_n_hex(*ab, buf, to_send);
  else
   audit_log_string(*ab, buf);

  p += to_send;
  len_left -= to_send;
  *len_sent += arg_num_len;
  if (has_cntl)
   *len_sent += to_send * 2;
  else
   *len_sent += to_send;
 }

 return len + 1;
}

static void audit_log_execve_info(struct audit_context *context,
      struct audit_buffer **ab)
{
 int i, len;
 size_t len_sent = 0;
 const char __user *p;
 char *buf;

 p = (const char __user *)current->mm->arg_start;

 audit_log_format(*ab, "argc=%d", context->execve.argc);







 buf = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
 if (!buf) {
  audit_panic("out of memory for argv string");
  return;
 }

 for (i = 0; i < context->execve.argc; i++) {
  len = audit_log_single_execve_arg(context, ab, i,
        &len_sent, p, buf);
  if (len <= 0)
   break;
  p += len;
 }
 kfree(buf);
}

static void show_special(struct audit_context *context, int *call_panic)
{
 struct audit_buffer *ab;
 int i;

 ab = audit_log_start(context, GFP_KERNEL, context->type);
 if (!ab)
  return;

 switch (context->type) {
 case AUDIT_SOCKETCALL: {
  int nargs = context->socketcall.nargs;
  audit_log_format(ab, "nargs=%d", nargs);
  for (i = 0; i < nargs; i++)
   audit_log_format(ab, " a%d=%lx", i,
    context->socketcall.args[i]);
  break; }
 case AUDIT_IPC: {
  u32 osid = context->ipc.osid;

  audit_log_format(ab, "ouid=%u ogid=%u mode=%#ho",
     from_kuid(&init_user_ns, context->ipc.uid),
     from_kgid(&init_user_ns, context->ipc.gid),
     context->ipc.mode);
  if (osid) {
   char *ctx = NULL;
   u32 len;
   if (security_secid_to_secctx(osid, &ctx, &len)) {
    audit_log_format(ab, " osid=%u", osid);
    *call_panic = 1;
   } else {
    audit_log_format(ab, " obj=%s", ctx);
    security_release_secctx(ctx, len);
   }
  }
  if (context->ipc.has_perm) {
   audit_log_end(ab);
   ab = audit_log_start(context, GFP_KERNEL,
          AUDIT_IPC_SET_PERM);
   if (unlikely(!ab))
    return;
   audit_log_format(ab,
    "qbytes=%lx ouid=%u ogid=%u mode=%#ho",
    context->ipc.qbytes,
    context->ipc.perm_uid,
    context->ipc.perm_gid,
    context->ipc.perm_mode);
  }
  break; }
 case AUDIT_MQ_OPEN: {
  audit_log_format(ab,
   "oflag=0x%x mode=%#ho mq_flags=0x%lx mq_maxmsg=%ld "
   "mq_msgsize=%ld mq_curmsgs=%ld",
   context->mq_open.oflag, context->mq_open.mode,
   context->mq_open.attr.mq_flags,
   context->mq_open.attr.mq_maxmsg,
   context->mq_open.attr.mq_msgsize,
   context->mq_open.attr.mq_curmsgs);
  break; }
 case AUDIT_MQ_SENDRECV: {
  audit_log_format(ab,
   "mqdes=%d msg_len=%zd msg_prio=%u "
   "abs_timeout_sec=%ld abs_timeout_nsec=%ld",
   context->mq_sendrecv.mqdes,
   context->mq_sendrecv.msg_len,
   context->mq_sendrecv.msg_prio,
   context->mq_sendrecv.abs_timeout.tv_sec,
   context->mq_sendrecv.abs_timeout.tv_nsec);
  break; }
 case AUDIT_MQ_NOTIFY: {
  audit_log_format(ab, "mqdes=%d sigev_signo=%d",
    context->mq_notify.mqdes,
    context->mq_notify.sigev_signo);
  break; }
 case AUDIT_MQ_GETSETATTR: {
  struct mq_attr *attr = &context->mq_getsetattr.mqstat;
  audit_log_format(ab,
   "mqdes=%d mq_flags=0x%lx mq_maxmsg=%ld mq_msgsize=%ld "
   "mq_curmsgs=%ld ",
   context->mq_getsetattr.mqdes,
   attr->mq_flags, attr->mq_maxmsg,
   attr->mq_msgsize, attr->mq_curmsgs);
  break; }
 case AUDIT_CAPSET: {
  audit_log_format(ab, "pid=%d", context->capset.pid);
  audit_log_cap(ab, "cap_pi", &context->capset.cap.inheritable);
  audit_log_cap(ab, "cap_pp", &context->capset.cap.permitted);
  audit_log_cap(ab, "cap_pe", &context->capset.cap.effective);
  break; }
 case AUDIT_MMAP: {
  audit_log_format(ab, "fd=%d flags=0x%x", context->mmap.fd,
     context->mmap.flags);
  break; }
 case AUDIT_EXECVE: {
  audit_log_execve_info(context, &ab);
  break; }
 }
 audit_log_end(ab);
}

static inline int audit_proctitle_rtrim(char *proctitle, int len)
{
 char *end = proctitle + len - 1;
 while (end > proctitle && !isprint(*end))
  end--;


 len = end - proctitle + 1;
 len -= isprint(proctitle[len-1]) == 0;
 return len;
}

static void audit_log_proctitle(struct task_struct *tsk,
    struct audit_context *context)
{
 int res;
 char *buf;
 char *msg = "(null)";
 int len = strlen(msg);
 struct audit_buffer *ab;

 ab = audit_log_start(context, GFP_KERNEL, AUDIT_PROCTITLE);
 if (!ab)
  return;

 audit_log_format(ab, "proctitle=");


 if (!context->proctitle.value) {
  buf = kmalloc(MAX_PROCTITLE_AUDIT_LEN, GFP_KERNEL);
  if (!buf)
   goto out;

  res = get_cmdline(tsk, buf, MAX_PROCTITLE_AUDIT_LEN);
  if (res == 0) {
   kfree(buf);
   goto out;
  }
  res = audit_proctitle_rtrim(buf, res);
  if (res == 0) {
   kfree(buf);
   goto out;
  }
  context->proctitle.value = buf;
  context->proctitle.len = res;
 }
 msg = context->proctitle.value;
 len = context->proctitle.len;
out:
 audit_log_n_untrustedstring(ab, msg, len);
 audit_log_end(ab);
}

static void audit_log_exit(struct audit_context *context, struct task_struct *tsk)
{
 int i, call_panic = 0;
 struct audit_buffer *ab;
 struct audit_aux_data *aux;
 struct audit_names *n;


 context->personality = tsk->personality;

 ab = audit_log_start(context, GFP_KERNEL, AUDIT_SYSCALL);
 if (!ab)
  return;
 audit_log_format(ab, "arch=%x syscall=%d",
    context->arch, context->major);
 if (context->personality != PER_LINUX)
  audit_log_format(ab, " per=%lx", context->personality);
 if (context->return_valid)
  audit_log_format(ab, " success=%s exit=%ld",
     (context->return_valid==AUDITSC_SUCCESS)?"yes":"no",
     context->return_code);

 audit_log_format(ab,
    " a0=%lx a1=%lx a2=%lx a3=%lx items=%d",
    context->argv[0],
    context->argv[1],
    context->argv[2],
    context->argv[3],
    context->name_count);

 audit_log_task_info(ab, tsk);
 audit_log_key(ab, context->filterkey);
 audit_log_end(ab);

 for (aux = context->aux; aux; aux = aux->next) {

  ab = audit_log_start(context, GFP_KERNEL, aux->type);
  if (!ab)
   continue;

  switch (aux->type) {

  case AUDIT_BPRM_FCAPS: {
   struct audit_aux_data_bprm_fcaps *axs = (void *)aux;
   audit_log_format(ab, "fver=%x", axs->fcap_ver);
   audit_log_cap(ab, "fp", &axs->fcap.permitted);
   audit_log_cap(ab, "fi", &axs->fcap.inheritable);
   audit_log_format(ab, " fe=%d", axs->fcap.fE);
   audit_log_cap(ab, "old_pp", &axs->old_pcap.permitted);
   audit_log_cap(ab, "old_pi", &axs->old_pcap.inheritable);
   audit_log_cap(ab, "old_pe", &axs->old_pcap.effective);
   audit_log_cap(ab, "new_pp", &axs->new_pcap.permitted);
   audit_log_cap(ab, "new_pi", &axs->new_pcap.inheritable);
   audit_log_cap(ab, "new_pe", &axs->new_pcap.effective);
   break; }

  }
  audit_log_end(ab);
 }

 if (context->type)
  show_special(context, &call_panic);

 if (context->fds[0] >= 0) {
  ab = audit_log_start(context, GFP_KERNEL, AUDIT_FD_PAIR);
  if (ab) {
   audit_log_format(ab, "fd0=%d fd1=%d",
     context->fds[0], context->fds[1]);
   audit_log_end(ab);
  }
 }

 if (context->sockaddr_len) {
  ab = audit_log_start(context, GFP_KERNEL, AUDIT_SOCKADDR);
  if (ab) {
   audit_log_format(ab, "saddr=");
   audit_log_n_hex(ab, (void *)context->sockaddr,
     context->sockaddr_len);
   audit_log_end(ab);
  }
 }

 for (aux = context->aux_pids; aux; aux = aux->next) {
  struct audit_aux_data_pids *axs = (void *)aux;

  for (i = 0; i < axs->pid_count; i++)
   if (audit_log_pid_context(context, axs->target_pid[i],
        axs->target_auid[i],
        axs->target_uid[i],
        axs->target_sessionid[i],
        axs->target_sid[i],
        axs->target_comm[i]))
    call_panic = 1;
 }

 if (context->target_pid &&
     audit_log_pid_context(context, context->target_pid,
      context->target_auid, context->target_uid,
      context->target_sessionid,
      context->target_sid, context->target_comm))
   call_panic = 1;

 if (context->pwd.dentry && context->pwd.mnt) {
  ab = audit_log_start(context, GFP_KERNEL, AUDIT_CWD);
  if (ab) {
   audit_log_d_path(ab, " cwd=", &context->pwd);
   audit_log_end(ab);
  }
 }

 i = 0;
 list_for_each_entry(n, &context->names_list, list) {
  if (n->hidden)
   continue;
  audit_log_name(context, n, NULL, i++, &call_panic);
 }

 audit_log_proctitle(tsk, context);


 ab = audit_log_start(context, GFP_KERNEL, AUDIT_EOE);
 if (ab)
  audit_log_end(ab);
 if (call_panic)
  audit_panic("error converting sid to string");
}







void __audit_free(struct task_struct *tsk)
{
 struct audit_context *context;

 context = audit_take_context(tsk, 0, 0);
 if (!context)
  return;






 if (context->in_syscall && context->current_state == AUDIT_RECORD_CONTEXT)
  audit_log_exit(context, tsk);
 if (!list_empty(&context->killed_trees))
  audit_kill_trees(&context->killed_trees);

 audit_free_context(context);
}
void __audit_syscall_entry(int major, unsigned long a1, unsigned long a2,
      unsigned long a3, unsigned long a4)
{
 struct task_struct *tsk = current;
 struct audit_context *context = tsk->audit_context;
 enum audit_state state;

 if (!context)
  return;

 BUG_ON(context->in_syscall || context->name_count);

 if (!audit_enabled)
  return;

 context->arch = syscall_get_arch();
 context->major = major;
 context->argv[0] = a1;
 context->argv[1] = a2;
 context->argv[2] = a3;
 context->argv[3] = a4;

 state = context->state;
 context->dummy = !audit_n_rules;
 if (!context->dummy && state == AUDIT_BUILD_CONTEXT) {
  context->prio = 0;
  state = audit_filter_syscall(tsk, context, &audit_filter_list[AUDIT_FILTER_ENTRY]);
 }
 if (state == AUDIT_DISABLED)
  return;

 context->serial = 0;
 context->ctime = CURRENT_TIME;
 context->in_syscall = 1;
 context->current_state = state;
 context->ppid = 0;
}
void __audit_syscall_exit(int success, long return_code)
{
 struct task_struct *tsk = current;
 struct audit_context *context;

 if (success)
  success = AUDITSC_SUCCESS;
 else
  success = AUDITSC_FAILURE;

 context = audit_take_context(tsk, success, return_code);
 if (!context)
  return;

 if (context->in_syscall && context->current_state == AUDIT_RECORD_CONTEXT)
  audit_log_exit(context, tsk);

 context->in_syscall = 0;
 context->prio = context->state == AUDIT_RECORD_CONTEXT ? ~0ULL : 0;

 if (!list_empty(&context->killed_trees))
  audit_kill_trees(&context->killed_trees);

 audit_free_names(context);
 unroll_tree_refs(context, NULL, 0);
 audit_free_aux(context);
 context->aux = NULL;
 context->aux_pids = NULL;
 context->target_pid = 0;
 context->target_sid = 0;
 context->sockaddr_len = 0;
 context->type = 0;
 context->fds[0] = -1;
 if (context->state != AUDIT_RECORD_CONTEXT) {
  kfree(context->filterkey);
  context->filterkey = NULL;
 }
 tsk->audit_context = context;
}

static inline void handle_one(const struct inode *inode)
{
 struct audit_context *context;
 struct audit_tree_refs *p;
 struct audit_chunk *chunk;
 int count;
 if (likely(hlist_empty(&inode->i_fsnotify_marks)))
  return;
 context = current->audit_context;
 p = context->trees;
 count = context->tree_count;
 rcu_read_lock();
 chunk = audit_tree_lookup(inode);
 rcu_read_unlock();
 if (!chunk)
  return;
 if (likely(put_tree_ref(context, chunk)))
  return;
 if (unlikely(!grow_tree_refs(context))) {
  pr_warn("out of memory, audit has lost a tree reference\n");
  audit_set_auditable(context);
  audit_put_chunk(chunk);
  unroll_tree_refs(context, p, count);
  return;
 }
 put_tree_ref(context, chunk);
}

static void handle_path(const struct dentry *dentry)
{
 struct audit_context *context;
 struct audit_tree_refs *p;
 const struct dentry *d, *parent;
 struct audit_chunk *drop;
 unsigned long seq;
 int count;

 context = current->audit_context;
 p = context->trees;
 count = context->tree_count;
retry:
 drop = NULL;
 d = dentry;
 rcu_read_lock();
 seq = read_seqbegin(&rename_lock);
 for(;;) {
  struct inode *inode = d_backing_inode(d);
  if (inode && unlikely(!hlist_empty(&inode->i_fsnotify_marks))) {
   struct audit_chunk *chunk;
   chunk = audit_tree_lookup(inode);
   if (chunk) {
    if (unlikely(!put_tree_ref(context, chunk))) {
     drop = chunk;
     break;
    }
   }
  }
  parent = d->d_parent;
  if (parent == d)
   break;
  d = parent;
 }
 if (unlikely(read_seqretry(&rename_lock, seq) || drop)) {
  rcu_read_unlock();
  if (!drop) {

   unroll_tree_refs(context, p, count);
   goto retry;
  }
  audit_put_chunk(drop);
  if (grow_tree_refs(context)) {

   unroll_tree_refs(context, p, count);
   goto retry;
  }

  pr_warn("out of memory, audit has lost a tree reference\n");
  unroll_tree_refs(context, p, count);
  audit_set_auditable(context);
  return;
 }
 rcu_read_unlock();
}

static struct audit_names *audit_alloc_name(struct audit_context *context,
      unsigned char type)
{
 struct audit_names *aname;

 if (context->name_count < AUDIT_NAMES) {
  aname = &context->preallocated_names[context->name_count];
  memset(aname, 0, sizeof(*aname));
 } else {
  aname = kzalloc(sizeof(*aname), GFP_NOFS);
  if (!aname)
   return NULL;
  aname->should_free = true;
 }

 aname->ino = AUDIT_INO_UNSET;
 aname->type = type;
 list_add_tail(&aname->list, &context->names_list);

 context->name_count++;
 return aname;
}
struct filename *
__audit_reusename(const __user char *uptr)
{
 struct audit_context *context = current->audit_context;
 struct audit_names *n;

 list_for_each_entry(n, &context->names_list, list) {
  if (!n->name)
   continue;
  if (n->name->uptr == uptr) {
   n->name->refcnt++;
   return n->name;
  }
 }
 return NULL;
}
void __audit_getname(struct filename *name)
{
 struct audit_context *context = current->audit_context;
 struct audit_names *n;

 if (!context->in_syscall)
  return;

 n = audit_alloc_name(context, AUDIT_TYPE_UNKNOWN);
 if (!n)
  return;

 n->name = name;
 n->name_len = AUDIT_NAME_FULL;
 name->aname = n;
 name->refcnt++;

 if (!context->pwd.dentry)
  get_fs_pwd(current->fs, &context->pwd);
}







void __audit_inode(struct filename *name, const struct dentry *dentry,
     unsigned int flags)
{
 struct audit_context *context = current->audit_context;
 struct inode *inode = d_backing_inode(dentry);
 struct audit_names *n;
 bool parent = flags & AUDIT_INODE_PARENT;

 if (!context->in_syscall)
  return;

 if (!name)
  goto out_alloc;





 n = name->aname;
 if (n) {
  if (parent) {
   if (n->type == AUDIT_TYPE_PARENT ||
       n->type == AUDIT_TYPE_UNKNOWN)
    goto out;
  } else {
   if (n->type != AUDIT_TYPE_PARENT)
    goto out;
  }
 }

 list_for_each_entry_reverse(n, &context->names_list, list) {
  if (n->ino) {

   if (n->ino != inode->i_ino ||
       n->dev != inode->i_sb->s_dev)
    continue;
  } else if (n->name) {

   if (strcmp(n->name->name, name->name))
    continue;
  } else

   continue;


  if (parent) {
   if (n->type == AUDIT_TYPE_PARENT ||
       n->type == AUDIT_TYPE_UNKNOWN)
    goto out;
  } else {
   if (n->type != AUDIT_TYPE_PARENT)
    goto out;
  }
 }

out_alloc:

 n = audit_alloc_name(context, AUDIT_TYPE_UNKNOWN);
 if (!n)
  return;
 if (name) {
  n->name = name;
  name->refcnt++;
 }

out:
 if (parent) {
  n->name_len = n->name ? parent_len(n->name->name) : AUDIT_NAME_FULL;
  n->type = AUDIT_TYPE_PARENT;
  if (flags & AUDIT_INODE_HIDDEN)
   n->hidden = true;
 } else {
  n->name_len = AUDIT_NAME_FULL;
  n->type = AUDIT_TYPE_NORMAL;
 }
 handle_path(dentry);
 audit_copy_inode(n, dentry, inode);
}

void __audit_file(const struct file *file)
{
 __audit_inode(NULL, file->f_path.dentry, 0);
}
void __audit_inode_child(struct inode *parent,
    const struct dentry *dentry,
    const unsigned char type)
{
 struct audit_context *context = current->audit_context;
 struct inode *inode = d_backing_inode(dentry);
 const char *dname = dentry->d_name.name;
 struct audit_names *n, *found_parent = NULL, *found_child = NULL;

 if (!context->in_syscall)
  return;

 if (inode)
  handle_one(inode);


 list_for_each_entry(n, &context->names_list, list) {
  if (!n->name ||
      (n->type != AUDIT_TYPE_PARENT &&
       n->type != AUDIT_TYPE_UNKNOWN))
   continue;

  if (n->ino == parent->i_ino && n->dev == parent->i_sb->s_dev &&
      !audit_compare_dname_path(dname,
           n->name->name, n->name_len)) {
   if (n->type == AUDIT_TYPE_UNKNOWN)
    n->type = AUDIT_TYPE_PARENT;
   found_parent = n;
   break;
  }
 }


 list_for_each_entry(n, &context->names_list, list) {

  if (!n->name ||
      (n->type != type && n->type != AUDIT_TYPE_UNKNOWN))
   continue;

  if (!strcmp(dname, n->name->name) ||
      !audit_compare_dname_path(dname, n->name->name,
      found_parent ?
      found_parent->name_len :
      AUDIT_NAME_FULL)) {
   if (n->type == AUDIT_TYPE_UNKNOWN)
    n->type = type;
   found_child = n;
   break;
  }
 }

 if (!found_parent) {

  n = audit_alloc_name(context, AUDIT_TYPE_PARENT);
  if (!n)
   return;
  audit_copy_inode(n, NULL, parent);
 }

 if (!found_child) {
  found_child = audit_alloc_name(context, type);
  if (!found_child)
   return;




  if (found_parent) {
   found_child->name = found_parent->name;
   found_child->name_len = AUDIT_NAME_FULL;
   found_child->name->refcnt++;
  }
 }

 if (inode)
  audit_copy_inode(found_child, dentry, inode);
 else
  found_child->ino = AUDIT_INO_UNSET;
}
EXPORT_SYMBOL_GPL(__audit_inode_child);
int auditsc_get_stamp(struct audit_context *ctx,
         struct timespec *t, unsigned int *serial)
{
 if (!ctx->in_syscall)
  return 0;
 if (!ctx->serial)
  ctx->serial = audit_serial();
 t->tv_sec = ctx->ctime.tv_sec;
 t->tv_nsec = ctx->ctime.tv_nsec;
 *serial = ctx->serial;
 if (!ctx->prio) {
  ctx->prio = 1;
  ctx->current_state = AUDIT_RECORD_CONTEXT;
 }
 return 1;
}


static atomic_t session_id = ATOMIC_INIT(0);

static int audit_set_loginuid_perm(kuid_t loginuid)
{

 if (!audit_loginuid_set(current))
  return 0;

 if (is_audit_feature_set(AUDIT_FEATURE_LOGINUID_IMMUTABLE))
  return -EPERM;

 if (!capable(CAP_AUDIT_CONTROL))
  return -EPERM;

 if (is_audit_feature_set(AUDIT_FEATURE_ONLY_UNSET_LOGINUID) && uid_valid(loginuid))
  return -EPERM;
 return 0;
}

static void audit_log_set_loginuid(kuid_t koldloginuid, kuid_t kloginuid,
       unsigned int oldsessionid, unsigned int sessionid,
       int rc)
{
 struct audit_buffer *ab;
 uid_t uid, oldloginuid, loginuid;
 struct tty_struct *tty;

 if (!audit_enabled)
  return;

 ab = audit_log_start(NULL, GFP_KERNEL, AUDIT_LOGIN);
 if (!ab)
  return;

 uid = from_kuid(&init_user_ns, task_uid(current));
 oldloginuid = from_kuid(&init_user_ns, koldloginuid);
 loginuid = from_kuid(&init_user_ns, kloginuid),
 tty = audit_get_tty(current);

 audit_log_format(ab, "pid=%d uid=%u", task_pid_nr(current), uid);
 audit_log_task_context(ab);
 audit_log_format(ab, " old-auid=%u auid=%u tty=%s old-ses=%u ses=%u res=%d",
    oldloginuid, loginuid, tty ? tty_name(tty) : "(none)",
    oldsessionid, sessionid, !rc);
 audit_put_tty(tty);
 audit_log_end(ab);
}
int audit_set_loginuid(kuid_t loginuid)
{
 struct task_struct *task = current;
 unsigned int oldsessionid, sessionid = (unsigned int)-1;
 kuid_t oldloginuid;
 int rc;

 oldloginuid = audit_get_loginuid(current);
 oldsessionid = audit_get_sessionid(current);

 rc = audit_set_loginuid_perm(loginuid);
 if (rc)
  goto out;


 if (uid_valid(loginuid))
  sessionid = (unsigned int)atomic_inc_return(&session_id);

 task->sessionid = sessionid;
 task->loginuid = loginuid;
out:
 audit_log_set_loginuid(oldloginuid, loginuid, oldsessionid, sessionid, rc);
 return rc;
}
void __audit_mq_open(int oflag, umode_t mode, struct mq_attr *attr)
{
 struct audit_context *context = current->audit_context;

 if (attr)
  memcpy(&context->mq_open.attr, attr, sizeof(struct mq_attr));
 else
  memset(&context->mq_open.attr, 0, sizeof(struct mq_attr));

 context->mq_open.oflag = oflag;
 context->mq_open.mode = mode;

 context->type = AUDIT_MQ_OPEN;
}
void __audit_mq_sendrecv(mqd_t mqdes, size_t msg_len, unsigned int msg_prio,
   const struct timespec *abs_timeout)
{
 struct audit_context *context = current->audit_context;
 struct timespec *p = &context->mq_sendrecv.abs_timeout;

 if (abs_timeout)
  memcpy(p, abs_timeout, sizeof(struct timespec));
 else
  memset(p, 0, sizeof(struct timespec));

 context->mq_sendrecv.mqdes = mqdes;
 context->mq_sendrecv.msg_len = msg_len;
 context->mq_sendrecv.msg_prio = msg_prio;

 context->type = AUDIT_MQ_SENDRECV;
}
void __audit_mq_notify(mqd_t mqdes, const struct sigevent *notification)
{
 struct audit_context *context = current->audit_context;

 if (notification)
  context->mq_notify.sigev_signo = notification->sigev_signo;
 else
  context->mq_notify.sigev_signo = 0;

 context->mq_notify.mqdes = mqdes;
 context->type = AUDIT_MQ_NOTIFY;
}







void __audit_mq_getsetattr(mqd_t mqdes, struct mq_attr *mqstat)
{
 struct audit_context *context = current->audit_context;
 context->mq_getsetattr.mqdes = mqdes;
 context->mq_getsetattr.mqstat = *mqstat;
 context->type = AUDIT_MQ_GETSETATTR;
}






void __audit_ipc_obj(struct kern_ipc_perm *ipcp)
{
 struct audit_context *context = current->audit_context;
 context->ipc.uid = ipcp->uid;
 context->ipc.gid = ipcp->gid;
 context->ipc.mode = ipcp->mode;
 context->ipc.has_perm = 0;
 security_ipc_getsecid(ipcp, &context->ipc.osid);
 context->type = AUDIT_IPC;
}
void __audit_ipc_set_perm(unsigned long qbytes, uid_t uid, gid_t gid, umode_t mode)
{
 struct audit_context *context = current->audit_context;

 context->ipc.qbytes = qbytes;
 context->ipc.perm_uid = uid;
 context->ipc.perm_gid = gid;
 context->ipc.perm_mode = mode;
 context->ipc.has_perm = 1;
}

void __audit_bprm(struct linux_binprm *bprm)
{
 struct audit_context *context = current->audit_context;

 context->type = AUDIT_EXECVE;
 context->execve.argc = bprm->argc;
}
int __audit_socketcall(int nargs, unsigned long *args)
{
 struct audit_context *context = current->audit_context;

 if (nargs <= 0 || nargs > AUDITSC_ARGS || !args)
  return -EINVAL;
 context->type = AUDIT_SOCKETCALL;
 context->socketcall.nargs = nargs;
 memcpy(context->socketcall.args, args, nargs * sizeof(unsigned long));
 return 0;
}







void __audit_fd_pair(int fd1, int fd2)
{
 struct audit_context *context = current->audit_context;
 context->fds[0] = fd1;
 context->fds[1] = fd2;
}
int __audit_sockaddr(int len, void *a)
{
 struct audit_context *context = current->audit_context;

 if (!context->sockaddr) {
  void *p = kmalloc(sizeof(struct sockaddr_storage), GFP_KERNEL);
  if (!p)
   return -ENOMEM;
  context->sockaddr = p;
 }

 context->sockaddr_len = len;
 memcpy(context->sockaddr, a, len);
 return 0;
}

void __audit_ptrace(struct task_struct *t)
{
 struct audit_context *context = current->audit_context;

 context->target_pid = task_pid_nr(t);
 context->target_auid = audit_get_loginuid(t);
 context->target_uid = task_uid(t);
 context->target_sessionid = audit_get_sessionid(t);
 security_task_getsecid(t, &context->target_sid);
 memcpy(context->target_comm, t->comm, TASK_COMM_LEN);
}
int __audit_signal_info(int sig, struct task_struct *t)
{
 struct audit_aux_data_pids *axp;
 struct task_struct *tsk = current;
 struct audit_context *ctx = tsk->audit_context;
 kuid_t uid = current_uid(), t_uid = task_uid(t);

 if (audit_pid && t->tgid == audit_pid) {
  if (sig == SIGTERM || sig == SIGHUP || sig == SIGUSR1 || sig == SIGUSR2) {
   audit_sig_pid = task_pid_nr(tsk);
   if (uid_valid(tsk->loginuid))
    audit_sig_uid = tsk->loginuid;
   else
    audit_sig_uid = uid;
   security_task_getsecid(tsk, &audit_sig_sid);
  }
  if (!audit_signals || audit_dummy_context())
   return 0;
 }



 if (!ctx->target_pid) {
  ctx->target_pid = task_tgid_nr(t);
  ctx->target_auid = audit_get_loginuid(t);
  ctx->target_uid = t_uid;
  ctx->target_sessionid = audit_get_sessionid(t);
  security_task_getsecid(t, &ctx->target_sid);
  memcpy(ctx->target_comm, t->comm, TASK_COMM_LEN);
  return 0;
 }

 axp = (void *)ctx->aux_pids;
 if (!axp || axp->pid_count == AUDIT_AUX_PIDS) {
  axp = kzalloc(sizeof(*axp), GFP_ATOMIC);
  if (!axp)
   return -ENOMEM;

  axp->d.type = AUDIT_OBJ_PID;
  axp->d.next = ctx->aux_pids;
  ctx->aux_pids = (void *)axp;
 }
 BUG_ON(axp->pid_count >= AUDIT_AUX_PIDS);

 axp->target_pid[axp->pid_count] = task_tgid_nr(t);
 axp->target_auid[axp->pid_count] = audit_get_loginuid(t);
 axp->target_uid[axp->pid_count] = t_uid;
 axp->target_sessionid[axp->pid_count] = audit_get_sessionid(t);
 security_task_getsecid(t, &axp->target_sid[axp->pid_count]);
 memcpy(axp->target_comm[axp->pid_count], t->comm, TASK_COMM_LEN);
 axp->pid_count++;

 return 0;
}
int __audit_log_bprm_fcaps(struct linux_binprm *bprm,
      const struct cred *new, const struct cred *old)
{
 struct audit_aux_data_bprm_fcaps *ax;
 struct audit_context *context = current->audit_context;
 struct cpu_vfs_cap_data vcaps;

 ax = kmalloc(sizeof(*ax), GFP_KERNEL);
 if (!ax)
  return -ENOMEM;

 ax->d.type = AUDIT_BPRM_FCAPS;
 ax->d.next = context->aux;
 context->aux = (void *)ax;

 get_vfs_caps_from_disk(bprm->file->f_path.dentry, &vcaps);

 ax->fcap.permitted = vcaps.permitted;
 ax->fcap.inheritable = vcaps.inheritable;
 ax->fcap.fE = !!(vcaps.magic_etc & VFS_CAP_FLAGS_EFFECTIVE);
 ax->fcap_ver = (vcaps.magic_etc & VFS_CAP_REVISION_MASK) >> VFS_CAP_REVISION_SHIFT;

 ax->old_pcap.permitted = old->cap_permitted;
 ax->old_pcap.inheritable = old->cap_inheritable;
 ax->old_pcap.effective = old->cap_effective;

 ax->new_pcap.permitted = new->cap_permitted;
 ax->new_pcap.inheritable = new->cap_inheritable;
 ax->new_pcap.effective = new->cap_effective;
 return 0;
}
void __audit_log_capset(const struct cred *new, const struct cred *old)
{
 struct audit_context *context = current->audit_context;
 context->capset.pid = task_pid_nr(current);
 context->capset.cap.effective = new->cap_effective;
 context->capset.cap.inheritable = new->cap_effective;
 context->capset.cap.permitted = new->cap_permitted;
 context->type = AUDIT_CAPSET;
}

void __audit_mmap_fd(int fd, int flags)
{
 struct audit_context *context = current->audit_context;
 context->mmap.fd = fd;
 context->mmap.flags = flags;
 context->type = AUDIT_MMAP;
}

static void audit_log_task(struct audit_buffer *ab)
{
 kuid_t auid, uid;
 kgid_t gid;
 unsigned int sessionid;
 char comm[sizeof(current->comm)];

 auid = audit_get_loginuid(current);
 sessionid = audit_get_sessionid(current);
 current_uid_gid(&uid, &gid);

 audit_log_format(ab, "auid=%u uid=%u gid=%u ses=%u",
    from_kuid(&init_user_ns, auid),
    from_kuid(&init_user_ns, uid),
    from_kgid(&init_user_ns, gid),
    sessionid);
 audit_log_task_context(ab);
 audit_log_format(ab, " pid=%d comm=", task_pid_nr(current));
 audit_log_untrustedstring(ab, get_task_comm(comm, current));
 audit_log_d_path_exe(ab, current->mm);
}
void audit_core_dumps(long signr)
{
 struct audit_buffer *ab;

 if (!audit_enabled)
  return;

 if (signr == SIGQUIT)
  return;

 ab = audit_log_start(NULL, GFP_KERNEL, AUDIT_ANOM_ABEND);
 if (unlikely(!ab))
  return;
 audit_log_task(ab);
 audit_log_format(ab, " sig=%ld", signr);
 audit_log_end(ab);
}

void __audit_seccomp(unsigned long syscall, long signr, int code)
{
 struct audit_buffer *ab;

 ab = audit_log_start(NULL, GFP_KERNEL, AUDIT_SECCOMP);
 if (unlikely(!ab))
  return;
 audit_log_task(ab);
 audit_log_format(ab, " sig=%ld arch=%x syscall=%ld compat=%d ip=0x%lx code=0x%x",
    signr, syscall_get_arch(), syscall,
    in_compat_syscall(), KSTK_EIP(current), code);
 audit_log_end(ab);
}

struct list_head *audit_killed_trees(void)
{
 struct audit_context *ctx = current->audit_context;
 if (likely(!ctx || !ctx->in_syscall))
  return NULL;
 return &ctx->killed_trees;
}

struct audit_tree;
struct audit_chunk;

struct audit_tree {
 atomic_t count;
 int goner;
 struct audit_chunk *root;
 struct list_head chunks;
 struct list_head rules;
 struct list_head list;
 struct list_head same_root;
 struct rcu_head head;
 char pathname[];
};

struct audit_chunk {
 struct list_head hash;
 struct fsnotify_mark mark;
 struct list_head trees;
 int dead;
 int count;
 atomic_long_t refs;
 struct rcu_head head;
 struct node {
  struct list_head list;
  struct audit_tree *owner;
  unsigned index;
 } owners[];
};

static LIST_HEAD(tree_list);
static LIST_HEAD(prune_list);
static struct task_struct *prune_thread;
static struct fsnotify_group *audit_tree_group;

static struct audit_tree *alloc_tree(const char *s)
{
 struct audit_tree *tree;

 tree = kmalloc(sizeof(struct audit_tree) + strlen(s) + 1, GFP_KERNEL);
 if (tree) {
  atomic_set(&tree->count, 1);
  tree->goner = 0;
  INIT_LIST_HEAD(&tree->chunks);
  INIT_LIST_HEAD(&tree->rules);
  INIT_LIST_HEAD(&tree->list);
  INIT_LIST_HEAD(&tree->same_root);
  tree->root = NULL;
  strcpy(tree->pathname, s);
 }
 return tree;
}

static inline void get_tree(struct audit_tree *tree)
{
 atomic_inc(&tree->count);
}

static inline void put_tree(struct audit_tree *tree)
{
 if (atomic_dec_and_test(&tree->count))
  kfree_rcu(tree, head);
}


const char *audit_tree_path(struct audit_tree *tree)
{
 return tree->pathname;
}

static void free_chunk(struct audit_chunk *chunk)
{
 int i;

 for (i = 0; i < chunk->count; i++) {
  if (chunk->owners[i].owner)
   put_tree(chunk->owners[i].owner);
 }
 kfree(chunk);
}

void audit_put_chunk(struct audit_chunk *chunk)
{
 if (atomic_long_dec_and_test(&chunk->refs))
  free_chunk(chunk);
}

static void __put_chunk(struct rcu_head *rcu)
{
 struct audit_chunk *chunk = container_of(rcu, struct audit_chunk, head);
 audit_put_chunk(chunk);
}

static void audit_tree_destroy_watch(struct fsnotify_mark *entry)
{
 struct audit_chunk *chunk = container_of(entry, struct audit_chunk, mark);
 call_rcu(&chunk->head, __put_chunk);
}

static struct audit_chunk *alloc_chunk(int count)
{
 struct audit_chunk *chunk;
 size_t size;
 int i;

 size = offsetof(struct audit_chunk, owners) + count * sizeof(struct node);
 chunk = kzalloc(size, GFP_KERNEL);
 if (!chunk)
  return NULL;

 INIT_LIST_HEAD(&chunk->hash);
 INIT_LIST_HEAD(&chunk->trees);
 chunk->count = count;
 atomic_long_set(&chunk->refs, 1);
 for (i = 0; i < count; i++) {
  INIT_LIST_HEAD(&chunk->owners[i].list);
  chunk->owners[i].index = i;
 }
 fsnotify_init_mark(&chunk->mark, audit_tree_destroy_watch);
 chunk->mark.mask = FS_IN_IGNORED;
 return chunk;
}

enum {HASH_SIZE = 128};
static struct list_head chunk_hash_heads[HASH_SIZE];
static __cacheline_aligned_in_smp DEFINE_SPINLOCK(hash_lock);

static inline struct list_head *chunk_hash(const struct inode *inode)
{
 unsigned long n = (unsigned long)inode / L1_CACHE_BYTES;
 return chunk_hash_heads + n % HASH_SIZE;
}


static void insert_hash(struct audit_chunk *chunk)
{
 struct fsnotify_mark *entry = &chunk->mark;
 struct list_head *list;

 if (!entry->inode)
  return;
 list = chunk_hash(entry->inode);
 list_add_rcu(&chunk->hash, list);
}


struct audit_chunk *audit_tree_lookup(const struct inode *inode)
{
 struct list_head *list = chunk_hash(inode);
 struct audit_chunk *p;

 list_for_each_entry_rcu(p, list, hash) {

  if (p->mark.inode == inode) {
   atomic_long_inc(&p->refs);
   return p;
  }
 }
 return NULL;
}

bool audit_tree_match(struct audit_chunk *chunk, struct audit_tree *tree)
{
 int n;
 for (n = 0; n < chunk->count; n++)
  if (chunk->owners[n].owner == tree)
   return true;
 return false;
}



static struct audit_chunk *find_chunk(struct node *p)
{
 int index = p->index & ~(1U<<31);
 p -= index;
 return container_of(p, struct audit_chunk, owners[0]);
}

static void untag_chunk(struct node *p)
{
 struct audit_chunk *chunk = find_chunk(p);
 struct fsnotify_mark *entry = &chunk->mark;
 struct audit_chunk *new = NULL;
 struct audit_tree *owner;
 int size = chunk->count - 1;
 int i, j;

 fsnotify_get_mark(entry);

 spin_unlock(&hash_lock);

 if (size)
  new = alloc_chunk(size);

 spin_lock(&entry->lock);
 if (chunk->dead || !entry->inode) {
  spin_unlock(&entry->lock);
  if (new)
   free_chunk(new);
  goto out;
 }

 owner = p->owner;

 if (!size) {
  chunk->dead = 1;
  spin_lock(&hash_lock);
  list_del_init(&chunk->trees);
  if (owner->root == chunk)
   owner->root = NULL;
  list_del_init(&p->list);
  list_del_rcu(&chunk->hash);
  spin_unlock(&hash_lock);
  spin_unlock(&entry->lock);
  fsnotify_destroy_mark(entry, audit_tree_group);
  goto out;
 }

 if (!new)
  goto Fallback;

 fsnotify_duplicate_mark(&new->mark, entry);
 if (fsnotify_add_mark(&new->mark, new->mark.group, new->mark.inode, NULL, 1)) {
  fsnotify_put_mark(&new->mark);
  goto Fallback;
 }

 chunk->dead = 1;
 spin_lock(&hash_lock);
 list_replace_init(&chunk->trees, &new->trees);
 if (owner->root == chunk) {
  list_del_init(&owner->same_root);
  owner->root = NULL;
 }

 for (i = j = 0; j <= size; i++, j++) {
  struct audit_tree *s;
  if (&chunk->owners[j] == p) {
   list_del_init(&p->list);
   i--;
   continue;
  }
  s = chunk->owners[j].owner;
  new->owners[i].owner = s;
  new->owners[i].index = chunk->owners[j].index - j + i;
  if (!s)
   continue;
  get_tree(s);
  list_replace_init(&chunk->owners[j].list, &new->owners[i].list);
 }

 list_replace_rcu(&chunk->hash, &new->hash);
 list_for_each_entry(owner, &new->trees, same_root)
  owner->root = new;
 spin_unlock(&hash_lock);
 spin_unlock(&entry->lock);
 fsnotify_destroy_mark(entry, audit_tree_group);
 fsnotify_put_mark(&new->mark);
 goto out;

Fallback:

 spin_lock(&hash_lock);
 if (owner->root == chunk) {
  list_del_init(&owner->same_root);
  owner->root = NULL;
 }
 list_del_init(&p->list);
 p->owner = NULL;
 put_tree(owner);
 spin_unlock(&hash_lock);
 spin_unlock(&entry->lock);
out:
 fsnotify_put_mark(entry);
 spin_lock(&hash_lock);
}

static int create_chunk(struct inode *inode, struct audit_tree *tree)
{
 struct fsnotify_mark *entry;
 struct audit_chunk *chunk = alloc_chunk(1);
 if (!chunk)
  return -ENOMEM;

 entry = &chunk->mark;
 if (fsnotify_add_mark(entry, audit_tree_group, inode, NULL, 0)) {
  fsnotify_put_mark(entry);
  return -ENOSPC;
 }

 spin_lock(&entry->lock);
 spin_lock(&hash_lock);
 if (tree->goner) {
  spin_unlock(&hash_lock);
  chunk->dead = 1;
  spin_unlock(&entry->lock);
  fsnotify_destroy_mark(entry, audit_tree_group);
  fsnotify_put_mark(entry);
  return 0;
 }
 chunk->owners[0].index = (1U << 31);
 chunk->owners[0].owner = tree;
 get_tree(tree);
 list_add(&chunk->owners[0].list, &tree->chunks);
 if (!tree->root) {
  tree->root = chunk;
  list_add(&tree->same_root, &chunk->trees);
 }
 insert_hash(chunk);
 spin_unlock(&hash_lock);
 spin_unlock(&entry->lock);
 fsnotify_put_mark(entry);
 return 0;
}


static int tag_chunk(struct inode *inode, struct audit_tree *tree)
{
 struct fsnotify_mark *old_entry, *chunk_entry;
 struct audit_tree *owner;
 struct audit_chunk *chunk, *old;
 struct node *p;
 int n;

 old_entry = fsnotify_find_inode_mark(audit_tree_group, inode);
 if (!old_entry)
  return create_chunk(inode, tree);

 old = container_of(old_entry, struct audit_chunk, mark);


 spin_lock(&hash_lock);
 for (n = 0; n < old->count; n++) {
  if (old->owners[n].owner == tree) {
   spin_unlock(&hash_lock);
   fsnotify_put_mark(old_entry);
   return 0;
  }
 }
 spin_unlock(&hash_lock);

 chunk = alloc_chunk(old->count + 1);
 if (!chunk) {
  fsnotify_put_mark(old_entry);
  return -ENOMEM;
 }

 chunk_entry = &chunk->mark;

 spin_lock(&old_entry->lock);
 if (!old_entry->inode) {

  spin_unlock(&old_entry->lock);
  fsnotify_put_mark(old_entry);
  free_chunk(chunk);
  return -ENOENT;
 }

 fsnotify_duplicate_mark(chunk_entry, old_entry);
 if (fsnotify_add_mark(chunk_entry, chunk_entry->group, chunk_entry->inode, NULL, 1)) {
  spin_unlock(&old_entry->lock);
  fsnotify_put_mark(chunk_entry);
  fsnotify_put_mark(old_entry);
  return -ENOSPC;
 }


 spin_lock(&chunk_entry->lock);
 spin_lock(&hash_lock);


 if (tree->goner) {
  spin_unlock(&hash_lock);
  chunk->dead = 1;
  spin_unlock(&chunk_entry->lock);
  spin_unlock(&old_entry->lock);

  fsnotify_destroy_mark(chunk_entry, audit_tree_group);

  fsnotify_put_mark(chunk_entry);
  fsnotify_put_mark(old_entry);
  return 0;
 }
 list_replace_init(&old->trees, &chunk->trees);
 for (n = 0, p = chunk->owners; n < old->count; n++, p++) {
  struct audit_tree *s = old->owners[n].owner;
  p->owner = s;
  p->index = old->owners[n].index;
  if (!s)
   continue;
  get_tree(s);
  list_replace_init(&old->owners[n].list, &p->list);
 }
 p->index = (chunk->count - 1) | (1U<<31);
 p->owner = tree;
 get_tree(tree);
 list_add(&p->list, &tree->chunks);
 list_replace_rcu(&old->hash, &chunk->hash);
 list_for_each_entry(owner, &chunk->trees, same_root)
  owner->root = chunk;
 old->dead = 1;
 if (!tree->root) {
  tree->root = chunk;
  list_add(&tree->same_root, &chunk->trees);
 }
 spin_unlock(&hash_lock);
 spin_unlock(&chunk_entry->lock);
 spin_unlock(&old_entry->lock);
 fsnotify_destroy_mark(old_entry, audit_tree_group);
 fsnotify_put_mark(chunk_entry);
 fsnotify_put_mark(old_entry);
 return 0;
}

static void audit_tree_log_remove_rule(struct audit_krule *rule)
{
 struct audit_buffer *ab;

 ab = audit_log_start(NULL, GFP_KERNEL, AUDIT_CONFIG_CHANGE);
 if (unlikely(!ab))
  return;
 audit_log_format(ab, "op=");
 audit_log_string(ab, "remove_rule");
 audit_log_format(ab, " dir=");
 audit_log_untrustedstring(ab, rule->tree->pathname);
 audit_log_key(ab, rule->filterkey);
 audit_log_format(ab, " list=%d res=1", rule->listnr);
 audit_log_end(ab);
}

static void kill_rules(struct audit_tree *tree)
{
 struct audit_krule *rule, *next;
 struct audit_entry *entry;

 list_for_each_entry_safe(rule, next, &tree->rules, rlist) {
  entry = container_of(rule, struct audit_entry, rule);

  list_del_init(&rule->rlist);
  if (rule->tree) {

   audit_tree_log_remove_rule(rule);
   if (entry->rule.exe)
    audit_remove_mark(entry->rule.exe);
   rule->tree = NULL;
   list_del_rcu(&entry->list);
   list_del(&entry->rule.list);
   call_rcu(&entry->rcu, audit_free_rule_rcu);
  }
 }
}




static void prune_one(struct audit_tree *victim)
{
 spin_lock(&hash_lock);
 while (!list_empty(&victim->chunks)) {
  struct node *p;

  p = list_entry(victim->chunks.next, struct node, list);

  untag_chunk(p);
 }
 spin_unlock(&hash_lock);
 put_tree(victim);
}



static void trim_marked(struct audit_tree *tree)
{
 struct list_head *p, *q;
 spin_lock(&hash_lock);
 if (tree->goner) {
  spin_unlock(&hash_lock);
  return;
 }

 for (p = tree->chunks.next; p != &tree->chunks; p = q) {
  struct node *node = list_entry(p, struct node, list);
  q = p->next;
  if (node->index & (1U<<31)) {
   list_del_init(p);
   list_add(p, &tree->chunks);
  }
 }

 while (!list_empty(&tree->chunks)) {
  struct node *node;

  node = list_entry(tree->chunks.next, struct node, list);


  if (!(node->index & (1U<<31)))
   break;

  untag_chunk(node);
 }
 if (!tree->root && !tree->goner) {
  tree->goner = 1;
  spin_unlock(&hash_lock);
  mutex_lock(&audit_filter_mutex);
  kill_rules(tree);
  list_del_init(&tree->list);
  mutex_unlock(&audit_filter_mutex);
  prune_one(tree);
 } else {
  spin_unlock(&hash_lock);
 }
}

static void audit_schedule_prune(void);


int audit_remove_tree_rule(struct audit_krule *rule)
{
 struct audit_tree *tree;
 tree = rule->tree;
 if (tree) {
  spin_lock(&hash_lock);
  list_del_init(&rule->rlist);
  if (list_empty(&tree->rules) && !tree->goner) {
   tree->root = NULL;
   list_del_init(&tree->same_root);
   tree->goner = 1;
   list_move(&tree->list, &prune_list);
   rule->tree = NULL;
   spin_unlock(&hash_lock);
   audit_schedule_prune();
   return 1;
  }
  rule->tree = NULL;
  spin_unlock(&hash_lock);
  return 1;
 }
 return 0;
}

static int compare_root(struct vfsmount *mnt, void *arg)
{
 return d_backing_inode(mnt->mnt_root) == arg;
}

void audit_trim_trees(void)
{
 struct list_head cursor;

 mutex_lock(&audit_filter_mutex);
 list_add(&cursor, &tree_list);
 while (cursor.next != &tree_list) {
  struct audit_tree *tree;
  struct path path;
  struct vfsmount *root_mnt;
  struct node *node;
  int err;

  tree = container_of(cursor.next, struct audit_tree, list);
  get_tree(tree);
  list_del(&cursor);
  list_add(&cursor, &tree->list);
  mutex_unlock(&audit_filter_mutex);

  err = kern_path(tree->pathname, 0, &path);
  if (err)
   goto skip_it;

  root_mnt = collect_mounts(&path);
  path_put(&path);
  if (IS_ERR(root_mnt))
   goto skip_it;

  spin_lock(&hash_lock);
  list_for_each_entry(node, &tree->chunks, list) {
   struct audit_chunk *chunk = find_chunk(node);

   struct inode *inode = chunk->mark.inode;
   node->index |= 1U<<31;
   if (iterate_mounts(compare_root, inode, root_mnt))
    node->index &= ~(1U<<31);
  }
  spin_unlock(&hash_lock);
  trim_marked(tree);
  drop_collected_mounts(root_mnt);
skip_it:
  put_tree(tree);
  mutex_lock(&audit_filter_mutex);
 }
 list_del(&cursor);
 mutex_unlock(&audit_filter_mutex);
}

int audit_make_tree(struct audit_krule *rule, char *pathname, u32 op)
{

 if (pathname[0] != '/' ||
     rule->listnr != AUDIT_FILTER_EXIT ||
     op != Audit_equal ||
     rule->inode_f || rule->watch || rule->tree)
  return -EINVAL;
 rule->tree = alloc_tree(pathname);
 if (!rule->tree)
  return -ENOMEM;
 return 0;
}

void audit_put_tree(struct audit_tree *tree)
{
 put_tree(tree);
}

static int tag_mount(struct vfsmount *mnt, void *arg)
{
 return tag_chunk(d_backing_inode(mnt->mnt_root), arg);
}





static int prune_tree_thread(void *unused)
{
 for (;;) {
  if (list_empty(&prune_list)) {
   set_current_state(TASK_INTERRUPTIBLE);
   schedule();
  }

  mutex_lock(&audit_cmd_mutex);
  mutex_lock(&audit_filter_mutex);

  while (!list_empty(&prune_list)) {
   struct audit_tree *victim;

   victim = list_entry(prune_list.next,
     struct audit_tree, list);
   list_del_init(&victim->list);

   mutex_unlock(&audit_filter_mutex);

   prune_one(victim);

   mutex_lock(&audit_filter_mutex);
  }

  mutex_unlock(&audit_filter_mutex);
  mutex_unlock(&audit_cmd_mutex);
 }
 return 0;
}

static int audit_launch_prune(void)
{
 if (prune_thread)
  return 0;
 prune_thread = kthread_run(prune_tree_thread, NULL,
    "audit_prune_tree");
 if (IS_ERR(prune_thread)) {
  pr_err("cannot start thread audit_prune_tree");
  prune_thread = NULL;
  return -ENOMEM;
 }
 return 0;
}


int audit_add_tree_rule(struct audit_krule *rule)
{
 struct audit_tree *seed = rule->tree, *tree;
 struct path path;
 struct vfsmount *mnt;
 int err;

 rule->tree = NULL;
 list_for_each_entry(tree, &tree_list, list) {
  if (!strcmp(seed->pathname, tree->pathname)) {
   put_tree(seed);
   rule->tree = tree;
   list_add(&rule->rlist, &tree->rules);
   return 0;
  }
 }
 tree = seed;
 list_add(&tree->list, &tree_list);
 list_add(&rule->rlist, &tree->rules);

 mutex_unlock(&audit_filter_mutex);

 if (unlikely(!prune_thread)) {
  err = audit_launch_prune();
  if (err)
   goto Err;
 }

 err = kern_path(tree->pathname, 0, &path);
 if (err)
  goto Err;
 mnt = collect_mounts(&path);
 path_put(&path);
 if (IS_ERR(mnt)) {
  err = PTR_ERR(mnt);
  goto Err;
 }

 get_tree(tree);
 err = iterate_mounts(tag_mount, tree, mnt);
 drop_collected_mounts(mnt);

 if (!err) {
  struct node *node;
  spin_lock(&hash_lock);
  list_for_each_entry(node, &tree->chunks, list)
   node->index &= ~(1U<<31);
  spin_unlock(&hash_lock);
 } else {
  trim_marked(tree);
  goto Err;
 }

 mutex_lock(&audit_filter_mutex);
 if (list_empty(&rule->rlist)) {
  put_tree(tree);
  return -ENOENT;
 }
 rule->tree = tree;
 put_tree(tree);

 return 0;
Err:
 mutex_lock(&audit_filter_mutex);
 list_del_init(&tree->list);
 list_del_init(&tree->rules);
 put_tree(tree);
 return err;
}

int audit_tag_tree(char *old, char *new)
{
 struct list_head cursor, barrier;
 int failed = 0;
 struct path path1, path2;
 struct vfsmount *tagged;
 int err;

 err = kern_path(new, 0, &path2);
 if (err)
  return err;
 tagged = collect_mounts(&path2);
 path_put(&path2);
 if (IS_ERR(tagged))
  return PTR_ERR(tagged);

 err = kern_path(old, 0, &path1);
 if (err) {
  drop_collected_mounts(tagged);
  return err;
 }

 mutex_lock(&audit_filter_mutex);
 list_add(&barrier, &tree_list);
 list_add(&cursor, &barrier);

 while (cursor.next != &tree_list) {
  struct audit_tree *tree;
  int good_one = 0;

  tree = container_of(cursor.next, struct audit_tree, list);
  get_tree(tree);
  list_del(&cursor);
  list_add(&cursor, &tree->list);
  mutex_unlock(&audit_filter_mutex);

  err = kern_path(tree->pathname, 0, &path2);
  if (!err) {
   good_one = path_is_under(&path1, &path2);
   path_put(&path2);
  }

  if (!good_one) {
   put_tree(tree);
   mutex_lock(&audit_filter_mutex);
   continue;
  }

  failed = iterate_mounts(tag_mount, tree, tagged);
  if (failed) {
   put_tree(tree);
   mutex_lock(&audit_filter_mutex);
   break;
  }

  mutex_lock(&audit_filter_mutex);
  spin_lock(&hash_lock);
  if (!tree->goner) {
   list_del(&tree->list);
   list_add(&tree->list, &tree_list);
  }
  spin_unlock(&hash_lock);
  put_tree(tree);
 }

 while (barrier.prev != &tree_list) {
  struct audit_tree *tree;

  tree = container_of(barrier.prev, struct audit_tree, list);
  get_tree(tree);
  list_del(&tree->list);
  list_add(&tree->list, &barrier);
  mutex_unlock(&audit_filter_mutex);

  if (!failed) {
   struct node *node;
   spin_lock(&hash_lock);
   list_for_each_entry(node, &tree->chunks, list)
    node->index &= ~(1U<<31);
   spin_unlock(&hash_lock);
  } else {
   trim_marked(tree);
  }

  put_tree(tree);
  mutex_lock(&audit_filter_mutex);
 }
 list_del(&barrier);
 list_del(&cursor);
 mutex_unlock(&audit_filter_mutex);
 path_put(&path1);
 drop_collected_mounts(tagged);
 return failed;
}


static void audit_schedule_prune(void)
{
 wake_up_process(prune_thread);
}





void audit_kill_trees(struct list_head *list)
{
 mutex_lock(&audit_cmd_mutex);
 mutex_lock(&audit_filter_mutex);

 while (!list_empty(list)) {
  struct audit_tree *victim;

  victim = list_entry(list->next, struct audit_tree, list);
  kill_rules(victim);
  list_del_init(&victim->list);

  mutex_unlock(&audit_filter_mutex);

  prune_one(victim);

  mutex_lock(&audit_filter_mutex);
 }

 mutex_unlock(&audit_filter_mutex);
 mutex_unlock(&audit_cmd_mutex);
}





static void evict_chunk(struct audit_chunk *chunk)
{
 struct audit_tree *owner;
 struct list_head *postponed = audit_killed_trees();
 int need_prune = 0;
 int n;

 if (chunk->dead)
  return;

 chunk->dead = 1;
 mutex_lock(&audit_filter_mutex);
 spin_lock(&hash_lock);
 while (!list_empty(&chunk->trees)) {
  owner = list_entry(chunk->trees.next,
       struct audit_tree, same_root);
  owner->goner = 1;
  owner->root = NULL;
  list_del_init(&owner->same_root);
  spin_unlock(&hash_lock);
  if (!postponed) {
   kill_rules(owner);
   list_move(&owner->list, &prune_list);
   need_prune = 1;
  } else {
   list_move(&owner->list, postponed);
  }
  spin_lock(&hash_lock);
 }
 list_del_rcu(&chunk->hash);
 for (n = 0; n < chunk->count; n++)
  list_del_init(&chunk->owners[n].list);
 spin_unlock(&hash_lock);
 mutex_unlock(&audit_filter_mutex);
 if (need_prune)
  audit_schedule_prune();
}

static int audit_tree_handle_event(struct fsnotify_group *group,
       struct inode *to_tell,
       struct fsnotify_mark *inode_mark,
       struct fsnotify_mark *vfsmount_mark,
       u32 mask, void *data, int data_type,
       const unsigned char *file_name, u32 cookie)
{
 return 0;
}

static void audit_tree_freeing_mark(struct fsnotify_mark *entry, struct fsnotify_group *group)
{
 struct audit_chunk *chunk = container_of(entry, struct audit_chunk, mark);

 evict_chunk(chunk);





 BUG_ON(atomic_read(&entry->refcnt) < 1);
}

static const struct fsnotify_ops audit_tree_ops = {
 .handle_event = audit_tree_handle_event,
 .freeing_mark = audit_tree_freeing_mark,
};

static int __init audit_tree_init(void)
{
 int i;

 audit_tree_group = fsnotify_alloc_group(&audit_tree_ops);
 if (IS_ERR(audit_tree_group))
  audit_panic("cannot initialize fsnotify group for rectree watches");

 for (i = 0; i < HASH_SIZE; i++)
  INIT_LIST_HEAD(&chunk_hash_heads[i]);

 return 0;
}
__initcall(audit_tree_init);
struct audit_watch {
 atomic_t count;
 dev_t dev;
 char *path;
 unsigned long ino;
 struct audit_parent *parent;
 struct list_head wlist;
 struct list_head rules;
};

struct audit_parent {
 struct list_head watches;
 struct fsnotify_mark mark;
};


static struct fsnotify_group *audit_watch_group;


   FS_MOVE_SELF | FS_EVENT_ON_CHILD)

static void audit_free_parent(struct audit_parent *parent)
{
 WARN_ON(!list_empty(&parent->watches));
 kfree(parent);
}

static void audit_watch_free_mark(struct fsnotify_mark *entry)
{
 struct audit_parent *parent;

 parent = container_of(entry, struct audit_parent, mark);
 audit_free_parent(parent);
}

static void audit_get_parent(struct audit_parent *parent)
{
 if (likely(parent))
  fsnotify_get_mark(&parent->mark);
}

static void audit_put_parent(struct audit_parent *parent)
{
 if (likely(parent))
  fsnotify_put_mark(&parent->mark);
}





static inline struct audit_parent *audit_find_parent(struct inode *inode)
{
 struct audit_parent *parent = NULL;
 struct fsnotify_mark *entry;

 entry = fsnotify_find_inode_mark(audit_watch_group, inode);
 if (entry)
  parent = container_of(entry, struct audit_parent, mark);

 return parent;
}

void audit_get_watch(struct audit_watch *watch)
{
 atomic_inc(&watch->count);
}

void audit_put_watch(struct audit_watch *watch)
{
 if (atomic_dec_and_test(&watch->count)) {
  WARN_ON(watch->parent);
  WARN_ON(!list_empty(&watch->rules));
  kfree(watch->path);
  kfree(watch);
 }
}

static void audit_remove_watch(struct audit_watch *watch)
{
 list_del(&watch->wlist);
 audit_put_parent(watch->parent);
 watch->parent = NULL;
 audit_put_watch(watch);
}

char *audit_watch_path(struct audit_watch *watch)
{
 return watch->path;
}

int audit_watch_compare(struct audit_watch *watch, unsigned long ino, dev_t dev)
{
 return (watch->ino != AUDIT_INO_UNSET) &&
  (watch->ino == ino) &&
  (watch->dev == dev);
}


static struct audit_parent *audit_init_parent(struct path *path)
{
 struct inode *inode = d_backing_inode(path->dentry);
 struct audit_parent *parent;
 int ret;

 parent = kzalloc(sizeof(*parent), GFP_KERNEL);
 if (unlikely(!parent))
  return ERR_PTR(-ENOMEM);

 INIT_LIST_HEAD(&parent->watches);

 fsnotify_init_mark(&parent->mark, audit_watch_free_mark);
 parent->mark.mask = AUDIT_FS_WATCH;
 ret = fsnotify_add_mark(&parent->mark, audit_watch_group, inode, NULL, 0);
 if (ret < 0) {
  audit_free_parent(parent);
  return ERR_PTR(ret);
 }

 return parent;
}


static struct audit_watch *audit_init_watch(char *path)
{
 struct audit_watch *watch;

 watch = kzalloc(sizeof(*watch), GFP_KERNEL);
 if (unlikely(!watch))
  return ERR_PTR(-ENOMEM);

 INIT_LIST_HEAD(&watch->rules);
 atomic_set(&watch->count, 1);
 watch->path = path;
 watch->dev = AUDIT_DEV_UNSET;
 watch->ino = AUDIT_INO_UNSET;

 return watch;
}


int audit_to_watch(struct audit_krule *krule, char *path, int len, u32 op)
{
 struct audit_watch *watch;

 if (!audit_watch_group)
  return -EOPNOTSUPP;

 if (path[0] != '/' || path[len-1] == '/' ||
     krule->listnr != AUDIT_FILTER_EXIT ||
     op != Audit_equal ||
     krule->inode_f || krule->watch || krule->tree)
  return -EINVAL;

 watch = audit_init_watch(path);
 if (IS_ERR(watch))
  return PTR_ERR(watch);

 krule->watch = watch;

 return 0;
}



static struct audit_watch *audit_dupe_watch(struct audit_watch *old)
{
 char *path;
 struct audit_watch *new;

 path = kstrdup(old->path, GFP_KERNEL);
 if (unlikely(!path))
  return ERR_PTR(-ENOMEM);

 new = audit_init_watch(path);
 if (IS_ERR(new)) {
  kfree(path);
  goto out;
 }

 new->dev = old->dev;
 new->ino = old->ino;
 audit_get_parent(old->parent);
 new->parent = old->parent;

out:
 return new;
}

static void audit_watch_log_rule_change(struct audit_krule *r, struct audit_watch *w, char *op)
{
 if (audit_enabled) {
  struct audit_buffer *ab;
  ab = audit_log_start(NULL, GFP_NOFS, AUDIT_CONFIG_CHANGE);
  if (unlikely(!ab))
   return;
  audit_log_format(ab, "auid=%u ses=%u op=",
     from_kuid(&init_user_ns, audit_get_loginuid(current)),
     audit_get_sessionid(current));
  audit_log_string(ab, op);
  audit_log_format(ab, " path=");
  audit_log_untrustedstring(ab, w->path);
  audit_log_key(ab, r->filterkey);
  audit_log_format(ab, " list=%d res=1", r->listnr);
  audit_log_end(ab);
 }
}


static void audit_update_watch(struct audit_parent *parent,
          const char *dname, dev_t dev,
          unsigned long ino, unsigned invalidating)
{
 struct audit_watch *owatch, *nwatch, *nextw;
 struct audit_krule *r, *nextr;
 struct audit_entry *oentry, *nentry;

 mutex_lock(&audit_filter_mutex);


 list_for_each_entry_safe(owatch, nextw, &parent->watches, wlist) {
  if (audit_compare_dname_path(dname, owatch->path,
          AUDIT_NAME_FULL))
   continue;



  if (invalidating && !audit_dummy_context())
   audit_filter_inodes(current, current->audit_context);



  nwatch = audit_dupe_watch(owatch);
  if (IS_ERR(nwatch)) {
   mutex_unlock(&audit_filter_mutex);
   audit_panic("error updating watch, skipping");
   return;
  }
  nwatch->dev = dev;
  nwatch->ino = ino;

  list_for_each_entry_safe(r, nextr, &owatch->rules, rlist) {

   oentry = container_of(r, struct audit_entry, rule);
   list_del(&oentry->rule.rlist);
   list_del_rcu(&oentry->list);

   nentry = audit_dupe_rule(&oentry->rule);
   if (IS_ERR(nentry)) {
    list_del(&oentry->rule.list);
    audit_panic("error updating watch, removing");
   } else {
    int h = audit_hash_ino((u32)ino);






    audit_put_watch(nentry->rule.watch);
    audit_get_watch(nwatch);
    nentry->rule.watch = nwatch;
    list_add(&nentry->rule.rlist, &nwatch->rules);
    list_add_rcu(&nentry->list, &audit_inode_hash[h]);
    list_replace(&oentry->rule.list,
          &nentry->rule.list);
   }
   if (oentry->rule.exe)
    audit_remove_mark(oentry->rule.exe);

   audit_watch_log_rule_change(r, owatch, "updated_rules");

   call_rcu(&oentry->rcu, audit_free_rule_rcu);
  }

  audit_remove_watch(owatch);
  goto add_watch_to_parent;
 }
 mutex_unlock(&audit_filter_mutex);
 return;

add_watch_to_parent:
 list_add(&nwatch->wlist, &parent->watches);
 mutex_unlock(&audit_filter_mutex);
 return;
}


static void audit_remove_parent_watches(struct audit_parent *parent)
{
 struct audit_watch *w, *nextw;
 struct audit_krule *r, *nextr;
 struct audit_entry *e;

 mutex_lock(&audit_filter_mutex);
 list_for_each_entry_safe(w, nextw, &parent->watches, wlist) {
  list_for_each_entry_safe(r, nextr, &w->rules, rlist) {
   e = container_of(r, struct audit_entry, rule);
   audit_watch_log_rule_change(r, w, "remove_rule");
   if (e->rule.exe)
    audit_remove_mark(e->rule.exe);
   list_del(&r->rlist);
   list_del(&r->list);
   list_del_rcu(&e->list);
   call_rcu(&e->rcu, audit_free_rule_rcu);
  }
  audit_remove_watch(w);
 }
 mutex_unlock(&audit_filter_mutex);

 fsnotify_destroy_mark(&parent->mark, audit_watch_group);
}


static int audit_get_nd(struct audit_watch *watch, struct path *parent)
{
 struct dentry *d = kern_path_locked(watch->path, parent);
 if (IS_ERR(d))
  return PTR_ERR(d);
 inode_unlock(d_backing_inode(parent->dentry));
 if (d_is_positive(d)) {

  watch->dev = d->d_sb->s_dev;
  watch->ino = d_backing_inode(d)->i_ino;
 }
 dput(d);
 return 0;
}



static void audit_add_to_parent(struct audit_krule *krule,
    struct audit_parent *parent)
{
 struct audit_watch *w, *watch = krule->watch;
 int watch_found = 0;

 BUG_ON(!mutex_is_locked(&audit_filter_mutex));

 list_for_each_entry(w, &parent->watches, wlist) {
  if (strcmp(watch->path, w->path))
   continue;

  watch_found = 1;


  audit_put_watch(watch);

  audit_get_watch(w);
  krule->watch = watch = w;

  audit_put_parent(parent);
  break;
 }

 if (!watch_found) {
  watch->parent = parent;

  audit_get_watch(watch);
  list_add(&watch->wlist, &parent->watches);
 }
 list_add(&krule->rlist, &watch->rules);
}



int audit_add_watch(struct audit_krule *krule, struct list_head **list)
{
 struct audit_watch *watch = krule->watch;
 struct audit_parent *parent;
 struct path parent_path;
 int h, ret = 0;

 mutex_unlock(&audit_filter_mutex);


 ret = audit_get_nd(watch, &parent_path);


 mutex_lock(&audit_filter_mutex);

 if (ret)
  return ret;


 parent = audit_find_parent(d_backing_inode(parent_path.dentry));
 if (!parent) {
  parent = audit_init_parent(&parent_path);
  if (IS_ERR(parent)) {
   ret = PTR_ERR(parent);
   goto error;
  }
 }

 audit_add_to_parent(krule, parent);

 h = audit_hash_ino((u32)watch->ino);
 *list = &audit_inode_hash[h];
error:
 path_put(&parent_path);
 return ret;
}

void audit_remove_watch_rule(struct audit_krule *krule)
{
 struct audit_watch *watch = krule->watch;
 struct audit_parent *parent = watch->parent;

 list_del(&krule->rlist);

 if (list_empty(&watch->rules)) {
  audit_remove_watch(watch);

  if (list_empty(&parent->watches)) {
   audit_get_parent(parent);
   fsnotify_destroy_mark(&parent->mark, audit_watch_group);
   audit_put_parent(parent);
  }
 }
}


static int audit_watch_handle_event(struct fsnotify_group *group,
        struct inode *to_tell,
        struct fsnotify_mark *inode_mark,
        struct fsnotify_mark *vfsmount_mark,
        u32 mask, void *data, int data_type,
        const unsigned char *dname, u32 cookie)
{
 struct inode *inode;
 struct audit_parent *parent;

 parent = container_of(inode_mark, struct audit_parent, mark);

 BUG_ON(group != audit_watch_group);

 switch (data_type) {
 case (FSNOTIFY_EVENT_PATH):
  inode = d_backing_inode(((struct path *)data)->dentry);
  break;
 case (FSNOTIFY_EVENT_INODE):
  inode = (struct inode *)data;
  break;
 default:
  BUG();
  inode = NULL;
  break;
 };

 if (mask & (FS_CREATE|FS_MOVED_TO) && inode)
  audit_update_watch(parent, dname, inode->i_sb->s_dev, inode->i_ino, 0);
 else if (mask & (FS_DELETE|FS_MOVED_FROM))
  audit_update_watch(parent, dname, AUDIT_DEV_UNSET, AUDIT_INO_UNSET, 1);
 else if (mask & (FS_DELETE_SELF|FS_UNMOUNT|FS_MOVE_SELF))
  audit_remove_parent_watches(parent);

 return 0;
}

static const struct fsnotify_ops audit_watch_fsnotify_ops = {
 .handle_event = audit_watch_handle_event,
};

static int __init audit_watch_init(void)
{
 audit_watch_group = fsnotify_alloc_group(&audit_watch_fsnotify_ops);
 if (IS_ERR(audit_watch_group)) {
  audit_watch_group = NULL;
  audit_panic("cannot create audit fsnotify group");
 }
 return 0;
}
device_initcall(audit_watch_init);

int audit_dupe_exe(struct audit_krule *new, struct audit_krule *old)
{
 struct audit_fsnotify_mark *audit_mark;
 char *pathname;

 pathname = kstrdup(audit_mark_path(old->exe), GFP_KERNEL);
 if (!pathname)
  return -ENOMEM;

 audit_mark = audit_alloc_mark(new, pathname, strlen(pathname));
 if (IS_ERR(audit_mark)) {
  kfree(pathname);
  return PTR_ERR(audit_mark);
 }
 new->exe = audit_mark;

 return 0;
}

int audit_exe_compare(struct task_struct *tsk, struct audit_fsnotify_mark *mark)
{
 struct file *exe_file;
 unsigned long ino;
 dev_t dev;

 rcu_read_lock();
 exe_file = rcu_dereference(tsk->mm->exe_file);
 ino = exe_file->f_inode->i_ino;
 dev = exe_file->f_inode->i_sb->s_dev;
 rcu_read_unlock();
 return audit_mark_compare(mark, ino, dev);
}







static DEFINE_MUTEX(probing_active);
unsigned long probe_irq_on(void)
{
 struct irq_desc *desc;
 unsigned long mask = 0;
 int i;




 async_synchronize_full();
 mutex_lock(&probing_active);




 for_each_irq_desc_reverse(i, desc) {
  raw_spin_lock_irq(&desc->lock);
  if (!desc->action && irq_settings_can_probe(desc)) {




   if (desc->irq_data.chip->irq_set_type)
    desc->irq_data.chip->irq_set_type(&desc->irq_data,
        IRQ_TYPE_PROBE);
   irq_startup(desc, false);
  }
  raw_spin_unlock_irq(&desc->lock);
 }


 msleep(20);






 for_each_irq_desc_reverse(i, desc) {
  raw_spin_lock_irq(&desc->lock);
  if (!desc->action && irq_settings_can_probe(desc)) {
   desc->istate |= IRQS_AUTODETECT | IRQS_WAITING;
   if (irq_startup(desc, false))
    desc->istate |= IRQS_PENDING;
  }
  raw_spin_unlock_irq(&desc->lock);
 }




 msleep(100);




 for_each_irq_desc(i, desc) {
  raw_spin_lock_irq(&desc->lock);

  if (desc->istate & IRQS_AUTODETECT) {

   if (!(desc->istate & IRQS_WAITING)) {
    desc->istate &= ~IRQS_AUTODETECT;
    irq_shutdown(desc);
   } else
    if (i < 32)
     mask |= 1 << i;
  }
  raw_spin_unlock_irq(&desc->lock);
 }

 return mask;
}
EXPORT_SYMBOL(probe_irq_on);
unsigned int probe_irq_mask(unsigned long val)
{
 unsigned int mask = 0;
 struct irq_desc *desc;
 int i;

 for_each_irq_desc(i, desc) {
  raw_spin_lock_irq(&desc->lock);
  if (desc->istate & IRQS_AUTODETECT) {
   if (i < 16 && !(desc->istate & IRQS_WAITING))
    mask |= 1 << i;

   desc->istate &= ~IRQS_AUTODETECT;
   irq_shutdown(desc);
  }
  raw_spin_unlock_irq(&desc->lock);
 }
 mutex_unlock(&probing_active);

 return mask & val;
}
EXPORT_SYMBOL(probe_irq_mask);
int probe_irq_off(unsigned long val)
{
 int i, irq_found = 0, nr_of_irqs = 0;
 struct irq_desc *desc;

 for_each_irq_desc(i, desc) {
  raw_spin_lock_irq(&desc->lock);

  if (desc->istate & IRQS_AUTODETECT) {
   if (!(desc->istate & IRQS_WAITING)) {
    if (!nr_of_irqs)
     irq_found = i;
    nr_of_irqs++;
   }
   desc->istate &= ~IRQS_AUTODETECT;
   irq_shutdown(desc);
  }
  raw_spin_unlock_irq(&desc->lock);
 }
 mutex_unlock(&probing_active);

 if (nr_of_irqs > 1)
  irq_found = -irq_found;

 return irq_found;
}
EXPORT_SYMBOL(probe_irq_off);


static suspend_state_t autosleep_state;
static struct workqueue_struct *autosleep_wq;






static DEFINE_MUTEX(autosleep_lock);
static struct wakeup_source *autosleep_ws;

static void try_to_suspend(struct work_struct *work)
{
 unsigned int initial_count, final_count;

 if (!pm_get_wakeup_count(&initial_count, true))
  goto out;

 mutex_lock(&autosleep_lock);

 if (!pm_save_wakeup_count(initial_count) ||
  system_state != SYSTEM_RUNNING) {
  mutex_unlock(&autosleep_lock);
  goto out;
 }

 if (autosleep_state == PM_SUSPEND_ON) {
  mutex_unlock(&autosleep_lock);
  return;
 }
 if (autosleep_state >= PM_SUSPEND_MAX)
  hibernate();
 else
  pm_suspend(autosleep_state);

 mutex_unlock(&autosleep_lock);

 if (!pm_get_wakeup_count(&final_count, false))
  goto out;





 if (final_count == initial_count)
  schedule_timeout_uninterruptible(HZ / 2);

 out:
 queue_up_suspend_work();
}

static DECLARE_WORK(suspend_work, try_to_suspend);

void queue_up_suspend_work(void)
{
 if (autosleep_state > PM_SUSPEND_ON)
  queue_work(autosleep_wq, &suspend_work);
}

suspend_state_t pm_autosleep_state(void)
{
 return autosleep_state;
}

int pm_autosleep_lock(void)
{
 return mutex_lock_interruptible(&autosleep_lock);
}

void pm_autosleep_unlock(void)
{
 mutex_unlock(&autosleep_lock);
}

int pm_autosleep_set_state(suspend_state_t state)
{

 if (state >= PM_SUSPEND_MAX)
  return -EINVAL;

 __pm_stay_awake(autosleep_ws);

 mutex_lock(&autosleep_lock);

 autosleep_state = state;

 __pm_relax(autosleep_ws);

 if (state > PM_SUSPEND_ON) {
  pm_wakep_autosleep_enabled(true);
  queue_up_suspend_work();
 } else {
  pm_wakep_autosleep_enabled(false);
 }

 mutex_unlock(&autosleep_lock);
 return 0;
}

int __init pm_autosleep_init(void)
{
 autosleep_ws = wakeup_source_register("autosleep");
 if (!autosleep_ws)
  return -ENOMEM;

 autosleep_wq = alloc_ordered_workqueue("autosleep", 0);
 if (autosleep_wq)
  return 0;

 wakeup_source_unregister(autosleep_ws);
 return -ENOMEM;
}

static void backtrace_test_normal(void)
{
 pr_info("Testing a backtrace from process context.\n");
 pr_info("The following trace is a kernel self test and not a bug!\n");

 dump_stack();
}

static DECLARE_COMPLETION(backtrace_work);

static void backtrace_test_irq_callback(unsigned long data)
{
 dump_stack();
 complete(&backtrace_work);
}

static DECLARE_TASKLET(backtrace_tasklet, &backtrace_test_irq_callback, 0);

static void backtrace_test_irq(void)
{
 pr_info("Testing a backtrace from irq context.\n");
 pr_info("The following trace is a kernel self test and not a bug!\n");

 init_completion(&backtrace_work);
 tasklet_schedule(&backtrace_tasklet);
 wait_for_completion(&backtrace_work);
}

static void backtrace_test_saved(void)
{
 struct stack_trace trace;
 unsigned long entries[8];

 pr_info("Testing a saved backtrace.\n");
 pr_info("The following trace is a kernel self test and not a bug!\n");

 trace.nr_entries = 0;
 trace.max_entries = ARRAY_SIZE(entries);
 trace.entries = entries;
 trace.skip = 0;

 save_stack_trace(&trace);
 print_stack_trace(&trace, 0);
}
static void backtrace_test_saved(void)
{
 pr_info("Saved backtrace test skipped.\n");
}

static int backtrace_regression_test(void)
{
 pr_info("====[ backtrace testing ]===========\n");

 backtrace_test_normal();
 backtrace_test_irq();
 backtrace_test_saved();

 pr_info("====[ end of backtrace testing ]====\n");
 return 0;
}

static void exitf(void)
{
}

module_init(backtrace_regression_test);
module_exit(exitf);
MODULE_LICENSE("GPL");
MODULE_AUTHOR("Arjan van de Ven <arjan@linux.intel.com>");








void foo(void)
{

 DEFINE(NR_PAGEFLAGS, __NR_PAGEFLAGS);
 DEFINE(MAX_NR_ZONES, __MAX_NR_ZONES);
 DEFINE(NR_CPUS_BITS, ilog2(CONFIG_NR_CPUS));
 DEFINE(SPINLOCK_SIZE, sizeof(spinlock_t));

}



char *_braille_console_setup(char **str, char **brl_options)
{
 if (!memcmp(*str, "brl,", 4)) {
  *brl_options = "";
  *str += 4;
 } else if (!memcmp(str, "brl=", 4)) {
  *brl_options = *str + 4;
  *str = strchr(*brl_options, ',');
  if (!*str)
   pr_err("need port name after brl=\n");
  else
   *((*str)++) = 0;
 } else
  return NULL;

 return *str;
}

int
_braille_register_console(struct console *console, struct console_cmdline *c)
{
 int rtn = 0;

 if (c->brl_options) {
  console->flags |= CON_BRL;
  rtn = braille_register_console(console, c->index, c->options,
            c->brl_options);
 }

 return rtn;
}

int
_braille_unregister_console(struct console *console)
{
 if (console->flags & CON_BRL)
  return braille_unregister_console(console);

 return 0;
}

struct callchain_cpus_entries {
 struct rcu_head rcu_head;
 struct perf_callchain_entry *cpu_entries[0];
};

int sysctl_perf_event_max_stack __read_mostly = PERF_MAX_STACK_DEPTH;
int sysctl_perf_event_max_contexts_per_stack __read_mostly = PERF_MAX_CONTEXTS_PER_STACK;

static inline size_t perf_callchain_entry__sizeof(void)
{
 return (sizeof(struct perf_callchain_entry) +
  sizeof(__u64) * (sysctl_perf_event_max_stack +
     sysctl_perf_event_max_contexts_per_stack));
}

static DEFINE_PER_CPU(int, callchain_recursion[PERF_NR_CONTEXTS]);
static atomic_t nr_callchain_events;
static DEFINE_MUTEX(callchain_mutex);
static struct callchain_cpus_entries *callchain_cpus_entries;


__weak void perf_callchain_kernel(struct perf_callchain_entry_ctx *entry,
      struct pt_regs *regs)
{
}

__weak void perf_callchain_user(struct perf_callchain_entry_ctx *entry,
    struct pt_regs *regs)
{
}

static void release_callchain_buffers_rcu(struct rcu_head *head)
{
 struct callchain_cpus_entries *entries;
 int cpu;

 entries = container_of(head, struct callchain_cpus_entries, rcu_head);

 for_each_possible_cpu(cpu)
  kfree(entries->cpu_entries[cpu]);

 kfree(entries);
}

static void release_callchain_buffers(void)
{
 struct callchain_cpus_entries *entries;

 entries = callchain_cpus_entries;
 RCU_INIT_POINTER(callchain_cpus_entries, NULL);
 call_rcu(&entries->rcu_head, release_callchain_buffers_rcu);
}

static int alloc_callchain_buffers(void)
{
 int cpu;
 int size;
 struct callchain_cpus_entries *entries;






 size = offsetof(struct callchain_cpus_entries, cpu_entries[nr_cpu_ids]);

 entries = kzalloc(size, GFP_KERNEL);
 if (!entries)
  return -ENOMEM;

 size = perf_callchain_entry__sizeof() * PERF_NR_CONTEXTS;

 for_each_possible_cpu(cpu) {
  entries->cpu_entries[cpu] = kmalloc_node(size, GFP_KERNEL,
        cpu_to_node(cpu));
  if (!entries->cpu_entries[cpu])
   goto fail;
 }

 rcu_assign_pointer(callchain_cpus_entries, entries);

 return 0;

fail:
 for_each_possible_cpu(cpu)
  kfree(entries->cpu_entries[cpu]);
 kfree(entries);

 return -ENOMEM;
}

int get_callchain_buffers(void)
{
 int err = 0;
 int count;

 mutex_lock(&callchain_mutex);

 count = atomic_inc_return(&nr_callchain_events);
 if (WARN_ON_ONCE(count < 1)) {
  err = -EINVAL;
  goto exit;
 }

 if (count > 1) {

  if (!callchain_cpus_entries)
   err = -ENOMEM;
  goto exit;
 }

 err = alloc_callchain_buffers();
exit:
 if (err)
  atomic_dec(&nr_callchain_events);

 mutex_unlock(&callchain_mutex);

 return err;
}

void put_callchain_buffers(void)
{
 if (atomic_dec_and_mutex_lock(&nr_callchain_events, &callchain_mutex)) {
  release_callchain_buffers();
  mutex_unlock(&callchain_mutex);
 }
}

static struct perf_callchain_entry *get_callchain_entry(int *rctx)
{
 int cpu;
 struct callchain_cpus_entries *entries;

 *rctx = get_recursion_context(this_cpu_ptr(callchain_recursion));
 if (*rctx == -1)
  return NULL;

 entries = rcu_dereference(callchain_cpus_entries);
 if (!entries)
  return NULL;

 cpu = smp_processor_id();

 return (((void *)entries->cpu_entries[cpu]) +
  (*rctx * perf_callchain_entry__sizeof()));
}

static void
put_callchain_entry(int rctx)
{
 put_recursion_context(this_cpu_ptr(callchain_recursion), rctx);
}

struct perf_callchain_entry *
perf_callchain(struct perf_event *event, struct pt_regs *regs)
{
 bool kernel = !event->attr.exclude_callchain_kernel;
 bool user = !event->attr.exclude_callchain_user;

 bool crosstask = event->ctx->task && event->ctx->task != current;

 if (!kernel && !user)
  return NULL;

 return get_perf_callchain(regs, 0, kernel, user, sysctl_perf_event_max_stack, crosstask, true);
}

struct perf_callchain_entry *
get_perf_callchain(struct pt_regs *regs, u32 init_nr, bool kernel, bool user,
     u32 max_stack, bool crosstask, bool add_mark)
{
 struct perf_callchain_entry *entry;
 struct perf_callchain_entry_ctx ctx;
 int rctx;

 entry = get_callchain_entry(&rctx);
 if (rctx == -1)
  return NULL;

 if (!entry)
  goto exit_put;

 ctx.entry = entry;
 ctx.max_stack = max_stack;
 ctx.nr = entry->nr = init_nr;
 ctx.contexts = 0;
 ctx.contexts_maxed = false;

 if (kernel && !user_mode(regs)) {
  if (add_mark)
   perf_callchain_store_context(&ctx, PERF_CONTEXT_KERNEL);
  perf_callchain_kernel(&ctx, regs);
 }

 if (user) {
  if (!user_mode(regs)) {
   if (current->mm)
    regs = task_pt_regs(current);
   else
    regs = NULL;
  }

  if (regs) {
   if (crosstask)
    goto exit_put;

   if (add_mark)
    perf_callchain_store_context(&ctx, PERF_CONTEXT_USER);
   perf_callchain_user(&ctx, regs);
  }
 }

exit_put:
 put_callchain_entry(rctx);

 return entry;
}





int perf_event_max_stack_handler(struct ctl_table *table, int write,
     void __user *buffer, size_t *lenp, loff_t *ppos)
{
 int *value = table->data;
 int new_value = *value, ret;
 struct ctl_table new_table = *table;

 new_table.data = &new_value;
 ret = proc_dointvec_minmax(&new_table, write, buffer, lenp, ppos);
 if (ret || !write)
  return ret;

 mutex_lock(&callchain_mutex);
 if (atomic_read(&nr_callchain_events))
  ret = -EBUSY;
 else
  *value = new_value;

 mutex_unlock(&callchain_mutex);

 return ret;
}






const kernel_cap_t __cap_empty_set = CAP_EMPTY_SET;
EXPORT_SYMBOL(__cap_empty_set);

int file_caps_enabled = 1;

static int __init file_caps_disable(char *str)
{
 file_caps_enabled = 0;
 return 1;
}
__setup("no_file_caps", file_caps_disable);







static void warn_legacy_capability_use(void)
{
 char name[sizeof(current->comm)];

 pr_info_once("warning: `%s' uses 32-bit capabilities (legacy support in use)\n",
       get_task_comm(name, current));
}
static void warn_deprecated_v2(void)
{
 char name[sizeof(current->comm)];

 pr_info_once("warning: `%s' uses deprecated v2 capabilities in a way that may be insecure\n",
       get_task_comm(name, current));
}





static int cap_validate_magic(cap_user_header_t header, unsigned *tocopy)
{
 __u32 version;

 if (get_user(version, &header->version))
  return -EFAULT;

 switch (version) {
 case _LINUX_CAPABILITY_VERSION_1:
  warn_legacy_capability_use();
  *tocopy = _LINUX_CAPABILITY_U32S_1;
  break;
 case _LINUX_CAPABILITY_VERSION_2:
  warn_deprecated_v2();



 case _LINUX_CAPABILITY_VERSION_3:
  *tocopy = _LINUX_CAPABILITY_U32S_3;
  break;
 default:
  if (put_user((u32)_KERNEL_CAPABILITY_VERSION, &header->version))
   return -EFAULT;
  return -EINVAL;
 }

 return 0;
}
static inline int cap_get_target_pid(pid_t pid, kernel_cap_t *pEp,
         kernel_cap_t *pIp, kernel_cap_t *pPp)
{
 int ret;

 if (pid && (pid != task_pid_vnr(current))) {
  struct task_struct *target;

  rcu_read_lock();

  target = find_task_by_vpid(pid);
  if (!target)
   ret = -ESRCH;
  else
   ret = security_capget(target, pEp, pIp, pPp);

  rcu_read_unlock();
 } else
  ret = security_capget(current, pEp, pIp, pPp);

 return ret;
}
SYSCALL_DEFINE2(capget, cap_user_header_t, header, cap_user_data_t, dataptr)
{
 int ret = 0;
 pid_t pid;
 unsigned tocopy;
 kernel_cap_t pE, pI, pP;

 ret = cap_validate_magic(header, &tocopy);
 if ((dataptr == NULL) || (ret != 0))
  return ((dataptr == NULL) && (ret == -EINVAL)) ? 0 : ret;

 if (get_user(pid, &header->pid))
  return -EFAULT;

 if (pid < 0)
  return -EINVAL;

 ret = cap_get_target_pid(pid, &pE, &pI, &pP);
 if (!ret) {
  struct __user_cap_data_struct kdata[_KERNEL_CAPABILITY_U32S];
  unsigned i;

  for (i = 0; i < tocopy; i++) {
   kdata[i].effective = pE.cap[i];
   kdata[i].permitted = pP.cap[i];
   kdata[i].inheritable = pI.cap[i];
  }
  if (copy_to_user(dataptr, kdata, tocopy
     * sizeof(struct __user_cap_data_struct))) {
   return -EFAULT;
  }
 }

 return ret;
}
SYSCALL_DEFINE2(capset, cap_user_header_t, header, const cap_user_data_t, data)
{
 struct __user_cap_data_struct kdata[_KERNEL_CAPABILITY_U32S];
 unsigned i, tocopy, copybytes;
 kernel_cap_t inheritable, permitted, effective;
 struct cred *new;
 int ret;
 pid_t pid;

 ret = cap_validate_magic(header, &tocopy);
 if (ret != 0)
  return ret;

 if (get_user(pid, &header->pid))
  return -EFAULT;


 if (pid != 0 && pid != task_pid_vnr(current))
  return -EPERM;

 copybytes = tocopy * sizeof(struct __user_cap_data_struct);
 if (copybytes > sizeof(kdata))
  return -EFAULT;

 if (copy_from_user(&kdata, data, copybytes))
  return -EFAULT;

 for (i = 0; i < tocopy; i++) {
  effective.cap[i] = kdata[i].effective;
  permitted.cap[i] = kdata[i].permitted;
  inheritable.cap[i] = kdata[i].inheritable;
 }
 while (i < _KERNEL_CAPABILITY_U32S) {
  effective.cap[i] = 0;
  permitted.cap[i] = 0;
  inheritable.cap[i] = 0;
  i++;
 }

 effective.cap[CAP_LAST_U32] &= CAP_LAST_U32_VALID_MASK;
 permitted.cap[CAP_LAST_U32] &= CAP_LAST_U32_VALID_MASK;
 inheritable.cap[CAP_LAST_U32] &= CAP_LAST_U32_VALID_MASK;

 new = prepare_creds();
 if (!new)
  return -ENOMEM;

 ret = security_capset(new, current_cred(),
         &effective, &inheritable, &permitted);
 if (ret < 0)
  goto error;

 audit_log_capset(new, current_cred());

 return commit_creds(new);

error:
 abort_creds(new);
 return ret;
}
bool has_ns_capability(struct task_struct *t,
         struct user_namespace *ns, int cap)
{
 int ret;

 rcu_read_lock();
 ret = security_capable(__task_cred(t), ns, cap);
 rcu_read_unlock();

 return (ret == 0);
}
bool has_capability(struct task_struct *t, int cap)
{
 return has_ns_capability(t, &init_user_ns, cap);
}
bool has_ns_capability_noaudit(struct task_struct *t,
          struct user_namespace *ns, int cap)
{
 int ret;

 rcu_read_lock();
 ret = security_capable_noaudit(__task_cred(t), ns, cap);
 rcu_read_unlock();

 return (ret == 0);
}
bool has_capability_noaudit(struct task_struct *t, int cap)
{
 return has_ns_capability_noaudit(t, &init_user_ns, cap);
}
bool ns_capable(struct user_namespace *ns, int cap)
{
 if (unlikely(!cap_valid(cap))) {
  pr_crit("capable() called with invalid cap=%u\n", cap);
  BUG();
 }

 if (security_capable(current_cred(), ns, cap) == 0) {
  current->flags |= PF_SUPERPRIV;
  return true;
 }
 return false;
}
EXPORT_SYMBOL(ns_capable);
bool capable(int cap)
{
 return ns_capable(&init_user_ns, cap);
}
EXPORT_SYMBOL(capable);
bool file_ns_capable(const struct file *file, struct user_namespace *ns,
       int cap)
{
 if (WARN_ON_ONCE(!cap_valid(cap)))
  return false;

 if (security_capable(file->f_cred, ns, cap) == 0)
  return true;

 return false;
}
EXPORT_SYMBOL(file_ns_capable);
bool capable_wrt_inode_uidgid(const struct inode *inode, int cap)
{
 struct user_namespace *ns = current_user_ns();

 return ns_capable(ns, cap) && kuid_has_mapping(ns, inode->i_uid) &&
  kgid_has_mapping(ns, inode->i_gid);
}
EXPORT_SYMBOL(capable_wrt_inode_uidgid);









      MAX_CFTYPE_NAME + 2)
DEFINE_MUTEX(cgroup_mutex);
DEFINE_SPINLOCK(css_set_lock);
EXPORT_SYMBOL_GPL(cgroup_mutex);
EXPORT_SYMBOL_GPL(css_set_lock);
static DEFINE_MUTEX(cgroup_mutex);
static DEFINE_SPINLOCK(css_set_lock);





static DEFINE_SPINLOCK(cgroup_idr_lock);





static DEFINE_SPINLOCK(cgroup_file_kn_lock);





static DEFINE_SPINLOCK(release_agent_path_lock);

struct percpu_rw_semaphore cgroup_threadgroup_rwsem;

 RCU_LOCKDEP_WARN(!rcu_read_lock_held() && \
      !lockdep_is_held(&cgroup_mutex), \
      "cgroup_mutex or RCU read lock required");







static struct workqueue_struct *cgroup_destroy_wq;





static struct workqueue_struct *cgroup_pidlist_destroy_wq;


static struct cgroup_subsys *cgroup_subsys[] = {
};


static const char *cgroup_subsys_name[] = {
};


 DEFINE_STATIC_KEY_TRUE(_x ## _cgrp_subsys_enabled_key); \
 DEFINE_STATIC_KEY_TRUE(_x ## _cgrp_subsys_on_dfl_key); \
 EXPORT_SYMBOL_GPL(_x ## _cgrp_subsys_enabled_key); \
 EXPORT_SYMBOL_GPL(_x ## _cgrp_subsys_on_dfl_key);

static struct static_key_true *cgroup_subsys_enabled_key[] = {
};

static struct static_key_true *cgroup_subsys_on_dfl_key[] = {
};






struct cgroup_root cgrp_dfl_root;
EXPORT_SYMBOL_GPL(cgrp_dfl_root);





static bool cgrp_dfl_visible;


static u16 cgroup_no_v1_mask;


static u16 cgrp_dfl_inhibit_ss_mask;


static unsigned long cgrp_dfl_implicit_ss_mask;



static LIST_HEAD(cgroup_roots);
static int cgroup_root_count;


static DEFINE_IDR(cgroup_hierarchy_idr);
static u64 css_serial_nr_next = 1;






static u16 have_fork_callback __read_mostly;
static u16 have_exit_callback __read_mostly;
static u16 have_free_callback __read_mostly;


struct cgroup_namespace init_cgroup_ns = {
 .count = { .counter = 2, },
 .user_ns = &init_user_ns,
 .ns.ops = &cgroupns_operations,
 .ns.inum = PROC_CGROUP_INIT_INO,
 .root_cset = &init_css_set,
};


static u16 have_canfork_callback __read_mostly;

static struct file_system_type cgroup2_fs_type;
static struct cftype cgroup_dfl_base_files[];
static struct cftype cgroup_legacy_base_files[];

static int rebind_subsystems(struct cgroup_root *dst_root, u16 ss_mask);
static void cgroup_lock_and_drain_offline(struct cgroup *cgrp);
static int cgroup_apply_control(struct cgroup *cgrp);
static void cgroup_finalize_control(struct cgroup *cgrp, int ret);
static void css_task_iter_advance(struct css_task_iter *it);
static int cgroup_destroy_locked(struct cgroup *cgrp);
static struct cgroup_subsys_state *css_create(struct cgroup *cgrp,
           struct cgroup_subsys *ss);
static void css_release(struct percpu_ref *ref);
static void kill_css(struct cgroup_subsys_state *css);
static int cgroup_addrm_files(struct cgroup_subsys_state *css,
         struct cgroup *cgrp, struct cftype cfts[],
         bool is_add);
static bool cgroup_ssid_enabled(int ssid)
{
 if (CGROUP_SUBSYS_COUNT == 0)
  return false;

 return static_key_enabled(cgroup_subsys_enabled_key[ssid]);
}

static bool cgroup_ssid_no_v1(int ssid)
{
 return cgroup_no_v1_mask & (1 << ssid);
}
static bool cgroup_on_dfl(const struct cgroup *cgrp)
{
 return cgrp->root == &cgrp_dfl_root;
}


static int cgroup_idr_alloc(struct idr *idr, void *ptr, int start, int end,
       gfp_t gfp_mask)
{
 int ret;

 idr_preload(gfp_mask);
 spin_lock_bh(&cgroup_idr_lock);
 ret = idr_alloc(idr, ptr, start, end, gfp_mask & ~__GFP_DIRECT_RECLAIM);
 spin_unlock_bh(&cgroup_idr_lock);
 idr_preload_end();
 return ret;
}

static void *cgroup_idr_replace(struct idr *idr, void *ptr, int id)
{
 void *ret;

 spin_lock_bh(&cgroup_idr_lock);
 ret = idr_replace(idr, ptr, id);
 spin_unlock_bh(&cgroup_idr_lock);
 return ret;
}

static void cgroup_idr_remove(struct idr *idr, int id)
{
 spin_lock_bh(&cgroup_idr_lock);
 idr_remove(idr, id);
 spin_unlock_bh(&cgroup_idr_lock);
}

static struct cgroup *cgroup_parent(struct cgroup *cgrp)
{
 struct cgroup_subsys_state *parent_css = cgrp->self.parent;

 if (parent_css)
  return container_of(parent_css, struct cgroup, self);
 return NULL;
}


static u16 cgroup_control(struct cgroup *cgrp)
{
 struct cgroup *parent = cgroup_parent(cgrp);
 u16 root_ss_mask = cgrp->root->subsys_mask;

 if (parent)
  return parent->subtree_control;

 if (cgroup_on_dfl(cgrp))
  root_ss_mask &= ~(cgrp_dfl_inhibit_ss_mask |
      cgrp_dfl_implicit_ss_mask);
 return root_ss_mask;
}


static u16 cgroup_ss_mask(struct cgroup *cgrp)
{
 struct cgroup *parent = cgroup_parent(cgrp);

 if (parent)
  return parent->subtree_ss_mask;

 return cgrp->root->subsys_mask;
}
static struct cgroup_subsys_state *cgroup_css(struct cgroup *cgrp,
           struct cgroup_subsys *ss)
{
 if (ss)
  return rcu_dereference_check(cgrp->subsys[ss->id],
     lockdep_is_held(&cgroup_mutex));
 else
  return &cgrp->self;
}
static struct cgroup_subsys_state *cgroup_e_css(struct cgroup *cgrp,
      struct cgroup_subsys *ss)
{
 lockdep_assert_held(&cgroup_mutex);

 if (!ss)
  return &cgrp->self;





 while (!(cgroup_ss_mask(cgrp) & (1 << ss->id))) {
  cgrp = cgroup_parent(cgrp);
  if (!cgrp)
   return NULL;
 }

 return cgroup_css(cgrp, ss);
}
struct cgroup_subsys_state *cgroup_get_e_css(struct cgroup *cgrp,
          struct cgroup_subsys *ss)
{
 struct cgroup_subsys_state *css;

 rcu_read_lock();

 do {
  css = cgroup_css(cgrp, ss);

  if (css && css_tryget_online(css))
   goto out_unlock;
  cgrp = cgroup_parent(cgrp);
 } while (cgrp);

 css = init_css_set.subsys[ss->id];
 css_get(css);
out_unlock:
 rcu_read_unlock();
 return css;
}


static inline bool cgroup_is_dead(const struct cgroup *cgrp)
{
 return !(cgrp->self.flags & CSS_ONLINE);
}

static void cgroup_get(struct cgroup *cgrp)
{
 WARN_ON_ONCE(cgroup_is_dead(cgrp));
 css_get(&cgrp->self);
}

static bool cgroup_tryget(struct cgroup *cgrp)
{
 return css_tryget(&cgrp->self);
}

struct cgroup_subsys_state *of_css(struct kernfs_open_file *of)
{
 struct cgroup *cgrp = of->kn->parent->priv;
 struct cftype *cft = of_cft(of);
 if (cft->ss)
  return rcu_dereference_raw(cgrp->subsys[cft->ss->id]);
 else
  return &cgrp->self;
}
EXPORT_SYMBOL_GPL(of_css);

static int notify_on_release(const struct cgroup *cgrp)
{
 return test_bit(CGRP_NOTIFY_ON_RELEASE, &cgrp->flags);
}
 for ((ssid) = 0; (ssid) < CGROUP_SUBSYS_COUNT; (ssid)++) \
  if (!((css) = rcu_dereference_check( \
    (cgrp)->subsys[(ssid)], \
    lockdep_is_held(&cgroup_mutex)))) { } \
  else
 for ((ssid) = 0; (ssid) < CGROUP_SUBSYS_COUNT; (ssid)++) \
  if (!((css) = cgroup_e_css(cgrp, cgroup_subsys[(ssid)]))) \
   ; \
  else






 for ((ssid) = 0; (ssid) < CGROUP_SUBSYS_COUNT && \
      (((ss) = cgroup_subsys[ssid]) || true); (ssid)++)
 unsigned long __ss_mask = (ss_mask); \
 if (!CGROUP_SUBSYS_COUNT) { \
  (ssid) = 0; \
  break; \
 } \
 for_each_set_bit(ssid, &__ss_mask, CGROUP_SUBSYS_COUNT) { \
  (ss) = cgroup_subsys[ssid]; \
  {

  } \
 } \
} while (false)


 list_for_each_entry((root), &cgroup_roots, root_list)


 list_for_each_entry((child), &(cgrp)->self.children, self.sibling) \
  if (({ lockdep_assert_held(&cgroup_mutex); \
         cgroup_is_dead(child); })) \
   ; \
  else


 css_for_each_descendant_pre((d_css), cgroup_css((cgrp), NULL)) \
  if (({ lockdep_assert_held(&cgroup_mutex); \
         (dsct) = (d_css)->cgroup; \
         cgroup_is_dead(dsct); })) \
   ; \
  else


 css_for_each_descendant_post((d_css), cgroup_css((cgrp), NULL)) \
  if (({ lockdep_assert_held(&cgroup_mutex); \
         (dsct) = (d_css)->cgroup; \
         cgroup_is_dead(dsct); })) \
   ; \
  else

static void cgroup_release_agent(struct work_struct *work);
static void check_for_release(struct cgroup *cgrp);
struct cgrp_cset_link {

 struct cgroup *cgrp;
 struct css_set *cset;


 struct list_head cset_link;


 struct list_head cgrp_link;
};
struct css_set init_css_set = {
 .refcount = ATOMIC_INIT(1),
 .cgrp_links = LIST_HEAD_INIT(init_css_set.cgrp_links),
 .tasks = LIST_HEAD_INIT(init_css_set.tasks),
 .mg_tasks = LIST_HEAD_INIT(init_css_set.mg_tasks),
 .mg_preload_node = LIST_HEAD_INIT(init_css_set.mg_preload_node),
 .mg_node = LIST_HEAD_INIT(init_css_set.mg_node),
 .task_iters = LIST_HEAD_INIT(init_css_set.task_iters),
};

static int css_set_count = 1;





static bool css_set_populated(struct css_set *cset)
{
 lockdep_assert_held(&css_set_lock);

 return !list_empty(&cset->tasks) || !list_empty(&cset->mg_tasks);
}
static void cgroup_update_populated(struct cgroup *cgrp, bool populated)
{
 lockdep_assert_held(&css_set_lock);

 do {
  bool trigger;

  if (populated)
   trigger = !cgrp->populated_cnt++;
  else
   trigger = !--cgrp->populated_cnt;

  if (!trigger)
   break;

  check_for_release(cgrp);
  cgroup_file_notify(&cgrp->events_file);

  cgrp = cgroup_parent(cgrp);
 } while (cgrp);
}
static void css_set_update_populated(struct css_set *cset, bool populated)
{
 struct cgrp_cset_link *link;

 lockdep_assert_held(&css_set_lock);

 list_for_each_entry(link, &cset->cgrp_links, cgrp_link)
  cgroup_update_populated(link->cgrp, populated);
}
static void css_set_move_task(struct task_struct *task,
         struct css_set *from_cset, struct css_set *to_cset,
         bool use_mg_tasks)
{
 lockdep_assert_held(&css_set_lock);

 if (to_cset && !css_set_populated(to_cset))
  css_set_update_populated(to_cset, true);

 if (from_cset) {
  struct css_task_iter *it, *pos;

  WARN_ON_ONCE(list_empty(&task->cg_list));
  list_for_each_entry_safe(it, pos, &from_cset->task_iters,
      iters_node)
   if (it->task_pos == &task->cg_list)
    css_task_iter_advance(it);

  list_del_init(&task->cg_list);
  if (!css_set_populated(from_cset))
   css_set_update_populated(from_cset, false);
 } else {
  WARN_ON_ONCE(!list_empty(&task->cg_list));
 }

 if (to_cset) {






  WARN_ON_ONCE(task->flags & PF_EXITING);

  rcu_assign_pointer(task->cgroups, to_cset);
  list_add_tail(&task->cg_list, use_mg_tasks ? &to_cset->mg_tasks :
            &to_cset->tasks);
 }
}






static DEFINE_HASHTABLE(css_set_table, CSS_SET_HASH_BITS);

static unsigned long css_set_hash(struct cgroup_subsys_state *css[])
{
 unsigned long key = 0UL;
 struct cgroup_subsys *ss;
 int i;

 for_each_subsys(ss, i)
  key += (unsigned long)css[i];
 key = (key >> 16) ^ key;

 return key;
}

static void put_css_set_locked(struct css_set *cset)
{
 struct cgrp_cset_link *link, *tmp_link;
 struct cgroup_subsys *ss;
 int ssid;

 lockdep_assert_held(&css_set_lock);

 if (!atomic_dec_and_test(&cset->refcount))
  return;


 for_each_subsys(ss, ssid) {
  list_del(&cset->e_cset_node[ssid]);
  css_put(cset->subsys[ssid]);
 }
 hash_del(&cset->hlist);
 css_set_count--;

 list_for_each_entry_safe(link, tmp_link, &cset->cgrp_links, cgrp_link) {
  list_del(&link->cset_link);
  list_del(&link->cgrp_link);
  if (cgroup_parent(link->cgrp))
   cgroup_put(link->cgrp);
  kfree(link);
 }

 kfree_rcu(cset, rcu_head);
}

static void put_css_set(struct css_set *cset)
{
 unsigned long flags;






 if (atomic_add_unless(&cset->refcount, -1, 1))
  return;

 spin_lock_irqsave(&css_set_lock, flags);
 put_css_set_locked(cset);
 spin_unlock_irqrestore(&css_set_lock, flags);
}




static inline void get_css_set(struct css_set *cset)
{
 atomic_inc(&cset->refcount);
}
static bool compare_css_sets(struct css_set *cset,
        struct css_set *old_cset,
        struct cgroup *new_cgrp,
        struct cgroup_subsys_state *template[])
{
 struct list_head *l1, *l2;






 if (memcmp(template, cset->subsys, sizeof(cset->subsys)))
  return false;







 l1 = &cset->cgrp_links;
 l2 = &old_cset->cgrp_links;
 while (1) {
  struct cgrp_cset_link *link1, *link2;
  struct cgroup *cgrp1, *cgrp2;

  l1 = l1->next;
  l2 = l2->next;

  if (l1 == &cset->cgrp_links) {
   BUG_ON(l2 != &old_cset->cgrp_links);
   break;
  } else {
   BUG_ON(l2 == &old_cset->cgrp_links);
  }

  link1 = list_entry(l1, struct cgrp_cset_link, cgrp_link);
  link2 = list_entry(l2, struct cgrp_cset_link, cgrp_link);
  cgrp1 = link1->cgrp;
  cgrp2 = link2->cgrp;

  BUG_ON(cgrp1->root != cgrp2->root);
  if (cgrp1->root == new_cgrp->root) {
   if (cgrp1 != new_cgrp)
    return false;
  } else {
   if (cgrp1 != cgrp2)
    return false;
  }
 }
 return true;
}







static struct css_set *find_existing_css_set(struct css_set *old_cset,
     struct cgroup *cgrp,
     struct cgroup_subsys_state *template[])
{
 struct cgroup_root *root = cgrp->root;
 struct cgroup_subsys *ss;
 struct css_set *cset;
 unsigned long key;
 int i;






 for_each_subsys(ss, i) {
  if (root->subsys_mask & (1UL << i)) {




   template[i] = cgroup_e_css(cgrp, ss);
  } else {




   template[i] = old_cset->subsys[i];
  }
 }

 key = css_set_hash(template);
 hash_for_each_possible(css_set_table, cset, hlist, key) {
  if (!compare_css_sets(cset, old_cset, cgrp, template))
   continue;


  return cset;
 }


 return NULL;
}

static void free_cgrp_cset_links(struct list_head *links_to_free)
{
 struct cgrp_cset_link *link, *tmp_link;

 list_for_each_entry_safe(link, tmp_link, links_to_free, cset_link) {
  list_del(&link->cset_link);
  kfree(link);
 }
}
static int allocate_cgrp_cset_links(int count, struct list_head *tmp_links)
{
 struct cgrp_cset_link *link;
 int i;

 INIT_LIST_HEAD(tmp_links);

 for (i = 0; i < count; i++) {
  link = kzalloc(sizeof(*link), GFP_KERNEL);
  if (!link) {
   free_cgrp_cset_links(tmp_links);
   return -ENOMEM;
  }
  list_add(&link->cset_link, tmp_links);
 }
 return 0;
}







static void link_css_set(struct list_head *tmp_links, struct css_set *cset,
    struct cgroup *cgrp)
{
 struct cgrp_cset_link *link;

 BUG_ON(list_empty(tmp_links));

 if (cgroup_on_dfl(cgrp))
  cset->dfl_cgrp = cgrp;

 link = list_first_entry(tmp_links, struct cgrp_cset_link, cset_link);
 link->cset = cset;
 link->cgrp = cgrp;





 list_move_tail(&link->cset_link, &cgrp->cset_links);
 list_add_tail(&link->cgrp_link, &cset->cgrp_links);

 if (cgroup_parent(cgrp))
  cgroup_get(cgrp);
}
static struct css_set *find_css_set(struct css_set *old_cset,
        struct cgroup *cgrp)
{
 struct cgroup_subsys_state *template[CGROUP_SUBSYS_COUNT] = { };
 struct css_set *cset;
 struct list_head tmp_links;
 struct cgrp_cset_link *link;
 struct cgroup_subsys *ss;
 unsigned long key;
 int ssid;

 lockdep_assert_held(&cgroup_mutex);



 spin_lock_irq(&css_set_lock);
 cset = find_existing_css_set(old_cset, cgrp, template);
 if (cset)
  get_css_set(cset);
 spin_unlock_irq(&css_set_lock);

 if (cset)
  return cset;

 cset = kzalloc(sizeof(*cset), GFP_KERNEL);
 if (!cset)
  return NULL;


 if (allocate_cgrp_cset_links(cgroup_root_count, &tmp_links) < 0) {
  kfree(cset);
  return NULL;
 }

 atomic_set(&cset->refcount, 1);
 INIT_LIST_HEAD(&cset->cgrp_links);
 INIT_LIST_HEAD(&cset->tasks);
 INIT_LIST_HEAD(&cset->mg_tasks);
 INIT_LIST_HEAD(&cset->mg_preload_node);
 INIT_LIST_HEAD(&cset->mg_node);
 INIT_LIST_HEAD(&cset->task_iters);
 INIT_HLIST_NODE(&cset->hlist);



 memcpy(cset->subsys, template, sizeof(cset->subsys));

 spin_lock_irq(&css_set_lock);

 list_for_each_entry(link, &old_cset->cgrp_links, cgrp_link) {
  struct cgroup *c = link->cgrp;

  if (c->root == cgrp->root)
   c = cgrp;
  link_css_set(&tmp_links, cset, c);
 }

 BUG_ON(!list_empty(&tmp_links));

 css_set_count++;


 key = css_set_hash(cset->subsys);
 hash_add(css_set_table, &cset->hlist, key);

 for_each_subsys(ss, ssid) {
  struct cgroup_subsys_state *css = cset->subsys[ssid];

  list_add_tail(&cset->e_cset_node[ssid],
         &css->cgroup->e_csets[ssid]);
  css_get(css);
 }

 spin_unlock_irq(&css_set_lock);

 return cset;
}

static struct cgroup_root *cgroup_root_from_kf(struct kernfs_root *kf_root)
{
 struct cgroup *root_cgrp = kf_root->kn->priv;

 return root_cgrp->root;
}

static int cgroup_init_root_id(struct cgroup_root *root)
{
 int id;

 lockdep_assert_held(&cgroup_mutex);

 id = idr_alloc_cyclic(&cgroup_hierarchy_idr, root, 0, 0, GFP_KERNEL);
 if (id < 0)
  return id;

 root->hierarchy_id = id;
 return 0;
}

static void cgroup_exit_root_id(struct cgroup_root *root)
{
 lockdep_assert_held(&cgroup_mutex);

 if (root->hierarchy_id) {
  idr_remove(&cgroup_hierarchy_idr, root->hierarchy_id);
  root->hierarchy_id = 0;
 }
}

static void cgroup_free_root(struct cgroup_root *root)
{
 if (root) {

  WARN_ON_ONCE(root->hierarchy_id);

  idr_destroy(&root->cgroup_idr);
  kfree(root);
 }
}

static void cgroup_destroy_root(struct cgroup_root *root)
{
 struct cgroup *cgrp = &root->cgrp;
 struct cgrp_cset_link *link, *tmp_link;

 cgroup_lock_and_drain_offline(&cgrp_dfl_root.cgrp);

 BUG_ON(atomic_read(&root->nr_cgrps));
 BUG_ON(!list_empty(&cgrp->self.children));


 WARN_ON(rebind_subsystems(&cgrp_dfl_root, root->subsys_mask));





 spin_lock_irq(&css_set_lock);

 list_for_each_entry_safe(link, tmp_link, &cgrp->cset_links, cset_link) {
  list_del(&link->cset_link);
  list_del(&link->cgrp_link);
  kfree(link);
 }

 spin_unlock_irq(&css_set_lock);

 if (!list_empty(&root->root_list)) {
  list_del(&root->root_list);
  cgroup_root_count--;
 }

 cgroup_exit_root_id(root);

 mutex_unlock(&cgroup_mutex);

 kernfs_destroy_root(root->kf_root);
 cgroup_free_root(root);
}





static struct cgroup *
current_cgns_cgroup_from_root(struct cgroup_root *root)
{
 struct cgroup *res = NULL;
 struct css_set *cset;

 lockdep_assert_held(&css_set_lock);

 rcu_read_lock();

 cset = current->nsproxy->cgroup_ns->root_cset;
 if (cset == &init_css_set) {
  res = &root->cgrp;
 } else {
  struct cgrp_cset_link *link;

  list_for_each_entry(link, &cset->cgrp_links, cgrp_link) {
   struct cgroup *c = link->cgrp;

   if (c->root == root) {
    res = c;
    break;
   }
  }
 }
 rcu_read_unlock();

 BUG_ON(!res);
 return res;
}


static struct cgroup *cset_cgroup_from_root(struct css_set *cset,
         struct cgroup_root *root)
{
 struct cgroup *res = NULL;

 lockdep_assert_held(&cgroup_mutex);
 lockdep_assert_held(&css_set_lock);

 if (cset == &init_css_set) {
  res = &root->cgrp;
 } else {
  struct cgrp_cset_link *link;

  list_for_each_entry(link, &cset->cgrp_links, cgrp_link) {
   struct cgroup *c = link->cgrp;

   if (c->root == root) {
    res = c;
    break;
   }
  }
 }

 BUG_ON(!res);
 return res;
}





static struct cgroup *task_cgroup_from_root(struct task_struct *task,
         struct cgroup_root *root)
{





 return cset_cgroup_from_root(task_css_set(task), root);
}
static struct kernfs_syscall_ops cgroup_kf_syscall_ops;
static const struct file_operations proc_cgroupstats_operations;

static char *cgroup_file_name(struct cgroup *cgrp, const struct cftype *cft,
         char *buf)
{
 struct cgroup_subsys *ss = cft->ss;

 if (cft->ss && !(cft->flags & CFTYPE_NO_PREFIX) &&
     !(cgrp->root->flags & CGRP_ROOT_NOPREFIX))
  snprintf(buf, CGROUP_FILE_NAME_MAX, "%s.%s",
    cgroup_on_dfl(cgrp) ? ss->name : ss->legacy_name,
    cft->name);
 else
  strncpy(buf, cft->name, CGROUP_FILE_NAME_MAX);
 return buf;
}







static umode_t cgroup_file_mode(const struct cftype *cft)
{
 umode_t mode = 0;

 if (cft->read_u64 || cft->read_s64 || cft->seq_show)
  mode |= S_IRUGO;

 if (cft->write_u64 || cft->write_s64 || cft->write) {
  if (cft->flags & CFTYPE_WORLD_WRITABLE)
   mode |= S_IWUGO;
  else
   mode |= S_IWUSR;
 }

 return mode;
}
static u16 cgroup_calc_subtree_ss_mask(u16 subtree_control, u16 this_ss_mask)
{
 u16 cur_ss_mask = subtree_control;
 struct cgroup_subsys *ss;
 int ssid;

 lockdep_assert_held(&cgroup_mutex);

 cur_ss_mask |= cgrp_dfl_implicit_ss_mask;

 while (true) {
  u16 new_ss_mask = cur_ss_mask;

  do_each_subsys_mask(ss, ssid, cur_ss_mask) {
   new_ss_mask |= ss->depends_on;
  } while_each_subsys_mask();






  new_ss_mask &= this_ss_mask;

  if (new_ss_mask == cur_ss_mask)
   break;
  cur_ss_mask = new_ss_mask;
 }

 return cur_ss_mask;
}
static void cgroup_kn_unlock(struct kernfs_node *kn)
{
 struct cgroup *cgrp;

 if (kernfs_type(kn) == KERNFS_DIR)
  cgrp = kn->priv;
 else
  cgrp = kn->parent->priv;

 mutex_unlock(&cgroup_mutex);

 kernfs_unbreak_active_protection(kn);
 cgroup_put(cgrp);
}
static struct cgroup *cgroup_kn_lock_live(struct kernfs_node *kn,
       bool drain_offline)
{
 struct cgroup *cgrp;

 if (kernfs_type(kn) == KERNFS_DIR)
  cgrp = kn->priv;
 else
  cgrp = kn->parent->priv;







 if (!cgroup_tryget(cgrp))
  return NULL;
 kernfs_break_active_protection(kn);

 if (drain_offline)
  cgroup_lock_and_drain_offline(cgrp);
 else
  mutex_lock(&cgroup_mutex);

 if (!cgroup_is_dead(cgrp))
  return cgrp;

 cgroup_kn_unlock(kn);
 return NULL;
}

static void cgroup_rm_file(struct cgroup *cgrp, const struct cftype *cft)
{
 char name[CGROUP_FILE_NAME_MAX];

 lockdep_assert_held(&cgroup_mutex);

 if (cft->file_offset) {
  struct cgroup_subsys_state *css = cgroup_css(cgrp, cft->ss);
  struct cgroup_file *cfile = (void *)css + cft->file_offset;

  spin_lock_irq(&cgroup_file_kn_lock);
  cfile->kn = NULL;
  spin_unlock_irq(&cgroup_file_kn_lock);
 }

 kernfs_remove_by_name(cgrp->kn, cgroup_file_name(cgrp, cft, name));
}





static void css_clear_dir(struct cgroup_subsys_state *css)
{
 struct cgroup *cgrp = css->cgroup;
 struct cftype *cfts;

 if (!(css->flags & CSS_VISIBLE))
  return;

 css->flags &= ~CSS_VISIBLE;

 list_for_each_entry(cfts, &css->ss->cfts, node)
  cgroup_addrm_files(css, cgrp, cfts, false);
}







static int css_populate_dir(struct cgroup_subsys_state *css)
{
 struct cgroup *cgrp = css->cgroup;
 struct cftype *cfts, *failed_cfts;
 int ret;

 if ((css->flags & CSS_VISIBLE) || !cgrp->kn)
  return 0;

 if (!css->ss) {
  if (cgroup_on_dfl(cgrp))
   cfts = cgroup_dfl_base_files;
  else
   cfts = cgroup_legacy_base_files;

  return cgroup_addrm_files(&cgrp->self, cgrp, cfts, true);
 }

 list_for_each_entry(cfts, &css->ss->cfts, node) {
  ret = cgroup_addrm_files(css, cgrp, cfts, true);
  if (ret < 0) {
   failed_cfts = cfts;
   goto err;
  }
 }

 css->flags |= CSS_VISIBLE;

 return 0;
err:
 list_for_each_entry(cfts, &css->ss->cfts, node) {
  if (cfts == failed_cfts)
   break;
  cgroup_addrm_files(css, cgrp, cfts, false);
 }
 return ret;
}

static int rebind_subsystems(struct cgroup_root *dst_root, u16 ss_mask)
{
 struct cgroup *dcgrp = &dst_root->cgrp;
 struct cgroup_subsys *ss;
 int ssid, i, ret;

 lockdep_assert_held(&cgroup_mutex);

 do_each_subsys_mask(ss, ssid, ss_mask) {





  if (css_next_child(NULL, cgroup_css(&ss->root->cgrp, ss)) &&
      !ss->implicit_on_dfl)
   return -EBUSY;


  if (ss->root != &cgrp_dfl_root && dst_root != &cgrp_dfl_root)
   return -EBUSY;
 } while_each_subsys_mask();

 do_each_subsys_mask(ss, ssid, ss_mask) {
  struct cgroup_root *src_root = ss->root;
  struct cgroup *scgrp = &src_root->cgrp;
  struct cgroup_subsys_state *css = cgroup_css(scgrp, ss);
  struct css_set *cset;

  WARN_ON(!css || cgroup_css(dcgrp, ss));


  src_root->subsys_mask &= ~(1 << ssid);
  WARN_ON(cgroup_apply_control(scgrp));
  cgroup_finalize_control(scgrp, 0);


  RCU_INIT_POINTER(scgrp->subsys[ssid], NULL);
  rcu_assign_pointer(dcgrp->subsys[ssid], css);
  ss->root = dst_root;
  css->cgroup = dcgrp;

  spin_lock_irq(&css_set_lock);
  hash_for_each(css_set_table, i, cset, hlist)
   list_move_tail(&cset->e_cset_node[ss->id],
           &dcgrp->e_csets[ss->id]);
  spin_unlock_irq(&css_set_lock);


  dst_root->subsys_mask |= 1 << ssid;
  if (dst_root == &cgrp_dfl_root) {
   static_branch_enable(cgroup_subsys_on_dfl_key[ssid]);
  } else {
   dcgrp->subtree_control |= 1 << ssid;
   static_branch_disable(cgroup_subsys_on_dfl_key[ssid]);
  }

  ret = cgroup_apply_control(dcgrp);
  if (ret)
   pr_warn("partial failure to rebind %s controller (err=%d)\n",
    ss->name, ret);

  if (ss->bind)
   ss->bind(css);
 } while_each_subsys_mask();

 kernfs_activate(dcgrp->kn);
 return 0;
}

static int cgroup_show_path(struct seq_file *sf, struct kernfs_node *kf_node,
       struct kernfs_root *kf_root)
{
 int len = 0;
 char *buf = NULL;
 struct cgroup_root *kf_cgroot = cgroup_root_from_kf(kf_root);
 struct cgroup *ns_cgroup;

 buf = kmalloc(PATH_MAX, GFP_KERNEL);
 if (!buf)
  return -ENOMEM;

 spin_lock_irq(&css_set_lock);
 ns_cgroup = current_cgns_cgroup_from_root(kf_cgroot);
 len = kernfs_path_from_node(kf_node, ns_cgroup->kn, buf, PATH_MAX);
 spin_unlock_irq(&css_set_lock);

 if (len >= PATH_MAX)
  len = -ERANGE;
 else if (len > 0) {
  seq_escape(sf, buf, " \t\n\\");
  len = 0;
 }
 kfree(buf);
 return len;
}

static int cgroup_show_options(struct seq_file *seq,
          struct kernfs_root *kf_root)
{
 struct cgroup_root *root = cgroup_root_from_kf(kf_root);
 struct cgroup_subsys *ss;
 int ssid;

 if (root != &cgrp_dfl_root)
  for_each_subsys(ss, ssid)
   if (root->subsys_mask & (1 << ssid))
    seq_show_option(seq, ss->legacy_name, NULL);
 if (root->flags & CGRP_ROOT_NOPREFIX)
  seq_puts(seq, ",noprefix");
 if (root->flags & CGRP_ROOT_XATTR)
  seq_puts(seq, ",xattr");

 spin_lock(&release_agent_path_lock);
 if (strlen(root->release_agent_path))
  seq_show_option(seq, "release_agent",
    root->release_agent_path);
 spin_unlock(&release_agent_path_lock);

 if (test_bit(CGRP_CPUSET_CLONE_CHILDREN, &root->cgrp.flags))
  seq_puts(seq, ",clone_children");
 if (strlen(root->name))
  seq_show_option(seq, "name", root->name);
 return 0;
}

struct cgroup_sb_opts {
 u16 subsys_mask;
 unsigned int flags;
 char *release_agent;
 bool cpuset_clone_children;
 char *name;

 bool none;
};

static int parse_cgroupfs_options(char *data, struct cgroup_sb_opts *opts)
{
 char *token, *o = data;
 bool all_ss = false, one_ss = false;
 u16 mask = U16_MAX;
 struct cgroup_subsys *ss;
 int nr_opts = 0;
 int i;

 mask = ~((u16)1 << cpuset_cgrp_id);

 memset(opts, 0, sizeof(*opts));

 while ((token = strsep(&o, ",")) != NULL) {
  nr_opts++;

  if (!*token)
   return -EINVAL;
  if (!strcmp(token, "none")) {

   opts->none = true;
   continue;
  }
  if (!strcmp(token, "all")) {

   if (one_ss)
    return -EINVAL;
   all_ss = true;
   continue;
  }
  if (!strcmp(token, "noprefix")) {
   opts->flags |= CGRP_ROOT_NOPREFIX;
   continue;
  }
  if (!strcmp(token, "clone_children")) {
   opts->cpuset_clone_children = true;
   continue;
  }
  if (!strcmp(token, "xattr")) {
   opts->flags |= CGRP_ROOT_XATTR;
   continue;
  }
  if (!strncmp(token, "release_agent=", 14)) {

   if (opts->release_agent)
    return -EINVAL;
   opts->release_agent =
    kstrndup(token + 14, PATH_MAX - 1, GFP_KERNEL);
   if (!opts->release_agent)
    return -ENOMEM;
   continue;
  }
  if (!strncmp(token, "name=", 5)) {
   const char *name = token + 5;

   if (!strlen(name))
    return -EINVAL;

   for (i = 0; i < strlen(name); i++) {
    char c = name[i];
    if (isalnum(c))
     continue;
    if ((c == '.') || (c == '-') || (c == '_'))
     continue;
    return -EINVAL;
   }

   if (opts->name)
    return -EINVAL;
   opts->name = kstrndup(name,
           MAX_CGROUP_ROOT_NAMELEN - 1,
           GFP_KERNEL);
   if (!opts->name)
    return -ENOMEM;

   continue;
  }

  for_each_subsys(ss, i) {
   if (strcmp(token, ss->legacy_name))
    continue;
   if (!cgroup_ssid_enabled(i))
    continue;
   if (cgroup_ssid_no_v1(i))
    continue;


   if (all_ss)
    return -EINVAL;
   opts->subsys_mask |= (1 << i);
   one_ss = true;

   break;
  }
  if (i == CGROUP_SUBSYS_COUNT)
   return -ENOENT;
 }






 if (all_ss || (!one_ss && !opts->none && !opts->name))
  for_each_subsys(ss, i)
   if (cgroup_ssid_enabled(i) && !cgroup_ssid_no_v1(i))
    opts->subsys_mask |= (1 << i);





 if (!opts->subsys_mask && !opts->name)
  return -EINVAL;






 if ((opts->flags & CGRP_ROOT_NOPREFIX) && (opts->subsys_mask & mask))
  return -EINVAL;


 if (opts->subsys_mask && opts->none)
  return -EINVAL;

 return 0;
}

static int cgroup_remount(struct kernfs_root *kf_root, int *flags, char *data)
{
 int ret = 0;
 struct cgroup_root *root = cgroup_root_from_kf(kf_root);
 struct cgroup_sb_opts opts;
 u16 added_mask, removed_mask;

 if (root == &cgrp_dfl_root) {
  pr_err("remount is not allowed\n");
  return -EINVAL;
 }

 cgroup_lock_and_drain_offline(&cgrp_dfl_root.cgrp);


 ret = parse_cgroupfs_options(data, &opts);
 if (ret)
  goto out_unlock;

 if (opts.subsys_mask != root->subsys_mask || opts.release_agent)
  pr_warn("option changes via remount are deprecated (pid=%d comm=%s)\n",
   task_tgid_nr(current), current->comm);

 added_mask = opts.subsys_mask & ~root->subsys_mask;
 removed_mask = root->subsys_mask & ~opts.subsys_mask;


 if ((opts.flags ^ root->flags) ||
     (opts.name && strcmp(opts.name, root->name))) {
  pr_err("option or name mismatch, new: 0x%x \"%s\", old: 0x%x \"%s\"\n",
         opts.flags, opts.name ?: "", root->flags, root->name);
  ret = -EINVAL;
  goto out_unlock;
 }


 if (!list_empty(&root->cgrp.self.children)) {
  ret = -EBUSY;
  goto out_unlock;
 }

 ret = rebind_subsystems(root, added_mask);
 if (ret)
  goto out_unlock;

 WARN_ON(rebind_subsystems(&cgrp_dfl_root, removed_mask));

 if (opts.release_agent) {
  spin_lock(&release_agent_path_lock);
  strcpy(root->release_agent_path, opts.release_agent);
  spin_unlock(&release_agent_path_lock);
 }
 out_unlock:
 kfree(opts.release_agent);
 kfree(opts.name);
 mutex_unlock(&cgroup_mutex);
 return ret;
}







static bool use_task_css_set_links __read_mostly;

static void cgroup_enable_task_cg_lists(void)
{
 struct task_struct *p, *g;

 spin_lock_irq(&css_set_lock);

 if (use_task_css_set_links)
  goto out_unlock;

 use_task_css_set_links = true;
 read_lock(&tasklist_lock);
 do_each_thread(g, p) {
  WARN_ON_ONCE(!list_empty(&p->cg_list) ||
        task_css_set(p) != &init_css_set);
  spin_lock(&p->sighand->siglock);
  if (!(p->flags & PF_EXITING)) {
   struct css_set *cset = task_css_set(p);

   if (!css_set_populated(cset))
    css_set_update_populated(cset, true);
   list_add_tail(&p->cg_list, &cset->tasks);
   get_css_set(cset);
  }
  spin_unlock(&p->sighand->siglock);
 } while_each_thread(g, p);
 read_unlock(&tasklist_lock);
out_unlock:
 spin_unlock_irq(&css_set_lock);
}

static void init_cgroup_housekeeping(struct cgroup *cgrp)
{
 struct cgroup_subsys *ss;
 int ssid;

 INIT_LIST_HEAD(&cgrp->self.sibling);
 INIT_LIST_HEAD(&cgrp->self.children);
 INIT_LIST_HEAD(&cgrp->cset_links);
 INIT_LIST_HEAD(&cgrp->pidlists);
 mutex_init(&cgrp->pidlist_mutex);
 cgrp->self.cgroup = cgrp;
 cgrp->self.flags |= CSS_ONLINE;

 for_each_subsys(ss, ssid)
  INIT_LIST_HEAD(&cgrp->e_csets[ssid]);

 init_waitqueue_head(&cgrp->offline_waitq);
 INIT_WORK(&cgrp->release_agent_work, cgroup_release_agent);
}

static void init_cgroup_root(struct cgroup_root *root,
        struct cgroup_sb_opts *opts)
{
 struct cgroup *cgrp = &root->cgrp;

 INIT_LIST_HEAD(&root->root_list);
 atomic_set(&root->nr_cgrps, 1);
 cgrp->root = root;
 init_cgroup_housekeeping(cgrp);
 idr_init(&root->cgroup_idr);

 root->flags = opts->flags;
 if (opts->release_agent)
  strcpy(root->release_agent_path, opts->release_agent);
 if (opts->name)
  strcpy(root->name, opts->name);
 if (opts->cpuset_clone_children)
  set_bit(CGRP_CPUSET_CLONE_CHILDREN, &root->cgrp.flags);
}

static int cgroup_setup_root(struct cgroup_root *root, u16 ss_mask)
{
 LIST_HEAD(tmp_links);
 struct cgroup *root_cgrp = &root->cgrp;
 struct css_set *cset;
 int i, ret;

 lockdep_assert_held(&cgroup_mutex);

 ret = cgroup_idr_alloc(&root->cgroup_idr, root_cgrp, 1, 2, GFP_KERNEL);
 if (ret < 0)
  goto out;
 root_cgrp->id = ret;
 root_cgrp->ancestor_ids[0] = ret;

 ret = percpu_ref_init(&root_cgrp->self.refcnt, css_release, 0,
         GFP_KERNEL);
 if (ret)
  goto out;
 ret = allocate_cgrp_cset_links(2 * css_set_count, &tmp_links);
 if (ret)
  goto cancel_ref;

 ret = cgroup_init_root_id(root);
 if (ret)
  goto cancel_ref;

 root->kf_root = kernfs_create_root(&cgroup_kf_syscall_ops,
        KERNFS_ROOT_CREATE_DEACTIVATED,
        root_cgrp);
 if (IS_ERR(root->kf_root)) {
  ret = PTR_ERR(root->kf_root);
  goto exit_root_id;
 }
 root_cgrp->kn = root->kf_root->kn;

 ret = css_populate_dir(&root_cgrp->self);
 if (ret)
  goto destroy_root;

 ret = rebind_subsystems(root, ss_mask);
 if (ret)
  goto destroy_root;






 list_add(&root->root_list, &cgroup_roots);
 cgroup_root_count++;





 spin_lock_irq(&css_set_lock);
 hash_for_each(css_set_table, i, cset, hlist) {
  link_css_set(&tmp_links, cset, root_cgrp);
  if (css_set_populated(cset))
   cgroup_update_populated(root_cgrp, true);
 }
 spin_unlock_irq(&css_set_lock);

 BUG_ON(!list_empty(&root_cgrp->self.children));
 BUG_ON(atomic_read(&root->nr_cgrps) != 1);

 kernfs_activate(root_cgrp->kn);
 ret = 0;
 goto out;

destroy_root:
 kernfs_destroy_root(root->kf_root);
 root->kf_root = NULL;
exit_root_id:
 cgroup_exit_root_id(root);
cancel_ref:
 percpu_ref_exit(&root_cgrp->self.refcnt);
out:
 free_cgrp_cset_links(&tmp_links);
 return ret;
}

static struct dentry *cgroup_mount(struct file_system_type *fs_type,
    int flags, const char *unused_dev_name,
    void *data)
{
 bool is_v2 = fs_type == &cgroup2_fs_type;
 struct super_block *pinned_sb = NULL;
 struct cgroup_namespace *ns = current->nsproxy->cgroup_ns;
 struct cgroup_subsys *ss;
 struct cgroup_root *root;
 struct cgroup_sb_opts opts;
 struct dentry *dentry;
 int ret;
 int i;
 bool new_sb;

 get_cgroup_ns(ns);


 if (!ns_capable(ns->user_ns, CAP_SYS_ADMIN)) {
  put_cgroup_ns(ns);
  return ERR_PTR(-EPERM);
 }





 if (!use_task_css_set_links)
  cgroup_enable_task_cg_lists();

 if (is_v2) {
  if (data) {
   pr_err("cgroup2: unknown option \"%s\"\n", (char *)data);
   put_cgroup_ns(ns);
   return ERR_PTR(-EINVAL);
  }
  cgrp_dfl_visible = true;
  root = &cgrp_dfl_root;
  cgroup_get(&root->cgrp);
  goto out_mount;
 }

 cgroup_lock_and_drain_offline(&cgrp_dfl_root.cgrp);


 ret = parse_cgroupfs_options(data, &opts);
 if (ret)
  goto out_unlock;
 for_each_subsys(ss, i) {
  if (!(opts.subsys_mask & (1 << i)) ||
      ss->root == &cgrp_dfl_root)
   continue;

  if (!percpu_ref_tryget_live(&ss->root->cgrp.self.refcnt)) {
   mutex_unlock(&cgroup_mutex);
   msleep(10);
   ret = restart_syscall();
   goto out_free;
  }
  cgroup_put(&ss->root->cgrp);
 }

 for_each_root(root) {
  bool name_match = false;

  if (root == &cgrp_dfl_root)
   continue;






  if (opts.name) {
   if (strcmp(opts.name, root->name))
    continue;
   name_match = true;
  }





  if ((opts.subsys_mask || opts.none) &&
      (opts.subsys_mask != root->subsys_mask)) {
   if (!name_match)
    continue;
   ret = -EBUSY;
   goto out_unlock;
  }

  if (root->flags ^ opts.flags)
   pr_warn("new mount options do not match the existing superblock, will be ignored\n");
  pinned_sb = kernfs_pin_sb(root->kf_root, NULL);
  if (IS_ERR(pinned_sb) ||
      !percpu_ref_tryget_live(&root->cgrp.self.refcnt)) {
   mutex_unlock(&cgroup_mutex);
   if (!IS_ERR_OR_NULL(pinned_sb))
    deactivate_super(pinned_sb);
   msleep(10);
   ret = restart_syscall();
   goto out_free;
  }

  ret = 0;
  goto out_unlock;
 }






 if (!opts.subsys_mask && !opts.none) {
  ret = -EINVAL;
  goto out_unlock;
 }






 if (!opts.none && !capable(CAP_SYS_ADMIN)) {
  ret = -EPERM;
  goto out_unlock;
 }

 root = kzalloc(sizeof(*root), GFP_KERNEL);
 if (!root) {
  ret = -ENOMEM;
  goto out_unlock;
 }

 init_cgroup_root(root, &opts);

 ret = cgroup_setup_root(root, opts.subsys_mask);
 if (ret)
  cgroup_free_root(root);

out_unlock:
 mutex_unlock(&cgroup_mutex);
out_free:
 kfree(opts.release_agent);
 kfree(opts.name);

 if (ret) {
  put_cgroup_ns(ns);
  return ERR_PTR(ret);
 }
out_mount:
 dentry = kernfs_mount(fs_type, flags, root->kf_root,
         is_v2 ? CGROUP2_SUPER_MAGIC : CGROUP_SUPER_MAGIC,
         &new_sb);






 if (!IS_ERR(dentry) && ns != &init_cgroup_ns) {
  struct dentry *nsdentry;
  struct cgroup *cgrp;

  mutex_lock(&cgroup_mutex);
  spin_lock_irq(&css_set_lock);

  cgrp = cset_cgroup_from_root(ns->root_cset, root);

  spin_unlock_irq(&css_set_lock);
  mutex_unlock(&cgroup_mutex);

  nsdentry = kernfs_node_dentry(cgrp->kn, dentry->d_sb);
  dput(dentry);
  dentry = nsdentry;
 }

 if (IS_ERR(dentry) || !new_sb)
  cgroup_put(&root->cgrp);





 if (pinned_sb) {
  WARN_ON(new_sb);
  deactivate_super(pinned_sb);
 }

 put_cgroup_ns(ns);
 return dentry;
}

static void cgroup_kill_sb(struct super_block *sb)
{
 struct kernfs_root *kf_root = kernfs_root_from_sb(sb);
 struct cgroup_root *root = cgroup_root_from_kf(kf_root);
 if (!list_empty(&root->cgrp.self.children) ||
     root == &cgrp_dfl_root)
  cgroup_put(&root->cgrp);
 else
  percpu_ref_kill(&root->cgrp.self.refcnt);

 kernfs_kill_sb(sb);
}

static struct file_system_type cgroup_fs_type = {
 .name = "cgroup",
 .mount = cgroup_mount,
 .kill_sb = cgroup_kill_sb,
 .fs_flags = FS_USERNS_MOUNT,
};

static struct file_system_type cgroup2_fs_type = {
 .name = "cgroup2",
 .mount = cgroup_mount,
 .kill_sb = cgroup_kill_sb,
 .fs_flags = FS_USERNS_MOUNT,
};

static char *cgroup_path_ns_locked(struct cgroup *cgrp, char *buf, size_t buflen,
       struct cgroup_namespace *ns)
{
 struct cgroup *root = cset_cgroup_from_root(ns->root_cset, cgrp->root);
 int ret;

 ret = kernfs_path_from_node(cgrp->kn, root->kn, buf, buflen);
 if (ret < 0 || ret >= buflen)
  return NULL;
 return buf;
}

char *cgroup_path_ns(struct cgroup *cgrp, char *buf, size_t buflen,
       struct cgroup_namespace *ns)
{
 char *ret;

 mutex_lock(&cgroup_mutex);
 spin_lock_irq(&css_set_lock);

 ret = cgroup_path_ns_locked(cgrp, buf, buflen, ns);

 spin_unlock_irq(&css_set_lock);
 mutex_unlock(&cgroup_mutex);

 return ret;
}
EXPORT_SYMBOL_GPL(cgroup_path_ns);
char *task_cgroup_path(struct task_struct *task, char *buf, size_t buflen)
{
 struct cgroup_root *root;
 struct cgroup *cgrp;
 int hierarchy_id = 1;
 char *path = NULL;

 mutex_lock(&cgroup_mutex);
 spin_lock_irq(&css_set_lock);

 root = idr_get_next(&cgroup_hierarchy_idr, &hierarchy_id);

 if (root) {
  cgrp = task_cgroup_from_root(task, root);
  path = cgroup_path_ns_locked(cgrp, buf, buflen, &init_cgroup_ns);
 } else {

  if (strlcpy(buf, "/", buflen) < buflen)
   path = buf;
 }

 spin_unlock_irq(&css_set_lock);
 mutex_unlock(&cgroup_mutex);
 return path;
}
EXPORT_SYMBOL_GPL(task_cgroup_path);


struct cgroup_taskset {

 struct list_head src_csets;
 struct list_head dst_csets;


 int ssid;
 struct list_head *csets;
 struct css_set *cur_cset;
 struct task_struct *cur_task;
};

 .src_csets = LIST_HEAD_INIT(tset.src_csets), \
 .dst_csets = LIST_HEAD_INIT(tset.dst_csets), \
 .csets = &tset.src_csets, \
}
static void cgroup_taskset_add(struct task_struct *task,
          struct cgroup_taskset *tset)
{
 struct css_set *cset;

 lockdep_assert_held(&css_set_lock);


 if (task->flags & PF_EXITING)
  return;


 if (list_empty(&task->cg_list))
  return;

 cset = task_css_set(task);
 if (!cset->mg_src_cgrp)
  return;

 list_move_tail(&task->cg_list, &cset->mg_tasks);
 if (list_empty(&cset->mg_node))
  list_add_tail(&cset->mg_node, &tset->src_csets);
 if (list_empty(&cset->mg_dst_cset->mg_node))
  list_move_tail(&cset->mg_dst_cset->mg_node,
          &tset->dst_csets);
}
struct task_struct *cgroup_taskset_first(struct cgroup_taskset *tset,
      struct cgroup_subsys_state **dst_cssp)
{
 tset->cur_cset = list_first_entry(tset->csets, struct css_set, mg_node);
 tset->cur_task = NULL;

 return cgroup_taskset_next(tset, dst_cssp);
}
struct task_struct *cgroup_taskset_next(struct cgroup_taskset *tset,
     struct cgroup_subsys_state **dst_cssp)
{
 struct css_set *cset = tset->cur_cset;
 struct task_struct *task = tset->cur_task;

 while (&cset->mg_node != tset->csets) {
  if (!task)
   task = list_first_entry(&cset->mg_tasks,
      struct task_struct, cg_list);
  else
   task = list_next_entry(task, cg_list);

  if (&task->cg_list != &cset->mg_tasks) {
   tset->cur_cset = cset;
   tset->cur_task = task;







   if (cset->mg_dst_cset)
    *dst_cssp = cset->mg_dst_cset->subsys[tset->ssid];
   else
    *dst_cssp = cset->subsys[tset->ssid];

   return task;
  }

  cset = list_next_entry(cset, mg_node);
  task = NULL;
 }

 return NULL;
}
static int cgroup_taskset_migrate(struct cgroup_taskset *tset,
      struct cgroup_root *root)
{
 struct cgroup_subsys *ss;
 struct task_struct *task, *tmp_task;
 struct css_set *cset, *tmp_cset;
 int ssid, failed_ssid, ret;


 if (list_empty(&tset->src_csets))
  return 0;


 do_each_subsys_mask(ss, ssid, root->subsys_mask) {
  if (ss->can_attach) {
   tset->ssid = ssid;
   ret = ss->can_attach(tset);
   if (ret) {
    failed_ssid = ssid;
    goto out_cancel_attach;
   }
  }
 } while_each_subsys_mask();






 spin_lock_irq(&css_set_lock);
 list_for_each_entry(cset, &tset->src_csets, mg_node) {
  list_for_each_entry_safe(task, tmp_task, &cset->mg_tasks, cg_list) {
   struct css_set *from_cset = task_css_set(task);
   struct css_set *to_cset = cset->mg_dst_cset;

   get_css_set(to_cset);
   css_set_move_task(task, from_cset, to_cset, true);
   put_css_set_locked(from_cset);
  }
 }
 spin_unlock_irq(&css_set_lock);






 tset->csets = &tset->dst_csets;

 do_each_subsys_mask(ss, ssid, root->subsys_mask) {
  if (ss->attach) {
   tset->ssid = ssid;
   ss->attach(tset);
  }
 } while_each_subsys_mask();

 ret = 0;
 goto out_release_tset;

out_cancel_attach:
 do_each_subsys_mask(ss, ssid, root->subsys_mask) {
  if (ssid == failed_ssid)
   break;
  if (ss->cancel_attach) {
   tset->ssid = ssid;
   ss->cancel_attach(tset);
  }
 } while_each_subsys_mask();
out_release_tset:
 spin_lock_irq(&css_set_lock);
 list_splice_init(&tset->dst_csets, &tset->src_csets);
 list_for_each_entry_safe(cset, tmp_cset, &tset->src_csets, mg_node) {
  list_splice_tail_init(&cset->mg_tasks, &cset->tasks);
  list_del_init(&cset->mg_node);
 }
 spin_unlock_irq(&css_set_lock);
 return ret;
}
static bool cgroup_may_migrate_to(struct cgroup *dst_cgrp)
{
 return !cgroup_on_dfl(dst_cgrp) || !cgroup_parent(dst_cgrp) ||
  !dst_cgrp->subtree_control;
}
static void cgroup_migrate_finish(struct list_head *preloaded_csets)
{
 struct css_set *cset, *tmp_cset;

 lockdep_assert_held(&cgroup_mutex);

 spin_lock_irq(&css_set_lock);
 list_for_each_entry_safe(cset, tmp_cset, preloaded_csets, mg_preload_node) {
  cset->mg_src_cgrp = NULL;
  cset->mg_dst_cgrp = NULL;
  cset->mg_dst_cset = NULL;
  list_del_init(&cset->mg_preload_node);
  put_css_set_locked(cset);
 }
 spin_unlock_irq(&css_set_lock);
}
static void cgroup_migrate_add_src(struct css_set *src_cset,
       struct cgroup *dst_cgrp,
       struct list_head *preloaded_csets)
{
 struct cgroup *src_cgrp;

 lockdep_assert_held(&cgroup_mutex);
 lockdep_assert_held(&css_set_lock);






 if (src_cset->dead)
  return;

 src_cgrp = cset_cgroup_from_root(src_cset, dst_cgrp->root);

 if (!list_empty(&src_cset->mg_preload_node))
  return;

 WARN_ON(src_cset->mg_src_cgrp);
 WARN_ON(src_cset->mg_dst_cgrp);
 WARN_ON(!list_empty(&src_cset->mg_tasks));
 WARN_ON(!list_empty(&src_cset->mg_node));

 src_cset->mg_src_cgrp = src_cgrp;
 src_cset->mg_dst_cgrp = dst_cgrp;
 get_css_set(src_cset);
 list_add(&src_cset->mg_preload_node, preloaded_csets);
}
static int cgroup_migrate_prepare_dst(struct list_head *preloaded_csets)
{
 LIST_HEAD(csets);
 struct css_set *src_cset, *tmp_cset;

 lockdep_assert_held(&cgroup_mutex);


 list_for_each_entry_safe(src_cset, tmp_cset, preloaded_csets, mg_preload_node) {
  struct css_set *dst_cset;

  dst_cset = find_css_set(src_cset, src_cset->mg_dst_cgrp);
  if (!dst_cset)
   goto err;

  WARN_ON_ONCE(src_cset->mg_dst_cset || dst_cset->mg_dst_cset);






  if (src_cset == dst_cset) {
   src_cset->mg_src_cgrp = NULL;
   src_cset->mg_dst_cgrp = NULL;
   list_del_init(&src_cset->mg_preload_node);
   put_css_set(src_cset);
   put_css_set(dst_cset);
   continue;
  }

  src_cset->mg_dst_cset = dst_cset;

  if (list_empty(&dst_cset->mg_preload_node))
   list_add(&dst_cset->mg_preload_node, &csets);
  else
   put_css_set(dst_cset);
 }

 list_splice_tail(&csets, preloaded_csets);
 return 0;
err:
 cgroup_migrate_finish(&csets);
 return -ENOMEM;
}
static int cgroup_migrate(struct task_struct *leader, bool threadgroup,
     struct cgroup_root *root)
{
 struct cgroup_taskset tset = CGROUP_TASKSET_INIT(tset);
 struct task_struct *task;






 spin_lock_irq(&css_set_lock);
 rcu_read_lock();
 task = leader;
 do {
  cgroup_taskset_add(task, &tset);
  if (!threadgroup)
   break;
 } while_each_thread(leader, task);
 rcu_read_unlock();
 spin_unlock_irq(&css_set_lock);

 return cgroup_taskset_migrate(&tset, root);
}
static int cgroup_attach_task(struct cgroup *dst_cgrp,
         struct task_struct *leader, bool threadgroup)
{
 LIST_HEAD(preloaded_csets);
 struct task_struct *task;
 int ret;

 if (!cgroup_may_migrate_to(dst_cgrp))
  return -EBUSY;


 spin_lock_irq(&css_set_lock);
 rcu_read_lock();
 task = leader;
 do {
  cgroup_migrate_add_src(task_css_set(task), dst_cgrp,
           &preloaded_csets);
  if (!threadgroup)
   break;
 } while_each_thread(leader, task);
 rcu_read_unlock();
 spin_unlock_irq(&css_set_lock);


 ret = cgroup_migrate_prepare_dst(&preloaded_csets);
 if (!ret)
  ret = cgroup_migrate(leader, threadgroup, dst_cgrp->root);

 cgroup_migrate_finish(&preloaded_csets);
 return ret;
}

static int cgroup_procs_write_permission(struct task_struct *task,
      struct cgroup *dst_cgrp,
      struct kernfs_open_file *of)
{
 const struct cred *cred = current_cred();
 const struct cred *tcred = get_task_cred(task);
 int ret = 0;





 if (!uid_eq(cred->euid, GLOBAL_ROOT_UID) &&
     !uid_eq(cred->euid, tcred->uid) &&
     !uid_eq(cred->euid, tcred->suid))
  ret = -EACCES;

 if (!ret && cgroup_on_dfl(dst_cgrp)) {
  struct super_block *sb = of->file->f_path.dentry->d_sb;
  struct cgroup *cgrp;
  struct inode *inode;

  spin_lock_irq(&css_set_lock);
  cgrp = task_cgroup_from_root(task, &cgrp_dfl_root);
  spin_unlock_irq(&css_set_lock);

  while (!cgroup_is_descendant(dst_cgrp, cgrp))
   cgrp = cgroup_parent(cgrp);

  ret = -ENOMEM;
  inode = kernfs_get_inode(sb, cgrp->procs_file.kn);
  if (inode) {
   ret = inode_permission(inode, MAY_WRITE);
   iput(inode);
  }
 }

 put_cred(tcred);
 return ret;
}






static ssize_t __cgroup_procs_write(struct kernfs_open_file *of, char *buf,
        size_t nbytes, loff_t off, bool threadgroup)
{
 struct task_struct *tsk;
 struct cgroup_subsys *ss;
 struct cgroup *cgrp;
 pid_t pid;
 int ssid, ret;

 if (kstrtoint(strstrip(buf), 0, &pid) || pid < 0)
  return -EINVAL;

 cgrp = cgroup_kn_lock_live(of->kn, false);
 if (!cgrp)
  return -ENODEV;

 percpu_down_write(&cgroup_threadgroup_rwsem);
 rcu_read_lock();
 if (pid) {
  tsk = find_task_by_vpid(pid);
  if (!tsk) {
   ret = -ESRCH;
   goto out_unlock_rcu;
  }
 } else {
  tsk = current;
 }

 if (threadgroup)
  tsk = tsk->group_leader;






 if (tsk == kthreadd_task || (tsk->flags & PF_NO_SETAFFINITY)) {
  ret = -EINVAL;
  goto out_unlock_rcu;
 }

 get_task_struct(tsk);
 rcu_read_unlock();

 ret = cgroup_procs_write_permission(tsk, cgrp, of);
 if (!ret)
  ret = cgroup_attach_task(cgrp, tsk, threadgroup);

 put_task_struct(tsk);
 goto out_unlock_threadgroup;

out_unlock_rcu:
 rcu_read_unlock();
out_unlock_threadgroup:
 percpu_up_write(&cgroup_threadgroup_rwsem);
 for_each_subsys(ss, ssid)
  if (ss->post_attach)
   ss->post_attach();
 cgroup_kn_unlock(of->kn);
 return ret ?: nbytes;
}






int cgroup_attach_task_all(struct task_struct *from, struct task_struct *tsk)
{
 struct cgroup_root *root;
 int retval = 0;

 mutex_lock(&cgroup_mutex);
 for_each_root(root) {
  struct cgroup *from_cgrp;

  if (root == &cgrp_dfl_root)
   continue;

  spin_lock_irq(&css_set_lock);
  from_cgrp = task_cgroup_from_root(from, root);
  spin_unlock_irq(&css_set_lock);

  retval = cgroup_attach_task(from_cgrp, tsk, false);
  if (retval)
   break;
 }
 mutex_unlock(&cgroup_mutex);

 return retval;
}
EXPORT_SYMBOL_GPL(cgroup_attach_task_all);

static ssize_t cgroup_tasks_write(struct kernfs_open_file *of,
      char *buf, size_t nbytes, loff_t off)
{
 return __cgroup_procs_write(of, buf, nbytes, off, false);
}

static ssize_t cgroup_procs_write(struct kernfs_open_file *of,
      char *buf, size_t nbytes, loff_t off)
{
 return __cgroup_procs_write(of, buf, nbytes, off, true);
}

static ssize_t cgroup_release_agent_write(struct kernfs_open_file *of,
       char *buf, size_t nbytes, loff_t off)
{
 struct cgroup *cgrp;

 BUILD_BUG_ON(sizeof(cgrp->root->release_agent_path) < PATH_MAX);

 cgrp = cgroup_kn_lock_live(of->kn, false);
 if (!cgrp)
  return -ENODEV;
 spin_lock(&release_agent_path_lock);
 strlcpy(cgrp->root->release_agent_path, strstrip(buf),
  sizeof(cgrp->root->release_agent_path));
 spin_unlock(&release_agent_path_lock);
 cgroup_kn_unlock(of->kn);
 return nbytes;
}

static int cgroup_release_agent_show(struct seq_file *seq, void *v)
{
 struct cgroup *cgrp = seq_css(seq)->cgroup;

 spin_lock(&release_agent_path_lock);
 seq_puts(seq, cgrp->root->release_agent_path);
 spin_unlock(&release_agent_path_lock);
 seq_putc(seq, '\n');
 return 0;
}

static int cgroup_sane_behavior_show(struct seq_file *seq, void *v)
{
 seq_puts(seq, "0\n");
 return 0;
}

static void cgroup_print_ss_mask(struct seq_file *seq, u16 ss_mask)
{
 struct cgroup_subsys *ss;
 bool printed = false;
 int ssid;

 do_each_subsys_mask(ss, ssid, ss_mask) {
  if (printed)
   seq_putc(seq, ' ');
  seq_printf(seq, "%s", ss->name);
  printed = true;
 } while_each_subsys_mask();
 if (printed)
  seq_putc(seq, '\n');
}


static int cgroup_controllers_show(struct seq_file *seq, void *v)
{
 struct cgroup *cgrp = seq_css(seq)->cgroup;

 cgroup_print_ss_mask(seq, cgroup_control(cgrp));
 return 0;
}


static int cgroup_subtree_control_show(struct seq_file *seq, void *v)
{
 struct cgroup *cgrp = seq_css(seq)->cgroup;

 cgroup_print_ss_mask(seq, cgrp->subtree_control);
 return 0;
}
static int cgroup_update_dfl_csses(struct cgroup *cgrp)
{
 LIST_HEAD(preloaded_csets);
 struct cgroup_taskset tset = CGROUP_TASKSET_INIT(tset);
 struct cgroup_subsys_state *d_css;
 struct cgroup *dsct;
 struct css_set *src_cset;
 int ret;

 lockdep_assert_held(&cgroup_mutex);

 percpu_down_write(&cgroup_threadgroup_rwsem);


 spin_lock_irq(&css_set_lock);
 cgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {
  struct cgrp_cset_link *link;

  list_for_each_entry(link, &dsct->cset_links, cset_link)
   cgroup_migrate_add_src(link->cset, dsct,
            &preloaded_csets);
 }
 spin_unlock_irq(&css_set_lock);


 ret = cgroup_migrate_prepare_dst(&preloaded_csets);
 if (ret)
  goto out_finish;

 spin_lock_irq(&css_set_lock);
 list_for_each_entry(src_cset, &preloaded_csets, mg_preload_node) {
  struct task_struct *task, *ntask;


  if (!src_cset->mg_src_cgrp)
   break;


  list_for_each_entry_safe(task, ntask, &src_cset->tasks, cg_list)
   cgroup_taskset_add(task, &tset);
 }
 spin_unlock_irq(&css_set_lock);

 ret = cgroup_taskset_migrate(&tset, cgrp->root);
out_finish:
 cgroup_migrate_finish(&preloaded_csets);
 percpu_up_write(&cgroup_threadgroup_rwsem);
 return ret;
}
static void cgroup_lock_and_drain_offline(struct cgroup *cgrp)
 __acquires(&cgroup_mutex)
{
 struct cgroup *dsct;
 struct cgroup_subsys_state *d_css;
 struct cgroup_subsys *ss;
 int ssid;

restart:
 mutex_lock(&cgroup_mutex);

 cgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {
  for_each_subsys(ss, ssid) {
   struct cgroup_subsys_state *css = cgroup_css(dsct, ss);
   DEFINE_WAIT(wait);

   if (!css || !percpu_ref_is_dying(&css->refcnt))
    continue;

   cgroup_get(dsct);
   prepare_to_wait(&dsct->offline_waitq, &wait,
     TASK_UNINTERRUPTIBLE);

   mutex_unlock(&cgroup_mutex);
   schedule();
   finish_wait(&dsct->offline_waitq, &wait);

   cgroup_put(dsct);
   goto restart;
  }
 }
}
static void cgroup_save_control(struct cgroup *cgrp)
{
 struct cgroup *dsct;
 struct cgroup_subsys_state *d_css;

 cgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {
  dsct->old_subtree_control = dsct->subtree_control;
  dsct->old_subtree_ss_mask = dsct->subtree_ss_mask;
 }
}
static void cgroup_propagate_control(struct cgroup *cgrp)
{
 struct cgroup *dsct;
 struct cgroup_subsys_state *d_css;

 cgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {
  dsct->subtree_control &= cgroup_control(dsct);
  dsct->subtree_ss_mask =
   cgroup_calc_subtree_ss_mask(dsct->subtree_control,
          cgroup_ss_mask(dsct));
 }
}
static void cgroup_restore_control(struct cgroup *cgrp)
{
 struct cgroup *dsct;
 struct cgroup_subsys_state *d_css;

 cgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {
  dsct->subtree_control = dsct->old_subtree_control;
  dsct->subtree_ss_mask = dsct->old_subtree_ss_mask;
 }
}

static bool css_visible(struct cgroup_subsys_state *css)
{
 struct cgroup_subsys *ss = css->ss;
 struct cgroup *cgrp = css->cgroup;

 if (cgroup_control(cgrp) & (1 << ss->id))
  return true;
 if (!(cgroup_ss_mask(cgrp) & (1 << ss->id)))
  return false;
 return cgroup_on_dfl(cgrp) && ss->implicit_on_dfl;
}
static int cgroup_apply_control_enable(struct cgroup *cgrp)
{
 struct cgroup *dsct;
 struct cgroup_subsys_state *d_css;
 struct cgroup_subsys *ss;
 int ssid, ret;

 cgroup_for_each_live_descendant_pre(dsct, d_css, cgrp) {
  for_each_subsys(ss, ssid) {
   struct cgroup_subsys_state *css = cgroup_css(dsct, ss);

   WARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));

   if (!(cgroup_ss_mask(dsct) & (1 << ss->id)))
    continue;

   if (!css) {
    css = css_create(dsct, ss);
    if (IS_ERR(css))
     return PTR_ERR(css);
   }

   if (css_visible(css)) {
    ret = css_populate_dir(css);
    if (ret)
     return ret;
   }
  }
 }

 return 0;
}
static void cgroup_apply_control_disable(struct cgroup *cgrp)
{
 struct cgroup *dsct;
 struct cgroup_subsys_state *d_css;
 struct cgroup_subsys *ss;
 int ssid;

 cgroup_for_each_live_descendant_post(dsct, d_css, cgrp) {
  for_each_subsys(ss, ssid) {
   struct cgroup_subsys_state *css = cgroup_css(dsct, ss);

   WARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));

   if (!css)
    continue;

   if (css->parent &&
       !(cgroup_ss_mask(dsct) & (1 << ss->id))) {
    kill_css(css);
   } else if (!css_visible(css)) {
    css_clear_dir(css);
    if (ss->css_reset)
     ss->css_reset(css);
   }
  }
 }
}
static int cgroup_apply_control(struct cgroup *cgrp)
{
 int ret;

 cgroup_propagate_control(cgrp);

 ret = cgroup_apply_control_enable(cgrp);
 if (ret)
  return ret;






 ret = cgroup_update_dfl_csses(cgrp);
 if (ret)
  return ret;

 return 0;
}
static void cgroup_finalize_control(struct cgroup *cgrp, int ret)
{
 if (ret) {
  cgroup_restore_control(cgrp);
  cgroup_propagate_control(cgrp);
 }

 cgroup_apply_control_disable(cgrp);
}


static ssize_t cgroup_subtree_control_write(struct kernfs_open_file *of,
         char *buf, size_t nbytes,
         loff_t off)
{
 u16 enable = 0, disable = 0;
 struct cgroup *cgrp, *child;
 struct cgroup_subsys *ss;
 char *tok;
 int ssid, ret;





 buf = strstrip(buf);
 while ((tok = strsep(&buf, " "))) {
  if (tok[0] == '\0')
   continue;
  do_each_subsys_mask(ss, ssid, ~cgrp_dfl_inhibit_ss_mask) {
   if (!cgroup_ssid_enabled(ssid) ||
       strcmp(tok + 1, ss->name))
    continue;

   if (*tok == '+') {
    enable |= 1 << ssid;
    disable &= ~(1 << ssid);
   } else if (*tok == '-') {
    disable |= 1 << ssid;
    enable &= ~(1 << ssid);
   } else {
    return -EINVAL;
   }
   break;
  } while_each_subsys_mask();
  if (ssid == CGROUP_SUBSYS_COUNT)
   return -EINVAL;
 }

 cgrp = cgroup_kn_lock_live(of->kn, true);
 if (!cgrp)
  return -ENODEV;

 for_each_subsys(ss, ssid) {
  if (enable & (1 << ssid)) {
   if (cgrp->subtree_control & (1 << ssid)) {
    enable &= ~(1 << ssid);
    continue;
   }

   if (!(cgroup_control(cgrp) & (1 << ssid))) {
    ret = -ENOENT;
    goto out_unlock;
   }
  } else if (disable & (1 << ssid)) {
   if (!(cgrp->subtree_control & (1 << ssid))) {
    disable &= ~(1 << ssid);
    continue;
   }


   cgroup_for_each_live_child(child, cgrp) {
    if (child->subtree_control & (1 << ssid)) {
     ret = -EBUSY;
     goto out_unlock;
    }
   }
  }
 }

 if (!enable && !disable) {
  ret = 0;
  goto out_unlock;
 }





 if (enable && cgroup_parent(cgrp) && !list_empty(&cgrp->cset_links)) {
  ret = -EBUSY;
  goto out_unlock;
 }


 cgroup_save_control(cgrp);

 cgrp->subtree_control |= enable;
 cgrp->subtree_control &= ~disable;

 ret = cgroup_apply_control(cgrp);

 cgroup_finalize_control(cgrp, ret);

 kernfs_activate(cgrp->kn);
 ret = 0;
out_unlock:
 cgroup_kn_unlock(of->kn);
 return ret ?: nbytes;
}

static int cgroup_events_show(struct seq_file *seq, void *v)
{
 seq_printf(seq, "populated %d\n",
     cgroup_is_populated(seq_css(seq)->cgroup));
 return 0;
}

static ssize_t cgroup_file_write(struct kernfs_open_file *of, char *buf,
     size_t nbytes, loff_t off)
{
 struct cgroup *cgrp = of->kn->parent->priv;
 struct cftype *cft = of->kn->priv;
 struct cgroup_subsys_state *css;
 int ret;

 if (cft->write)
  return cft->write(of, buf, nbytes, off);







 rcu_read_lock();
 css = cgroup_css(cgrp, cft->ss);
 rcu_read_unlock();

 if (cft->write_u64) {
  unsigned long long v;
  ret = kstrtoull(buf, 0, &v);
  if (!ret)
   ret = cft->write_u64(css, cft, v);
 } else if (cft->write_s64) {
  long long v;
  ret = kstrtoll(buf, 0, &v);
  if (!ret)
   ret = cft->write_s64(css, cft, v);
 } else {
  ret = -EINVAL;
 }

 return ret ?: nbytes;
}

static void *cgroup_seqfile_start(struct seq_file *seq, loff_t *ppos)
{
 return seq_cft(seq)->seq_start(seq, ppos);
}

static void *cgroup_seqfile_next(struct seq_file *seq, void *v, loff_t *ppos)
{
 return seq_cft(seq)->seq_next(seq, v, ppos);
}

static void cgroup_seqfile_stop(struct seq_file *seq, void *v)
{
 seq_cft(seq)->seq_stop(seq, v);
}

static int cgroup_seqfile_show(struct seq_file *m, void *arg)
{
 struct cftype *cft = seq_cft(m);
 struct cgroup_subsys_state *css = seq_css(m);

 if (cft->seq_show)
  return cft->seq_show(m, arg);

 if (cft->read_u64)
  seq_printf(m, "%llu\n", cft->read_u64(css, cft));
 else if (cft->read_s64)
  seq_printf(m, "%lld\n", cft->read_s64(css, cft));
 else
  return -EINVAL;
 return 0;
}

static struct kernfs_ops cgroup_kf_single_ops = {
 .atomic_write_len = PAGE_SIZE,
 .write = cgroup_file_write,
 .seq_show = cgroup_seqfile_show,
};

static struct kernfs_ops cgroup_kf_ops = {
 .atomic_write_len = PAGE_SIZE,
 .write = cgroup_file_write,
 .seq_start = cgroup_seqfile_start,
 .seq_next = cgroup_seqfile_next,
 .seq_stop = cgroup_seqfile_stop,
 .seq_show = cgroup_seqfile_show,
};




static int cgroup_rename(struct kernfs_node *kn, struct kernfs_node *new_parent,
    const char *new_name_str)
{
 struct cgroup *cgrp = kn->priv;
 int ret;

 if (kernfs_type(kn) != KERNFS_DIR)
  return -ENOTDIR;
 if (kn->parent != new_parent)
  return -EIO;





 if (cgroup_on_dfl(cgrp))
  return -EPERM;






 kernfs_break_active_protection(new_parent);
 kernfs_break_active_protection(kn);

 mutex_lock(&cgroup_mutex);

 ret = kernfs_rename(kn, new_parent, new_name_str);

 mutex_unlock(&cgroup_mutex);

 kernfs_unbreak_active_protection(kn);
 kernfs_unbreak_active_protection(new_parent);
 return ret;
}


static int cgroup_kn_set_ugid(struct kernfs_node *kn)
{
 struct iattr iattr = { .ia_valid = ATTR_UID | ATTR_GID,
          .ia_uid = current_fsuid(),
          .ia_gid = current_fsgid(), };

 if (uid_eq(iattr.ia_uid, GLOBAL_ROOT_UID) &&
     gid_eq(iattr.ia_gid, GLOBAL_ROOT_GID))
  return 0;

 return kernfs_setattr(kn, &iattr);
}

static int cgroup_add_file(struct cgroup_subsys_state *css, struct cgroup *cgrp,
      struct cftype *cft)
{
 char name[CGROUP_FILE_NAME_MAX];
 struct kernfs_node *kn;
 struct lock_class_key *key = NULL;
 int ret;

 key = &cft->lockdep_key;
 kn = __kernfs_create_file(cgrp->kn, cgroup_file_name(cgrp, cft, name),
      cgroup_file_mode(cft), 0, cft->kf_ops, cft,
      NULL, key);
 if (IS_ERR(kn))
  return PTR_ERR(kn);

 ret = cgroup_kn_set_ugid(kn);
 if (ret) {
  kernfs_remove(kn);
  return ret;
 }

 if (cft->file_offset) {
  struct cgroup_file *cfile = (void *)css + cft->file_offset;

  spin_lock_irq(&cgroup_file_kn_lock);
  cfile->kn = kn;
  spin_unlock_irq(&cgroup_file_kn_lock);
 }

 return 0;
}
static int cgroup_addrm_files(struct cgroup_subsys_state *css,
         struct cgroup *cgrp, struct cftype cfts[],
         bool is_add)
{
 struct cftype *cft, *cft_end = NULL;
 int ret = 0;

 lockdep_assert_held(&cgroup_mutex);

restart:
 for (cft = cfts; cft != cft_end && cft->name[0] != '\0'; cft++) {

  if ((cft->flags & __CFTYPE_ONLY_ON_DFL) && !cgroup_on_dfl(cgrp))
   continue;
  if ((cft->flags & __CFTYPE_NOT_ON_DFL) && cgroup_on_dfl(cgrp))
   continue;
  if ((cft->flags & CFTYPE_NOT_ON_ROOT) && !cgroup_parent(cgrp))
   continue;
  if ((cft->flags & CFTYPE_ONLY_ON_ROOT) && cgroup_parent(cgrp))
   continue;

  if (is_add) {
   ret = cgroup_add_file(css, cgrp, cft);
   if (ret) {
    pr_warn("%s: failed to add %s, err=%d\n",
     __func__, cft->name, ret);
    cft_end = cft;
    is_add = false;
    goto restart;
   }
  } else {
   cgroup_rm_file(cgrp, cft);
  }
 }
 return ret;
}

static int cgroup_apply_cftypes(struct cftype *cfts, bool is_add)
{
 LIST_HEAD(pending);
 struct cgroup_subsys *ss = cfts[0].ss;
 struct cgroup *root = &ss->root->cgrp;
 struct cgroup_subsys_state *css;
 int ret = 0;

 lockdep_assert_held(&cgroup_mutex);


 css_for_each_descendant_pre(css, cgroup_css(root, ss)) {
  struct cgroup *cgrp = css->cgroup;

  if (!(css->flags & CSS_VISIBLE))
   continue;

  ret = cgroup_addrm_files(css, cgrp, cfts, is_add);
  if (ret)
   break;
 }

 if (is_add && !ret)
  kernfs_activate(root->kn);
 return ret;
}

static void cgroup_exit_cftypes(struct cftype *cfts)
{
 struct cftype *cft;

 for (cft = cfts; cft->name[0] != '\0'; cft++) {

  if (cft->max_write_len && cft->max_write_len != PAGE_SIZE)
   kfree(cft->kf_ops);
  cft->kf_ops = NULL;
  cft->ss = NULL;


  cft->flags &= ~(__CFTYPE_ONLY_ON_DFL | __CFTYPE_NOT_ON_DFL);
 }
}

static int cgroup_init_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)
{
 struct cftype *cft;

 for (cft = cfts; cft->name[0] != '\0'; cft++) {
  struct kernfs_ops *kf_ops;

  WARN_ON(cft->ss || cft->kf_ops);

  if (cft->seq_start)
   kf_ops = &cgroup_kf_ops;
  else
   kf_ops = &cgroup_kf_single_ops;





  if (cft->max_write_len && cft->max_write_len != PAGE_SIZE) {
   kf_ops = kmemdup(kf_ops, sizeof(*kf_ops), GFP_KERNEL);
   if (!kf_ops) {
    cgroup_exit_cftypes(cfts);
    return -ENOMEM;
   }
   kf_ops->atomic_write_len = cft->max_write_len;
  }

  cft->kf_ops = kf_ops;
  cft->ss = ss;
 }

 return 0;
}

static int cgroup_rm_cftypes_locked(struct cftype *cfts)
{
 lockdep_assert_held(&cgroup_mutex);

 if (!cfts || !cfts[0].ss)
  return -ENOENT;

 list_del(&cfts->node);
 cgroup_apply_cftypes(cfts, false);
 cgroup_exit_cftypes(cfts);
 return 0;
}
int cgroup_rm_cftypes(struct cftype *cfts)
{
 int ret;

 mutex_lock(&cgroup_mutex);
 ret = cgroup_rm_cftypes_locked(cfts);
 mutex_unlock(&cgroup_mutex);
 return ret;
}
static int cgroup_add_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)
{
 int ret;

 if (!cgroup_ssid_enabled(ss->id))
  return 0;

 if (!cfts || cfts[0].name[0] == '\0')
  return 0;

 ret = cgroup_init_cftypes(ss, cfts);
 if (ret)
  return ret;

 mutex_lock(&cgroup_mutex);

 list_add_tail(&cfts->node, &ss->cfts);
 ret = cgroup_apply_cftypes(cfts, true);
 if (ret)
  cgroup_rm_cftypes_locked(cfts);

 mutex_unlock(&cgroup_mutex);
 return ret;
}
int cgroup_add_dfl_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)
{
 struct cftype *cft;

 for (cft = cfts; cft && cft->name[0] != '\0'; cft++)
  cft->flags |= __CFTYPE_ONLY_ON_DFL;
 return cgroup_add_cftypes(ss, cfts);
}
int cgroup_add_legacy_cftypes(struct cgroup_subsys *ss, struct cftype *cfts)
{
 struct cftype *cft;

 for (cft = cfts; cft && cft->name[0] != '\0'; cft++)
  cft->flags |= __CFTYPE_NOT_ON_DFL;
 return cgroup_add_cftypes(ss, cfts);
}







void cgroup_file_notify(struct cgroup_file *cfile)
{
 unsigned long flags;

 spin_lock_irqsave(&cgroup_file_kn_lock, flags);
 if (cfile->kn)
  kernfs_notify(cfile->kn);
 spin_unlock_irqrestore(&cgroup_file_kn_lock, flags);
}







static int cgroup_task_count(const struct cgroup *cgrp)
{
 int count = 0;
 struct cgrp_cset_link *link;

 spin_lock_irq(&css_set_lock);
 list_for_each_entry(link, &cgrp->cset_links, cset_link)
  count += atomic_read(&link->cset->refcount);
 spin_unlock_irq(&css_set_lock);
 return count;
}
struct cgroup_subsys_state *css_next_child(struct cgroup_subsys_state *pos,
        struct cgroup_subsys_state *parent)
{
 struct cgroup_subsys_state *next;

 cgroup_assert_mutex_or_rcu_locked();
 if (!pos) {
  next = list_entry_rcu(parent->children.next, struct cgroup_subsys_state, sibling);
 } else if (likely(!(pos->flags & CSS_RELEASED))) {
  next = list_entry_rcu(pos->sibling.next, struct cgroup_subsys_state, sibling);
 } else {
  list_for_each_entry_rcu(next, &parent->children, sibling)
   if (next->serial_nr > pos->serial_nr)
    break;
 }





 if (&next->sibling != &parent->children)
  return next;
 return NULL;
}
struct cgroup_subsys_state *
css_next_descendant_pre(struct cgroup_subsys_state *pos,
   struct cgroup_subsys_state *root)
{
 struct cgroup_subsys_state *next;

 cgroup_assert_mutex_or_rcu_locked();


 if (!pos)
  return root;


 next = css_next_child(NULL, pos);
 if (next)
  return next;


 while (pos != root) {
  next = css_next_child(pos, pos->parent);
  if (next)
   return next;
  pos = pos->parent;
 }

 return NULL;
}
struct cgroup_subsys_state *
css_rightmost_descendant(struct cgroup_subsys_state *pos)
{
 struct cgroup_subsys_state *last, *tmp;

 cgroup_assert_mutex_or_rcu_locked();

 do {
  last = pos;

  pos = NULL;
  css_for_each_child(tmp, last)
   pos = tmp;
 } while (pos);

 return last;
}

static struct cgroup_subsys_state *
css_leftmost_descendant(struct cgroup_subsys_state *pos)
{
 struct cgroup_subsys_state *last;

 do {
  last = pos;
  pos = css_next_child(NULL, pos);
 } while (pos);

 return last;
}
struct cgroup_subsys_state *
css_next_descendant_post(struct cgroup_subsys_state *pos,
    struct cgroup_subsys_state *root)
{
 struct cgroup_subsys_state *next;

 cgroup_assert_mutex_or_rcu_locked();


 if (!pos)
  return css_leftmost_descendant(root);


 if (pos == root)
  return NULL;


 next = css_next_child(pos, pos->parent);
 if (next)
  return css_leftmost_descendant(next);


 return pos->parent;
}
bool css_has_online_children(struct cgroup_subsys_state *css)
{
 struct cgroup_subsys_state *child;
 bool ret = false;

 rcu_read_lock();
 css_for_each_child(child, css) {
  if (child->flags & CSS_ONLINE) {
   ret = true;
   break;
  }
 }
 rcu_read_unlock();
 return ret;
}







static void css_task_iter_advance_css_set(struct css_task_iter *it)
{
 struct list_head *l = it->cset_pos;
 struct cgrp_cset_link *link;
 struct css_set *cset;

 lockdep_assert_held(&css_set_lock);


 do {
  l = l->next;
  if (l == it->cset_head) {
   it->cset_pos = NULL;
   it->task_pos = NULL;
   return;
  }

  if (it->ss) {
   cset = container_of(l, struct css_set,
         e_cset_node[it->ss->id]);
  } else {
   link = list_entry(l, struct cgrp_cset_link, cset_link);
   cset = link->cset;
  }
 } while (!css_set_populated(cset));

 it->cset_pos = l;

 if (!list_empty(&cset->tasks))
  it->task_pos = cset->tasks.next;
 else
  it->task_pos = cset->mg_tasks.next;

 it->tasks_head = &cset->tasks;
 it->mg_tasks_head = &cset->mg_tasks;
 if (it->cur_cset) {
  list_del(&it->iters_node);
  put_css_set_locked(it->cur_cset);
 }
 get_css_set(cset);
 it->cur_cset = cset;
 list_add(&it->iters_node, &cset->task_iters);
}

static void css_task_iter_advance(struct css_task_iter *it)
{
 struct list_head *l = it->task_pos;

 lockdep_assert_held(&css_set_lock);
 WARN_ON_ONCE(!l);






 l = l->next;

 if (l == it->tasks_head)
  l = it->mg_tasks_head->next;

 if (l == it->mg_tasks_head)
  css_task_iter_advance_css_set(it);
 else
  it->task_pos = l;
}
void css_task_iter_start(struct cgroup_subsys_state *css,
    struct css_task_iter *it)
{

 WARN_ON_ONCE(!use_task_css_set_links);

 memset(it, 0, sizeof(*it));

 spin_lock_irq(&css_set_lock);

 it->ss = css->ss;

 if (it->ss)
  it->cset_pos = &css->cgroup->e_csets[css->ss->id];
 else
  it->cset_pos = &css->cgroup->cset_links;

 it->cset_head = it->cset_pos;

 css_task_iter_advance_css_set(it);

 spin_unlock_irq(&css_set_lock);
}
struct task_struct *css_task_iter_next(struct css_task_iter *it)
{
 if (it->cur_task) {
  put_task_struct(it->cur_task);
  it->cur_task = NULL;
 }

 spin_lock_irq(&css_set_lock);

 if (it->task_pos) {
  it->cur_task = list_entry(it->task_pos, struct task_struct,
       cg_list);
  get_task_struct(it->cur_task);
  css_task_iter_advance(it);
 }

 spin_unlock_irq(&css_set_lock);

 return it->cur_task;
}







void css_task_iter_end(struct css_task_iter *it)
{
 if (it->cur_cset) {
  spin_lock_irq(&css_set_lock);
  list_del(&it->iters_node);
  put_css_set_locked(it->cur_cset);
  spin_unlock_irq(&css_set_lock);
 }

 if (it->cur_task)
  put_task_struct(it->cur_task);
}
int cgroup_transfer_tasks(struct cgroup *to, struct cgroup *from)
{
 LIST_HEAD(preloaded_csets);
 struct cgrp_cset_link *link;
 struct css_task_iter it;
 struct task_struct *task;
 int ret;

 if (!cgroup_may_migrate_to(to))
  return -EBUSY;

 mutex_lock(&cgroup_mutex);


 spin_lock_irq(&css_set_lock);
 list_for_each_entry(link, &from->cset_links, cset_link)
  cgroup_migrate_add_src(link->cset, to, &preloaded_csets);
 spin_unlock_irq(&css_set_lock);

 ret = cgroup_migrate_prepare_dst(&preloaded_csets);
 if (ret)
  goto out_err;





 do {
  css_task_iter_start(&from->self, &it);
  task = css_task_iter_next(&it);
  if (task)
   get_task_struct(task);
  css_task_iter_end(&it);

  if (task) {
   ret = cgroup_migrate(task, false, to->root);
   put_task_struct(task);
  }
 } while (task && !ret);
out_err:
 cgroup_migrate_finish(&preloaded_csets);
 mutex_unlock(&cgroup_mutex);
 return ret;
}
enum cgroup_filetype {
 CGROUP_FILE_PROCS,
 CGROUP_FILE_TASKS,
};







struct cgroup_pidlist {




 struct { enum cgroup_filetype type; struct pid_namespace *ns; } key;

 pid_t *list;

 int length;

 struct list_head links;

 struct cgroup *owner;

 struct delayed_work destroy_dwork;
};






static void *pidlist_allocate(int count)
{
 if (PIDLIST_TOO_LARGE(count))
  return vmalloc(count * sizeof(pid_t));
 else
  return kmalloc(count * sizeof(pid_t), GFP_KERNEL);
}

static void pidlist_free(void *p)
{
 kvfree(p);
}





static void cgroup_pidlist_destroy_all(struct cgroup *cgrp)
{
 struct cgroup_pidlist *l, *tmp_l;

 mutex_lock(&cgrp->pidlist_mutex);
 list_for_each_entry_safe(l, tmp_l, &cgrp->pidlists, links)
  mod_delayed_work(cgroup_pidlist_destroy_wq, &l->destroy_dwork, 0);
 mutex_unlock(&cgrp->pidlist_mutex);

 flush_workqueue(cgroup_pidlist_destroy_wq);
 BUG_ON(!list_empty(&cgrp->pidlists));
}

static void cgroup_pidlist_destroy_work_fn(struct work_struct *work)
{
 struct delayed_work *dwork = to_delayed_work(work);
 struct cgroup_pidlist *l = container_of(dwork, struct cgroup_pidlist,
      destroy_dwork);
 struct cgroup_pidlist *tofree = NULL;

 mutex_lock(&l->owner->pidlist_mutex);





 if (!delayed_work_pending(dwork)) {
  list_del(&l->links);
  pidlist_free(l->list);
  put_pid_ns(l->key.ns);
  tofree = l;
 }

 mutex_unlock(&l->owner->pidlist_mutex);
 kfree(tofree);
}





static int pidlist_uniq(pid_t *list, int length)
{
 int src, dest = 1;





 if (length == 0 || length == 1)
  return length;

 for (src = 1; src < length; src++) {

  while (list[src] == list[src-1]) {
   src++;
   if (src == length)
    goto after;
  }

  list[dest] = list[src];
  dest++;
 }
after:
 return dest;
}
static pid_t pid_fry(pid_t pid)
{
 unsigned a = pid & 0x55555555;
 unsigned b = pid & 0xAAAAAAAA;

 return (a << 1) | (b >> 1);
}

static pid_t cgroup_pid_fry(struct cgroup *cgrp, pid_t pid)
{
 if (cgroup_on_dfl(cgrp))
  return pid_fry(pid);
 else
  return pid;
}

static int cmppid(const void *a, const void *b)
{
 return *(pid_t *)a - *(pid_t *)b;
}

static int fried_cmppid(const void *a, const void *b)
{
 return pid_fry(*(pid_t *)a) - pid_fry(*(pid_t *)b);
}

static struct cgroup_pidlist *cgroup_pidlist_find(struct cgroup *cgrp,
        enum cgroup_filetype type)
{
 struct cgroup_pidlist *l;

 struct pid_namespace *ns = task_active_pid_ns(current);

 lockdep_assert_held(&cgrp->pidlist_mutex);

 list_for_each_entry(l, &cgrp->pidlists, links)
  if (l->key.type == type && l->key.ns == ns)
   return l;
 return NULL;
}







static struct cgroup_pidlist *cgroup_pidlist_find_create(struct cgroup *cgrp,
      enum cgroup_filetype type)
{
 struct cgroup_pidlist *l;

 lockdep_assert_held(&cgrp->pidlist_mutex);

 l = cgroup_pidlist_find(cgrp, type);
 if (l)
  return l;


 l = kzalloc(sizeof(struct cgroup_pidlist), GFP_KERNEL);
 if (!l)
  return l;

 INIT_DELAYED_WORK(&l->destroy_dwork, cgroup_pidlist_destroy_work_fn);
 l->key.type = type;

 l->key.ns = get_pid_ns(task_active_pid_ns(current));
 l->owner = cgrp;
 list_add(&l->links, &cgrp->pidlists);
 return l;
}




static int pidlist_array_load(struct cgroup *cgrp, enum cgroup_filetype type,
         struct cgroup_pidlist **lp)
{
 pid_t *array;
 int length;
 int pid, n = 0;
 struct css_task_iter it;
 struct task_struct *tsk;
 struct cgroup_pidlist *l;

 lockdep_assert_held(&cgrp->pidlist_mutex);







 length = cgroup_task_count(cgrp);
 array = pidlist_allocate(length);
 if (!array)
  return -ENOMEM;

 css_task_iter_start(&cgrp->self, &it);
 while ((tsk = css_task_iter_next(&it))) {
  if (unlikely(n == length))
   break;

  if (type == CGROUP_FILE_PROCS)
   pid = task_tgid_vnr(tsk);
  else
   pid = task_pid_vnr(tsk);
  if (pid > 0)
   array[n++] = pid;
 }
 css_task_iter_end(&it);
 length = n;

 if (cgroup_on_dfl(cgrp))
  sort(array, length, sizeof(pid_t), fried_cmppid, NULL);
 else
  sort(array, length, sizeof(pid_t), cmppid, NULL);
 if (type == CGROUP_FILE_PROCS)
  length = pidlist_uniq(array, length);

 l = cgroup_pidlist_find_create(cgrp, type);
 if (!l) {
  pidlist_free(array);
  return -ENOMEM;
 }


 pidlist_free(l->list);
 l->list = array;
 l->length = length;
 *lp = l;
 return 0;
}
int cgroupstats_build(struct cgroupstats *stats, struct dentry *dentry)
{
 struct kernfs_node *kn = kernfs_node_from_dentry(dentry);
 struct cgroup *cgrp;
 struct css_task_iter it;
 struct task_struct *tsk;


 if (dentry->d_sb->s_type != &cgroup_fs_type || !kn ||
     kernfs_type(kn) != KERNFS_DIR)
  return -EINVAL;

 mutex_lock(&cgroup_mutex);






 rcu_read_lock();
 cgrp = rcu_dereference(kn->priv);
 if (!cgrp || cgroup_is_dead(cgrp)) {
  rcu_read_unlock();
  mutex_unlock(&cgroup_mutex);
  return -ENOENT;
 }
 rcu_read_unlock();

 css_task_iter_start(&cgrp->self, &it);
 while ((tsk = css_task_iter_next(&it))) {
  switch (tsk->state) {
  case TASK_RUNNING:
   stats->nr_running++;
   break;
  case TASK_INTERRUPTIBLE:
   stats->nr_sleeping++;
   break;
  case TASK_UNINTERRUPTIBLE:
   stats->nr_uninterruptible++;
   break;
  case TASK_STOPPED:
   stats->nr_stopped++;
   break;
  default:
   if (delayacct_is_task_waiting_on_io(tsk))
    stats->nr_io_wait++;
   break;
  }
 }
 css_task_iter_end(&it);

 mutex_unlock(&cgroup_mutex);
 return 0;
}
static void *cgroup_pidlist_start(struct seq_file *s, loff_t *pos)
{






 struct kernfs_open_file *of = s->private;
 struct cgroup *cgrp = seq_css(s)->cgroup;
 struct cgroup_pidlist *l;
 enum cgroup_filetype type = seq_cft(s)->private;
 int index = 0, pid = *pos;
 int *iter, ret;

 mutex_lock(&cgrp->pidlist_mutex);







 if (of->priv)
  of->priv = cgroup_pidlist_find(cgrp, type);





 if (!of->priv) {
  ret = pidlist_array_load(cgrp, type,
      (struct cgroup_pidlist **)&of->priv);
  if (ret)
   return ERR_PTR(ret);
 }
 l = of->priv;

 if (pid) {
  int end = l->length;

  while (index < end) {
   int mid = (index + end) / 2;
   if (cgroup_pid_fry(cgrp, l->list[mid]) == pid) {
    index = mid;
    break;
   } else if (cgroup_pid_fry(cgrp, l->list[mid]) <= pid)
    index = mid + 1;
   else
    end = mid;
  }
 }

 if (index >= l->length)
  return NULL;

 iter = l->list + index;
 *pos = cgroup_pid_fry(cgrp, *iter);
 return iter;
}

static void cgroup_pidlist_stop(struct seq_file *s, void *v)
{
 struct kernfs_open_file *of = s->private;
 struct cgroup_pidlist *l = of->priv;

 if (l)
  mod_delayed_work(cgroup_pidlist_destroy_wq, &l->destroy_dwork,
     CGROUP_PIDLIST_DESTROY_DELAY);
 mutex_unlock(&seq_css(s)->cgroup->pidlist_mutex);
}

static void *cgroup_pidlist_next(struct seq_file *s, void *v, loff_t *pos)
{
 struct kernfs_open_file *of = s->private;
 struct cgroup_pidlist *l = of->priv;
 pid_t *p = v;
 pid_t *end = l->list + l->length;




 p++;
 if (p >= end) {
  return NULL;
 } else {
  *pos = cgroup_pid_fry(seq_css(s)->cgroup, *p);
  return p;
 }
}

static int cgroup_pidlist_show(struct seq_file *s, void *v)
{
 seq_printf(s, "%d\n", *(int *)v);

 return 0;
}

static u64 cgroup_read_notify_on_release(struct cgroup_subsys_state *css,
      struct cftype *cft)
{
 return notify_on_release(css->cgroup);
}

static int cgroup_write_notify_on_release(struct cgroup_subsys_state *css,
       struct cftype *cft, u64 val)
{
 if (val)
  set_bit(CGRP_NOTIFY_ON_RELEASE, &css->cgroup->flags);
 else
  clear_bit(CGRP_NOTIFY_ON_RELEASE, &css->cgroup->flags);
 return 0;
}

static u64 cgroup_clone_children_read(struct cgroup_subsys_state *css,
          struct cftype *cft)
{
 return test_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags);
}

static int cgroup_clone_children_write(struct cgroup_subsys_state *css,
           struct cftype *cft, u64 val)
{
 if (val)
  set_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags);
 else
  clear_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags);
 return 0;
}


static struct cftype cgroup_dfl_base_files[] = {
 {
  .name = "cgroup.procs",
  .file_offset = offsetof(struct cgroup, procs_file),
  .seq_start = cgroup_pidlist_start,
  .seq_next = cgroup_pidlist_next,
  .seq_stop = cgroup_pidlist_stop,
  .seq_show = cgroup_pidlist_show,
  .private = CGROUP_FILE_PROCS,
  .write = cgroup_procs_write,
 },
 {
  .name = "cgroup.controllers",
  .seq_show = cgroup_controllers_show,
 },
 {
  .name = "cgroup.subtree_control",
  .seq_show = cgroup_subtree_control_show,
  .write = cgroup_subtree_control_write,
 },
 {
  .name = "cgroup.events",
  .flags = CFTYPE_NOT_ON_ROOT,
  .file_offset = offsetof(struct cgroup, events_file),
  .seq_show = cgroup_events_show,
 },
 { }
};


static struct cftype cgroup_legacy_base_files[] = {
 {
  .name = "cgroup.procs",
  .seq_start = cgroup_pidlist_start,
  .seq_next = cgroup_pidlist_next,
  .seq_stop = cgroup_pidlist_stop,
  .seq_show = cgroup_pidlist_show,
  .private = CGROUP_FILE_PROCS,
  .write = cgroup_procs_write,
 },
 {
  .name = "cgroup.clone_children",
  .read_u64 = cgroup_clone_children_read,
  .write_u64 = cgroup_clone_children_write,
 },
 {
  .name = "cgroup.sane_behavior",
  .flags = CFTYPE_ONLY_ON_ROOT,
  .seq_show = cgroup_sane_behavior_show,
 },
 {
  .name = "tasks",
  .seq_start = cgroup_pidlist_start,
  .seq_next = cgroup_pidlist_next,
  .seq_stop = cgroup_pidlist_stop,
  .seq_show = cgroup_pidlist_show,
  .private = CGROUP_FILE_TASKS,
  .write = cgroup_tasks_write,
 },
 {
  .name = "notify_on_release",
  .read_u64 = cgroup_read_notify_on_release,
  .write_u64 = cgroup_write_notify_on_release,
 },
 {
  .name = "release_agent",
  .flags = CFTYPE_ONLY_ON_ROOT,
  .seq_show = cgroup_release_agent_show,
  .write = cgroup_release_agent_write,
  .max_write_len = PATH_MAX - 1,
 },
 { }
};
static void css_free_work_fn(struct work_struct *work)
{
 struct cgroup_subsys_state *css =
  container_of(work, struct cgroup_subsys_state, destroy_work);
 struct cgroup_subsys *ss = css->ss;
 struct cgroup *cgrp = css->cgroup;

 percpu_ref_exit(&css->refcnt);

 if (ss) {

  struct cgroup_subsys_state *parent = css->parent;
  int id = css->id;

  ss->css_free(css);
  cgroup_idr_remove(&ss->css_idr, id);
  cgroup_put(cgrp);

  if (parent)
   css_put(parent);
 } else {

  atomic_dec(&cgrp->root->nr_cgrps);
  cgroup_pidlist_destroy_all(cgrp);
  cancel_work_sync(&cgrp->release_agent_work);

  if (cgroup_parent(cgrp)) {






   cgroup_put(cgroup_parent(cgrp));
   kernfs_put(cgrp->kn);
   kfree(cgrp);
  } else {





   cgroup_destroy_root(cgrp->root);
  }
 }
}

static void css_free_rcu_fn(struct rcu_head *rcu_head)
{
 struct cgroup_subsys_state *css =
  container_of(rcu_head, struct cgroup_subsys_state, rcu_head);

 INIT_WORK(&css->destroy_work, css_free_work_fn);
 queue_work(cgroup_destroy_wq, &css->destroy_work);
}

static void css_release_work_fn(struct work_struct *work)
{
 struct cgroup_subsys_state *css =
  container_of(work, struct cgroup_subsys_state, destroy_work);
 struct cgroup_subsys *ss = css->ss;
 struct cgroup *cgrp = css->cgroup;

 mutex_lock(&cgroup_mutex);

 css->flags |= CSS_RELEASED;
 list_del_rcu(&css->sibling);

 if (ss) {

  cgroup_idr_replace(&ss->css_idr, NULL, css->id);
  if (ss->css_released)
   ss->css_released(css);
 } else {

  cgroup_idr_remove(&cgrp->root->cgroup_idr, cgrp->id);
  cgrp->id = -1;
  if (cgrp->kn)
   RCU_INIT_POINTER(*(void __rcu __force **)&cgrp->kn->priv,
      NULL);
 }

 mutex_unlock(&cgroup_mutex);

 call_rcu(&css->rcu_head, css_free_rcu_fn);
}

static void css_release(struct percpu_ref *ref)
{
 struct cgroup_subsys_state *css =
  container_of(ref, struct cgroup_subsys_state, refcnt);

 INIT_WORK(&css->destroy_work, css_release_work_fn);
 queue_work(cgroup_destroy_wq, &css->destroy_work);
}

static void init_and_link_css(struct cgroup_subsys_state *css,
         struct cgroup_subsys *ss, struct cgroup *cgrp)
{
 lockdep_assert_held(&cgroup_mutex);

 cgroup_get(cgrp);

 memset(css, 0, sizeof(*css));
 css->cgroup = cgrp;
 css->ss = ss;
 css->id = -1;
 INIT_LIST_HEAD(&css->sibling);
 INIT_LIST_HEAD(&css->children);
 css->serial_nr = css_serial_nr_next++;
 atomic_set(&css->online_cnt, 0);

 if (cgroup_parent(cgrp)) {
  css->parent = cgroup_css(cgroup_parent(cgrp), ss);
  css_get(css->parent);
 }

 BUG_ON(cgroup_css(cgrp, ss));
}


static int online_css(struct cgroup_subsys_state *css)
{
 struct cgroup_subsys *ss = css->ss;
 int ret = 0;

 lockdep_assert_held(&cgroup_mutex);

 if (ss->css_online)
  ret = ss->css_online(css);
 if (!ret) {
  css->flags |= CSS_ONLINE;
  rcu_assign_pointer(css->cgroup->subsys[ss->id], css);

  atomic_inc(&css->online_cnt);
  if (css->parent)
   atomic_inc(&css->parent->online_cnt);
 }
 return ret;
}


static void offline_css(struct cgroup_subsys_state *css)
{
 struct cgroup_subsys *ss = css->ss;

 lockdep_assert_held(&cgroup_mutex);

 if (!(css->flags & CSS_ONLINE))
  return;

 if (ss->css_reset)
  ss->css_reset(css);

 if (ss->css_offline)
  ss->css_offline(css);

 css->flags &= ~CSS_ONLINE;
 RCU_INIT_POINTER(css->cgroup->subsys[ss->id], NULL);

 wake_up_all(&css->cgroup->offline_waitq);
}
static struct cgroup_subsys_state *css_create(struct cgroup *cgrp,
           struct cgroup_subsys *ss)
{
 struct cgroup *parent = cgroup_parent(cgrp);
 struct cgroup_subsys_state *parent_css = cgroup_css(parent, ss);
 struct cgroup_subsys_state *css;
 int err;

 lockdep_assert_held(&cgroup_mutex);

 css = ss->css_alloc(parent_css);
 if (IS_ERR(css))
  return css;

 init_and_link_css(css, ss, cgrp);

 err = percpu_ref_init(&css->refcnt, css_release, 0, GFP_KERNEL);
 if (err)
  goto err_free_css;

 err = cgroup_idr_alloc(&ss->css_idr, NULL, 2, 0, GFP_KERNEL);
 if (err < 0)
  goto err_free_css;
 css->id = err;


 list_add_tail_rcu(&css->sibling, &parent_css->children);
 cgroup_idr_replace(&ss->css_idr, css, css->id);

 err = online_css(css);
 if (err)
  goto err_list_del;

 if (ss->broken_hierarchy && !ss->warned_broken_hierarchy &&
     cgroup_parent(parent)) {
  pr_warn("%s (%d) created nested cgroup for controller \"%s\" which has incomplete hierarchy support. Nested cgroups may change behavior in the future.\n",
   current->comm, current->pid, ss->name);
  if (!strcmp(ss->name, "memory"))
   pr_warn("\"memory\" requires setting use_hierarchy to 1 on the root\n");
  ss->warned_broken_hierarchy = true;
 }

 return css;

err_list_del:
 list_del_rcu(&css->sibling);
err_free_css:
 call_rcu(&css->rcu_head, css_free_rcu_fn);
 return ERR_PTR(err);
}

static struct cgroup *cgroup_create(struct cgroup *parent)
{
 struct cgroup_root *root = parent->root;
 struct cgroup *cgrp, *tcgrp;
 int level = parent->level + 1;
 int ret;


 cgrp = kzalloc(sizeof(*cgrp) +
         sizeof(cgrp->ancestor_ids[0]) * (level + 1), GFP_KERNEL);
 if (!cgrp)
  return ERR_PTR(-ENOMEM);

 ret = percpu_ref_init(&cgrp->self.refcnt, css_release, 0, GFP_KERNEL);
 if (ret)
  goto out_free_cgrp;





 cgrp->id = cgroup_idr_alloc(&root->cgroup_idr, NULL, 2, 0, GFP_KERNEL);
 if (cgrp->id < 0) {
  ret = -ENOMEM;
  goto out_cancel_ref;
 }

 init_cgroup_housekeeping(cgrp);

 cgrp->self.parent = &parent->self;
 cgrp->root = root;
 cgrp->level = level;

 for (tcgrp = cgrp; tcgrp; tcgrp = cgroup_parent(tcgrp))
  cgrp->ancestor_ids[tcgrp->level] = tcgrp->id;

 if (notify_on_release(parent))
  set_bit(CGRP_NOTIFY_ON_RELEASE, &cgrp->flags);

 if (test_bit(CGRP_CPUSET_CLONE_CHILDREN, &parent->flags))
  set_bit(CGRP_CPUSET_CLONE_CHILDREN, &cgrp->flags);

 cgrp->self.serial_nr = css_serial_nr_next++;


 list_add_tail_rcu(&cgrp->self.sibling, &cgroup_parent(cgrp)->self.children);
 atomic_inc(&root->nr_cgrps);
 cgroup_get(parent);





 cgroup_idr_replace(&root->cgroup_idr, cgrp, cgrp->id);





 if (!cgroup_on_dfl(cgrp))
  cgrp->subtree_control = cgroup_control(cgrp);

 cgroup_propagate_control(cgrp);


 ret = cgroup_apply_control_enable(cgrp);
 if (ret)
  goto out_destroy;

 return cgrp;

out_cancel_ref:
 percpu_ref_exit(&cgrp->self.refcnt);
out_free_cgrp:
 kfree(cgrp);
 return ERR_PTR(ret);
out_destroy:
 cgroup_destroy_locked(cgrp);
 return ERR_PTR(ret);
}

static int cgroup_mkdir(struct kernfs_node *parent_kn, const char *name,
   umode_t mode)
{
 struct cgroup *parent, *cgrp;
 struct kernfs_node *kn;
 int ret;


 if (strchr(name, '\n'))
  return -EINVAL;

 parent = cgroup_kn_lock_live(parent_kn, false);
 if (!parent)
  return -ENODEV;

 cgrp = cgroup_create(parent);
 if (IS_ERR(cgrp)) {
  ret = PTR_ERR(cgrp);
  goto out_unlock;
 }


 kn = kernfs_create_dir(parent->kn, name, mode, cgrp);
 if (IS_ERR(kn)) {
  ret = PTR_ERR(kn);
  goto out_destroy;
 }
 cgrp->kn = kn;





 kernfs_get(kn);

 ret = cgroup_kn_set_ugid(kn);
 if (ret)
  goto out_destroy;

 ret = css_populate_dir(&cgrp->self);
 if (ret)
  goto out_destroy;

 ret = cgroup_apply_control_enable(cgrp);
 if (ret)
  goto out_destroy;


 kernfs_activate(kn);

 ret = 0;
 goto out_unlock;

out_destroy:
 cgroup_destroy_locked(cgrp);
out_unlock:
 cgroup_kn_unlock(parent_kn);
 return ret;
}






static void css_killed_work_fn(struct work_struct *work)
{
 struct cgroup_subsys_state *css =
  container_of(work, struct cgroup_subsys_state, destroy_work);

 mutex_lock(&cgroup_mutex);

 do {
  offline_css(css);
  css_put(css);

  css = css->parent;
 } while (css && atomic_dec_and_test(&css->online_cnt));

 mutex_unlock(&cgroup_mutex);
}


static void css_killed_ref_fn(struct percpu_ref *ref)
{
 struct cgroup_subsys_state *css =
  container_of(ref, struct cgroup_subsys_state, refcnt);

 if (atomic_dec_and_test(&css->online_cnt)) {
  INIT_WORK(&css->destroy_work, css_killed_work_fn);
  queue_work(cgroup_destroy_wq, &css->destroy_work);
 }
}
static void kill_css(struct cgroup_subsys_state *css)
{
 lockdep_assert_held(&cgroup_mutex);





 css_clear_dir(css);





 css_get(css);
 percpu_ref_kill_and_confirm(&css->refcnt, css_killed_ref_fn);
}
static int cgroup_destroy_locked(struct cgroup *cgrp)
 __releases(&cgroup_mutex) __acquires(&cgroup_mutex)
{
 struct cgroup_subsys_state *css;
 struct cgrp_cset_link *link;
 int ssid;

 lockdep_assert_held(&cgroup_mutex);





 if (cgroup_is_populated(cgrp))
  return -EBUSY;






 if (css_has_online_children(&cgrp->self))
  return -EBUSY;







 cgrp->self.flags &= ~CSS_ONLINE;

 spin_lock_irq(&css_set_lock);
 list_for_each_entry(link, &cgrp->cset_links, cset_link)
  link->cset->dead = true;
 spin_unlock_irq(&css_set_lock);


 for_each_css(css, ssid, cgrp)
  kill_css(css);





 kernfs_remove(cgrp->kn);

 check_for_release(cgroup_parent(cgrp));


 percpu_ref_kill(&cgrp->self.refcnt);

 return 0;
};

static int cgroup_rmdir(struct kernfs_node *kn)
{
 struct cgroup *cgrp;
 int ret = 0;

 cgrp = cgroup_kn_lock_live(kn, false);
 if (!cgrp)
  return 0;

 ret = cgroup_destroy_locked(cgrp);

 cgroup_kn_unlock(kn);
 return ret;
}

static struct kernfs_syscall_ops cgroup_kf_syscall_ops = {
 .remount_fs = cgroup_remount,
 .show_options = cgroup_show_options,
 .mkdir = cgroup_mkdir,
 .rmdir = cgroup_rmdir,
 .rename = cgroup_rename,
 .show_path = cgroup_show_path,
};

static void __init cgroup_init_subsys(struct cgroup_subsys *ss, bool early)
{
 struct cgroup_subsys_state *css;

 pr_debug("Initializing cgroup subsys %s\n", ss->name);

 mutex_lock(&cgroup_mutex);

 idr_init(&ss->css_idr);
 INIT_LIST_HEAD(&ss->cfts);


 ss->root = &cgrp_dfl_root;
 css = ss->css_alloc(cgroup_css(&cgrp_dfl_root.cgrp, ss));

 BUG_ON(IS_ERR(css));
 init_and_link_css(css, ss, &cgrp_dfl_root.cgrp);





 css->flags |= CSS_NO_REF;

 if (early) {

  css->id = 1;
 } else {
  css->id = cgroup_idr_alloc(&ss->css_idr, css, 1, 2, GFP_KERNEL);
  BUG_ON(css->id < 0);
 }





 init_css_set.subsys[ss->id] = css;

 have_fork_callback |= (bool)ss->fork << ss->id;
 have_exit_callback |= (bool)ss->exit << ss->id;
 have_free_callback |= (bool)ss->free << ss->id;
 have_canfork_callback |= (bool)ss->can_fork << ss->id;




 BUG_ON(!list_empty(&init_task.tasks));

 BUG_ON(online_css(css));

 mutex_unlock(&cgroup_mutex);
}







int __init cgroup_init_early(void)
{
 static struct cgroup_sb_opts __initdata opts;
 struct cgroup_subsys *ss;
 int i;

 init_cgroup_root(&cgrp_dfl_root, &opts);
 cgrp_dfl_root.cgrp.self.flags |= CSS_NO_REF;

 RCU_INIT_POINTER(init_task.cgroups, &init_css_set);

 for_each_subsys(ss, i) {
  WARN(!ss->css_alloc || !ss->css_free || ss->name || ss->id,
       "invalid cgroup_subsys %d:%s css_alloc=%p css_free=%p id:name=%d:%s\n",
       i, cgroup_subsys_name[i], ss->css_alloc, ss->css_free,
       ss->id, ss->name);
  WARN(strlen(cgroup_subsys_name[i]) > MAX_CGROUP_TYPE_NAMELEN,
       "cgroup_subsys_name %s too long\n", cgroup_subsys_name[i]);

  ss->id = i;
  ss->name = cgroup_subsys_name[i];
  if (!ss->legacy_name)
   ss->legacy_name = cgroup_subsys_name[i];

  if (ss->early_init)
   cgroup_init_subsys(ss, true);
 }
 return 0;
}

static u16 cgroup_disable_mask __initdata;







int __init cgroup_init(void)
{
 struct cgroup_subsys *ss;
 int ssid;

 BUILD_BUG_ON(CGROUP_SUBSYS_COUNT > 16);
 BUG_ON(percpu_init_rwsem(&cgroup_threadgroup_rwsem));
 BUG_ON(cgroup_init_cftypes(NULL, cgroup_dfl_base_files));
 BUG_ON(cgroup_init_cftypes(NULL, cgroup_legacy_base_files));

 get_user_ns(init_cgroup_ns.user_ns);

 mutex_lock(&cgroup_mutex);





 hash_add(css_set_table, &init_css_set.hlist,
   css_set_hash(init_css_set.subsys));

 BUG_ON(cgroup_setup_root(&cgrp_dfl_root, 0));

 mutex_unlock(&cgroup_mutex);

 for_each_subsys(ss, ssid) {
  if (ss->early_init) {
   struct cgroup_subsys_state *css =
    init_css_set.subsys[ss->id];

   css->id = cgroup_idr_alloc(&ss->css_idr, css, 1, 2,
         GFP_KERNEL);
   BUG_ON(css->id < 0);
  } else {
   cgroup_init_subsys(ss, false);
  }

  list_add_tail(&init_css_set.e_cset_node[ssid],
         &cgrp_dfl_root.cgrp.e_csets[ssid]);






  if (cgroup_disable_mask & (1 << ssid)) {
   static_branch_disable(cgroup_subsys_enabled_key[ssid]);
   printk(KERN_INFO "Disabling %s control group subsystem\n",
          ss->name);
   continue;
  }

  if (cgroup_ssid_no_v1(ssid))
   printk(KERN_INFO "Disabling %s control group subsystem in v1 mounts\n",
          ss->name);

  cgrp_dfl_root.subsys_mask |= 1 << ss->id;

  if (ss->implicit_on_dfl)
   cgrp_dfl_implicit_ss_mask |= 1 << ss->id;
  else if (!ss->dfl_cftypes)
   cgrp_dfl_inhibit_ss_mask |= 1 << ss->id;

  if (ss->dfl_cftypes == ss->legacy_cftypes) {
   WARN_ON(cgroup_add_cftypes(ss, ss->dfl_cftypes));
  } else {
   WARN_ON(cgroup_add_dfl_cftypes(ss, ss->dfl_cftypes));
   WARN_ON(cgroup_add_legacy_cftypes(ss, ss->legacy_cftypes));
  }

  if (ss->bind)
   ss->bind(init_css_set.subsys[ssid]);
 }


 hash_del(&init_css_set.hlist);
 hash_add(css_set_table, &init_css_set.hlist,
   css_set_hash(init_css_set.subsys));

 WARN_ON(sysfs_create_mount_point(fs_kobj, "cgroup"));
 WARN_ON(register_filesystem(&cgroup_fs_type));
 WARN_ON(register_filesystem(&cgroup2_fs_type));
 WARN_ON(!proc_create("cgroups", 0, NULL, &proc_cgroupstats_operations));

 return 0;
}

static int __init cgroup_wq_init(void)
{
 cgroup_destroy_wq = alloc_workqueue("cgroup_destroy", 0, 1);
 BUG_ON(!cgroup_destroy_wq);





 cgroup_pidlist_destroy_wq = alloc_workqueue("cgroup_pidlist_destroy",
          0, 1);
 BUG_ON(!cgroup_pidlist_destroy_wq);

 return 0;
}
core_initcall(cgroup_wq_init);






int proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,
       struct pid *pid, struct task_struct *tsk)
{
 char *buf, *path;
 int retval;
 struct cgroup_root *root;

 retval = -ENOMEM;
 buf = kmalloc(PATH_MAX, GFP_KERNEL);
 if (!buf)
  goto out;

 mutex_lock(&cgroup_mutex);
 spin_lock_irq(&css_set_lock);

 for_each_root(root) {
  struct cgroup_subsys *ss;
  struct cgroup *cgrp;
  int ssid, count = 0;

  if (root == &cgrp_dfl_root && !cgrp_dfl_visible)
   continue;

  seq_printf(m, "%d:", root->hierarchy_id);
  if (root != &cgrp_dfl_root)
   for_each_subsys(ss, ssid)
    if (root->subsys_mask & (1 << ssid))
     seq_printf(m, "%s%s", count++ ? "," : "",
         ss->legacy_name);
  if (strlen(root->name))
   seq_printf(m, "%sname=%s", count ? "," : "",
       root->name);
  seq_putc(m, ':');

  cgrp = task_cgroup_from_root(tsk, root);
  if (cgroup_on_dfl(cgrp) || !(tsk->flags & PF_EXITING)) {
   path = cgroup_path_ns_locked(cgrp, buf, PATH_MAX,
      current->nsproxy->cgroup_ns);
   if (!path) {
    retval = -ENAMETOOLONG;
    goto out_unlock;
   }
  } else {
   path = "/";
  }

  seq_puts(m, path);

  if (cgroup_on_dfl(cgrp) && cgroup_is_dead(cgrp))
   seq_puts(m, " (deleted)\n");
  else
   seq_putc(m, '\n');
 }

 retval = 0;
out_unlock:
 spin_unlock_irq(&css_set_lock);
 mutex_unlock(&cgroup_mutex);
 kfree(buf);
out:
 return retval;
}


static int proc_cgroupstats_show(struct seq_file *m, void *v)
{
 struct cgroup_subsys *ss;
 int i;

 seq_puts(m, "#subsys_name\thierarchy\tnum_cgroups\tenabled\n");





 mutex_lock(&cgroup_mutex);

 for_each_subsys(ss, i)
  seq_printf(m, "%s\t%d\t%d\t%d\n",
      ss->legacy_name, ss->root->hierarchy_id,
      atomic_read(&ss->root->nr_cgrps),
      cgroup_ssid_enabled(i));

 mutex_unlock(&cgroup_mutex);
 return 0;
}

static int cgroupstats_open(struct inode *inode, struct file *file)
{
 return single_open(file, proc_cgroupstats_show, NULL);
}

static const struct file_operations proc_cgroupstats_operations = {
 .open = cgroupstats_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
};
void cgroup_fork(struct task_struct *child)
{
 RCU_INIT_POINTER(child->cgroups, &init_css_set);
 INIT_LIST_HEAD(&child->cg_list);
}
int cgroup_can_fork(struct task_struct *child)
{
 struct cgroup_subsys *ss;
 int i, j, ret;

 do_each_subsys_mask(ss, i, have_canfork_callback) {
  ret = ss->can_fork(child);
  if (ret)
   goto out_revert;
 } while_each_subsys_mask();

 return 0;

out_revert:
 for_each_subsys(ss, j) {
  if (j >= i)
   break;
  if (ss->cancel_fork)
   ss->cancel_fork(child);
 }

 return ret;
}
void cgroup_cancel_fork(struct task_struct *child)
{
 struct cgroup_subsys *ss;
 int i;

 for_each_subsys(ss, i)
  if (ss->cancel_fork)
   ss->cancel_fork(child);
}
void cgroup_post_fork(struct task_struct *child)
{
 struct cgroup_subsys *ss;
 int i;
 if (use_task_css_set_links) {
  struct css_set *cset;

  spin_lock_irq(&css_set_lock);
  cset = task_css_set(current);
  if (list_empty(&child->cg_list)) {
   get_css_set(cset);
   css_set_move_task(child, NULL, cset, false);
  }
  spin_unlock_irq(&css_set_lock);
 }






 do_each_subsys_mask(ss, i, have_fork_callback) {
  ss->fork(child);
 } while_each_subsys_mask();
}
void cgroup_exit(struct task_struct *tsk)
{
 struct cgroup_subsys *ss;
 struct css_set *cset;
 int i;





 cset = task_css_set(tsk);

 if (!list_empty(&tsk->cg_list)) {
  spin_lock_irq(&css_set_lock);
  css_set_move_task(tsk, cset, NULL, false);
  spin_unlock_irq(&css_set_lock);
 } else {
  get_css_set(cset);
 }


 do_each_subsys_mask(ss, i, have_exit_callback) {
  ss->exit(tsk);
 } while_each_subsys_mask();
}

void cgroup_free(struct task_struct *task)
{
 struct css_set *cset = task_css_set(task);
 struct cgroup_subsys *ss;
 int ssid;

 do_each_subsys_mask(ss, ssid, have_free_callback) {
  ss->free(task);
 } while_each_subsys_mask();

 put_css_set(cset);
}

static void check_for_release(struct cgroup *cgrp)
{
 if (notify_on_release(cgrp) && !cgroup_is_populated(cgrp) &&
     !css_has_online_children(&cgrp->self) && !cgroup_is_dead(cgrp))
  schedule_work(&cgrp->release_agent_work);
}
static void cgroup_release_agent(struct work_struct *work)
{
 struct cgroup *cgrp =
  container_of(work, struct cgroup, release_agent_work);
 char *pathbuf = NULL, *agentbuf = NULL, *path;
 char *argv[3], *envp[3];

 mutex_lock(&cgroup_mutex);

 pathbuf = kmalloc(PATH_MAX, GFP_KERNEL);
 agentbuf = kstrdup(cgrp->root->release_agent_path, GFP_KERNEL);
 if (!pathbuf || !agentbuf)
  goto out;

 spin_lock_irq(&css_set_lock);
 path = cgroup_path_ns_locked(cgrp, pathbuf, PATH_MAX, &init_cgroup_ns);
 spin_unlock_irq(&css_set_lock);
 if (!path)
  goto out;

 argv[0] = agentbuf;
 argv[1] = path;
 argv[2] = NULL;


 envp[0] = "HOME=/";
 envp[1] = "PATH=/sbin:/bin:/usr/sbin:/usr/bin";
 envp[2] = NULL;

 mutex_unlock(&cgroup_mutex);
 call_usermodehelper(argv[0], argv, envp, UMH_WAIT_EXEC);
 goto out_free;
out:
 mutex_unlock(&cgroup_mutex);
out_free:
 kfree(agentbuf);
 kfree(pathbuf);
}

static int __init cgroup_disable(char *str)
{
 struct cgroup_subsys *ss;
 char *token;
 int i;

 while ((token = strsep(&str, ",")) != NULL) {
  if (!*token)
   continue;

  for_each_subsys(ss, i) {
   if (strcmp(token, ss->name) &&
       strcmp(token, ss->legacy_name))
    continue;
   cgroup_disable_mask |= 1 << i;
  }
 }
 return 1;
}
__setup("cgroup_disable=", cgroup_disable);

static int __init cgroup_no_v1(char *str)
{
 struct cgroup_subsys *ss;
 char *token;
 int i;

 while ((token = strsep(&str, ",")) != NULL) {
  if (!*token)
   continue;

  if (!strcmp(token, "all")) {
   cgroup_no_v1_mask = U16_MAX;
   break;
  }

  for_each_subsys(ss, i) {
   if (strcmp(token, ss->name) &&
       strcmp(token, ss->legacy_name))
    continue;

   cgroup_no_v1_mask |= 1 << i;
  }
 }
 return 1;
}
__setup("cgroup_no_v1=", cgroup_no_v1);
struct cgroup_subsys_state *css_tryget_online_from_dir(struct dentry *dentry,
             struct cgroup_subsys *ss)
{
 struct kernfs_node *kn = kernfs_node_from_dentry(dentry);
 struct file_system_type *s_type = dentry->d_sb->s_type;
 struct cgroup_subsys_state *css = NULL;
 struct cgroup *cgrp;


 if ((s_type != &cgroup_fs_type && s_type != &cgroup2_fs_type) ||
     !kn || kernfs_type(kn) != KERNFS_DIR)
  return ERR_PTR(-EBADF);

 rcu_read_lock();






 cgrp = rcu_dereference(kn->priv);
 if (cgrp)
  css = cgroup_css(cgrp, ss);

 if (!css || !css_tryget_online(css))
  css = ERR_PTR(-ENOENT);

 rcu_read_unlock();
 return css;
}
struct cgroup_subsys_state *css_from_id(int id, struct cgroup_subsys *ss)
{
 WARN_ON_ONCE(!rcu_read_lock_held());
 return id > 0 ? idr_find(&ss->css_idr, id) : NULL;
}
struct cgroup *cgroup_get_from_path(const char *path)
{
 struct kernfs_node *kn;
 struct cgroup *cgrp;

 mutex_lock(&cgroup_mutex);

 kn = kernfs_walk_and_get(cgrp_dfl_root.cgrp.kn, path);
 if (kn) {
  if (kernfs_type(kn) == KERNFS_DIR) {
   cgrp = kn->priv;
   cgroup_get(cgrp);
  } else {
   cgrp = ERR_PTR(-ENOTDIR);
  }
  kernfs_put(kn);
 } else {
  cgrp = ERR_PTR(-ENOENT);
 }

 mutex_unlock(&cgroup_mutex);
 return cgrp;
}
EXPORT_SYMBOL_GPL(cgroup_get_from_path);







DEFINE_SPINLOCK(cgroup_sk_update_lock);
static bool cgroup_sk_alloc_disabled __read_mostly;

void cgroup_sk_alloc_disable(void)
{
 if (cgroup_sk_alloc_disabled)
  return;
 pr_info("cgroup: disabling cgroup2 socket matching due to net_prio or net_cls activation\n");
 cgroup_sk_alloc_disabled = true;
}




void cgroup_sk_alloc(struct sock_cgroup_data *skcd)
{
 if (cgroup_sk_alloc_disabled)
  return;

 rcu_read_lock();

 while (true) {
  struct css_set *cset;

  cset = task_css_set(current);
  if (likely(cgroup_tryget(cset->dfl_cgrp))) {
   skcd->val = (unsigned long)cset->dfl_cgrp;
   break;
  }
  cpu_relax();
 }

 rcu_read_unlock();
}

void cgroup_sk_free(struct sock_cgroup_data *skcd)
{
 cgroup_put(sock_cgroup_ptr(skcd));
}




static struct cgroup_namespace *alloc_cgroup_ns(void)
{
 struct cgroup_namespace *new_ns;
 int ret;

 new_ns = kzalloc(sizeof(struct cgroup_namespace), GFP_KERNEL);
 if (!new_ns)
  return ERR_PTR(-ENOMEM);
 ret = ns_alloc_inum(&new_ns->ns);
 if (ret) {
  kfree(new_ns);
  return ERR_PTR(ret);
 }
 atomic_set(&new_ns->count, 1);
 new_ns->ns.ops = &cgroupns_operations;
 return new_ns;
}

void free_cgroup_ns(struct cgroup_namespace *ns)
{
 put_css_set(ns->root_cset);
 put_user_ns(ns->user_ns);
 ns_free_inum(&ns->ns);
 kfree(ns);
}
EXPORT_SYMBOL(free_cgroup_ns);

struct cgroup_namespace *copy_cgroup_ns(unsigned long flags,
     struct user_namespace *user_ns,
     struct cgroup_namespace *old_ns)
{
 struct cgroup_namespace *new_ns;
 struct css_set *cset;

 BUG_ON(!old_ns);

 if (!(flags & CLONE_NEWCGROUP)) {
  get_cgroup_ns(old_ns);
  return old_ns;
 }


 if (!ns_capable(user_ns, CAP_SYS_ADMIN))
  return ERR_PTR(-EPERM);

 mutex_lock(&cgroup_mutex);
 spin_lock_irq(&css_set_lock);

 cset = task_css_set(current);
 get_css_set(cset);

 spin_unlock_irq(&css_set_lock);
 mutex_unlock(&cgroup_mutex);

 new_ns = alloc_cgroup_ns();
 if (IS_ERR(new_ns)) {
  put_css_set(cset);
  return new_ns;
 }

 new_ns->user_ns = get_user_ns(user_ns);
 new_ns->root_cset = cset;

 return new_ns;
}

static inline struct cgroup_namespace *to_cg_ns(struct ns_common *ns)
{
 return container_of(ns, struct cgroup_namespace, ns);
}

static int cgroupns_install(struct nsproxy *nsproxy, struct ns_common *ns)
{
 struct cgroup_namespace *cgroup_ns = to_cg_ns(ns);

 if (!ns_capable(current_user_ns(), CAP_SYS_ADMIN) ||
     !ns_capable(cgroup_ns->user_ns, CAP_SYS_ADMIN))
  return -EPERM;


 if (cgroup_ns == nsproxy->cgroup_ns)
  return 0;

 get_cgroup_ns(cgroup_ns);
 put_cgroup_ns(nsproxy->cgroup_ns);
 nsproxy->cgroup_ns = cgroup_ns;

 return 0;
}

static struct ns_common *cgroupns_get(struct task_struct *task)
{
 struct cgroup_namespace *ns = NULL;
 struct nsproxy *nsproxy;

 task_lock(task);
 nsproxy = task->nsproxy;
 if (nsproxy) {
  ns = nsproxy->cgroup_ns;
  get_cgroup_ns(ns);
 }
 task_unlock(task);

 return ns ? &ns->ns : NULL;
}

static void cgroupns_put(struct ns_common *ns)
{
 put_cgroup_ns(to_cg_ns(ns));
}

const struct proc_ns_operations cgroupns_operations = {
 .name = "cgroup",
 .type = CLONE_NEWCGROUP,
 .get = cgroupns_get,
 .put = cgroupns_put,
 .install = cgroupns_install,
};

static __init int cgroup_namespaces_init(void)
{
 return 0;
}
subsys_initcall(cgroup_namespaces_init);

static struct cgroup_subsys_state *
debug_css_alloc(struct cgroup_subsys_state *parent_css)
{
 struct cgroup_subsys_state *css = kzalloc(sizeof(*css), GFP_KERNEL);

 if (!css)
  return ERR_PTR(-ENOMEM);

 return css;
}

static void debug_css_free(struct cgroup_subsys_state *css)
{
 kfree(css);
}

static u64 debug_taskcount_read(struct cgroup_subsys_state *css,
    struct cftype *cft)
{
 return cgroup_task_count(css->cgroup);
}

static u64 current_css_set_read(struct cgroup_subsys_state *css,
    struct cftype *cft)
{
 return (u64)(unsigned long)current->cgroups;
}

static u64 current_css_set_refcount_read(struct cgroup_subsys_state *css,
      struct cftype *cft)
{
 u64 count;

 rcu_read_lock();
 count = atomic_read(&task_css_set(current)->refcount);
 rcu_read_unlock();
 return count;
}

static int current_css_set_cg_links_read(struct seq_file *seq, void *v)
{
 struct cgrp_cset_link *link;
 struct css_set *cset;
 char *name_buf;

 name_buf = kmalloc(NAME_MAX + 1, GFP_KERNEL);
 if (!name_buf)
  return -ENOMEM;

 spin_lock_irq(&css_set_lock);
 rcu_read_lock();
 cset = rcu_dereference(current->cgroups);
 list_for_each_entry(link, &cset->cgrp_links, cgrp_link) {
  struct cgroup *c = link->cgrp;

  cgroup_name(c, name_buf, NAME_MAX + 1);
  seq_printf(seq, "Root %d group %s\n",
      c->root->hierarchy_id, name_buf);
 }
 rcu_read_unlock();
 spin_unlock_irq(&css_set_lock);
 kfree(name_buf);
 return 0;
}

static int cgroup_css_links_read(struct seq_file *seq, void *v)
{
 struct cgroup_subsys_state *css = seq_css(seq);
 struct cgrp_cset_link *link;

 spin_lock_irq(&css_set_lock);
 list_for_each_entry(link, &css->cgroup->cset_links, cset_link) {
  struct css_set *cset = link->cset;
  struct task_struct *task;
  int count = 0;

  seq_printf(seq, "css_set %p\n", cset);

  list_for_each_entry(task, &cset->tasks, cg_list) {
   if (count++ > MAX_TASKS_SHOWN_PER_CSS)
    goto overflow;
   seq_printf(seq, "  task %d\n", task_pid_vnr(task));
  }

  list_for_each_entry(task, &cset->mg_tasks, cg_list) {
   if (count++ > MAX_TASKS_SHOWN_PER_CSS)
    goto overflow;
   seq_printf(seq, "  task %d\n", task_pid_vnr(task));
  }
  continue;
 overflow:
  seq_puts(seq, "  ...\n");
 }
 spin_unlock_irq(&css_set_lock);
 return 0;
}

static u64 releasable_read(struct cgroup_subsys_state *css, struct cftype *cft)
{
 return (!cgroup_is_populated(css->cgroup) &&
  !css_has_online_children(&css->cgroup->self));
}

static struct cftype debug_files[] = {
 {
  .name = "taskcount",
  .read_u64 = debug_taskcount_read,
 },

 {
  .name = "current_css_set",
  .read_u64 = current_css_set_read,
 },

 {
  .name = "current_css_set_refcount",
  .read_u64 = current_css_set_refcount_read,
 },

 {
  .name = "current_css_set_cg_links",
  .seq_show = current_css_set_cg_links_read,
 },

 {
  .name = "cgroup_css_links",
  .seq_show = cgroup_css_links_read,
 },

 {
  .name = "releasable",
  .read_u64 = releasable_read,
 },

 { }
};

struct cgroup_subsys debug_cgrp_subsys = {
 .css_alloc = debug_css_alloc,
 .css_free = debug_css_free,
 .legacy_cftypes = debug_files,
};
enum freezer_state_flags {
 CGROUP_FREEZER_ONLINE = (1 << 0),
 CGROUP_FREEZING_SELF = (1 << 1),
 CGROUP_FREEZING_PARENT = (1 << 2),
 CGROUP_FROZEN = (1 << 3),


 CGROUP_FREEZING = CGROUP_FREEZING_SELF | CGROUP_FREEZING_PARENT,
};

struct freezer {
 struct cgroup_subsys_state css;
 unsigned int state;
};

static DEFINE_MUTEX(freezer_mutex);

static inline struct freezer *css_freezer(struct cgroup_subsys_state *css)
{
 return css ? container_of(css, struct freezer, css) : NULL;
}

static inline struct freezer *task_freezer(struct task_struct *task)
{
 return css_freezer(task_css(task, freezer_cgrp_id));
}

static struct freezer *parent_freezer(struct freezer *freezer)
{
 return css_freezer(freezer->css.parent);
}

bool cgroup_freezing(struct task_struct *task)
{
 bool ret;

 rcu_read_lock();
 ret = task_freezer(task)->state & CGROUP_FREEZING;
 rcu_read_unlock();

 return ret;
}

static const char *freezer_state_strs(unsigned int state)
{
 if (state & CGROUP_FROZEN)
  return "FROZEN";
 if (state & CGROUP_FREEZING)
  return "FREEZING";
 return "THAWED";
};

static struct cgroup_subsys_state *
freezer_css_alloc(struct cgroup_subsys_state *parent_css)
{
 struct freezer *freezer;

 freezer = kzalloc(sizeof(struct freezer), GFP_KERNEL);
 if (!freezer)
  return ERR_PTR(-ENOMEM);

 return &freezer->css;
}
static int freezer_css_online(struct cgroup_subsys_state *css)
{
 struct freezer *freezer = css_freezer(css);
 struct freezer *parent = parent_freezer(freezer);

 mutex_lock(&freezer_mutex);

 freezer->state |= CGROUP_FREEZER_ONLINE;

 if (parent && (parent->state & CGROUP_FREEZING)) {
  freezer->state |= CGROUP_FREEZING_PARENT | CGROUP_FROZEN;
  atomic_inc(&system_freezing_cnt);
 }

 mutex_unlock(&freezer_mutex);
 return 0;
}
static void freezer_css_offline(struct cgroup_subsys_state *css)
{
 struct freezer *freezer = css_freezer(css);

 mutex_lock(&freezer_mutex);

 if (freezer->state & CGROUP_FREEZING)
  atomic_dec(&system_freezing_cnt);

 freezer->state = 0;

 mutex_unlock(&freezer_mutex);
}

static void freezer_css_free(struct cgroup_subsys_state *css)
{
 kfree(css_freezer(css));
}
static void freezer_attach(struct cgroup_taskset *tset)
{
 struct task_struct *task;
 struct cgroup_subsys_state *new_css;

 mutex_lock(&freezer_mutex);
 cgroup_taskset_for_each(task, new_css, tset) {
  struct freezer *freezer = css_freezer(new_css);

  if (!(freezer->state & CGROUP_FREEZING)) {
   __thaw_task(task);
  } else {
   freeze_task(task);

   while (freezer && (freezer->state & CGROUP_FROZEN)) {
    freezer->state &= ~CGROUP_FROZEN;
    freezer = parent_freezer(freezer);
   }
  }
 }

 mutex_unlock(&freezer_mutex);
}
static void freezer_fork(struct task_struct *task)
{
 struct freezer *freezer;
 if (task_css_is_root(task, freezer_cgrp_id))
  return;

 mutex_lock(&freezer_mutex);
 rcu_read_lock();

 freezer = task_freezer(task);
 if (freezer->state & CGROUP_FREEZING)
  freeze_task(task);

 rcu_read_unlock();
 mutex_unlock(&freezer_mutex);
}
static void update_if_frozen(struct cgroup_subsys_state *css)
{
 struct freezer *freezer = css_freezer(css);
 struct cgroup_subsys_state *pos;
 struct css_task_iter it;
 struct task_struct *task;

 lockdep_assert_held(&freezer_mutex);

 if (!(freezer->state & CGROUP_FREEZING) ||
     (freezer->state & CGROUP_FROZEN))
  return;


 rcu_read_lock();
 css_for_each_child(pos, css) {
  struct freezer *child = css_freezer(pos);

  if ((child->state & CGROUP_FREEZER_ONLINE) &&
      !(child->state & CGROUP_FROZEN)) {
   rcu_read_unlock();
   return;
  }
 }
 rcu_read_unlock();


 css_task_iter_start(css, &it);

 while ((task = css_task_iter_next(&it))) {
  if (freezing(task)) {






   if (!frozen(task) && !freezer_should_skip(task))
    goto out_iter_end;
  }
 }

 freezer->state |= CGROUP_FROZEN;
out_iter_end:
 css_task_iter_end(&it);
}

static int freezer_read(struct seq_file *m, void *v)
{
 struct cgroup_subsys_state *css = seq_css(m), *pos;

 mutex_lock(&freezer_mutex);
 rcu_read_lock();


 css_for_each_descendant_post(pos, css) {
  if (!css_tryget_online(pos))
   continue;
  rcu_read_unlock();

  update_if_frozen(pos);

  rcu_read_lock();
  css_put(pos);
 }

 rcu_read_unlock();
 mutex_unlock(&freezer_mutex);

 seq_puts(m, freezer_state_strs(css_freezer(css)->state));
 seq_putc(m, '\n');
 return 0;
}

static void freeze_cgroup(struct freezer *freezer)
{
 struct css_task_iter it;
 struct task_struct *task;

 css_task_iter_start(&freezer->css, &it);
 while ((task = css_task_iter_next(&it)))
  freeze_task(task);
 css_task_iter_end(&it);
}

static void unfreeze_cgroup(struct freezer *freezer)
{
 struct css_task_iter it;
 struct task_struct *task;

 css_task_iter_start(&freezer->css, &it);
 while ((task = css_task_iter_next(&it)))
  __thaw_task(task);
 css_task_iter_end(&it);
}
static void freezer_apply_state(struct freezer *freezer, bool freeze,
    unsigned int state)
{

 lockdep_assert_held(&freezer_mutex);

 if (!(freezer->state & CGROUP_FREEZER_ONLINE))
  return;

 if (freeze) {
  if (!(freezer->state & CGROUP_FREEZING))
   atomic_inc(&system_freezing_cnt);
  freezer->state |= state;
  freeze_cgroup(freezer);
 } else {
  bool was_freezing = freezer->state & CGROUP_FREEZING;

  freezer->state &= ~state;

  if (!(freezer->state & CGROUP_FREEZING)) {
   if (was_freezing)
    atomic_dec(&system_freezing_cnt);
   freezer->state &= ~CGROUP_FROZEN;
   unfreeze_cgroup(freezer);
  }
 }
}
static void freezer_change_state(struct freezer *freezer, bool freeze)
{
 struct cgroup_subsys_state *pos;






 mutex_lock(&freezer_mutex);
 rcu_read_lock();
 css_for_each_descendant_pre(pos, &freezer->css) {
  struct freezer *pos_f = css_freezer(pos);
  struct freezer *parent = parent_freezer(pos_f);

  if (!css_tryget_online(pos))
   continue;
  rcu_read_unlock();

  if (pos_f == freezer)
   freezer_apply_state(pos_f, freeze,
         CGROUP_FREEZING_SELF);
  else
   freezer_apply_state(pos_f,
         parent->state & CGROUP_FREEZING,
         CGROUP_FREEZING_PARENT);

  rcu_read_lock();
  css_put(pos);
 }
 rcu_read_unlock();
 mutex_unlock(&freezer_mutex);
}

static ssize_t freezer_write(struct kernfs_open_file *of,
        char *buf, size_t nbytes, loff_t off)
{
 bool freeze;

 buf = strstrip(buf);

 if (strcmp(buf, freezer_state_strs(0)) == 0)
  freeze = false;
 else if (strcmp(buf, freezer_state_strs(CGROUP_FROZEN)) == 0)
  freeze = true;
 else
  return -EINVAL;

 freezer_change_state(css_freezer(of_css(of)), freeze);
 return nbytes;
}

static u64 freezer_self_freezing_read(struct cgroup_subsys_state *css,
          struct cftype *cft)
{
 struct freezer *freezer = css_freezer(css);

 return (bool)(freezer->state & CGROUP_FREEZING_SELF);
}

static u64 freezer_parent_freezing_read(struct cgroup_subsys_state *css,
     struct cftype *cft)
{
 struct freezer *freezer = css_freezer(css);

 return (bool)(freezer->state & CGROUP_FREEZING_PARENT);
}

static struct cftype files[] = {
 {
  .name = "state",
  .flags = CFTYPE_NOT_ON_ROOT,
  .seq_show = freezer_read,
  .write = freezer_write,
 },
 {
  .name = "self_freezing",
  .flags = CFTYPE_NOT_ON_ROOT,
  .read_u64 = freezer_self_freezing_read,
 },
 {
  .name = "parent_freezing",
  .flags = CFTYPE_NOT_ON_ROOT,
  .read_u64 = freezer_parent_freezing_read,
 },
 { }
};

struct cgroup_subsys freezer_cgrp_subsys = {
 .css_alloc = freezer_css_alloc,
 .css_online = freezer_css_online,
 .css_offline = freezer_css_offline,
 .css_free = freezer_css_free,
 .attach = freezer_attach,
 .fork = freezer_fork,
 .legacy_cftypes = files,
};


struct pids_cgroup {
 struct cgroup_subsys_state css;





 atomic64_t counter;
 int64_t limit;
};

static struct pids_cgroup *css_pids(struct cgroup_subsys_state *css)
{
 return container_of(css, struct pids_cgroup, css);
}

static struct pids_cgroup *parent_pids(struct pids_cgroup *pids)
{
 return css_pids(pids->css.parent);
}

static struct cgroup_subsys_state *
pids_css_alloc(struct cgroup_subsys_state *parent)
{
 struct pids_cgroup *pids;

 pids = kzalloc(sizeof(struct pids_cgroup), GFP_KERNEL);
 if (!pids)
  return ERR_PTR(-ENOMEM);

 pids->limit = PIDS_MAX;
 atomic64_set(&pids->counter, 0);
 return &pids->css;
}

static void pids_css_free(struct cgroup_subsys_state *css)
{
 kfree(css_pids(css));
}
static void pids_cancel(struct pids_cgroup *pids, int num)
{




 WARN_ON_ONCE(atomic64_add_negative(-num, &pids->counter));
}






static void pids_uncharge(struct pids_cgroup *pids, int num)
{
 struct pids_cgroup *p;

 for (p = pids; parent_pids(p); p = parent_pids(p))
  pids_cancel(p, num);
}
static void pids_charge(struct pids_cgroup *pids, int num)
{
 struct pids_cgroup *p;

 for (p = pids; parent_pids(p); p = parent_pids(p))
  atomic64_add(num, &p->counter);
}
static int pids_try_charge(struct pids_cgroup *pids, int num)
{
 struct pids_cgroup *p, *q;

 for (p = pids; parent_pids(p); p = parent_pids(p)) {
  int64_t new = atomic64_add_return(num, &p->counter);






  if (new > p->limit)
   goto revert;
 }

 return 0;

revert:
 for (q = pids; q != p; q = parent_pids(q))
  pids_cancel(q, num);
 pids_cancel(p, num);

 return -EAGAIN;
}

static int pids_can_attach(struct cgroup_taskset *tset)
{
 struct task_struct *task;
 struct cgroup_subsys_state *dst_css;

 cgroup_taskset_for_each(task, dst_css, tset) {
  struct pids_cgroup *pids = css_pids(dst_css);
  struct cgroup_subsys_state *old_css;
  struct pids_cgroup *old_pids;






  old_css = task_css(task, pids_cgrp_id);
  old_pids = css_pids(old_css);

  pids_charge(pids, 1);
  pids_uncharge(old_pids, 1);
 }

 return 0;
}

static void pids_cancel_attach(struct cgroup_taskset *tset)
{
 struct task_struct *task;
 struct cgroup_subsys_state *dst_css;

 cgroup_taskset_for_each(task, dst_css, tset) {
  struct pids_cgroup *pids = css_pids(dst_css);
  struct cgroup_subsys_state *old_css;
  struct pids_cgroup *old_pids;

  old_css = task_css(task, pids_cgrp_id);
  old_pids = css_pids(old_css);

  pids_charge(old_pids, 1);
  pids_uncharge(pids, 1);
 }
}





static int pids_can_fork(struct task_struct *task)
{
 struct cgroup_subsys_state *css;
 struct pids_cgroup *pids;

 css = task_css_check(current, pids_cgrp_id, true);
 pids = css_pids(css);
 return pids_try_charge(pids, 1);
}

static void pids_cancel_fork(struct task_struct *task)
{
 struct cgroup_subsys_state *css;
 struct pids_cgroup *pids;

 css = task_css_check(current, pids_cgrp_id, true);
 pids = css_pids(css);
 pids_uncharge(pids, 1);
}

static void pids_free(struct task_struct *task)
{
 struct pids_cgroup *pids = css_pids(task_css(task, pids_cgrp_id));

 pids_uncharge(pids, 1);
}

static ssize_t pids_max_write(struct kernfs_open_file *of, char *buf,
         size_t nbytes, loff_t off)
{
 struct cgroup_subsys_state *css = of_css(of);
 struct pids_cgroup *pids = css_pids(css);
 int64_t limit;
 int err;

 buf = strstrip(buf);
 if (!strcmp(buf, PIDS_MAX_STR)) {
  limit = PIDS_MAX;
  goto set_limit;
 }

 err = kstrtoll(buf, 0, &limit);
 if (err)
  return err;

 if (limit < 0 || limit >= PIDS_MAX)
  return -EINVAL;

set_limit:




 pids->limit = limit;
 return nbytes;
}

static int pids_max_show(struct seq_file *sf, void *v)
{
 struct cgroup_subsys_state *css = seq_css(sf);
 struct pids_cgroup *pids = css_pids(css);
 int64_t limit = pids->limit;

 if (limit >= PIDS_MAX)
  seq_printf(sf, "%s\n", PIDS_MAX_STR);
 else
  seq_printf(sf, "%lld\n", limit);

 return 0;
}

static s64 pids_current_read(struct cgroup_subsys_state *css,
        struct cftype *cft)
{
 struct pids_cgroup *pids = css_pids(css);

 return atomic64_read(&pids->counter);
}

static struct cftype pids_files[] = {
 {
  .name = "max",
  .write = pids_max_write,
  .seq_show = pids_max_show,
  .flags = CFTYPE_NOT_ON_ROOT,
 },
 {
  .name = "current",
  .read_s64 = pids_current_read,
  .flags = CFTYPE_NOT_ON_ROOT,
 },
 { }
};

struct cgroup_subsys pids_cgrp_subsys = {
 .css_alloc = pids_css_alloc,
 .css_free = pids_css_free,
 .can_attach = pids_can_attach,
 .cancel_attach = pids_cancel_attach,
 .can_fork = pids_can_fork,
 .cancel_fork = pids_cancel_fork,
 .free = pids_free,
 .legacy_cftypes = pids_files,
 .dfl_cftypes = pids_files,
};



static irqreturn_t bad_chained_irq(int irq, void *dev_id)
{
 WARN_ONCE(1, "Chained irq %d should not call an action\n", irq);
 return IRQ_NONE;
}





struct irqaction chained_action = {
 .handler = bad_chained_irq,
};






int irq_set_chip(unsigned int irq, struct irq_chip *chip)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_lock(irq, &flags, 0);

 if (!desc)
  return -EINVAL;

 if (!chip)
  chip = &no_irq_chip;

 desc->irq_data.chip = chip;
 irq_put_desc_unlock(desc, flags);




 irq_mark_irq(irq);
 return 0;
}
EXPORT_SYMBOL(irq_set_chip);






int irq_set_irq_type(unsigned int irq, unsigned int type)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_buslock(irq, &flags, IRQ_GET_DESC_CHECK_GLOBAL);
 int ret = 0;

 if (!desc)
  return -EINVAL;

 type &= IRQ_TYPE_SENSE_MASK;
 ret = __irq_set_trigger(desc, type);
 irq_put_desc_busunlock(desc, flags);
 return ret;
}
EXPORT_SYMBOL(irq_set_irq_type);
int irq_set_handler_data(unsigned int irq, void *data)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_lock(irq, &flags, 0);

 if (!desc)
  return -EINVAL;
 desc->irq_common_data.handler_data = data;
 irq_put_desc_unlock(desc, flags);
 return 0;
}
EXPORT_SYMBOL(irq_set_handler_data);
int irq_set_msi_desc_off(unsigned int irq_base, unsigned int irq_offset,
    struct msi_desc *entry)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_lock(irq_base + irq_offset, &flags, IRQ_GET_DESC_CHECK_GLOBAL);

 if (!desc)
  return -EINVAL;
 desc->irq_common_data.msi_desc = entry;
 if (entry && !irq_offset)
  entry->irq = irq_base;
 irq_put_desc_unlock(desc, flags);
 return 0;
}
int irq_set_msi_desc(unsigned int irq, struct msi_desc *entry)
{
 return irq_set_msi_desc_off(irq, 0, entry);
}
int irq_set_chip_data(unsigned int irq, void *data)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_lock(irq, &flags, 0);

 if (!desc)
  return -EINVAL;
 desc->irq_data.chip_data = data;
 irq_put_desc_unlock(desc, flags);
 return 0;
}
EXPORT_SYMBOL(irq_set_chip_data);

struct irq_data *irq_get_irq_data(unsigned int irq)
{
 struct irq_desc *desc = irq_to_desc(irq);

 return desc ? &desc->irq_data : NULL;
}
EXPORT_SYMBOL_GPL(irq_get_irq_data);

static void irq_state_clr_disabled(struct irq_desc *desc)
{
 irqd_clear(&desc->irq_data, IRQD_IRQ_DISABLED);
}

static void irq_state_set_disabled(struct irq_desc *desc)
{
 irqd_set(&desc->irq_data, IRQD_IRQ_DISABLED);
}

static void irq_state_clr_masked(struct irq_desc *desc)
{
 irqd_clear(&desc->irq_data, IRQD_IRQ_MASKED);
}

static void irq_state_set_masked(struct irq_desc *desc)
{
 irqd_set(&desc->irq_data, IRQD_IRQ_MASKED);
}

int irq_startup(struct irq_desc *desc, bool resend)
{
 int ret = 0;

 irq_state_clr_disabled(desc);
 desc->depth = 0;

 irq_domain_activate_irq(&desc->irq_data);
 if (desc->irq_data.chip->irq_startup) {
  ret = desc->irq_data.chip->irq_startup(&desc->irq_data);
  irq_state_clr_masked(desc);
 } else {
  irq_enable(desc);
 }
 if (resend)
  check_irq_resend(desc);
 return ret;
}

void irq_shutdown(struct irq_desc *desc)
{
 irq_state_set_disabled(desc);
 desc->depth = 1;
 if (desc->irq_data.chip->irq_shutdown)
  desc->irq_data.chip->irq_shutdown(&desc->irq_data);
 else if (desc->irq_data.chip->irq_disable)
  desc->irq_data.chip->irq_disable(&desc->irq_data);
 else
  desc->irq_data.chip->irq_mask(&desc->irq_data);
 irq_domain_deactivate_irq(&desc->irq_data);
 irq_state_set_masked(desc);
}

void irq_enable(struct irq_desc *desc)
{
 irq_state_clr_disabled(desc);
 if (desc->irq_data.chip->irq_enable)
  desc->irq_data.chip->irq_enable(&desc->irq_data);
 else
  desc->irq_data.chip->irq_unmask(&desc->irq_data);
 irq_state_clr_masked(desc);
}
void irq_disable(struct irq_desc *desc)
{
 irq_state_set_disabled(desc);
 if (desc->irq_data.chip->irq_disable) {
  desc->irq_data.chip->irq_disable(&desc->irq_data);
  irq_state_set_masked(desc);
 } else if (irq_settings_disable_unlazy(desc)) {
  mask_irq(desc);
 }
}

void irq_percpu_enable(struct irq_desc *desc, unsigned int cpu)
{
 if (desc->irq_data.chip->irq_enable)
  desc->irq_data.chip->irq_enable(&desc->irq_data);
 else
  desc->irq_data.chip->irq_unmask(&desc->irq_data);
 cpumask_set_cpu(cpu, desc->percpu_enabled);
}

void irq_percpu_disable(struct irq_desc *desc, unsigned int cpu)
{
 if (desc->irq_data.chip->irq_disable)
  desc->irq_data.chip->irq_disable(&desc->irq_data);
 else
  desc->irq_data.chip->irq_mask(&desc->irq_data);
 cpumask_clear_cpu(cpu, desc->percpu_enabled);
}

static inline void mask_ack_irq(struct irq_desc *desc)
{
 if (desc->irq_data.chip->irq_mask_ack)
  desc->irq_data.chip->irq_mask_ack(&desc->irq_data);
 else {
  desc->irq_data.chip->irq_mask(&desc->irq_data);
  if (desc->irq_data.chip->irq_ack)
   desc->irq_data.chip->irq_ack(&desc->irq_data);
 }
 irq_state_set_masked(desc);
}

void mask_irq(struct irq_desc *desc)
{
 if (desc->irq_data.chip->irq_mask) {
  desc->irq_data.chip->irq_mask(&desc->irq_data);
  irq_state_set_masked(desc);
 }
}

void unmask_irq(struct irq_desc *desc)
{
 if (desc->irq_data.chip->irq_unmask) {
  desc->irq_data.chip->irq_unmask(&desc->irq_data);
  irq_state_clr_masked(desc);
 }
}

void unmask_threaded_irq(struct irq_desc *desc)
{
 struct irq_chip *chip = desc->irq_data.chip;

 if (chip->flags & IRQCHIP_EOI_THREADED)
  chip->irq_eoi(&desc->irq_data);

 if (chip->irq_unmask) {
  chip->irq_unmask(&desc->irq_data);
  irq_state_clr_masked(desc);
 }
}
void handle_nested_irq(unsigned int irq)
{
 struct irq_desc *desc = irq_to_desc(irq);
 struct irqaction *action;
 irqreturn_t action_ret;

 might_sleep();

 raw_spin_lock_irq(&desc->lock);

 desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);

 action = desc->action;
 if (unlikely(!action || irqd_irq_disabled(&desc->irq_data))) {
  desc->istate |= IRQS_PENDING;
  goto out_unlock;
 }

 kstat_incr_irqs_this_cpu(desc);
 irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
 raw_spin_unlock_irq(&desc->lock);

 action_ret = action->thread_fn(action->irq, action->dev_id);
 if (!noirqdebug)
  note_interrupt(desc, action_ret);

 raw_spin_lock_irq(&desc->lock);
 irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);

out_unlock:
 raw_spin_unlock_irq(&desc->lock);
}
EXPORT_SYMBOL_GPL(handle_nested_irq);

static bool irq_check_poll(struct irq_desc *desc)
{
 if (!(desc->istate & IRQS_POLL_INPROGRESS))
  return false;
 return irq_wait_for_poll(desc);
}

static bool irq_may_run(struct irq_desc *desc)
{
 unsigned int mask = IRQD_IRQ_INPROGRESS | IRQD_WAKEUP_ARMED;





 if (!irqd_has_set(&desc->irq_data, mask))
  return true;






 if (irq_pm_check_wakeup(desc))
  return false;




 return irq_check_poll(desc);
}
void handle_simple_irq(struct irq_desc *desc)
{
 raw_spin_lock(&desc->lock);

 if (!irq_may_run(desc))
  goto out_unlock;

 desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);

 if (unlikely(!desc->action || irqd_irq_disabled(&desc->irq_data))) {
  desc->istate |= IRQS_PENDING;
  goto out_unlock;
 }

 kstat_incr_irqs_this_cpu(desc);
 handle_irq_event(desc);

out_unlock:
 raw_spin_unlock(&desc->lock);
}
EXPORT_SYMBOL_GPL(handle_simple_irq);





static void cond_unmask_irq(struct irq_desc *desc)
{







 if (!irqd_irq_disabled(&desc->irq_data) &&
     irqd_irq_masked(&desc->irq_data) && !desc->threads_oneshot)
  unmask_irq(desc);
}
void handle_level_irq(struct irq_desc *desc)
{
 raw_spin_lock(&desc->lock);
 mask_ack_irq(desc);

 if (!irq_may_run(desc))
  goto out_unlock;

 desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);





 if (unlikely(!desc->action || irqd_irq_disabled(&desc->irq_data))) {
  desc->istate |= IRQS_PENDING;
  goto out_unlock;
 }

 kstat_incr_irqs_this_cpu(desc);
 handle_irq_event(desc);

 cond_unmask_irq(desc);

out_unlock:
 raw_spin_unlock(&desc->lock);
}
EXPORT_SYMBOL_GPL(handle_level_irq);

static inline void preflow_handler(struct irq_desc *desc)
{
 if (desc->preflow_handler)
  desc->preflow_handler(&desc->irq_data);
}
static inline void preflow_handler(struct irq_desc *desc) { }

static void cond_unmask_eoi_irq(struct irq_desc *desc, struct irq_chip *chip)
{
 if (!(desc->istate & IRQS_ONESHOT)) {
  chip->irq_eoi(&desc->irq_data);
  return;
 }






 if (!irqd_irq_disabled(&desc->irq_data) &&
     irqd_irq_masked(&desc->irq_data) && !desc->threads_oneshot) {
  chip->irq_eoi(&desc->irq_data);
  unmask_irq(desc);
 } else if (!(chip->flags & IRQCHIP_EOI_THREADED)) {
  chip->irq_eoi(&desc->irq_data);
 }
}
void handle_fasteoi_irq(struct irq_desc *desc)
{
 struct irq_chip *chip = desc->irq_data.chip;

 raw_spin_lock(&desc->lock);

 if (!irq_may_run(desc))
  goto out;

 desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);





 if (unlikely(!desc->action || irqd_irq_disabled(&desc->irq_data))) {
  desc->istate |= IRQS_PENDING;
  mask_irq(desc);
  goto out;
 }

 kstat_incr_irqs_this_cpu(desc);
 if (desc->istate & IRQS_ONESHOT)
  mask_irq(desc);

 preflow_handler(desc);
 handle_irq_event(desc);

 cond_unmask_eoi_irq(desc, chip);

 raw_spin_unlock(&desc->lock);
 return;
out:
 if (!(chip->flags & IRQCHIP_EOI_IF_HANDLED))
  chip->irq_eoi(&desc->irq_data);
 raw_spin_unlock(&desc->lock);
}
EXPORT_SYMBOL_GPL(handle_fasteoi_irq);
void handle_edge_irq(struct irq_desc *desc)
{
 raw_spin_lock(&desc->lock);

 desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);

 if (!irq_may_run(desc)) {
  desc->istate |= IRQS_PENDING;
  mask_ack_irq(desc);
  goto out_unlock;
 }





 if (irqd_irq_disabled(&desc->irq_data) || !desc->action) {
  desc->istate |= IRQS_PENDING;
  mask_ack_irq(desc);
  goto out_unlock;
 }

 kstat_incr_irqs_this_cpu(desc);


 desc->irq_data.chip->irq_ack(&desc->irq_data);

 do {
  if (unlikely(!desc->action)) {
   mask_irq(desc);
   goto out_unlock;
  }






  if (unlikely(desc->istate & IRQS_PENDING)) {
   if (!irqd_irq_disabled(&desc->irq_data) &&
       irqd_irq_masked(&desc->irq_data))
    unmask_irq(desc);
  }

  handle_irq_event(desc);

 } while ((desc->istate & IRQS_PENDING) &&
   !irqd_irq_disabled(&desc->irq_data));

out_unlock:
 raw_spin_unlock(&desc->lock);
}
EXPORT_SYMBOL(handle_edge_irq);








void handle_edge_eoi_irq(struct irq_desc *desc)
{
 struct irq_chip *chip = irq_desc_get_chip(desc);

 raw_spin_lock(&desc->lock);

 desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);

 if (!irq_may_run(desc)) {
  desc->istate |= IRQS_PENDING;
  goto out_eoi;
 }





 if (irqd_irq_disabled(&desc->irq_data) || !desc->action) {
  desc->istate |= IRQS_PENDING;
  goto out_eoi;
 }

 kstat_incr_irqs_this_cpu(desc);

 do {
  if (unlikely(!desc->action))
   goto out_eoi;

  handle_irq_event(desc);

 } while ((desc->istate & IRQS_PENDING) &&
   !irqd_irq_disabled(&desc->irq_data));

out_eoi:
 chip->irq_eoi(&desc->irq_data);
 raw_spin_unlock(&desc->lock);
}







void handle_percpu_irq(struct irq_desc *desc)
{
 struct irq_chip *chip = irq_desc_get_chip(desc);

 kstat_incr_irqs_this_cpu(desc);

 if (chip->irq_ack)
  chip->irq_ack(&desc->irq_data);

 handle_irq_event_percpu(desc);

 if (chip->irq_eoi)
  chip->irq_eoi(&desc->irq_data);
}
void handle_percpu_devid_irq(struct irq_desc *desc)
{
 struct irq_chip *chip = irq_desc_get_chip(desc);
 struct irqaction *action = desc->action;
 void *dev_id = raw_cpu_ptr(action->percpu_dev_id);
 unsigned int irq = irq_desc_get_irq(desc);
 irqreturn_t res;

 kstat_incr_irqs_this_cpu(desc);

 if (chip->irq_ack)
  chip->irq_ack(&desc->irq_data);

 trace_irq_handler_entry(irq, action);
 res = action->handler(irq, dev_id);
 trace_irq_handler_exit(irq, action, res);

 if (chip->irq_eoi)
  chip->irq_eoi(&desc->irq_data);
}

void
__irq_do_set_handler(struct irq_desc *desc, irq_flow_handler_t handle,
       int is_chained, const char *name)
{
 if (!handle) {
  handle = handle_bad_irq;
 } else {
  struct irq_data *irq_data = &desc->irq_data;







  while (irq_data) {
   if (irq_data->chip != &no_irq_chip)
    break;





   if (WARN_ON(is_chained))
    return;

   irq_data = irq_data->parent_data;
  }
  if (WARN_ON(!irq_data || irq_data->chip == &no_irq_chip))
   return;
 }


 if (handle == handle_bad_irq) {
  if (desc->irq_data.chip != &no_irq_chip)
   mask_ack_irq(desc);
  irq_state_set_disabled(desc);
  if (is_chained)
   desc->action = NULL;
  desc->depth = 1;
 }
 desc->handle_irq = handle;
 desc->name = name;

 if (handle != handle_bad_irq && is_chained) {
  irq_settings_set_noprobe(desc);
  irq_settings_set_norequest(desc);
  irq_settings_set_nothread(desc);
  desc->action = &chained_action;
  irq_startup(desc, true);
 }
}

void
__irq_set_handler(unsigned int irq, irq_flow_handler_t handle, int is_chained,
    const char *name)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_buslock(irq, &flags, 0);

 if (!desc)
  return;

 __irq_do_set_handler(desc, handle, is_chained, name);
 irq_put_desc_busunlock(desc, flags);
}
EXPORT_SYMBOL_GPL(__irq_set_handler);

void
irq_set_chained_handler_and_data(unsigned int irq, irq_flow_handler_t handle,
     void *data)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_buslock(irq, &flags, 0);

 if (!desc)
  return;

 __irq_do_set_handler(desc, handle, 1, NULL);
 desc->irq_common_data.handler_data = data;

 irq_put_desc_busunlock(desc, flags);
}
EXPORT_SYMBOL_GPL(irq_set_chained_handler_and_data);

void
irq_set_chip_and_handler_name(unsigned int irq, struct irq_chip *chip,
         irq_flow_handler_t handle, const char *name)
{
 irq_set_chip(irq, chip);
 __irq_set_handler(irq, handle, 0, name);
}
EXPORT_SYMBOL_GPL(irq_set_chip_and_handler_name);

void irq_modify_status(unsigned int irq, unsigned long clr, unsigned long set)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_lock(irq, &flags, 0);

 if (!desc)
  return;
 irq_settings_clr_and_set(desc, clr, set);

 irqd_clear(&desc->irq_data, IRQD_NO_BALANCING | IRQD_PER_CPU |
     IRQD_TRIGGER_MASK | IRQD_LEVEL | IRQD_MOVE_PCNTXT);
 if (irq_settings_has_no_balance_set(desc))
  irqd_set(&desc->irq_data, IRQD_NO_BALANCING);
 if (irq_settings_is_per_cpu(desc))
  irqd_set(&desc->irq_data, IRQD_PER_CPU);
 if (irq_settings_can_move_pcntxt(desc))
  irqd_set(&desc->irq_data, IRQD_MOVE_PCNTXT);
 if (irq_settings_is_level(desc))
  irqd_set(&desc->irq_data, IRQD_LEVEL);

 irqd_set(&desc->irq_data, irq_settings_get_trigger_mask(desc));

 irq_put_desc_unlock(desc, flags);
}
EXPORT_SYMBOL_GPL(irq_modify_status);







void irq_cpu_online(void)
{
 struct irq_desc *desc;
 struct irq_chip *chip;
 unsigned long flags;
 unsigned int irq;

 for_each_active_irq(irq) {
  desc = irq_to_desc(irq);
  if (!desc)
   continue;

  raw_spin_lock_irqsave(&desc->lock, flags);

  chip = irq_data_get_irq_chip(&desc->irq_data);
  if (chip && chip->irq_cpu_online &&
      (!(chip->flags & IRQCHIP_ONOFFLINE_ENABLED) ||
       !irqd_irq_disabled(&desc->irq_data)))
   chip->irq_cpu_online(&desc->irq_data);

  raw_spin_unlock_irqrestore(&desc->lock, flags);
 }
}







void irq_cpu_offline(void)
{
 struct irq_desc *desc;
 struct irq_chip *chip;
 unsigned long flags;
 unsigned int irq;

 for_each_active_irq(irq) {
  desc = irq_to_desc(irq);
  if (!desc)
   continue;

  raw_spin_lock_irqsave(&desc->lock, flags);

  chip = irq_data_get_irq_chip(&desc->irq_data);
  if (chip && chip->irq_cpu_offline &&
      (!(chip->flags & IRQCHIP_ONOFFLINE_ENABLED) ||
       !irqd_irq_disabled(&desc->irq_data)))
   chip->irq_cpu_offline(&desc->irq_data);

  raw_spin_unlock_irqrestore(&desc->lock, flags);
 }
}






void irq_chip_enable_parent(struct irq_data *data)
{
 data = data->parent_data;
 if (data->chip->irq_enable)
  data->chip->irq_enable(data);
 else
  data->chip->irq_unmask(data);
}






void irq_chip_disable_parent(struct irq_data *data)
{
 data = data->parent_data;
 if (data->chip->irq_disable)
  data->chip->irq_disable(data);
 else
  data->chip->irq_mask(data);
}





void irq_chip_ack_parent(struct irq_data *data)
{
 data = data->parent_data;
 data->chip->irq_ack(data);
}
EXPORT_SYMBOL_GPL(irq_chip_ack_parent);





void irq_chip_mask_parent(struct irq_data *data)
{
 data = data->parent_data;
 data->chip->irq_mask(data);
}
EXPORT_SYMBOL_GPL(irq_chip_mask_parent);





void irq_chip_unmask_parent(struct irq_data *data)
{
 data = data->parent_data;
 data->chip->irq_unmask(data);
}
EXPORT_SYMBOL_GPL(irq_chip_unmask_parent);





void irq_chip_eoi_parent(struct irq_data *data)
{
 data = data->parent_data;
 data->chip->irq_eoi(data);
}
EXPORT_SYMBOL_GPL(irq_chip_eoi_parent);
int irq_chip_set_affinity_parent(struct irq_data *data,
     const struct cpumask *dest, bool force)
{
 data = data->parent_data;
 if (data->chip->irq_set_affinity)
  return data->chip->irq_set_affinity(data, dest, force);

 return -ENOSYS;
}
int irq_chip_set_type_parent(struct irq_data *data, unsigned int type)
{
 data = data->parent_data;

 if (data->chip->irq_set_type)
  return data->chip->irq_set_type(data, type);

 return -ENOSYS;
}
EXPORT_SYMBOL_GPL(irq_chip_set_type_parent);
int irq_chip_retrigger_hierarchy(struct irq_data *data)
{
 for (data = data->parent_data; data; data = data->parent_data)
  if (data->chip && data->chip->irq_retrigger)
   return data->chip->irq_retrigger(data);

 return 0;
}






int irq_chip_set_vcpu_affinity_parent(struct irq_data *data, void *vcpu_info)
{
 data = data->parent_data;
 if (data->chip->irq_set_vcpu_affinity)
  return data->chip->irq_set_vcpu_affinity(data, vcpu_info);

 return -ENOSYS;
}
int irq_chip_set_wake_parent(struct irq_data *data, unsigned int on)
{
 data = data->parent_data;
 if (data->chip->irq_set_wake)
  return data->chip->irq_set_wake(data, on);

 return -ENOSYS;
}
int irq_chip_compose_msi_msg(struct irq_data *data, struct msi_msg *msg)
{
 struct irq_data *pos = NULL;

 for (; data; data = data->parent_data)
  if (data->chip && data->chip->irq_compose_msi_msg)
   pos = data;
 if (!pos)
  return -ENOSYS;

 pos->chip->irq_compose_msi_msg(pos, msg);

 return 0;
}



static LIST_HEAD(clockevent_devices);
static LIST_HEAD(clockevents_released);

static DEFINE_RAW_SPINLOCK(clockevents_lock);

static DEFINE_MUTEX(clockevents_mutex);

struct ce_unbind {
 struct clock_event_device *ce;
 int res;
};

static u64 cev_delta2ns(unsigned long latch, struct clock_event_device *evt,
   bool ismax)
{
 u64 clc = (u64) latch << evt->shift;
 u64 rnd;

 if (unlikely(!evt->mult)) {
  evt->mult = 1;
  WARN_ON(1);
 }
 rnd = (u64) evt->mult - 1;





 if ((clc >> evt->shift) != (u64)latch)
  clc = ~0ULL;
 if ((~0ULL - clc > rnd) &&
     (!ismax || evt->mult <= (1ULL << evt->shift)))
  clc += rnd;

 do_div(clc, evt->mult);


 return clc > 1000 ? clc : 1000;
}
u64 clockevent_delta2ns(unsigned long latch, struct clock_event_device *evt)
{
 return cev_delta2ns(latch, evt, false);
}
EXPORT_SYMBOL_GPL(clockevent_delta2ns);

static int __clockevents_switch_state(struct clock_event_device *dev,
          enum clock_event_state state)
{
 if (dev->features & CLOCK_EVT_FEAT_DUMMY)
  return 0;


 switch (state) {
 case CLOCK_EVT_STATE_DETACHED:


 case CLOCK_EVT_STATE_SHUTDOWN:
  if (dev->set_state_shutdown)
   return dev->set_state_shutdown(dev);
  return 0;

 case CLOCK_EVT_STATE_PERIODIC:

  if (!(dev->features & CLOCK_EVT_FEAT_PERIODIC))
   return -ENOSYS;
  if (dev->set_state_periodic)
   return dev->set_state_periodic(dev);
  return 0;

 case CLOCK_EVT_STATE_ONESHOT:

  if (!(dev->features & CLOCK_EVT_FEAT_ONESHOT))
   return -ENOSYS;
  if (dev->set_state_oneshot)
   return dev->set_state_oneshot(dev);
  return 0;

 case CLOCK_EVT_STATE_ONESHOT_STOPPED:

  if (WARN_ONCE(!clockevent_state_oneshot(dev),
         "Current state: %d\n",
         clockevent_get_state(dev)))
   return -EINVAL;

  if (dev->set_state_oneshot_stopped)
   return dev->set_state_oneshot_stopped(dev);
  else
   return -ENOSYS;

 default:
  return -ENOSYS;
 }
}
void clockevents_switch_state(struct clock_event_device *dev,
         enum clock_event_state state)
{
 if (clockevent_get_state(dev) != state) {
  if (__clockevents_switch_state(dev, state))
   return;

  clockevent_set_state(dev, state);





  if (clockevent_state_oneshot(dev)) {
   if (unlikely(!dev->mult)) {
    dev->mult = 1;
    WARN_ON(1);
   }
  }
 }
}





void clockevents_shutdown(struct clock_event_device *dev)
{
 clockevents_switch_state(dev, CLOCK_EVT_STATE_SHUTDOWN);
 dev->next_event.tv64 = KTIME_MAX;
}





int clockevents_tick_resume(struct clock_event_device *dev)
{
 int ret = 0;

 if (dev->tick_resume)
  ret = dev->tick_resume(dev);

 return ret;
}










static int clockevents_increase_min_delta(struct clock_event_device *dev)
{

 if (dev->min_delta_ns >= MIN_DELTA_LIMIT) {
  printk_deferred(KERN_WARNING
    "CE: Reprogramming failure. Giving up\n");
  dev->next_event.tv64 = KTIME_MAX;
  return -ETIME;
 }

 if (dev->min_delta_ns < 5000)
  dev->min_delta_ns = 5000;
 else
  dev->min_delta_ns += dev->min_delta_ns >> 1;

 if (dev->min_delta_ns > MIN_DELTA_LIMIT)
  dev->min_delta_ns = MIN_DELTA_LIMIT;

 printk_deferred(KERN_WARNING
   "CE: %s increased min_delta_ns to %llu nsec\n",
   dev->name ? dev->name : "?",
   (unsigned long long) dev->min_delta_ns);
 return 0;
}







static int clockevents_program_min_delta(struct clock_event_device *dev)
{
 unsigned long long clc;
 int64_t delta;
 int i;

 for (i = 0;;) {
  delta = dev->min_delta_ns;
  dev->next_event = ktime_add_ns(ktime_get(), delta);

  if (clockevent_state_shutdown(dev))
   return 0;

  dev->retries++;
  clc = ((unsigned long long) delta * dev->mult) >> dev->shift;
  if (dev->set_next_event((unsigned long) clc, dev) == 0)
   return 0;

  if (++i > 2) {





   if (clockevents_increase_min_delta(dev))
    return -ETIME;
   i = 0;
  }
 }
}








static int clockevents_program_min_delta(struct clock_event_device *dev)
{
 unsigned long long clc;
 int64_t delta;

 delta = dev->min_delta_ns;
 dev->next_event = ktime_add_ns(ktime_get(), delta);

 if (clockevent_state_shutdown(dev))
  return 0;

 dev->retries++;
 clc = ((unsigned long long) delta * dev->mult) >> dev->shift;
 return dev->set_next_event((unsigned long) clc, dev);
}

int clockevents_program_event(struct clock_event_device *dev, ktime_t expires,
         bool force)
{
 unsigned long long clc;
 int64_t delta;
 int rc;

 if (unlikely(expires.tv64 < 0)) {
  WARN_ON_ONCE(1);
  return -ETIME;
 }

 dev->next_event = expires;

 if (clockevent_state_shutdown(dev))
  return 0;


 WARN_ONCE(!clockevent_state_oneshot(dev), "Current state: %d\n",
    clockevent_get_state(dev));


 if (dev->features & CLOCK_EVT_FEAT_KTIME)
  return dev->set_next_ktime(expires, dev);

 delta = ktime_to_ns(ktime_sub(expires, ktime_get()));
 if (delta <= 0)
  return force ? clockevents_program_min_delta(dev) : -ETIME;

 delta = min(delta, (int64_t) dev->max_delta_ns);
 delta = max(delta, (int64_t) dev->min_delta_ns);

 clc = ((unsigned long long) delta * dev->mult) >> dev->shift;
 rc = dev->set_next_event((unsigned long) clc, dev);

 return (rc && force) ? clockevents_program_min_delta(dev) : rc;
}





static void clockevents_notify_released(void)
{
 struct clock_event_device *dev;

 while (!list_empty(&clockevents_released)) {
  dev = list_entry(clockevents_released.next,
     struct clock_event_device, list);
  list_del(&dev->list);
  list_add(&dev->list, &clockevent_devices);
  tick_check_new_device(dev);
 }
}




static int clockevents_replace(struct clock_event_device *ced)
{
 struct clock_event_device *dev, *newdev = NULL;

 list_for_each_entry(dev, &clockevent_devices, list) {
  if (dev == ced || !clockevent_state_detached(dev))
   continue;

  if (!tick_check_replacement(newdev, dev))
   continue;

  if (!try_module_get(dev->owner))
   continue;

  if (newdev)
   module_put(newdev->owner);
  newdev = dev;
 }
 if (newdev) {
  tick_install_replacement(newdev);
  list_del_init(&ced->list);
 }
 return newdev ? 0 : -EBUSY;
}




static int __clockevents_try_unbind(struct clock_event_device *ced, int cpu)
{

 if (clockevent_state_detached(ced)) {
  list_del_init(&ced->list);
  return 0;
 }

 return ced == per_cpu(tick_cpu_device, cpu).evtdev ? -EAGAIN : -EBUSY;
}




static void __clockevents_unbind(void *arg)
{
 struct ce_unbind *cu = arg;
 int res;

 raw_spin_lock(&clockevents_lock);
 res = __clockevents_try_unbind(cu->ce, smp_processor_id());
 if (res == -EAGAIN)
  res = clockevents_replace(cu->ce);
 cu->res = res;
 raw_spin_unlock(&clockevents_lock);
}





static int clockevents_unbind(struct clock_event_device *ced, int cpu)
{
 struct ce_unbind cu = { .ce = ced, .res = -ENODEV };

 smp_call_function_single(cpu, __clockevents_unbind, &cu, 1);
 return cu.res;
}




int clockevents_unbind_device(struct clock_event_device *ced, int cpu)
{
 int ret;

 mutex_lock(&clockevents_mutex);
 ret = clockevents_unbind(ced, cpu);
 mutex_unlock(&clockevents_mutex);
 return ret;
}
EXPORT_SYMBOL_GPL(clockevents_unbind_device);





void clockevents_register_device(struct clock_event_device *dev)
{
 unsigned long flags;


 clockevent_set_state(dev, CLOCK_EVT_STATE_DETACHED);

 if (!dev->cpumask) {
  WARN_ON(num_possible_cpus() > 1);
  dev->cpumask = cpumask_of(smp_processor_id());
 }

 raw_spin_lock_irqsave(&clockevents_lock, flags);

 list_add(&dev->list, &clockevent_devices);
 tick_check_new_device(dev);
 clockevents_notify_released();

 raw_spin_unlock_irqrestore(&clockevents_lock, flags);
}
EXPORT_SYMBOL_GPL(clockevents_register_device);

void clockevents_config(struct clock_event_device *dev, u32 freq)
{
 u64 sec;

 if (!(dev->features & CLOCK_EVT_FEAT_ONESHOT))
  return;






 sec = dev->max_delta_ticks;
 do_div(sec, freq);
 if (!sec)
  sec = 1;
 else if (sec > 600 && dev->max_delta_ticks > UINT_MAX)
  sec = 600;

 clockevents_calc_mult_shift(dev, freq, sec);
 dev->min_delta_ns = cev_delta2ns(dev->min_delta_ticks, dev, false);
 dev->max_delta_ns = cev_delta2ns(dev->max_delta_ticks, dev, true);
}
void clockevents_config_and_register(struct clock_event_device *dev,
         u32 freq, unsigned long min_delta,
         unsigned long max_delta)
{
 dev->min_delta_ticks = min_delta;
 dev->max_delta_ticks = max_delta;
 clockevents_config(dev, freq);
 clockevents_register_device(dev);
}
EXPORT_SYMBOL_GPL(clockevents_config_and_register);

int __clockevents_update_freq(struct clock_event_device *dev, u32 freq)
{
 clockevents_config(dev, freq);

 if (clockevent_state_oneshot(dev))
  return clockevents_program_event(dev, dev->next_event, false);

 if (clockevent_state_periodic(dev))
  return __clockevents_switch_state(dev, CLOCK_EVT_STATE_PERIODIC);

 return 0;
}
int clockevents_update_freq(struct clock_event_device *dev, u32 freq)
{
 unsigned long flags;
 int ret;

 local_irq_save(flags);
 ret = tick_broadcast_update_freq(dev, freq);
 if (ret == -ENODEV)
  ret = __clockevents_update_freq(dev, freq);
 local_irq_restore(flags);
 return ret;
}




void clockevents_handle_noop(struct clock_event_device *dev)
{
}
void clockevents_exchange_device(struct clock_event_device *old,
     struct clock_event_device *new)
{




 if (old) {
  module_put(old->owner);
  clockevents_switch_state(old, CLOCK_EVT_STATE_DETACHED);
  list_del(&old->list);
  list_add(&old->list, &clockevents_released);
 }

 if (new) {
  BUG_ON(!clockevent_state_detached(new));
  clockevents_shutdown(new);
 }
}




void clockevents_suspend(void)
{
 struct clock_event_device *dev;

 list_for_each_entry_reverse(dev, &clockevent_devices, list)
  if (dev->suspend && !clockevent_state_detached(dev))
   dev->suspend(dev);
}




void clockevents_resume(void)
{
 struct clock_event_device *dev;

 list_for_each_entry(dev, &clockevent_devices, list)
  if (dev->resume && !clockevent_state_detached(dev))
   dev->resume(dev);
}




void tick_cleanup_dead_cpu(int cpu)
{
 struct clock_event_device *dev, *tmp;
 unsigned long flags;

 raw_spin_lock_irqsave(&clockevents_lock, flags);

 tick_shutdown_broadcast_oneshot(cpu);
 tick_shutdown_broadcast(cpu);
 tick_shutdown(cpu);




 list_for_each_entry_safe(dev, tmp, &clockevents_released, list)
  list_del(&dev->list);



 list_for_each_entry_safe(dev, tmp, &clockevent_devices, list) {
  if (cpumask_test_cpu(cpu, dev->cpumask) &&
      cpumask_weight(dev->cpumask) == 1 &&
      !tick_is_broadcast_device(dev)) {
   BUG_ON(!clockevent_state_detached(dev));
   list_del(&dev->list);
  }
 }
 raw_spin_unlock_irqrestore(&clockevents_lock, flags);
}

struct bus_type clockevents_subsys = {
 .name = "clockevents",
 .dev_name = "clockevent",
};

static DEFINE_PER_CPU(struct device, tick_percpu_dev);
static struct tick_device *tick_get_tick_dev(struct device *dev);

static ssize_t sysfs_show_current_tick_dev(struct device *dev,
        struct device_attribute *attr,
        char *buf)
{
 struct tick_device *td;
 ssize_t count = 0;

 raw_spin_lock_irq(&clockevents_lock);
 td = tick_get_tick_dev(dev);
 if (td && td->evtdev)
  count = snprintf(buf, PAGE_SIZE, "%s\n", td->evtdev->name);
 raw_spin_unlock_irq(&clockevents_lock);
 return count;
}
static DEVICE_ATTR(current_device, 0444, sysfs_show_current_tick_dev, NULL);


static ssize_t sysfs_unbind_tick_dev(struct device *dev,
         struct device_attribute *attr,
         const char *buf, size_t count)
{
 char name[CS_NAME_LEN];
 ssize_t ret = sysfs_get_uname(buf, name, count);
 struct clock_event_device *ce;

 if (ret < 0)
  return ret;

 ret = -ENODEV;
 mutex_lock(&clockevents_mutex);
 raw_spin_lock_irq(&clockevents_lock);
 list_for_each_entry(ce, &clockevent_devices, list) {
  if (!strcmp(ce->name, name)) {
   ret = __clockevents_try_unbind(ce, dev->id);
   break;
  }
 }
 raw_spin_unlock_irq(&clockevents_lock);



 if (ret == -EAGAIN)
  ret = clockevents_unbind(ce, dev->id);
 mutex_unlock(&clockevents_mutex);
 return ret ? ret : count;
}
static DEVICE_ATTR(unbind_device, 0200, NULL, sysfs_unbind_tick_dev);

static struct device tick_bc_dev = {
 .init_name = "broadcast",
 .id = 0,
 .bus = &clockevents_subsys,
};

static struct tick_device *tick_get_tick_dev(struct device *dev)
{
 return dev == &tick_bc_dev ? tick_get_broadcast_device() :
  &per_cpu(tick_cpu_device, dev->id);
}

static __init int tick_broadcast_init_sysfs(void)
{
 int err = device_register(&tick_bc_dev);

 if (!err)
  err = device_create_file(&tick_bc_dev, &dev_attr_current_device);
 return err;
}
static struct tick_device *tick_get_tick_dev(struct device *dev)
{
 return &per_cpu(tick_cpu_device, dev->id);
}
static inline int tick_broadcast_init_sysfs(void) { return 0; }

static int __init tick_init_sysfs(void)
{
 int cpu;

 for_each_possible_cpu(cpu) {
  struct device *dev = &per_cpu(tick_percpu_dev, cpu);
  int err;

  dev->id = cpu;
  dev->bus = &clockevents_subsys;
  err = device_register(dev);
  if (!err)
   err = device_create_file(dev, &dev_attr_current_device);
  if (!err)
   err = device_create_file(dev, &dev_attr_unbind_device);
  if (err)
   return err;
 }
 return tick_broadcast_init_sysfs();
}

static int __init clockevents_init_sysfs(void)
{
 int err = subsys_system_register(&clockevents_subsys, NULL);

 if (!err)
  err = tick_init_sysfs();
 return err;
}
device_initcall(clockevents_init_sysfs);


void
clocks_calc_mult_shift(u32 *mult, u32 *shift, u32 from, u32 to, u32 maxsec)
{
 u64 tmp;
 u32 sft, sftacc= 32;





 tmp = ((u64)maxsec * from) >> 32;
 while (tmp) {
  tmp >>=1;
  sftacc--;
 }





 for (sft = 32; sft > 0; sft--) {
  tmp = (u64) to << sft;
  tmp += from / 2;
  do_div(tmp, from);
  if ((tmp >> sftacc) == 0)
   break;
 }
 *mult = tmp;
 *shift = sft;
}
static struct clocksource *curr_clocksource;
static LIST_HEAD(clocksource_list);
static DEFINE_MUTEX(clocksource_mutex);
static char override_name[CS_NAME_LEN];
static int finished_booting;

static void clocksource_watchdog_work(struct work_struct *work);
static void clocksource_select(void);

static LIST_HEAD(watchdog_list);
static struct clocksource *watchdog;
static struct timer_list watchdog_timer;
static DECLARE_WORK(watchdog_work, clocksource_watchdog_work);
static DEFINE_SPINLOCK(watchdog_lock);
static int watchdog_running;
static atomic_t watchdog_reset_pending;

static int clocksource_watchdog_kthread(void *data);
static void __clocksource_change_rating(struct clocksource *cs, int rating);





static void clocksource_watchdog_work(struct work_struct *work)
{




 kthread_run(clocksource_watchdog_kthread, NULL, "kwatchdog");
}

static void __clocksource_unstable(struct clocksource *cs)
{
 cs->flags &= ~(CLOCK_SOURCE_VALID_FOR_HRES | CLOCK_SOURCE_WATCHDOG);
 cs->flags |= CLOCK_SOURCE_UNSTABLE;
 if (finished_booting)
  schedule_work(&watchdog_work);
}
void clocksource_mark_unstable(struct clocksource *cs)
{
 unsigned long flags;

 spin_lock_irqsave(&watchdog_lock, flags);
 if (!(cs->flags & CLOCK_SOURCE_UNSTABLE)) {
  if (list_empty(&cs->wd_list))
   list_add(&cs->wd_list, &watchdog_list);
  __clocksource_unstable(cs);
 }
 spin_unlock_irqrestore(&watchdog_lock, flags);
}

static void clocksource_watchdog(unsigned long data)
{
 struct clocksource *cs;
 cycle_t csnow, wdnow, cslast, wdlast, delta;
 int64_t wd_nsec, cs_nsec;
 int next_cpu, reset_pending;

 spin_lock(&watchdog_lock);
 if (!watchdog_running)
  goto out;

 reset_pending = atomic_read(&watchdog_reset_pending);

 list_for_each_entry(cs, &watchdog_list, wd_list) {


  if (cs->flags & CLOCK_SOURCE_UNSTABLE) {
   if (finished_booting)
    schedule_work(&watchdog_work);
   continue;
  }

  local_irq_disable();
  csnow = cs->read(cs);
  wdnow = watchdog->read(watchdog);
  local_irq_enable();


  if (!(cs->flags & CLOCK_SOURCE_WATCHDOG) ||
      atomic_read(&watchdog_reset_pending)) {
   cs->flags |= CLOCK_SOURCE_WATCHDOG;
   cs->wd_last = wdnow;
   cs->cs_last = csnow;
   continue;
  }

  delta = clocksource_delta(wdnow, cs->wd_last, watchdog->mask);
  wd_nsec = clocksource_cyc2ns(delta, watchdog->mult,
          watchdog->shift);

  delta = clocksource_delta(csnow, cs->cs_last, cs->mask);
  cs_nsec = clocksource_cyc2ns(delta, cs->mult, cs->shift);
  wdlast = cs->wd_last;
  cslast = cs->cs_last;
  cs->cs_last = csnow;
  cs->wd_last = wdnow;

  if (atomic_read(&watchdog_reset_pending))
   continue;


  if (abs(cs_nsec - wd_nsec) > WATCHDOG_THRESHOLD) {
   pr_warn("timekeeping watchdog on CPU%d: Marking clocksource '%s' as unstable because the skew is too large:\n",
    smp_processor_id(), cs->name);
   pr_warn("                      '%s' wd_now: %llx wd_last: %llx mask: %llx\n",
    watchdog->name, wdnow, wdlast, watchdog->mask);
   pr_warn("                      '%s' cs_now: %llx cs_last: %llx mask: %llx\n",
    cs->name, csnow, cslast, cs->mask);
   __clocksource_unstable(cs);
   continue;
  }

  if (!(cs->flags & CLOCK_SOURCE_VALID_FOR_HRES) &&
      (cs->flags & CLOCK_SOURCE_IS_CONTINUOUS) &&
      (watchdog->flags & CLOCK_SOURCE_IS_CONTINUOUS)) {

   cs->flags |= CLOCK_SOURCE_VALID_FOR_HRES;





   if (!finished_booting)
    continue;
   if (cs != curr_clocksource) {
    cs->flags |= CLOCK_SOURCE_RESELECT;
    schedule_work(&watchdog_work);
   } else {
    tick_clock_notify();
   }
  }
 }





 if (reset_pending)
  atomic_dec(&watchdog_reset_pending);





 next_cpu = cpumask_next(raw_smp_processor_id(), cpu_online_mask);
 if (next_cpu >= nr_cpu_ids)
  next_cpu = cpumask_first(cpu_online_mask);
 watchdog_timer.expires += WATCHDOG_INTERVAL;
 add_timer_on(&watchdog_timer, next_cpu);
out:
 spin_unlock(&watchdog_lock);
}

static inline void clocksource_start_watchdog(void)
{
 if (watchdog_running || !watchdog || list_empty(&watchdog_list))
  return;
 init_timer(&watchdog_timer);
 watchdog_timer.function = clocksource_watchdog;
 watchdog_timer.expires = jiffies + WATCHDOG_INTERVAL;
 add_timer_on(&watchdog_timer, cpumask_first(cpu_online_mask));
 watchdog_running = 1;
}

static inline void clocksource_stop_watchdog(void)
{
 if (!watchdog_running || (watchdog && !list_empty(&watchdog_list)))
  return;
 del_timer(&watchdog_timer);
 watchdog_running = 0;
}

static inline void clocksource_reset_watchdog(void)
{
 struct clocksource *cs;

 list_for_each_entry(cs, &watchdog_list, wd_list)
  cs->flags &= ~CLOCK_SOURCE_WATCHDOG;
}

static void clocksource_resume_watchdog(void)
{
 atomic_inc(&watchdog_reset_pending);
}

static void clocksource_enqueue_watchdog(struct clocksource *cs)
{
 unsigned long flags;

 spin_lock_irqsave(&watchdog_lock, flags);
 if (cs->flags & CLOCK_SOURCE_MUST_VERIFY) {

  list_add(&cs->wd_list, &watchdog_list);
  cs->flags &= ~CLOCK_SOURCE_WATCHDOG;
 } else {

  if (cs->flags & CLOCK_SOURCE_IS_CONTINUOUS)
   cs->flags |= CLOCK_SOURCE_VALID_FOR_HRES;
 }
 spin_unlock_irqrestore(&watchdog_lock, flags);
}

static void clocksource_select_watchdog(bool fallback)
{
 struct clocksource *cs, *old_wd;
 unsigned long flags;

 spin_lock_irqsave(&watchdog_lock, flags);

 old_wd = watchdog;
 if (fallback)
  watchdog = NULL;

 list_for_each_entry(cs, &clocksource_list, list) {

  if (cs->flags & CLOCK_SOURCE_MUST_VERIFY)
   continue;


  if (fallback && cs == old_wd)
   continue;


  if (!watchdog || cs->rating > watchdog->rating)
   watchdog = cs;
 }

 if (!watchdog)
  watchdog = old_wd;


 if (watchdog != old_wd)
  clocksource_reset_watchdog();


 clocksource_start_watchdog();
 spin_unlock_irqrestore(&watchdog_lock, flags);
}

static void clocksource_dequeue_watchdog(struct clocksource *cs)
{
 unsigned long flags;

 spin_lock_irqsave(&watchdog_lock, flags);
 if (cs != watchdog) {
  if (cs->flags & CLOCK_SOURCE_MUST_VERIFY) {

   list_del_init(&cs->wd_list);

   clocksource_stop_watchdog();
  }
 }
 spin_unlock_irqrestore(&watchdog_lock, flags);
}

static int __clocksource_watchdog_kthread(void)
{
 struct clocksource *cs, *tmp;
 unsigned long flags;
 LIST_HEAD(unstable);
 int select = 0;

 spin_lock_irqsave(&watchdog_lock, flags);
 list_for_each_entry_safe(cs, tmp, &watchdog_list, wd_list) {
  if (cs->flags & CLOCK_SOURCE_UNSTABLE) {
   list_del_init(&cs->wd_list);
   list_add(&cs->wd_list, &unstable);
   select = 1;
  }
  if (cs->flags & CLOCK_SOURCE_RESELECT) {
   cs->flags &= ~CLOCK_SOURCE_RESELECT;
   select = 1;
  }
 }

 clocksource_stop_watchdog();
 spin_unlock_irqrestore(&watchdog_lock, flags);


 list_for_each_entry_safe(cs, tmp, &unstable, wd_list) {
  list_del_init(&cs->wd_list);
  __clocksource_change_rating(cs, 0);
 }
 return select;
}

static int clocksource_watchdog_kthread(void *data)
{
 mutex_lock(&clocksource_mutex);
 if (__clocksource_watchdog_kthread())
  clocksource_select();
 mutex_unlock(&clocksource_mutex);
 return 0;
}

static bool clocksource_is_watchdog(struct clocksource *cs)
{
 return cs == watchdog;
}


static void clocksource_enqueue_watchdog(struct clocksource *cs)
{
 if (cs->flags & CLOCK_SOURCE_IS_CONTINUOUS)
  cs->flags |= CLOCK_SOURCE_VALID_FOR_HRES;
}

static void clocksource_select_watchdog(bool fallback) { }
static inline void clocksource_dequeue_watchdog(struct clocksource *cs) { }
static inline void clocksource_resume_watchdog(void) { }
static inline int __clocksource_watchdog_kthread(void) { return 0; }
static bool clocksource_is_watchdog(struct clocksource *cs) { return false; }
void clocksource_mark_unstable(struct clocksource *cs) { }





void clocksource_suspend(void)
{
 struct clocksource *cs;

 list_for_each_entry_reverse(cs, &clocksource_list, list)
  if (cs->suspend)
   cs->suspend(cs);
}




void clocksource_resume(void)
{
 struct clocksource *cs;

 list_for_each_entry(cs, &clocksource_list, list)
  if (cs->resume)
   cs->resume(cs);

 clocksource_resume_watchdog();
}
void clocksource_touch_watchdog(void)
{
 clocksource_resume_watchdog();
}






static u32 clocksource_max_adjustment(struct clocksource *cs)
{
 u64 ret;



 ret = (u64)cs->mult * 11;
 do_div(ret,100);
 return (u32)ret;
}
u64 clocks_calc_max_nsecs(u32 mult, u32 shift, u32 maxadj, u64 mask, u64 *max_cyc)
{
 u64 max_nsecs, max_cycles;





 max_cycles = ULLONG_MAX;
 do_div(max_cycles, mult+maxadj);







 max_cycles = min(max_cycles, mask);
 max_nsecs = clocksource_cyc2ns(max_cycles, mult - maxadj, shift);


 if (max_cyc)
  *max_cyc = max_cycles;


 max_nsecs >>= 1;

 return max_nsecs;
}






static inline void clocksource_update_max_deferment(struct clocksource *cs)
{
 cs->max_idle_ns = clocks_calc_max_nsecs(cs->mult, cs->shift,
      cs->maxadj, cs->mask,
      &cs->max_cycles);
}


static struct clocksource *clocksource_find_best(bool oneshot, bool skipcur)
{
 struct clocksource *cs;

 if (!finished_booting || list_empty(&clocksource_list))
  return NULL;






 list_for_each_entry(cs, &clocksource_list, list) {
  if (skipcur && cs == curr_clocksource)
   continue;
  if (oneshot && !(cs->flags & CLOCK_SOURCE_VALID_FOR_HRES))
   continue;
  return cs;
 }
 return NULL;
}

static void __clocksource_select(bool skipcur)
{
 bool oneshot = tick_oneshot_mode_active();
 struct clocksource *best, *cs;


 best = clocksource_find_best(oneshot, skipcur);
 if (!best)
  return;


 list_for_each_entry(cs, &clocksource_list, list) {
  if (skipcur && cs == curr_clocksource)
   continue;
  if (strcmp(cs->name, override_name) != 0)
   continue;





  if (!(cs->flags & CLOCK_SOURCE_VALID_FOR_HRES) && oneshot) {

   pr_warn("Override clocksource %s is not HRT compatible - cannot switch while in HRT/NOHZ mode\n",
    cs->name);
   override_name[0] = 0;
  } else

   best = cs;
  break;
 }

 if (curr_clocksource != best && !timekeeping_notify(best)) {
  pr_info("Switched to clocksource %s\n", best->name);
  curr_clocksource = best;
 }
}
static void clocksource_select(void)
{
 __clocksource_select(false);
}

static void clocksource_select_fallback(void)
{
 __clocksource_select(true);
}

static inline void clocksource_select(void) { }
static inline void clocksource_select_fallback(void) { }

static int __init clocksource_done_booting(void)
{
 mutex_lock(&clocksource_mutex);
 curr_clocksource = clocksource_default_clock();
 finished_booting = 1;



 __clocksource_watchdog_kthread();
 clocksource_select();
 mutex_unlock(&clocksource_mutex);
 return 0;
}
fs_initcall(clocksource_done_booting);




static void clocksource_enqueue(struct clocksource *cs)
{
 struct list_head *entry = &clocksource_list;
 struct clocksource *tmp;

 list_for_each_entry(tmp, &clocksource_list, list)

  if (tmp->rating >= cs->rating)
   entry = &tmp->list;
 list_add(&cs->list, entry);
}
void __clocksource_update_freq_scale(struct clocksource *cs, u32 scale, u32 freq)
{
 u64 sec;





 if (freq) {
  sec = cs->mask;
  do_div(sec, freq);
  do_div(sec, scale);
  if (!sec)
   sec = 1;
  else if (sec > 600 && cs->mask > UINT_MAX)
   sec = 600;

  clocks_calc_mult_shift(&cs->mult, &cs->shift, freq,
           NSEC_PER_SEC / scale, sec * scale);
 }




 cs->maxadj = clocksource_max_adjustment(cs);
 while (freq && ((cs->mult + cs->maxadj < cs->mult)
  || (cs->mult - cs->maxadj > cs->mult))) {
  cs->mult >>= 1;
  cs->shift--;
  cs->maxadj = clocksource_max_adjustment(cs);
 }





 WARN_ONCE(cs->mult + cs->maxadj < cs->mult,
  "timekeeping: Clocksource %s might overflow on 11%% adjustment\n",
  cs->name);

 clocksource_update_max_deferment(cs);

 pr_info("%s: mask: 0x%llx max_cycles: 0x%llx, max_idle_ns: %lld ns\n",
  cs->name, cs->mask, cs->max_cycles, cs->max_idle_ns);
}
EXPORT_SYMBOL_GPL(__clocksource_update_freq_scale);
int __clocksource_register_scale(struct clocksource *cs, u32 scale, u32 freq)
{


 __clocksource_update_freq_scale(cs, scale, freq);


 mutex_lock(&clocksource_mutex);
 clocksource_enqueue(cs);
 clocksource_enqueue_watchdog(cs);
 clocksource_select();
 clocksource_select_watchdog(false);
 mutex_unlock(&clocksource_mutex);
 return 0;
}
EXPORT_SYMBOL_GPL(__clocksource_register_scale);

static void __clocksource_change_rating(struct clocksource *cs, int rating)
{
 list_del(&cs->list);
 cs->rating = rating;
 clocksource_enqueue(cs);
}






void clocksource_change_rating(struct clocksource *cs, int rating)
{
 mutex_lock(&clocksource_mutex);
 __clocksource_change_rating(cs, rating);
 clocksource_select();
 clocksource_select_watchdog(false);
 mutex_unlock(&clocksource_mutex);
}
EXPORT_SYMBOL(clocksource_change_rating);




static int clocksource_unbind(struct clocksource *cs)
{
 if (clocksource_is_watchdog(cs)) {

  clocksource_select_watchdog(true);
  if (clocksource_is_watchdog(cs))
   return -EBUSY;
 }

 if (cs == curr_clocksource) {

  clocksource_select_fallback();
  if (curr_clocksource == cs)
   return -EBUSY;
 }
 clocksource_dequeue_watchdog(cs);
 list_del_init(&cs->list);
 return 0;
}





int clocksource_unregister(struct clocksource *cs)
{
 int ret = 0;

 mutex_lock(&clocksource_mutex);
 if (!list_empty(&cs->list))
  ret = clocksource_unbind(cs);
 mutex_unlock(&clocksource_mutex);
 return ret;
}
EXPORT_SYMBOL(clocksource_unregister);

static ssize_t
sysfs_show_current_clocksources(struct device *dev,
    struct device_attribute *attr, char *buf)
{
 ssize_t count = 0;

 mutex_lock(&clocksource_mutex);
 count = snprintf(buf, PAGE_SIZE, "%s\n", curr_clocksource->name);
 mutex_unlock(&clocksource_mutex);

 return count;
}

ssize_t sysfs_get_uname(const char *buf, char *dst, size_t cnt)
{
 size_t ret = cnt;


 if (!cnt || cnt >= CS_NAME_LEN)
  return -EINVAL;


 if (buf[cnt-1] == '\n')
  cnt--;
 if (cnt > 0)
  memcpy(dst, buf, cnt);
 dst[cnt] = 0;
 return ret;
}
static ssize_t sysfs_override_clocksource(struct device *dev,
       struct device_attribute *attr,
       const char *buf, size_t count)
{
 ssize_t ret;

 mutex_lock(&clocksource_mutex);

 ret = sysfs_get_uname(buf, override_name, count);
 if (ret >= 0)
  clocksource_select();

 mutex_unlock(&clocksource_mutex);

 return ret;
}
static ssize_t sysfs_unbind_clocksource(struct device *dev,
     struct device_attribute *attr,
     const char *buf, size_t count)
{
 struct clocksource *cs;
 char name[CS_NAME_LEN];
 ssize_t ret;

 ret = sysfs_get_uname(buf, name, count);
 if (ret < 0)
  return ret;

 ret = -ENODEV;
 mutex_lock(&clocksource_mutex);
 list_for_each_entry(cs, &clocksource_list, list) {
  if (strcmp(cs->name, name))
   continue;
  ret = clocksource_unbind(cs);
  break;
 }
 mutex_unlock(&clocksource_mutex);

 return ret ? ret : count;
}
static ssize_t
sysfs_show_available_clocksources(struct device *dev,
      struct device_attribute *attr,
      char *buf)
{
 struct clocksource *src;
 ssize_t count = 0;

 mutex_lock(&clocksource_mutex);
 list_for_each_entry(src, &clocksource_list, list) {




  if (!tick_oneshot_mode_active() ||
      (src->flags & CLOCK_SOURCE_VALID_FOR_HRES))
   count += snprintf(buf + count,
      max((ssize_t)PAGE_SIZE - count, (ssize_t)0),
      "%s ", src->name);
 }
 mutex_unlock(&clocksource_mutex);

 count += snprintf(buf + count,
     max((ssize_t)PAGE_SIZE - count, (ssize_t)0), "\n");

 return count;
}




static DEVICE_ATTR(current_clocksource, 0644, sysfs_show_current_clocksources,
     sysfs_override_clocksource);

static DEVICE_ATTR(unbind_clocksource, 0200, NULL, sysfs_unbind_clocksource);

static DEVICE_ATTR(available_clocksource, 0444,
     sysfs_show_available_clocksources, NULL);

static struct bus_type clocksource_subsys = {
 .name = "clocksource",
 .dev_name = "clocksource",
};

static struct device device_clocksource = {
 .id = 0,
 .bus = &clocksource_subsys,
};

static int __init init_clocksource_sysfs(void)
{
 int error = subsys_system_register(&clocksource_subsys, NULL);

 if (!error)
  error = device_register(&device_clocksource);
 if (!error)
  error = device_create_file(
    &device_clocksource,
    &dev_attr_current_clocksource);
 if (!error)
  error = device_create_file(&device_clocksource,
        &dev_attr_unbind_clocksource);
 if (!error)
  error = device_create_file(
    &device_clocksource,
    &dev_attr_available_clocksource);
 return error;
}

device_initcall(init_clocksource_sysfs);
static int __init boot_override_clocksource(char* str)
{
 mutex_lock(&clocksource_mutex);
 if (str)
  strlcpy(override_name, str, sizeof(override_name));
 mutex_unlock(&clocksource_mutex);
 return 1;
}

__setup("clocksource=", boot_override_clocksource);
static int __init boot_override_clock(char* str)
{
 if (!strcmp(str, "pmtmr")) {
  pr_warn("clock=pmtmr is deprecated - use clocksource=acpi_pm\n");
  return boot_override_clocksource("acpi_pm");
 }
 pr_warn("clock= boot option is deprecated - use clocksource=xyz\n");
 return boot_override_clocksource(str);
}

__setup("clock=", boot_override_clock);


static int compat_get_timex(struct timex *txc, struct compat_timex __user *utp)
{
 memset(txc, 0, sizeof(struct timex));

 if (!access_ok(VERIFY_READ, utp, sizeof(struct compat_timex)) ||
   __get_user(txc->modes, &utp->modes) ||
   __get_user(txc->offset, &utp->offset) ||
   __get_user(txc->freq, &utp->freq) ||
   __get_user(txc->maxerror, &utp->maxerror) ||
   __get_user(txc->esterror, &utp->esterror) ||
   __get_user(txc->status, &utp->status) ||
   __get_user(txc->constant, &utp->constant) ||
   __get_user(txc->precision, &utp->precision) ||
   __get_user(txc->tolerance, &utp->tolerance) ||
   __get_user(txc->time.tv_sec, &utp->time.tv_sec) ||
   __get_user(txc->time.tv_usec, &utp->time.tv_usec) ||
   __get_user(txc->tick, &utp->tick) ||
   __get_user(txc->ppsfreq, &utp->ppsfreq) ||
   __get_user(txc->jitter, &utp->jitter) ||
   __get_user(txc->shift, &utp->shift) ||
   __get_user(txc->stabil, &utp->stabil) ||
   __get_user(txc->jitcnt, &utp->jitcnt) ||
   __get_user(txc->calcnt, &utp->calcnt) ||
   __get_user(txc->errcnt, &utp->errcnt) ||
   __get_user(txc->stbcnt, &utp->stbcnt))
  return -EFAULT;

 return 0;
}

static int compat_put_timex(struct compat_timex __user *utp, struct timex *txc)
{
 if (!access_ok(VERIFY_WRITE, utp, sizeof(struct compat_timex)) ||
   __put_user(txc->modes, &utp->modes) ||
   __put_user(txc->offset, &utp->offset) ||
   __put_user(txc->freq, &utp->freq) ||
   __put_user(txc->maxerror, &utp->maxerror) ||
   __put_user(txc->esterror, &utp->esterror) ||
   __put_user(txc->status, &utp->status) ||
   __put_user(txc->constant, &utp->constant) ||
   __put_user(txc->precision, &utp->precision) ||
   __put_user(txc->tolerance, &utp->tolerance) ||
   __put_user(txc->time.tv_sec, &utp->time.tv_sec) ||
   __put_user(txc->time.tv_usec, &utp->time.tv_usec) ||
   __put_user(txc->tick, &utp->tick) ||
   __put_user(txc->ppsfreq, &utp->ppsfreq) ||
   __put_user(txc->jitter, &utp->jitter) ||
   __put_user(txc->shift, &utp->shift) ||
   __put_user(txc->stabil, &utp->stabil) ||
   __put_user(txc->jitcnt, &utp->jitcnt) ||
   __put_user(txc->calcnt, &utp->calcnt) ||
   __put_user(txc->errcnt, &utp->errcnt) ||
   __put_user(txc->stbcnt, &utp->stbcnt) ||
   __put_user(txc->tai, &utp->tai))
  return -EFAULT;
 return 0;
}

COMPAT_SYSCALL_DEFINE2(gettimeofday, struct compat_timeval __user *, tv,
         struct timezone __user *, tz)
{
 if (tv) {
  struct timeval ktv;
  do_gettimeofday(&ktv);
  if (compat_put_timeval(&ktv, tv))
   return -EFAULT;
 }
 if (tz) {
  if (copy_to_user(tz, &sys_tz, sizeof(sys_tz)))
   return -EFAULT;
 }

 return 0;
}

COMPAT_SYSCALL_DEFINE2(settimeofday, struct compat_timeval __user *, tv,
         struct timezone __user *, tz)
{
 struct timeval user_tv;
 struct timespec new_ts;
 struct timezone new_tz;

 if (tv) {
  if (compat_get_timeval(&user_tv, tv))
   return -EFAULT;
  new_ts.tv_sec = user_tv.tv_sec;
  new_ts.tv_nsec = user_tv.tv_usec * NSEC_PER_USEC;
 }
 if (tz) {
  if (copy_from_user(&new_tz, tz, sizeof(*tz)))
   return -EFAULT;
 }

 return do_sys_settimeofday(tv ? &new_ts : NULL, tz ? &new_tz : NULL);
}

static int __compat_get_timeval(struct timeval *tv, const struct compat_timeval __user *ctv)
{
 return (!access_ok(VERIFY_READ, ctv, sizeof(*ctv)) ||
   __get_user(tv->tv_sec, &ctv->tv_sec) ||
   __get_user(tv->tv_usec, &ctv->tv_usec)) ? -EFAULT : 0;
}

static int __compat_put_timeval(const struct timeval *tv, struct compat_timeval __user *ctv)
{
 return (!access_ok(VERIFY_WRITE, ctv, sizeof(*ctv)) ||
   __put_user(tv->tv_sec, &ctv->tv_sec) ||
   __put_user(tv->tv_usec, &ctv->tv_usec)) ? -EFAULT : 0;
}

static int __compat_get_timespec(struct timespec *ts, const struct compat_timespec __user *cts)
{
 return (!access_ok(VERIFY_READ, cts, sizeof(*cts)) ||
   __get_user(ts->tv_sec, &cts->tv_sec) ||
   __get_user(ts->tv_nsec, &cts->tv_nsec)) ? -EFAULT : 0;
}

static int __compat_put_timespec(const struct timespec *ts, struct compat_timespec __user *cts)
{
 return (!access_ok(VERIFY_WRITE, cts, sizeof(*cts)) ||
   __put_user(ts->tv_sec, &cts->tv_sec) ||
   __put_user(ts->tv_nsec, &cts->tv_nsec)) ? -EFAULT : 0;
}

int compat_get_timeval(struct timeval *tv, const void __user *utv)
{
 if (COMPAT_USE_64BIT_TIME)
  return copy_from_user(tv, utv, sizeof(*tv)) ? -EFAULT : 0;
 else
  return __compat_get_timeval(tv, utv);
}
EXPORT_SYMBOL_GPL(compat_get_timeval);

int compat_put_timeval(const struct timeval *tv, void __user *utv)
{
 if (COMPAT_USE_64BIT_TIME)
  return copy_to_user(utv, tv, sizeof(*tv)) ? -EFAULT : 0;
 else
  return __compat_put_timeval(tv, utv);
}
EXPORT_SYMBOL_GPL(compat_put_timeval);

int compat_get_timespec(struct timespec *ts, const void __user *uts)
{
 if (COMPAT_USE_64BIT_TIME)
  return copy_from_user(ts, uts, sizeof(*ts)) ? -EFAULT : 0;
 else
  return __compat_get_timespec(ts, uts);
}
EXPORT_SYMBOL_GPL(compat_get_timespec);

int compat_put_timespec(const struct timespec *ts, void __user *uts)
{
 if (COMPAT_USE_64BIT_TIME)
  return copy_to_user(uts, ts, sizeof(*ts)) ? -EFAULT : 0;
 else
  return __compat_put_timespec(ts, uts);
}
EXPORT_SYMBOL_GPL(compat_put_timespec);

int compat_convert_timespec(struct timespec __user **kts,
       const void __user *cts)
{
 struct timespec ts;
 struct timespec __user *uts;

 if (!cts || COMPAT_USE_64BIT_TIME) {
  *kts = (struct timespec __user *)cts;
  return 0;
 }

 uts = compat_alloc_user_space(sizeof(ts));
 if (!uts)
  return -EFAULT;
 if (compat_get_timespec(&ts, cts))
  return -EFAULT;
 if (copy_to_user(uts, &ts, sizeof(ts)))
  return -EFAULT;

 *kts = uts;
 return 0;
}

static long compat_nanosleep_restart(struct restart_block *restart)
{
 struct compat_timespec __user *rmtp;
 struct timespec rmt;
 mm_segment_t oldfs;
 long ret;

 restart->nanosleep.rmtp = (struct timespec __user *) &rmt;
 oldfs = get_fs();
 set_fs(KERNEL_DS);
 ret = hrtimer_nanosleep_restart(restart);
 set_fs(oldfs);

 if (ret == -ERESTART_RESTARTBLOCK) {
  rmtp = restart->nanosleep.compat_rmtp;

  if (rmtp && compat_put_timespec(&rmt, rmtp))
   return -EFAULT;
 }

 return ret;
}

COMPAT_SYSCALL_DEFINE2(nanosleep, struct compat_timespec __user *, rqtp,
         struct compat_timespec __user *, rmtp)
{
 struct timespec tu, rmt;
 mm_segment_t oldfs;
 long ret;

 if (compat_get_timespec(&tu, rqtp))
  return -EFAULT;

 if (!timespec_valid(&tu))
  return -EINVAL;

 oldfs = get_fs();
 set_fs(KERNEL_DS);
 ret = hrtimer_nanosleep(&tu,
    rmtp ? (struct timespec __user *)&rmt : NULL,
    HRTIMER_MODE_REL, CLOCK_MONOTONIC);
 set_fs(oldfs);
 if (ret == -ERESTART_RESTARTBLOCK) {
  struct restart_block *restart = &current->restart_block;

  restart->fn = compat_nanosleep_restart;
  restart->nanosleep.compat_rmtp = rmtp;

  if (rmtp && compat_put_timespec(&rmt, rmtp))
   return -EFAULT;
 }
 return ret;
}

static inline long get_compat_itimerval(struct itimerval *o,
  struct compat_itimerval __user *i)
{
 return (!access_ok(VERIFY_READ, i, sizeof(*i)) ||
  (__get_user(o->it_interval.tv_sec, &i->it_interval.tv_sec) |
   __get_user(o->it_interval.tv_usec, &i->it_interval.tv_usec) |
   __get_user(o->it_value.tv_sec, &i->it_value.tv_sec) |
   __get_user(o->it_value.tv_usec, &i->it_value.tv_usec)));
}

static inline long put_compat_itimerval(struct compat_itimerval __user *o,
  struct itimerval *i)
{
 return (!access_ok(VERIFY_WRITE, o, sizeof(*o)) ||
  (__put_user(i->it_interval.tv_sec, &o->it_interval.tv_sec) |
   __put_user(i->it_interval.tv_usec, &o->it_interval.tv_usec) |
   __put_user(i->it_value.tv_sec, &o->it_value.tv_sec) |
   __put_user(i->it_value.tv_usec, &o->it_value.tv_usec)));
}

COMPAT_SYSCALL_DEFINE2(getitimer, int, which,
  struct compat_itimerval __user *, it)
{
 struct itimerval kit;
 int error;

 error = do_getitimer(which, &kit);
 if (!error && put_compat_itimerval(it, &kit))
  error = -EFAULT;
 return error;
}

COMPAT_SYSCALL_DEFINE3(setitimer, int, which,
  struct compat_itimerval __user *, in,
  struct compat_itimerval __user *, out)
{
 struct itimerval kin, kout;
 int error;

 if (in) {
  if (get_compat_itimerval(&kin, in))
   return -EFAULT;
 } else
  memset(&kin, 0, sizeof(kin));

 error = do_setitimer(which, &kin, out ? &kout : NULL);
 if (error || !out)
  return error;
 if (put_compat_itimerval(out, &kout))
  return -EFAULT;
 return 0;
}

static compat_clock_t clock_t_to_compat_clock_t(clock_t x)
{
 return compat_jiffies_to_clock_t(clock_t_to_jiffies(x));
}

COMPAT_SYSCALL_DEFINE1(times, struct compat_tms __user *, tbuf)
{
 if (tbuf) {
  struct tms tms;
  struct compat_tms tmp;

  do_sys_times(&tms);

  tmp.tms_utime = clock_t_to_compat_clock_t(tms.tms_utime);
  tmp.tms_stime = clock_t_to_compat_clock_t(tms.tms_stime);
  tmp.tms_cutime = clock_t_to_compat_clock_t(tms.tms_cutime);
  tmp.tms_cstime = clock_t_to_compat_clock_t(tms.tms_cstime);
  if (copy_to_user(tbuf, &tmp, sizeof(tmp)))
   return -EFAULT;
 }
 force_successful_syscall_return();
 return compat_jiffies_to_clock_t(jiffies);
}







COMPAT_SYSCALL_DEFINE1(sigpending, compat_old_sigset_t __user *, set)
{
 old_sigset_t s;
 long ret;
 mm_segment_t old_fs = get_fs();

 set_fs(KERNEL_DS);
 ret = sys_sigpending((old_sigset_t __user *) &s);
 set_fs(old_fs);
 if (ret == 0)
  ret = put_user(s, set);
 return ret;
}







static inline void compat_sig_setmask(sigset_t *blocked, compat_sigset_word set)
{
 memcpy(blocked->sig, &set, sizeof(set));
}

COMPAT_SYSCALL_DEFINE3(sigprocmask, int, how,
         compat_old_sigset_t __user *, nset,
         compat_old_sigset_t __user *, oset)
{
 old_sigset_t old_set, new_set;
 sigset_t new_blocked;

 old_set = current->blocked.sig[0];

 if (nset) {
  if (get_user(new_set, nset))
   return -EFAULT;
  new_set &= ~(sigmask(SIGKILL) | sigmask(SIGSTOP));

  new_blocked = current->blocked;

  switch (how) {
  case SIG_BLOCK:
   sigaddsetmask(&new_blocked, new_set);
   break;
  case SIG_UNBLOCK:
   sigdelsetmask(&new_blocked, new_set);
   break;
  case SIG_SETMASK:
   compat_sig_setmask(&new_blocked, new_set);
   break;
  default:
   return -EINVAL;
  }

  set_current_blocked(&new_blocked);
 }

 if (oset) {
  if (put_user(old_set, oset))
   return -EFAULT;
 }

 return 0;
}


COMPAT_SYSCALL_DEFINE2(setrlimit, unsigned int, resource,
         struct compat_rlimit __user *, rlim)
{
 struct rlimit r;

 if (!access_ok(VERIFY_READ, rlim, sizeof(*rlim)) ||
     __get_user(r.rlim_cur, &rlim->rlim_cur) ||
     __get_user(r.rlim_max, &rlim->rlim_max))
  return -EFAULT;

 if (r.rlim_cur == COMPAT_RLIM_INFINITY)
  r.rlim_cur = RLIM_INFINITY;
 if (r.rlim_max == COMPAT_RLIM_INFINITY)
  r.rlim_max = RLIM_INFINITY;
 return do_prlimit(current, resource, &r, NULL);
}


COMPAT_SYSCALL_DEFINE2(old_getrlimit, unsigned int, resource,
         struct compat_rlimit __user *, rlim)
{
 struct rlimit r;
 int ret;
 mm_segment_t old_fs = get_fs();

 set_fs(KERNEL_DS);
 ret = sys_old_getrlimit(resource, (struct rlimit __user *)&r);
 set_fs(old_fs);

 if (!ret) {
  if (r.rlim_cur > COMPAT_RLIM_OLD_INFINITY)
   r.rlim_cur = COMPAT_RLIM_INFINITY;
  if (r.rlim_max > COMPAT_RLIM_OLD_INFINITY)
   r.rlim_max = COMPAT_RLIM_INFINITY;

  if (!access_ok(VERIFY_WRITE, rlim, sizeof(*rlim)) ||
      __put_user(r.rlim_cur, &rlim->rlim_cur) ||
      __put_user(r.rlim_max, &rlim->rlim_max))
   return -EFAULT;
 }
 return ret;
}


COMPAT_SYSCALL_DEFINE2(getrlimit, unsigned int, resource,
         struct compat_rlimit __user *, rlim)
{
 struct rlimit r;
 int ret;

 ret = do_prlimit(current, resource, NULL, &r);
 if (!ret) {
  if (r.rlim_cur > COMPAT_RLIM_INFINITY)
   r.rlim_cur = COMPAT_RLIM_INFINITY;
  if (r.rlim_max > COMPAT_RLIM_INFINITY)
   r.rlim_max = COMPAT_RLIM_INFINITY;

  if (!access_ok(VERIFY_WRITE, rlim, sizeof(*rlim)) ||
      __put_user(r.rlim_cur, &rlim->rlim_cur) ||
      __put_user(r.rlim_max, &rlim->rlim_max))
   return -EFAULT;
 }
 return ret;
}

int put_compat_rusage(const struct rusage *r, struct compat_rusage __user *ru)
{
 if (!access_ok(VERIFY_WRITE, ru, sizeof(*ru)) ||
     __put_user(r->ru_utime.tv_sec, &ru->ru_utime.tv_sec) ||
     __put_user(r->ru_utime.tv_usec, &ru->ru_utime.tv_usec) ||
     __put_user(r->ru_stime.tv_sec, &ru->ru_stime.tv_sec) ||
     __put_user(r->ru_stime.tv_usec, &ru->ru_stime.tv_usec) ||
     __put_user(r->ru_maxrss, &ru->ru_maxrss) ||
     __put_user(r->ru_ixrss, &ru->ru_ixrss) ||
     __put_user(r->ru_idrss, &ru->ru_idrss) ||
     __put_user(r->ru_isrss, &ru->ru_isrss) ||
     __put_user(r->ru_minflt, &ru->ru_minflt) ||
     __put_user(r->ru_majflt, &ru->ru_majflt) ||
     __put_user(r->ru_nswap, &ru->ru_nswap) ||
     __put_user(r->ru_inblock, &ru->ru_inblock) ||
     __put_user(r->ru_oublock, &ru->ru_oublock) ||
     __put_user(r->ru_msgsnd, &ru->ru_msgsnd) ||
     __put_user(r->ru_msgrcv, &ru->ru_msgrcv) ||
     __put_user(r->ru_nsignals, &ru->ru_nsignals) ||
     __put_user(r->ru_nvcsw, &ru->ru_nvcsw) ||
     __put_user(r->ru_nivcsw, &ru->ru_nivcsw))
  return -EFAULT;
 return 0;
}

COMPAT_SYSCALL_DEFINE4(wait4,
 compat_pid_t, pid,
 compat_uint_t __user *, stat_addr,
 int, options,
 struct compat_rusage __user *, ru)
{
 if (!ru) {
  return sys_wait4(pid, stat_addr, options, NULL);
 } else {
  struct rusage r;
  int ret;
  unsigned int status;
  mm_segment_t old_fs = get_fs();

  set_fs (KERNEL_DS);
  ret = sys_wait4(pid,
    (stat_addr ?
     (unsigned int __user *) &status : NULL),
    options, (struct rusage __user *) &r);
  set_fs (old_fs);

  if (ret > 0) {
   if (put_compat_rusage(&r, ru))
    return -EFAULT;
   if (stat_addr && put_user(status, stat_addr))
    return -EFAULT;
  }
  return ret;
 }
}

COMPAT_SYSCALL_DEFINE5(waitid,
  int, which, compat_pid_t, pid,
  struct compat_siginfo __user *, uinfo, int, options,
  struct compat_rusage __user *, uru)
{
 siginfo_t info;
 struct rusage ru;
 long ret;
 mm_segment_t old_fs = get_fs();

 memset(&info, 0, sizeof(info));

 set_fs(KERNEL_DS);
 ret = sys_waitid(which, pid, (siginfo_t __user *)&info, options,
    uru ? (struct rusage __user *)&ru : NULL);
 set_fs(old_fs);

 if ((ret < 0) || (info.si_signo == 0))
  return ret;

 if (uru) {

  if (COMPAT_USE_64BIT_TIME)
   ret = copy_to_user(uru, &ru, sizeof(ru));
  else
   ret = put_compat_rusage(&ru, uru);
  if (ret)
   return -EFAULT;
 }

 BUG_ON(info.si_code & __SI_MASK);
 info.si_code |= __SI_CHLD;
 return copy_siginfo_to_user32(uinfo, &info);
}

static int compat_get_user_cpu_mask(compat_ulong_t __user *user_mask_ptr,
        unsigned len, struct cpumask *new_mask)
{
 unsigned long *k;

 if (len < cpumask_size())
  memset(new_mask, 0, cpumask_size());
 else if (len > cpumask_size())
  len = cpumask_size();

 k = cpumask_bits(new_mask);
 return compat_get_bitmap(k, user_mask_ptr, len * 8);
}

COMPAT_SYSCALL_DEFINE3(sched_setaffinity, compat_pid_t, pid,
         unsigned int, len,
         compat_ulong_t __user *, user_mask_ptr)
{
 cpumask_var_t new_mask;
 int retval;

 if (!alloc_cpumask_var(&new_mask, GFP_KERNEL))
  return -ENOMEM;

 retval = compat_get_user_cpu_mask(user_mask_ptr, len, new_mask);
 if (retval)
  goto out;

 retval = sched_setaffinity(pid, new_mask);
out:
 free_cpumask_var(new_mask);
 return retval;
}

COMPAT_SYSCALL_DEFINE3(sched_getaffinity, compat_pid_t, pid, unsigned int, len,
         compat_ulong_t __user *, user_mask_ptr)
{
 int ret;
 cpumask_var_t mask;

 if ((len * BITS_PER_BYTE) < nr_cpu_ids)
  return -EINVAL;
 if (len & (sizeof(compat_ulong_t)-1))
  return -EINVAL;

 if (!alloc_cpumask_var(&mask, GFP_KERNEL))
  return -ENOMEM;

 ret = sched_getaffinity(pid, mask);
 if (ret == 0) {
  size_t retlen = min_t(size_t, len, cpumask_size());

  if (compat_put_bitmap(user_mask_ptr, cpumask_bits(mask), retlen * 8))
   ret = -EFAULT;
  else
   ret = retlen;
 }
 free_cpumask_var(mask);

 return ret;
}

int get_compat_itimerspec(struct itimerspec *dst,
     const struct compat_itimerspec __user *src)
{
 if (__compat_get_timespec(&dst->it_interval, &src->it_interval) ||
     __compat_get_timespec(&dst->it_value, &src->it_value))
  return -EFAULT;
 return 0;
}

int put_compat_itimerspec(struct compat_itimerspec __user *dst,
     const struct itimerspec *src)
{
 if (__compat_put_timespec(&src->it_interval, &dst->it_interval) ||
     __compat_put_timespec(&src->it_value, &dst->it_value))
  return -EFAULT;
 return 0;
}

COMPAT_SYSCALL_DEFINE3(timer_create, clockid_t, which_clock,
         struct compat_sigevent __user *, timer_event_spec,
         timer_t __user *, created_timer_id)
{
 struct sigevent __user *event = NULL;

 if (timer_event_spec) {
  struct sigevent kevent;

  event = compat_alloc_user_space(sizeof(*event));
  if (get_compat_sigevent(&kevent, timer_event_spec) ||
      copy_to_user(event, &kevent, sizeof(*event)))
   return -EFAULT;
 }

 return sys_timer_create(which_clock, event, created_timer_id);
}

COMPAT_SYSCALL_DEFINE4(timer_settime, timer_t, timer_id, int, flags,
         struct compat_itimerspec __user *, new,
         struct compat_itimerspec __user *, old)
{
 long err;
 mm_segment_t oldfs;
 struct itimerspec newts, oldts;

 if (!new)
  return -EINVAL;
 if (get_compat_itimerspec(&newts, new))
  return -EFAULT;
 oldfs = get_fs();
 set_fs(KERNEL_DS);
 err = sys_timer_settime(timer_id, flags,
    (struct itimerspec __user *) &newts,
    (struct itimerspec __user *) &oldts);
 set_fs(oldfs);
 if (!err && old && put_compat_itimerspec(old, &oldts))
  return -EFAULT;
 return err;
}

COMPAT_SYSCALL_DEFINE2(timer_gettime, timer_t, timer_id,
         struct compat_itimerspec __user *, setting)
{
 long err;
 mm_segment_t oldfs;
 struct itimerspec ts;

 oldfs = get_fs();
 set_fs(KERNEL_DS);
 err = sys_timer_gettime(timer_id,
    (struct itimerspec __user *) &ts);
 set_fs(oldfs);
 if (!err && put_compat_itimerspec(setting, &ts))
  return -EFAULT;
 return err;
}

COMPAT_SYSCALL_DEFINE2(clock_settime, clockid_t, which_clock,
         struct compat_timespec __user *, tp)
{
 long err;
 mm_segment_t oldfs;
 struct timespec ts;

 if (compat_get_timespec(&ts, tp))
  return -EFAULT;
 oldfs = get_fs();
 set_fs(KERNEL_DS);
 err = sys_clock_settime(which_clock,
    (struct timespec __user *) &ts);
 set_fs(oldfs);
 return err;
}

COMPAT_SYSCALL_DEFINE2(clock_gettime, clockid_t, which_clock,
         struct compat_timespec __user *, tp)
{
 long err;
 mm_segment_t oldfs;
 struct timespec ts;

 oldfs = get_fs();
 set_fs(KERNEL_DS);
 err = sys_clock_gettime(which_clock,
    (struct timespec __user *) &ts);
 set_fs(oldfs);
 if (!err && compat_put_timespec(&ts, tp))
  return -EFAULT;
 return err;
}

COMPAT_SYSCALL_DEFINE2(clock_adjtime, clockid_t, which_clock,
         struct compat_timex __user *, utp)
{
 struct timex txc;
 mm_segment_t oldfs;
 int err, ret;

 err = compat_get_timex(&txc, utp);
 if (err)
  return err;

 oldfs = get_fs();
 set_fs(KERNEL_DS);
 ret = sys_clock_adjtime(which_clock, (struct timex __user *) &txc);
 set_fs(oldfs);

 err = compat_put_timex(utp, &txc);
 if (err)
  return err;

 return ret;
}

COMPAT_SYSCALL_DEFINE2(clock_getres, clockid_t, which_clock,
         struct compat_timespec __user *, tp)
{
 long err;
 mm_segment_t oldfs;
 struct timespec ts;

 oldfs = get_fs();
 set_fs(KERNEL_DS);
 err = sys_clock_getres(which_clock,
          (struct timespec __user *) &ts);
 set_fs(oldfs);
 if (!err && tp && compat_put_timespec(&ts, tp))
  return -EFAULT;
 return err;
}

static long compat_clock_nanosleep_restart(struct restart_block *restart)
{
 long err;
 mm_segment_t oldfs;
 struct timespec tu;
 struct compat_timespec __user *rmtp = restart->nanosleep.compat_rmtp;

 restart->nanosleep.rmtp = (struct timespec __user *) &tu;
 oldfs = get_fs();
 set_fs(KERNEL_DS);
 err = clock_nanosleep_restart(restart);
 set_fs(oldfs);

 if ((err == -ERESTART_RESTARTBLOCK) && rmtp &&
     compat_put_timespec(&tu, rmtp))
  return -EFAULT;

 if (err == -ERESTART_RESTARTBLOCK) {
  restart->fn = compat_clock_nanosleep_restart;
  restart->nanosleep.compat_rmtp = rmtp;
 }
 return err;
}

COMPAT_SYSCALL_DEFINE4(clock_nanosleep, clockid_t, which_clock, int, flags,
         struct compat_timespec __user *, rqtp,
         struct compat_timespec __user *, rmtp)
{
 long err;
 mm_segment_t oldfs;
 struct timespec in, out;
 struct restart_block *restart;

 if (compat_get_timespec(&in, rqtp))
  return -EFAULT;

 oldfs = get_fs();
 set_fs(KERNEL_DS);
 err = sys_clock_nanosleep(which_clock, flags,
      (struct timespec __user *) &in,
      (struct timespec __user *) &out);
 set_fs(oldfs);

 if ((err == -ERESTART_RESTARTBLOCK) && rmtp &&
     compat_put_timespec(&out, rmtp))
  return -EFAULT;

 if (err == -ERESTART_RESTARTBLOCK) {
  restart = &current->restart_block;
  restart->fn = compat_clock_nanosleep_restart;
  restart->nanosleep.compat_rmtp = rmtp;
 }
 return err;
}
int get_compat_sigevent(struct sigevent *event,
  const struct compat_sigevent __user *u_event)
{
 memset(event, 0, sizeof(*event));
 return (!access_ok(VERIFY_READ, u_event, sizeof(*u_event)) ||
  __get_user(event->sigev_value.sival_int,
   &u_event->sigev_value.sival_int) ||
  __get_user(event->sigev_signo, &u_event->sigev_signo) ||
  __get_user(event->sigev_notify, &u_event->sigev_notify) ||
  __get_user(event->sigev_notify_thread_id,
   &u_event->sigev_notify_thread_id))
  ? -EFAULT : 0;
}

long compat_get_bitmap(unsigned long *mask, const compat_ulong_t __user *umask,
         unsigned long bitmap_size)
{
 int i, j;
 unsigned long m;
 compat_ulong_t um;
 unsigned long nr_compat_longs;


 bitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);

 if (!access_ok(VERIFY_READ, umask, bitmap_size / 8))
  return -EFAULT;

 nr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);

 for (i = 0; i < BITS_TO_LONGS(bitmap_size); i++) {
  m = 0;

  for (j = 0; j < sizeof(m)/sizeof(um); j++) {





   if (nr_compat_longs) {
    nr_compat_longs--;
    if (__get_user(um, umask))
     return -EFAULT;
   } else {
    um = 0;
   }

   umask++;
   m |= (long)um << (j * BITS_PER_COMPAT_LONG);
  }
  *mask++ = m;
 }

 return 0;
}

long compat_put_bitmap(compat_ulong_t __user *umask, unsigned long *mask,
         unsigned long bitmap_size)
{
 int i, j;
 unsigned long m;
 compat_ulong_t um;
 unsigned long nr_compat_longs;


 bitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);

 if (!access_ok(VERIFY_WRITE, umask, bitmap_size / 8))
  return -EFAULT;

 nr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);

 for (i = 0; i < BITS_TO_LONGS(bitmap_size); i++) {
  m = *mask++;

  for (j = 0; j < sizeof(m)/sizeof(um); j++) {
   um = m;





   if (nr_compat_longs) {
    nr_compat_longs--;
    if (__put_user(um, umask))
     return -EFAULT;
   }

   umask++;
   m >>= 4*sizeof(um);
   m >>= 4*sizeof(um);
  }
 }

 return 0;
}

void
sigset_from_compat(sigset_t *set, const compat_sigset_t *compat)
{
 switch (_NSIG_WORDS) {
 case 4: set->sig[3] = compat->sig[6] | (((long)compat->sig[7]) << 32 );
 case 3: set->sig[2] = compat->sig[4] | (((long)compat->sig[5]) << 32 );
 case 2: set->sig[1] = compat->sig[2] | (((long)compat->sig[3]) << 32 );
 case 1: set->sig[0] = compat->sig[0] | (((long)compat->sig[1]) << 32 );
 }
}
EXPORT_SYMBOL_GPL(sigset_from_compat);

void
sigset_to_compat(compat_sigset_t *compat, const sigset_t *set)
{
 switch (_NSIG_WORDS) {
 case 4: compat->sig[7] = (set->sig[3] >> 32); compat->sig[6] = set->sig[3];
 case 3: compat->sig[5] = (set->sig[2] >> 32); compat->sig[4] = set->sig[2];
 case 2: compat->sig[3] = (set->sig[1] >> 32); compat->sig[2] = set->sig[1];
 case 1: compat->sig[1] = (set->sig[0] >> 32); compat->sig[0] = set->sig[0];
 }
}

COMPAT_SYSCALL_DEFINE4(rt_sigtimedwait, compat_sigset_t __user *, uthese,
  struct compat_siginfo __user *, uinfo,
  struct compat_timespec __user *, uts, compat_size_t, sigsetsize)
{
 compat_sigset_t s32;
 sigset_t s;
 struct timespec t;
 siginfo_t info;
 long ret;

 if (sigsetsize != sizeof(sigset_t))
  return -EINVAL;

 if (copy_from_user(&s32, uthese, sizeof(compat_sigset_t)))
  return -EFAULT;
 sigset_from_compat(&s, &s32);

 if (uts) {
  if (compat_get_timespec(&t, uts))
   return -EFAULT;
 }

 ret = do_sigtimedwait(&s, &info, uts ? &t : NULL);

 if (ret > 0 && uinfo) {
  if (copy_siginfo_to_user32(uinfo, &info))
   ret = -EFAULT;
 }

 return ret;
}




COMPAT_SYSCALL_DEFINE1(time, compat_time_t __user *, tloc)
{
 compat_time_t i;
 struct timeval tv;

 do_gettimeofday(&tv);
 i = tv.tv_sec;

 if (tloc) {
  if (put_user(i,tloc))
   return -EFAULT;
 }
 force_successful_syscall_return();
 return i;
}

COMPAT_SYSCALL_DEFINE1(stime, compat_time_t __user *, tptr)
{
 struct timespec tv;
 int err;

 if (get_user(tv.tv_sec, tptr))
  return -EFAULT;

 tv.tv_nsec = 0;

 err = security_settime(&tv, NULL);
 if (err)
  return err;

 do_settimeofday(&tv);
 return 0;
}


COMPAT_SYSCALL_DEFINE1(adjtimex, struct compat_timex __user *, utp)
{
 struct timex txc;
 int err, ret;

 err = compat_get_timex(&txc, utp);
 if (err)
  return err;

 ret = do_adjtimex(&txc);

 err = compat_put_timex(utp, &txc);
 if (err)
  return err;

 return ret;
}

COMPAT_SYSCALL_DEFINE6(move_pages, pid_t, pid, compat_ulong_t, nr_pages,
         compat_uptr_t __user *, pages32,
         const int __user *, nodes,
         int __user *, status,
         int, flags)
{
 const void __user * __user *pages;
 int i;

 pages = compat_alloc_user_space(nr_pages * sizeof(void *));
 for (i = 0; i < nr_pages; i++) {
  compat_uptr_t p;

  if (get_user(p, pages32 + i) ||
   put_user(compat_ptr(p), pages + i))
   return -EFAULT;
 }
 return sys_move_pages(pid, nr_pages, pages, nodes, status, flags);
}

COMPAT_SYSCALL_DEFINE4(migrate_pages, compat_pid_t, pid,
         compat_ulong_t, maxnode,
         const compat_ulong_t __user *, old_nodes,
         const compat_ulong_t __user *, new_nodes)
{
 unsigned long __user *old = NULL;
 unsigned long __user *new = NULL;
 nodemask_t tmp_mask;
 unsigned long nr_bits;
 unsigned long size;

 nr_bits = min_t(unsigned long, maxnode - 1, MAX_NUMNODES);
 size = ALIGN(nr_bits, BITS_PER_LONG) / 8;
 if (old_nodes) {
  if (compat_get_bitmap(nodes_addr(tmp_mask), old_nodes, nr_bits))
   return -EFAULT;
  old = compat_alloc_user_space(new_nodes ? size * 2 : size);
  if (new_nodes)
   new = old + size / sizeof(unsigned long);
  if (copy_to_user(old, nodes_addr(tmp_mask), size))
   return -EFAULT;
 }
 if (new_nodes) {
  if (compat_get_bitmap(nodes_addr(tmp_mask), new_nodes, nr_bits))
   return -EFAULT;
  if (new == NULL)
   new = compat_alloc_user_space(size);
  if (copy_to_user(new, nodes_addr(tmp_mask), size))
   return -EFAULT;
 }
 return sys_migrate_pages(pid, nr_bits + 1, old, new);
}

COMPAT_SYSCALL_DEFINE2(sched_rr_get_interval,
         compat_pid_t, pid,
         struct compat_timespec __user *, interval)
{
 struct timespec t;
 int ret;
 mm_segment_t old_fs = get_fs();

 set_fs(KERNEL_DS);
 ret = sys_sched_rr_get_interval(pid, (struct timespec __user *)&t);
 set_fs(old_fs);
 if (compat_put_timespec(&t, interval))
  return -EFAULT;
 return ret;
}





void __user *compat_alloc_user_space(unsigned long len)
{
 void __user *ptr;


 if (unlikely(len > (((compat_uptr_t)~0) >> 1)))
  return NULL;

 ptr = arch_compat_alloc_user_space(len);

 if (unlikely(!access_ok(VERIFY_WRITE, ptr, len)))
  return NULL;

 return ptr;
}
EXPORT_SYMBOL_GPL(compat_alloc_user_space);


 (sizeof(kernel_config_data) - 1 - MAGIC_SIZE * 2)


static ssize_t
ikconfig_read_current(struct file *file, char __user *buf,
        size_t len, loff_t * offset)
{
 return simple_read_from_buffer(buf, len, offset,
           kernel_config_data + MAGIC_SIZE,
           kernel_config_data_size);
}

static const struct file_operations ikconfig_file_ops = {
 .owner = THIS_MODULE,
 .read = ikconfig_read_current,
 .llseek = default_llseek,
};

static int __init ikconfig_init(void)
{
 struct proc_dir_entry *entry;


 entry = proc_create("config.gz", S_IFREG | S_IRUGO, NULL,
       &ikconfig_file_ops);
 if (!entry)
  return -ENOMEM;

 proc_set_size(entry, kernel_config_data_size);

 return 0;
}

static void __exit ikconfig_cleanup(void)
{
 remove_proc_entry("config.gz", NULL);
}

module_init(ikconfig_init);
module_exit(ikconfig_cleanup);


MODULE_LICENSE("GPL");
MODULE_AUTHOR("Randy Dunlap");
MODULE_DESCRIPTION("Echo the kernel .config file used to build the kernel");








static int orig_fgconsole, orig_kmsg;

static DEFINE_MUTEX(vt_switch_mutex);

struct pm_vt_switch {
 struct list_head head;
 struct device *dev;
 bool required;
};

static LIST_HEAD(pm_vt_switch_list);
void pm_vt_switch_required(struct device *dev, bool required)
{
 struct pm_vt_switch *entry, *tmp;

 mutex_lock(&vt_switch_mutex);
 list_for_each_entry(tmp, &pm_vt_switch_list, head) {
  if (tmp->dev == dev) {

   tmp->required = required;
   goto out;
  }
 }

 entry = kmalloc(sizeof(*entry), GFP_KERNEL);
 if (!entry)
  goto out;

 entry->required = required;
 entry->dev = dev;

 list_add(&entry->head, &pm_vt_switch_list);
out:
 mutex_unlock(&vt_switch_mutex);
}
EXPORT_SYMBOL(pm_vt_switch_required);







void pm_vt_switch_unregister(struct device *dev)
{
 struct pm_vt_switch *tmp;

 mutex_lock(&vt_switch_mutex);
 list_for_each_entry(tmp, &pm_vt_switch_list, head) {
  if (tmp->dev == dev) {
   list_del(&tmp->head);
   kfree(tmp);
   break;
  }
 }
 mutex_unlock(&vt_switch_mutex);
}
EXPORT_SYMBOL(pm_vt_switch_unregister);
static bool pm_vt_switch(void)
{
 struct pm_vt_switch *entry;
 bool ret = true;

 mutex_lock(&vt_switch_mutex);
 if (list_empty(&pm_vt_switch_list))
  goto out;

 if (!console_suspend_enabled)
  goto out;

 list_for_each_entry(entry, &pm_vt_switch_list, head) {
  if (entry->required)
   goto out;
 }

 ret = false;
out:
 mutex_unlock(&vt_switch_mutex);
 return ret;
}

int pm_prepare_console(void)
{
 if (!pm_vt_switch())
  return 0;

 orig_fgconsole = vt_move_to_console(SUSPEND_CONSOLE, 1);
 if (orig_fgconsole < 0)
  return 1;

 orig_kmsg = vt_kmsg_redirect(SUSPEND_CONSOLE);
 return 0;
}

void pm_restore_console(void)
{
 if (!pm_vt_switch())
  return;

 if (orig_fgconsole >= 0) {
  vt_move_to_console(orig_fgconsole, 0);
  vt_kmsg_redirect(orig_kmsg);
 }
}


DEFINE_STATIC_KEY_FALSE(context_tracking_enabled);
EXPORT_SYMBOL_GPL(context_tracking_enabled);

DEFINE_PER_CPU(struct context_tracking, context_tracking);
EXPORT_SYMBOL_GPL(context_tracking);

static bool context_tracking_recursion_enter(void)
{
 int recursion;

 recursion = __this_cpu_inc_return(context_tracking.recursion);
 if (recursion == 1)
  return true;

 WARN_ONCE((recursion < 1), "Invalid context tracking recursion value %d\n", recursion);
 __this_cpu_dec(context_tracking.recursion);

 return false;
}

static void context_tracking_recursion_exit(void)
{
 __this_cpu_dec(context_tracking.recursion);
}
void __context_tracking_enter(enum ctx_state state)
{

 WARN_ON_ONCE(!current->mm);

 if (!context_tracking_recursion_enter())
  return;

 if ( __this_cpu_read(context_tracking.state) != state) {
  if (__this_cpu_read(context_tracking.active)) {







   if (state == CONTEXT_USER) {
    trace_user_enter(0);
    vtime_user_enter(current);
   }
   rcu_user_enter();
  }
  __this_cpu_write(context_tracking.state, state);
 }
 context_tracking_recursion_exit();
}
NOKPROBE_SYMBOL(__context_tracking_enter);
EXPORT_SYMBOL_GPL(__context_tracking_enter);

void context_tracking_enter(enum ctx_state state)
{
 unsigned long flags;
 if (in_interrupt())
  return;

 local_irq_save(flags);
 __context_tracking_enter(state);
 local_irq_restore(flags);
}
NOKPROBE_SYMBOL(context_tracking_enter);
EXPORT_SYMBOL_GPL(context_tracking_enter);

void context_tracking_user_enter(void)
{
 user_enter();
}
NOKPROBE_SYMBOL(context_tracking_user_enter);
void __context_tracking_exit(enum ctx_state state)
{
 if (!context_tracking_recursion_enter())
  return;

 if (__this_cpu_read(context_tracking.state) == state) {
  if (__this_cpu_read(context_tracking.active)) {




   rcu_user_exit();
   if (state == CONTEXT_USER) {
    vtime_user_exit(current);
    trace_user_exit(0);
   }
  }
  __this_cpu_write(context_tracking.state, CONTEXT_KERNEL);
 }
 context_tracking_recursion_exit();
}
NOKPROBE_SYMBOL(__context_tracking_exit);
EXPORT_SYMBOL_GPL(__context_tracking_exit);

void context_tracking_exit(enum ctx_state state)
{
 unsigned long flags;

 if (in_interrupt())
  return;

 local_irq_save(flags);
 __context_tracking_exit(state);
 local_irq_restore(flags);
}
NOKPROBE_SYMBOL(context_tracking_exit);
EXPORT_SYMBOL_GPL(context_tracking_exit);

void context_tracking_user_exit(void)
{
 user_exit();
}
NOKPROBE_SYMBOL(context_tracking_user_exit);

void __init context_tracking_cpu_set(int cpu)
{
 static __initdata bool initialized = false;

 if (!per_cpu(context_tracking.active, cpu)) {
  per_cpu(context_tracking.active, cpu) = true;
  static_branch_inc(&context_tracking_enabled);
 }

 if (initialized)
  return;





 set_tsk_thread_flag(&init_task, TIF_NOHZ);
 WARN_ON_ONCE(!tasklist_empty());

 initialized = true;
}

void __init context_tracking_init(void)
{
 int cpu;

 for_each_possible_cpu(cpu)
  context_tracking_cpu_set(cpu);
}










void *bpf_internal_load_pointer_neg_helper(const struct sk_buff *skb, int k, unsigned int size)
{
 u8 *ptr = NULL;

 if (k >= SKF_NET_OFF)
  ptr = skb_network_header(skb) + k - SKF_NET_OFF;
 else if (k >= SKF_LL_OFF)
  ptr = skb_mac_header(skb) + k - SKF_LL_OFF;

 if (ptr >= skb->head && ptr + size <= skb_tail_pointer(skb))
  return ptr;

 return NULL;
}

struct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags)
{
 gfp_t gfp_flags = GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO |
     gfp_extra_flags;
 struct bpf_prog_aux *aux;
 struct bpf_prog *fp;

 size = round_up(size, PAGE_SIZE);
 fp = __vmalloc(size, gfp_flags, PAGE_KERNEL);
 if (fp == NULL)
  return NULL;

 kmemcheck_annotate_bitfield(fp, meta);

 aux = kzalloc(sizeof(*aux), GFP_KERNEL | gfp_extra_flags);
 if (aux == NULL) {
  vfree(fp);
  return NULL;
 }

 fp->pages = size / PAGE_SIZE;
 fp->aux = aux;
 fp->aux->prog = fp;

 return fp;
}
EXPORT_SYMBOL_GPL(bpf_prog_alloc);

struct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,
      gfp_t gfp_extra_flags)
{
 gfp_t gfp_flags = GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO |
     gfp_extra_flags;
 struct bpf_prog *fp;

 BUG_ON(fp_old == NULL);

 size = round_up(size, PAGE_SIZE);
 if (size <= fp_old->pages * PAGE_SIZE)
  return fp_old;

 fp = __vmalloc(size, gfp_flags, PAGE_KERNEL);
 if (fp != NULL) {
  kmemcheck_annotate_bitfield(fp, meta);

  memcpy(fp, fp_old, fp_old->pages * PAGE_SIZE);
  fp->pages = size / PAGE_SIZE;
  fp->aux->prog = fp;




  fp_old->aux = NULL;
  __bpf_prog_free(fp_old);
 }

 return fp;
}

void __bpf_prog_free(struct bpf_prog *fp)
{
 kfree(fp->aux);
 vfree(fp);
}

static bool bpf_is_jmp_and_has_target(const struct bpf_insn *insn)
{
 return BPF_CLASS(insn->code) == BPF_JMP &&



        BPF_OP(insn->code) != BPF_CALL &&
        BPF_OP(insn->code) != BPF_EXIT;
}

static void bpf_adj_branches(struct bpf_prog *prog, u32 pos, u32 delta)
{
 struct bpf_insn *insn = prog->insnsi;
 u32 i, insn_cnt = prog->len;

 for (i = 0; i < insn_cnt; i++, insn++) {
  if (!bpf_is_jmp_and_has_target(insn))
   continue;


  if (i < pos && i + insn->off + 1 > pos)
   insn->off += delta;
  else if (i > pos + delta && i + insn->off + 1 <= pos + delta)
   insn->off -= delta;
 }
}

struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
           const struct bpf_insn *patch, u32 len)
{
 u32 insn_adj_cnt, insn_rest, insn_delta = len - 1;
 struct bpf_prog *prog_adj;


 if (insn_delta == 0) {
  memcpy(prog->insnsi + off, patch, sizeof(*patch));
  return prog;
 }

 insn_adj_cnt = prog->len + insn_delta;





 prog_adj = bpf_prog_realloc(prog, bpf_prog_size(insn_adj_cnt),
        GFP_USER);
 if (!prog_adj)
  return NULL;

 prog_adj->len = insn_adj_cnt;
 insn_rest = insn_adj_cnt - off - len;

 memmove(prog_adj->insnsi + off + len, prog_adj->insnsi + off + 1,
  sizeof(*patch) * insn_rest);
 memcpy(prog_adj->insnsi + off, patch, sizeof(*patch) * len);

 bpf_adj_branches(prog_adj, off, insn_delta);

 return prog_adj;
}

struct bpf_binary_header *
bpf_jit_binary_alloc(unsigned int proglen, u8 **image_ptr,
       unsigned int alignment,
       bpf_jit_fill_hole_t bpf_fill_ill_insns)
{
 struct bpf_binary_header *hdr;
 unsigned int size, hole, start;





 size = round_up(proglen + sizeof(*hdr) + 128, PAGE_SIZE);
 hdr = module_alloc(size);
 if (hdr == NULL)
  return NULL;


 bpf_fill_ill_insns(hdr, size);

 hdr->pages = size / PAGE_SIZE;
 hole = min_t(unsigned int, size - (proglen + sizeof(*hdr)),
       PAGE_SIZE - sizeof(*hdr));
 start = (get_random_int() % hole) & ~(alignment - 1);


 *image_ptr = &hdr->image[start];

 return hdr;
}

void bpf_jit_binary_free(struct bpf_binary_header *hdr)
{
 module_memfree(hdr);
}

int bpf_jit_harden __read_mostly;

static int bpf_jit_blind_insn(const struct bpf_insn *from,
         const struct bpf_insn *aux,
         struct bpf_insn *to_buff)
{
 struct bpf_insn *to = to_buff;
 u32 imm_rnd = get_random_int();
 s16 off;

 BUILD_BUG_ON(BPF_REG_AX + 1 != MAX_BPF_JIT_REG);
 BUILD_BUG_ON(MAX_BPF_REG + 1 != MAX_BPF_JIT_REG);

 if (from->imm == 0 &&
     (from->code == (BPF_ALU | BPF_MOV | BPF_K) ||
      from->code == (BPF_ALU64 | BPF_MOV | BPF_K))) {
  *to++ = BPF_ALU64_REG(BPF_XOR, from->dst_reg, from->dst_reg);
  goto out;
 }

 switch (from->code) {
 case BPF_ALU | BPF_ADD | BPF_K:
 case BPF_ALU | BPF_SUB | BPF_K:
 case BPF_ALU | BPF_AND | BPF_K:
 case BPF_ALU | BPF_OR | BPF_K:
 case BPF_ALU | BPF_XOR | BPF_K:
 case BPF_ALU | BPF_MUL | BPF_K:
 case BPF_ALU | BPF_MOV | BPF_K:
 case BPF_ALU | BPF_DIV | BPF_K:
 case BPF_ALU | BPF_MOD | BPF_K:
  *to++ = BPF_ALU32_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);
  *to++ = BPF_ALU32_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);
  *to++ = BPF_ALU32_REG(from->code, from->dst_reg, BPF_REG_AX);
  break;

 case BPF_ALU64 | BPF_ADD | BPF_K:
 case BPF_ALU64 | BPF_SUB | BPF_K:
 case BPF_ALU64 | BPF_AND | BPF_K:
 case BPF_ALU64 | BPF_OR | BPF_K:
 case BPF_ALU64 | BPF_XOR | BPF_K:
 case BPF_ALU64 | BPF_MUL | BPF_K:
 case BPF_ALU64 | BPF_MOV | BPF_K:
 case BPF_ALU64 | BPF_DIV | BPF_K:
 case BPF_ALU64 | BPF_MOD | BPF_K:
  *to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);
  *to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);
  *to++ = BPF_ALU64_REG(from->code, from->dst_reg, BPF_REG_AX);
  break;

 case BPF_JMP | BPF_JEQ | BPF_K:
 case BPF_JMP | BPF_JNE | BPF_K:
 case BPF_JMP | BPF_JGT | BPF_K:
 case BPF_JMP | BPF_JGE | BPF_K:
 case BPF_JMP | BPF_JSGT | BPF_K:
 case BPF_JMP | BPF_JSGE | BPF_K:
 case BPF_JMP | BPF_JSET | BPF_K:

  off = from->off;
  if (off < 0)
   off -= 2;
  *to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);
  *to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);
  *to++ = BPF_JMP_REG(from->code, from->dst_reg, BPF_REG_AX, off);
  break;

 case BPF_LD | BPF_ABS | BPF_W:
 case BPF_LD | BPF_ABS | BPF_H:
 case BPF_LD | BPF_ABS | BPF_B:
  *to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);
  *to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);
  *to++ = BPF_LD_IND(from->code, BPF_REG_AX, 0);
  break;

 case BPF_LD | BPF_IND | BPF_W:
 case BPF_LD | BPF_IND | BPF_H:
 case BPF_LD | BPF_IND | BPF_B:
  *to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);
  *to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);
  *to++ = BPF_ALU32_REG(BPF_ADD, BPF_REG_AX, from->src_reg);
  *to++ = BPF_LD_IND(from->code, BPF_REG_AX, 0);
  break;

 case BPF_LD | BPF_IMM | BPF_DW:
  *to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ aux[1].imm);
  *to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);
  *to++ = BPF_ALU64_IMM(BPF_LSH, BPF_REG_AX, 32);
  *to++ = BPF_ALU64_REG(BPF_MOV, aux[0].dst_reg, BPF_REG_AX);
  break;
 case 0:
  *to++ = BPF_ALU32_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ aux[0].imm);
  *to++ = BPF_ALU32_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);
  *to++ = BPF_ALU64_REG(BPF_OR, aux[0].dst_reg, BPF_REG_AX);
  break;

 case BPF_ST | BPF_MEM | BPF_DW:
 case BPF_ST | BPF_MEM | BPF_W:
 case BPF_ST | BPF_MEM | BPF_H:
 case BPF_ST | BPF_MEM | BPF_B:
  *to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);
  *to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);
  *to++ = BPF_STX_MEM(from->code, from->dst_reg, BPF_REG_AX, from->off);
  break;
 }
out:
 return to - to_buff;
}

static struct bpf_prog *bpf_prog_clone_create(struct bpf_prog *fp_other,
           gfp_t gfp_extra_flags)
{
 gfp_t gfp_flags = GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO |
     gfp_extra_flags;
 struct bpf_prog *fp;

 fp = __vmalloc(fp_other->pages * PAGE_SIZE, gfp_flags, PAGE_KERNEL);
 if (fp != NULL) {
  kmemcheck_annotate_bitfield(fp, meta);





  memcpy(fp, fp_other, fp_other->pages * PAGE_SIZE);
 }

 return fp;
}

static void bpf_prog_clone_free(struct bpf_prog *fp)
{







 fp->aux = NULL;
 __bpf_prog_free(fp);
}

void bpf_jit_prog_release_other(struct bpf_prog *fp, struct bpf_prog *fp_other)
{



 fp->aux->prog = fp;
 bpf_prog_clone_free(fp_other);
}

struct bpf_prog *bpf_jit_blind_constants(struct bpf_prog *prog)
{
 struct bpf_insn insn_buff[16], aux[2];
 struct bpf_prog *clone, *tmp;
 int insn_delta, insn_cnt;
 struct bpf_insn *insn;
 int i, rewritten;

 if (!bpf_jit_blinding_enabled())
  return prog;

 clone = bpf_prog_clone_create(prog, GFP_USER);
 if (!clone)
  return ERR_PTR(-ENOMEM);

 insn_cnt = clone->len;
 insn = clone->insnsi;

 for (i = 0; i < insn_cnt; i++, insn++) {




  if (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW) &&
      insn[1].code == 0)
   memcpy(aux, insn, sizeof(aux));

  rewritten = bpf_jit_blind_insn(insn, aux, insn_buff);
  if (!rewritten)
   continue;

  tmp = bpf_patch_insn_single(clone, i, insn_buff, rewritten);
  if (!tmp) {




   bpf_jit_prog_release_other(prog, clone);
   return ERR_PTR(-ENOMEM);
  }

  clone = tmp;
  insn_delta = rewritten - 1;


  insn = clone->insnsi + i + insn_delta;
  insn_cnt += insn_delta;
  i += insn_delta;
 }

 return clone;
}





noinline u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
{
 return 0;
}
EXPORT_SYMBOL_GPL(__bpf_call_base);
static unsigned int __bpf_prog_run(void *ctx, const struct bpf_insn *insn)
{
 u64 stack[MAX_BPF_STACK / sizeof(u64)];
 u64 regs[MAX_BPF_REG], tmp;
 static const void *jumptable[256] = {
  [0 ... 255] = &&default_label,


  [BPF_ALU | BPF_ADD | BPF_X] = &&ALU_ADD_X,
  [BPF_ALU | BPF_ADD | BPF_K] = &&ALU_ADD_K,
  [BPF_ALU | BPF_SUB | BPF_X] = &&ALU_SUB_X,
  [BPF_ALU | BPF_SUB | BPF_K] = &&ALU_SUB_K,
  [BPF_ALU | BPF_AND | BPF_X] = &&ALU_AND_X,
  [BPF_ALU | BPF_AND | BPF_K] = &&ALU_AND_K,
  [BPF_ALU | BPF_OR | BPF_X] = &&ALU_OR_X,
  [BPF_ALU | BPF_OR | BPF_K] = &&ALU_OR_K,
  [BPF_ALU | BPF_LSH | BPF_X] = &&ALU_LSH_X,
  [BPF_ALU | BPF_LSH | BPF_K] = &&ALU_LSH_K,
  [BPF_ALU | BPF_RSH | BPF_X] = &&ALU_RSH_X,
  [BPF_ALU | BPF_RSH | BPF_K] = &&ALU_RSH_K,
  [BPF_ALU | BPF_XOR | BPF_X] = &&ALU_XOR_X,
  [BPF_ALU | BPF_XOR | BPF_K] = &&ALU_XOR_K,
  [BPF_ALU | BPF_MUL | BPF_X] = &&ALU_MUL_X,
  [BPF_ALU | BPF_MUL | BPF_K] = &&ALU_MUL_K,
  [BPF_ALU | BPF_MOV | BPF_X] = &&ALU_MOV_X,
  [BPF_ALU | BPF_MOV | BPF_K] = &&ALU_MOV_K,
  [BPF_ALU | BPF_DIV | BPF_X] = &&ALU_DIV_X,
  [BPF_ALU | BPF_DIV | BPF_K] = &&ALU_DIV_K,
  [BPF_ALU | BPF_MOD | BPF_X] = &&ALU_MOD_X,
  [BPF_ALU | BPF_MOD | BPF_K] = &&ALU_MOD_K,
  [BPF_ALU | BPF_NEG] = &&ALU_NEG,
  [BPF_ALU | BPF_END | BPF_TO_BE] = &&ALU_END_TO_BE,
  [BPF_ALU | BPF_END | BPF_TO_LE] = &&ALU_END_TO_LE,

  [BPF_ALU64 | BPF_ADD | BPF_X] = &&ALU64_ADD_X,
  [BPF_ALU64 | BPF_ADD | BPF_K] = &&ALU64_ADD_K,
  [BPF_ALU64 | BPF_SUB | BPF_X] = &&ALU64_SUB_X,
  [BPF_ALU64 | BPF_SUB | BPF_K] = &&ALU64_SUB_K,
  [BPF_ALU64 | BPF_AND | BPF_X] = &&ALU64_AND_X,
  [BPF_ALU64 | BPF_AND | BPF_K] = &&ALU64_AND_K,
  [BPF_ALU64 | BPF_OR | BPF_X] = &&ALU64_OR_X,
  [BPF_ALU64 | BPF_OR | BPF_K] = &&ALU64_OR_K,
  [BPF_ALU64 | BPF_LSH | BPF_X] = &&ALU64_LSH_X,
  [BPF_ALU64 | BPF_LSH | BPF_K] = &&ALU64_LSH_K,
  [BPF_ALU64 | BPF_RSH | BPF_X] = &&ALU64_RSH_X,
  [BPF_ALU64 | BPF_RSH | BPF_K] = &&ALU64_RSH_K,
  [BPF_ALU64 | BPF_XOR | BPF_X] = &&ALU64_XOR_X,
  [BPF_ALU64 | BPF_XOR | BPF_K] = &&ALU64_XOR_K,
  [BPF_ALU64 | BPF_MUL | BPF_X] = &&ALU64_MUL_X,
  [BPF_ALU64 | BPF_MUL | BPF_K] = &&ALU64_MUL_K,
  [BPF_ALU64 | BPF_MOV | BPF_X] = &&ALU64_MOV_X,
  [BPF_ALU64 | BPF_MOV | BPF_K] = &&ALU64_MOV_K,
  [BPF_ALU64 | BPF_ARSH | BPF_X] = &&ALU64_ARSH_X,
  [BPF_ALU64 | BPF_ARSH | BPF_K] = &&ALU64_ARSH_K,
  [BPF_ALU64 | BPF_DIV | BPF_X] = &&ALU64_DIV_X,
  [BPF_ALU64 | BPF_DIV | BPF_K] = &&ALU64_DIV_K,
  [BPF_ALU64 | BPF_MOD | BPF_X] = &&ALU64_MOD_X,
  [BPF_ALU64 | BPF_MOD | BPF_K] = &&ALU64_MOD_K,
  [BPF_ALU64 | BPF_NEG] = &&ALU64_NEG,

  [BPF_JMP | BPF_CALL] = &&JMP_CALL,
  [BPF_JMP | BPF_CALL | BPF_X] = &&JMP_TAIL_CALL,

  [BPF_JMP | BPF_JA] = &&JMP_JA,
  [BPF_JMP | BPF_JEQ | BPF_X] = &&JMP_JEQ_X,
  [BPF_JMP | BPF_JEQ | BPF_K] = &&JMP_JEQ_K,
  [BPF_JMP | BPF_JNE | BPF_X] = &&JMP_JNE_X,
  [BPF_JMP | BPF_JNE | BPF_K] = &&JMP_JNE_K,
  [BPF_JMP | BPF_JGT | BPF_X] = &&JMP_JGT_X,
  [BPF_JMP | BPF_JGT | BPF_K] = &&JMP_JGT_K,
  [BPF_JMP | BPF_JGE | BPF_X] = &&JMP_JGE_X,
  [BPF_JMP | BPF_JGE | BPF_K] = &&JMP_JGE_K,
  [BPF_JMP | BPF_JSGT | BPF_X] = &&JMP_JSGT_X,
  [BPF_JMP | BPF_JSGT | BPF_K] = &&JMP_JSGT_K,
  [BPF_JMP | BPF_JSGE | BPF_X] = &&JMP_JSGE_X,
  [BPF_JMP | BPF_JSGE | BPF_K] = &&JMP_JSGE_K,
  [BPF_JMP | BPF_JSET | BPF_X] = &&JMP_JSET_X,
  [BPF_JMP | BPF_JSET | BPF_K] = &&JMP_JSET_K,

  [BPF_JMP | BPF_EXIT] = &&JMP_EXIT,

  [BPF_STX | BPF_MEM | BPF_B] = &&STX_MEM_B,
  [BPF_STX | BPF_MEM | BPF_H] = &&STX_MEM_H,
  [BPF_STX | BPF_MEM | BPF_W] = &&STX_MEM_W,
  [BPF_STX | BPF_MEM | BPF_DW] = &&STX_MEM_DW,
  [BPF_STX | BPF_XADD | BPF_W] = &&STX_XADD_W,
  [BPF_STX | BPF_XADD | BPF_DW] = &&STX_XADD_DW,
  [BPF_ST | BPF_MEM | BPF_B] = &&ST_MEM_B,
  [BPF_ST | BPF_MEM | BPF_H] = &&ST_MEM_H,
  [BPF_ST | BPF_MEM | BPF_W] = &&ST_MEM_W,
  [BPF_ST | BPF_MEM | BPF_DW] = &&ST_MEM_DW,

  [BPF_LDX | BPF_MEM | BPF_B] = &&LDX_MEM_B,
  [BPF_LDX | BPF_MEM | BPF_H] = &&LDX_MEM_H,
  [BPF_LDX | BPF_MEM | BPF_W] = &&LDX_MEM_W,
  [BPF_LDX | BPF_MEM | BPF_DW] = &&LDX_MEM_DW,
  [BPF_LD | BPF_ABS | BPF_W] = &&LD_ABS_W,
  [BPF_LD | BPF_ABS | BPF_H] = &&LD_ABS_H,
  [BPF_LD | BPF_ABS | BPF_B] = &&LD_ABS_B,
  [BPF_LD | BPF_IND | BPF_W] = &&LD_IND_W,
  [BPF_LD | BPF_IND | BPF_H] = &&LD_IND_H,
  [BPF_LD | BPF_IND | BPF_B] = &&LD_IND_B,
  [BPF_LD | BPF_IMM | BPF_DW] = &&LD_IMM_DW,
 };
 u32 tail_call_cnt = 0;
 void *ptr;
 int off;


 FP = (u64) (unsigned long) &stack[ARRAY_SIZE(stack)];
 ARG1 = (u64) (unsigned long) ctx;

select_insn:
 goto *jumptable[insn->code];


 ALU64_##OPCODE##_X: \
  DST = DST OP SRC; \
  CONT; \
 ALU_##OPCODE##_X: \
  DST = (u32) DST OP (u32) SRC; \
  CONT; \
 ALU64_##OPCODE##_K: \
  DST = DST OP IMM; \
  CONT; \
 ALU_##OPCODE##_K: \
  DST = (u32) DST OP (u32) IMM; \
  CONT;

 ALU(ADD, +)
 ALU(SUB, -)
 ALU(AND, &)
 ALU(OR, |)
 ALU(LSH, <<)
 ALU(RSH, >>)
 ALU(XOR, ^)
 ALU(MUL, *)
 ALU_NEG:
  DST = (u32) -DST;
  CONT;
 ALU64_NEG:
  DST = -DST;
  CONT;
 ALU_MOV_X:
  DST = (u32) SRC;
  CONT;
 ALU_MOV_K:
  DST = (u32) IMM;
  CONT;
 ALU64_MOV_X:
  DST = SRC;
  CONT;
 ALU64_MOV_K:
  DST = IMM;
  CONT;
 LD_IMM_DW:
  DST = (u64) (u32) insn[0].imm | ((u64) (u32) insn[1].imm) << 32;
  insn++;
  CONT;
 ALU64_ARSH_X:
  (*(s64 *) &DST) >>= SRC;
  CONT;
 ALU64_ARSH_K:
  (*(s64 *) &DST) >>= IMM;
  CONT;
 ALU64_MOD_X:
  if (unlikely(SRC == 0))
   return 0;
  div64_u64_rem(DST, SRC, &tmp);
  DST = tmp;
  CONT;
 ALU_MOD_X:
  if (unlikely(SRC == 0))
   return 0;
  tmp = (u32) DST;
  DST = do_div(tmp, (u32) SRC);
  CONT;
 ALU64_MOD_K:
  div64_u64_rem(DST, IMM, &tmp);
  DST = tmp;
  CONT;
 ALU_MOD_K:
  tmp = (u32) DST;
  DST = do_div(tmp, (u32) IMM);
  CONT;
 ALU64_DIV_X:
  if (unlikely(SRC == 0))
   return 0;
  DST = div64_u64(DST, SRC);
  CONT;
 ALU_DIV_X:
  if (unlikely(SRC == 0))
   return 0;
  tmp = (u32) DST;
  do_div(tmp, (u32) SRC);
  DST = (u32) tmp;
  CONT;
 ALU64_DIV_K:
  DST = div64_u64(DST, IMM);
  CONT;
 ALU_DIV_K:
  tmp = (u32) DST;
  do_div(tmp, (u32) IMM);
  DST = (u32) tmp;
  CONT;
 ALU_END_TO_BE:
  switch (IMM) {
  case 16:
   DST = (__force u16) cpu_to_be16(DST);
   break;
  case 32:
   DST = (__force u32) cpu_to_be32(DST);
   break;
  case 64:
   DST = (__force u64) cpu_to_be64(DST);
   break;
  }
  CONT;
 ALU_END_TO_LE:
  switch (IMM) {
  case 16:
   DST = (__force u16) cpu_to_le16(DST);
   break;
  case 32:
   DST = (__force u32) cpu_to_le32(DST);
   break;
  case 64:
   DST = (__force u64) cpu_to_le64(DST);
   break;
  }
  CONT;


 JMP_CALL:




  BPF_R0 = (__bpf_call_base + insn->imm)(BPF_R1, BPF_R2, BPF_R3,
             BPF_R4, BPF_R5);
  CONT;

 JMP_TAIL_CALL: {
  struct bpf_map *map = (struct bpf_map *) (unsigned long) BPF_R2;
  struct bpf_array *array = container_of(map, struct bpf_array, map);
  struct bpf_prog *prog;
  u64 index = BPF_R3;

  if (unlikely(index >= array->map.max_entries))
   goto out;

  if (unlikely(tail_call_cnt > MAX_TAIL_CALL_CNT))
   goto out;

  tail_call_cnt++;

  prog = READ_ONCE(array->ptrs[index]);
  if (unlikely(!prog))
   goto out;






  insn = prog->insnsi;
  goto select_insn;
out:
  CONT;
 }

 JMP_JA:
  insn += insn->off;
  CONT;
 JMP_JEQ_X:
  if (DST == SRC) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JEQ_K:
  if (DST == IMM) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JNE_X:
  if (DST != SRC) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JNE_K:
  if (DST != IMM) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JGT_X:
  if (DST > SRC) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JGT_K:
  if (DST > IMM) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JGE_X:
  if (DST >= SRC) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JGE_K:
  if (DST >= IMM) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JSGT_X:
  if (((s64) DST) > ((s64) SRC)) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JSGT_K:
  if (((s64) DST) > ((s64) IMM)) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JSGE_X:
  if (((s64) DST) >= ((s64) SRC)) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JSGE_K:
  if (((s64) DST) >= ((s64) IMM)) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JSET_X:
  if (DST & SRC) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_JSET_K:
  if (DST & IMM) {
   insn += insn->off;
   CONT_JMP;
  }
  CONT;
 JMP_EXIT:
  return BPF_R0;


 STX_MEM_##SIZEOP: \
  *(SIZE *)(unsigned long) (DST + insn->off) = SRC; \
  CONT; \
 ST_MEM_##SIZEOP: \
  *(SIZE *)(unsigned long) (DST + insn->off) = IMM; \
  CONT; \
 LDX_MEM_##SIZEOP: \
  DST = *(SIZE *)(unsigned long) (SRC + insn->off); \
  CONT;

 LDST(B, u8)
 LDST(H, u16)
 LDST(W, u32)
 LDST(DW, u64)
 STX_XADD_W:
  atomic_add((u32) SRC, (atomic_t *)(unsigned long)
      (DST + insn->off));
  CONT;
 STX_XADD_DW:
  atomic64_add((u64) SRC, (atomic64_t *)(unsigned long)
        (DST + insn->off));
  CONT;
 LD_ABS_W:
  off = IMM;
load_word:
  ptr = bpf_load_pointer((struct sk_buff *) (unsigned long) CTX, off, 4, &tmp);
  if (likely(ptr != NULL)) {
   BPF_R0 = get_unaligned_be32(ptr);
   CONT;
  }

  return 0;
 LD_ABS_H:
  off = IMM;
load_half:
  ptr = bpf_load_pointer((struct sk_buff *) (unsigned long) CTX, off, 2, &tmp);
  if (likely(ptr != NULL)) {
   BPF_R0 = get_unaligned_be16(ptr);
   CONT;
  }

  return 0;
 LD_ABS_B:
  off = IMM;
load_byte:
  ptr = bpf_load_pointer((struct sk_buff *) (unsigned long) CTX, off, 1, &tmp);
  if (likely(ptr != NULL)) {
   BPF_R0 = *(u8 *)ptr;
   CONT;
  }

  return 0;
 LD_IND_W:
  off = IMM + SRC;
  goto load_word;
 LD_IND_H:
  off = IMM + SRC;
  goto load_half;
 LD_IND_B:
  off = IMM + SRC;
  goto load_byte;

 default_label:

  WARN_RATELIMIT(1, "unknown opcode %02x\n", insn->code);
  return 0;
}
STACK_FRAME_NON_STANDARD(__bpf_prog_run);

bool bpf_prog_array_compatible(struct bpf_array *array,
          const struct bpf_prog *fp)
{
 if (!array->owner_prog_type) {



  array->owner_prog_type = fp->type;
  array->owner_jited = fp->jited;

  return true;
 }

 return array->owner_prog_type == fp->type &&
        array->owner_jited == fp->jited;
}

static int bpf_check_tail_call(const struct bpf_prog *fp)
{
 struct bpf_prog_aux *aux = fp->aux;
 int i;

 for (i = 0; i < aux->used_map_cnt; i++) {
  struct bpf_map *map = aux->used_maps[i];
  struct bpf_array *array;

  if (map->map_type != BPF_MAP_TYPE_PROG_ARRAY)
   continue;

  array = container_of(map, struct bpf_array, map);
  if (!bpf_prog_array_compatible(array, fp))
   return -EINVAL;
 }

 return 0;
}
struct bpf_prog *bpf_prog_select_runtime(struct bpf_prog *fp, int *err)
{
 fp->bpf_func = (void *) __bpf_prog_run;







 fp = bpf_int_jit_compile(fp);
 bpf_prog_lock_ro(fp);






 *err = bpf_check_tail_call(fp);

 return fp;
}
EXPORT_SYMBOL_GPL(bpf_prog_select_runtime);

static void bpf_prog_free_deferred(struct work_struct *work)
{
 struct bpf_prog_aux *aux;

 aux = container_of(work, struct bpf_prog_aux, work);
 bpf_jit_free(aux->prog);
}


void bpf_prog_free(struct bpf_prog *fp)
{
 struct bpf_prog_aux *aux = fp->aux;

 INIT_WORK(&aux->work, bpf_prog_free_deferred);
 schedule_work(&aux->work);
}
EXPORT_SYMBOL_GPL(bpf_prog_free);


static DEFINE_PER_CPU(struct rnd_state, bpf_user_rnd_state);

void bpf_user_rnd_init_once(void)
{
 prandom_init_once(&bpf_user_rnd_state);
}

u64 bpf_user_rnd_u32(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
{






 struct rnd_state *state;
 u32 res;

 state = &get_cpu_var(bpf_user_rnd_state);
 res = prandom_u32_state(state);
 put_cpu_var(state);

 return res;
}


const struct bpf_func_proto bpf_map_lookup_elem_proto __weak;
const struct bpf_func_proto bpf_map_update_elem_proto __weak;
const struct bpf_func_proto bpf_map_delete_elem_proto __weak;

const struct bpf_func_proto bpf_get_prandom_u32_proto __weak;
const struct bpf_func_proto bpf_get_smp_processor_id_proto __weak;
const struct bpf_func_proto bpf_ktime_get_ns_proto __weak;

const struct bpf_func_proto bpf_get_current_pid_tgid_proto __weak;
const struct bpf_func_proto bpf_get_current_uid_gid_proto __weak;
const struct bpf_func_proto bpf_get_current_comm_proto __weak;

const struct bpf_func_proto * __weak bpf_get_trace_printk_proto(void)
{
 return NULL;
}

const struct bpf_func_proto * __weak bpf_get_event_output_proto(void)
{
 return NULL;
}


const struct bpf_func_proto bpf_tail_call_proto = {
 .func = NULL,
 .gpl_only = false,
 .ret_type = RET_VOID,
 .arg1_type = ARG_PTR_TO_CTX,
 .arg2_type = ARG_CONST_MAP_PTR,
 .arg3_type = ARG_ANYTHING,
};


struct bpf_prog * __weak bpf_int_jit_compile(struct bpf_prog *prog)
{
 return prog;
}

bool __weak bpf_helper_changes_skb_data(void *func)
{
 return false;
}




int __weak skb_copy_bits(const struct sk_buff *skb, int offset, void *to,
    int len)
{
 return -EFAULT;
}







struct cpuhp_cpu_state {
 enum cpuhp_state state;
 enum cpuhp_state target;
 struct task_struct *thread;
 bool should_run;
 bool rollback;
 enum cpuhp_state cb_state;
 int (*cb)(unsigned int cpu);
 int result;
 struct completion done;
};

static DEFINE_PER_CPU(struct cpuhp_cpu_state, cpuhp_state);
struct cpuhp_step {
 const char *name;
 int (*startup)(unsigned int cpu);
 int (*teardown)(unsigned int cpu);
 bool skip_onerr;
 bool cant_stop;
};

static DEFINE_MUTEX(cpuhp_state_mutex);
static struct cpuhp_step cpuhp_bp_states[];
static struct cpuhp_step cpuhp_ap_states[];
static int cpuhp_invoke_callback(unsigned int cpu, enum cpuhp_state step,
     int (*cb)(unsigned int))
{
 struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 int ret = 0;

 if (cb) {
  trace_cpuhp_enter(cpu, st->target, step, cb);
  ret = cb(cpu);
  trace_cpuhp_exit(cpu, st->state, step, ret);
 }
 return ret;
}


static DEFINE_MUTEX(cpu_add_remove_lock);
bool cpuhp_tasks_frozen;
EXPORT_SYMBOL_GPL(cpuhp_tasks_frozen);
void cpu_maps_update_begin(void)
{
 mutex_lock(&cpu_add_remove_lock);
}
EXPORT_SYMBOL(cpu_notifier_register_begin);

void cpu_maps_update_done(void)
{
 mutex_unlock(&cpu_add_remove_lock);
}
EXPORT_SYMBOL(cpu_notifier_register_done);

static RAW_NOTIFIER_HEAD(cpu_chain);




static int cpu_hotplug_disabled;


static struct {
 struct task_struct *active_writer;

 wait_queue_head_t wq;

 struct mutex lock;




 atomic_t refcount;

 struct lockdep_map dep_map;
} cpu_hotplug = {
 .active_writer = NULL,
 .wq = __WAIT_QUEUE_HEAD_INITIALIZER(cpu_hotplug.wq),
 .lock = __MUTEX_INITIALIZER(cpu_hotplug.lock),
 .dep_map = {.name = "cpu_hotplug.lock" },
};


      lock_map_acquire_tryread(&cpu_hotplug.dep_map)


void get_online_cpus(void)
{
 might_sleep();
 if (cpu_hotplug.active_writer == current)
  return;
 cpuhp_lock_acquire_read();
 mutex_lock(&cpu_hotplug.lock);
 atomic_inc(&cpu_hotplug.refcount);
 mutex_unlock(&cpu_hotplug.lock);
}
EXPORT_SYMBOL_GPL(get_online_cpus);

void put_online_cpus(void)
{
 int refcount;

 if (cpu_hotplug.active_writer == current)
  return;

 refcount = atomic_dec_return(&cpu_hotplug.refcount);
 if (WARN_ON(refcount < 0))
  atomic_inc(&cpu_hotplug.refcount);

 if (refcount <= 0 && waitqueue_active(&cpu_hotplug.wq))
  wake_up(&cpu_hotplug.wq);

 cpuhp_lock_release();

}
EXPORT_SYMBOL_GPL(put_online_cpus);
void cpu_hotplug_begin(void)
{
 DEFINE_WAIT(wait);

 cpu_hotplug.active_writer = current;
 cpuhp_lock_acquire();

 for (;;) {
  mutex_lock(&cpu_hotplug.lock);
  prepare_to_wait(&cpu_hotplug.wq, &wait, TASK_UNINTERRUPTIBLE);
  if (likely(!atomic_read(&cpu_hotplug.refcount)))
    break;
  mutex_unlock(&cpu_hotplug.lock);
  schedule();
 }
 finish_wait(&cpu_hotplug.wq, &wait);
}

void cpu_hotplug_done(void)
{
 cpu_hotplug.active_writer = NULL;
 mutex_unlock(&cpu_hotplug.lock);
 cpuhp_lock_release();
}
void cpu_hotplug_disable(void)
{
 cpu_maps_update_begin();
 cpu_hotplug_disabled++;
 cpu_maps_update_done();
}
EXPORT_SYMBOL_GPL(cpu_hotplug_disable);

void cpu_hotplug_enable(void)
{
 cpu_maps_update_begin();
 WARN_ON(--cpu_hotplug_disabled < 0);
 cpu_maps_update_done();
}
EXPORT_SYMBOL_GPL(cpu_hotplug_enable);


int register_cpu_notifier(struct notifier_block *nb)
{
 int ret;
 cpu_maps_update_begin();
 ret = raw_notifier_chain_register(&cpu_chain, nb);
 cpu_maps_update_done();
 return ret;
}

int __register_cpu_notifier(struct notifier_block *nb)
{
 return raw_notifier_chain_register(&cpu_chain, nb);
}

static int __cpu_notify(unsigned long val, unsigned int cpu, int nr_to_call,
   int *nr_calls)
{
 unsigned long mod = cpuhp_tasks_frozen ? CPU_TASKS_FROZEN : 0;
 void *hcpu = (void *)(long)cpu;

 int ret;

 ret = __raw_notifier_call_chain(&cpu_chain, val | mod, hcpu, nr_to_call,
     nr_calls);

 return notifier_to_errno(ret);
}

static int cpu_notify(unsigned long val, unsigned int cpu)
{
 return __cpu_notify(val, cpu, -1, NULL);
}

static void cpu_notify_nofail(unsigned long val, unsigned int cpu)
{
 BUG_ON(cpu_notify(val, cpu));
}


static int notify_prepare(unsigned int cpu)
{
 int nr_calls = 0;
 int ret;

 ret = __cpu_notify(CPU_UP_PREPARE, cpu, -1, &nr_calls);
 if (ret) {
  nr_calls--;
  printk(KERN_WARNING "%s: attempt to bring up CPU %u failed\n",
    __func__, cpu);
  __cpu_notify(CPU_UP_CANCELED, cpu, nr_calls, NULL);
 }
 return ret;
}

static int notify_online(unsigned int cpu)
{
 cpu_notify(CPU_ONLINE, cpu);
 return 0;
}

static int notify_starting(unsigned int cpu)
{
 cpu_notify(CPU_STARTING, cpu);
 return 0;
}

static int bringup_wait_for_ap(unsigned int cpu)
{
 struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);

 wait_for_completion(&st->done);
 return st->result;
}

static int bringup_cpu(unsigned int cpu)
{
 struct task_struct *idle = idle_thread_get(cpu);
 int ret;


 ret = __cpu_up(cpu, idle);
 if (ret) {
  cpu_notify(CPU_UP_CANCELED, cpu);
  return ret;
 }
 ret = bringup_wait_for_ap(cpu);
 BUG_ON(!cpu_online(cpu));
 return ret;
}




static void undo_cpu_down(unsigned int cpu, struct cpuhp_cpu_state *st,
     struct cpuhp_step *steps)
{
 for (st->state++; st->state < st->target; st->state++) {
  struct cpuhp_step *step = steps + st->state;

  if (!step->skip_onerr)
   cpuhp_invoke_callback(cpu, st->state, step->startup);
 }
}

static int cpuhp_down_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,
    struct cpuhp_step *steps, enum cpuhp_state target)
{
 enum cpuhp_state prev_state = st->state;
 int ret = 0;

 for (; st->state > target; st->state--) {
  struct cpuhp_step *step = steps + st->state;

  ret = cpuhp_invoke_callback(cpu, st->state, step->teardown);
  if (ret) {
   st->target = prev_state;
   undo_cpu_down(cpu, st, steps);
   break;
  }
 }
 return ret;
}

static void undo_cpu_up(unsigned int cpu, struct cpuhp_cpu_state *st,
   struct cpuhp_step *steps)
{
 for (st->state--; st->state > st->target; st->state--) {
  struct cpuhp_step *step = steps + st->state;

  if (!step->skip_onerr)
   cpuhp_invoke_callback(cpu, st->state, step->teardown);
 }
}

static int cpuhp_up_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,
         struct cpuhp_step *steps, enum cpuhp_state target)
{
 enum cpuhp_state prev_state = st->state;
 int ret = 0;

 while (st->state < target) {
  struct cpuhp_step *step;

  st->state++;
  step = steps + st->state;
  ret = cpuhp_invoke_callback(cpu, st->state, step->startup);
  if (ret) {
   st->target = prev_state;
   undo_cpu_up(cpu, st, steps);
   break;
  }
 }
 return ret;
}




static void cpuhp_create(unsigned int cpu)
{
 struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);

 init_completion(&st->done);
}

static int cpuhp_should_run(unsigned int cpu)
{
 struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);

 return st->should_run;
}


static int cpuhp_ap_offline(unsigned int cpu, struct cpuhp_cpu_state *st)
{
 enum cpuhp_state target = max((int)st->target, CPUHP_TEARDOWN_CPU);

 return cpuhp_down_callbacks(cpu, st, cpuhp_ap_states, target);
}


static int cpuhp_ap_online(unsigned int cpu, struct cpuhp_cpu_state *st)
{
 return cpuhp_up_callbacks(cpu, st, cpuhp_ap_states, st->target);
}





static void cpuhp_thread_fun(unsigned int cpu)
{
 struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);
 int ret = 0;





 smp_mb();
 if (!st->should_run)
  return;

 st->should_run = false;


 if (st->cb) {
  if (st->cb_state < CPUHP_AP_ONLINE) {
   local_irq_disable();
   ret = cpuhp_invoke_callback(cpu, st->cb_state, st->cb);
   local_irq_enable();
  } else {
   ret = cpuhp_invoke_callback(cpu, st->cb_state, st->cb);
  }
 } else if (st->rollback) {
  BUG_ON(st->state < CPUHP_AP_ONLINE_IDLE);

  undo_cpu_down(cpu, st, cpuhp_ap_states);




  cpu_notify_nofail(CPU_DOWN_FAILED, cpu);
  st->rollback = false;
 } else {

  BUG_ON(st->state < CPUHP_AP_ONLINE_IDLE);


  if (st->state < st->target)
   ret = cpuhp_ap_online(cpu, st);
  else if (st->state > st->target)
   ret = cpuhp_ap_offline(cpu, st);
 }
 st->result = ret;
 complete(&st->done);
}


static int cpuhp_invoke_ap_callback(int cpu, enum cpuhp_state state,
        int (*cb)(unsigned int))
{
 struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);

 if (!cpu_online(cpu))
  return 0;

 st->cb_state = state;
 st->cb = cb;




 smp_mb();
 st->should_run = true;
 wake_up_process(st->thread);
 wait_for_completion(&st->done);
 return st->result;
}


static void __cpuhp_kick_ap_work(struct cpuhp_cpu_state *st)
{
 st->result = 0;
 st->cb = NULL;




 smp_mb();
 st->should_run = true;
 wake_up_process(st->thread);
}

static int cpuhp_kick_ap_work(unsigned int cpu)
{
 struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 enum cpuhp_state state = st->state;

 trace_cpuhp_enter(cpu, st->target, state, cpuhp_kick_ap_work);
 __cpuhp_kick_ap_work(st);
 wait_for_completion(&st->done);
 trace_cpuhp_exit(cpu, st->state, state, st->result);
 return st->result;
}

static struct smp_hotplug_thread cpuhp_threads = {
 .store = &cpuhp_state.thread,
 .create = &cpuhp_create,
 .thread_should_run = cpuhp_should_run,
 .thread_fn = cpuhp_thread_fun,
 .thread_comm = "cpuhp/%u",
 .selfparking = true,
};

void __init cpuhp_threads_init(void)
{
 BUG_ON(smpboot_register_percpu_thread(&cpuhp_threads));
 kthread_unpark(this_cpu_read(cpuhp_state.thread));
}

EXPORT_SYMBOL(register_cpu_notifier);
EXPORT_SYMBOL(__register_cpu_notifier);
void unregister_cpu_notifier(struct notifier_block *nb)
{
 cpu_maps_update_begin();
 raw_notifier_chain_unregister(&cpu_chain, nb);
 cpu_maps_update_done();
}
EXPORT_SYMBOL(unregister_cpu_notifier);

void __unregister_cpu_notifier(struct notifier_block *nb)
{
 raw_notifier_chain_unregister(&cpu_chain, nb);
}
EXPORT_SYMBOL(__unregister_cpu_notifier);
void clear_tasks_mm_cpumask(int cpu)
{
 struct task_struct *p;
 WARN_ON(cpu_online(cpu));
 rcu_read_lock();
 for_each_process(p) {
  struct task_struct *t;





  t = find_lock_task_mm(p);
  if (!t)
   continue;
  cpumask_clear_cpu(cpu, mm_cpumask(t->mm));
  task_unlock(t);
 }
 rcu_read_unlock();
}

static inline void check_for_tasks(int dead_cpu)
{
 struct task_struct *g, *p;

 read_lock(&tasklist_lock);
 for_each_process_thread(g, p) {
  if (!p->on_rq)
   continue;






  rmb();
  if (task_cpu(p) != dead_cpu)
   continue;

  pr_warn("Task %s (pid=%d) is on cpu %d (state=%ld, flags=%x)\n",
   p->comm, task_pid_nr(p), dead_cpu, p->state, p->flags);
 }
 read_unlock(&tasklist_lock);
}

static int notify_down_prepare(unsigned int cpu)
{
 int err, nr_calls = 0;

 err = __cpu_notify(CPU_DOWN_PREPARE, cpu, -1, &nr_calls);
 if (err) {
  nr_calls--;
  __cpu_notify(CPU_DOWN_FAILED, cpu, nr_calls, NULL);
  pr_warn("%s: attempt to take down CPU %u failed\n",
    __func__, cpu);
 }
 return err;
}

static int notify_dying(unsigned int cpu)
{
 cpu_notify(CPU_DYING, cpu);
 return 0;
}


static int take_cpu_down(void *_param)
{
 struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);
 enum cpuhp_state target = max((int)st->target, CPUHP_AP_OFFLINE);
 int err, cpu = smp_processor_id();


 err = __cpu_disable();
 if (err < 0)
  return err;


 for (; st->state > target; st->state--) {
  struct cpuhp_step *step = cpuhp_ap_states + st->state;

  cpuhp_invoke_callback(cpu, st->state, step->teardown);
 }

 tick_handover_do_timer();

 stop_machine_park(cpu);
 return 0;
}

static int takedown_cpu(unsigned int cpu)
{
 struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 int err;


 kthread_park(per_cpu_ptr(&cpuhp_state, cpu)->thread);
 smpboot_park_threads(cpu);





 irq_lock_sparse();




 err = stop_machine(take_cpu_down, NULL, cpumask_of(cpu));
 if (err) {

  irq_unlock_sparse();

  kthread_unpark(per_cpu_ptr(&cpuhp_state, cpu)->thread);
  return err;
 }
 BUG_ON(cpu_online(cpu));
 wait_for_completion(&st->done);
 BUG_ON(st->state != CPUHP_AP_IDLE_DEAD);


 irq_unlock_sparse();

 hotplug_cpu__broadcast_tick_pull(cpu);

 __cpu_die(cpu);

 tick_cleanup_dead_cpu(cpu);
 return 0;
}

static int notify_dead(unsigned int cpu)
{
 cpu_notify_nofail(CPU_DEAD, cpu);
 check_for_tasks(cpu);
 return 0;
}

static void cpuhp_complete_idle_dead(void *arg)
{
 struct cpuhp_cpu_state *st = arg;

 complete(&st->done);
}

void cpuhp_report_idle_dead(void)
{
 struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);

 BUG_ON(st->state != CPUHP_AP_OFFLINE);
 rcu_report_dead(smp_processor_id());
 st->state = CPUHP_AP_IDLE_DEAD;




 smp_call_function_single(cpumask_first(cpu_online_mask),
     cpuhp_complete_idle_dead, st, 0);
}




static int __ref _cpu_down(unsigned int cpu, int tasks_frozen,
      enum cpuhp_state target)
{
 struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 int prev_state, ret = 0;
 bool hasdied = false;

 if (num_online_cpus() == 1)
  return -EBUSY;

 if (!cpu_present(cpu))
  return -EINVAL;

 cpu_hotplug_begin();

 cpuhp_tasks_frozen = tasks_frozen;

 prev_state = st->state;
 st->target = target;




 if (st->state > CPUHP_TEARDOWN_CPU) {
  ret = cpuhp_kick_ap_work(cpu);




  if (ret)
   goto out;





  if (st->state > CPUHP_TEARDOWN_CPU)
   goto out;
 }




 ret = cpuhp_down_callbacks(cpu, st, cpuhp_bp_states, target);
 if (ret && st->state > CPUHP_TEARDOWN_CPU && st->state < prev_state) {
  st->target = prev_state;
  st->rollback = true;
  cpuhp_kick_ap_work(cpu);
 }

 hasdied = prev_state != st->state && st->state == CPUHP_OFFLINE;
out:
 cpu_hotplug_done();

 if (!ret && hasdied)
  cpu_notify_nofail(CPU_POST_DEAD, cpu);
 return ret;
}

static int do_cpu_down(unsigned int cpu, enum cpuhp_state target)
{
 int err;

 cpu_maps_update_begin();

 if (cpu_hotplug_disabled) {
  err = -EBUSY;
  goto out;
 }

 err = _cpu_down(cpu, 0, target);

out:
 cpu_maps_update_done();
 return err;
}
int cpu_down(unsigned int cpu)
{
 return do_cpu_down(cpu, CPUHP_OFFLINE);
}
EXPORT_SYMBOL(cpu_down);
void notify_cpu_starting(unsigned int cpu)
{
 struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 enum cpuhp_state target = min((int)st->target, CPUHP_AP_ONLINE);

 while (st->state < target) {
  struct cpuhp_step *step;

  st->state++;
  step = cpuhp_ap_states + st->state;
  cpuhp_invoke_callback(cpu, st->state, step->startup);
 }
}







void cpuhp_online_idle(enum cpuhp_state state)
{
 struct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);
 unsigned int cpu = smp_processor_id();


 if (state != CPUHP_AP_ONLINE_IDLE)
  return;

 st->state = CPUHP_AP_ONLINE_IDLE;


 stop_machine_unpark(cpu);
 kthread_unpark(st->thread);


 if (st->target > CPUHP_AP_ONLINE_IDLE)
  __cpuhp_kick_ap_work(st);
 else
  complete(&st->done);
}


static int _cpu_up(unsigned int cpu, int tasks_frozen, enum cpuhp_state target)
{
 struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
 struct task_struct *idle;
 int ret = 0;

 cpu_hotplug_begin();

 if (!cpu_present(cpu)) {
  ret = -EINVAL;
  goto out;
 }





 if (st->state >= target)
  goto out;

 if (st->state == CPUHP_OFFLINE) {

  idle = idle_thread_get(cpu);
  if (IS_ERR(idle)) {
   ret = PTR_ERR(idle);
   goto out;
  }
 }

 cpuhp_tasks_frozen = tasks_frozen;

 st->target = target;




 if (st->state > CPUHP_BRINGUP_CPU) {
  ret = cpuhp_kick_ap_work(cpu);




  if (ret)
   goto out;
 }






 target = min((int)target, CPUHP_BRINGUP_CPU);
 ret = cpuhp_up_callbacks(cpu, st, cpuhp_bp_states, target);
out:
 cpu_hotplug_done();
 return ret;
}

static int do_cpu_up(unsigned int cpu, enum cpuhp_state target)
{
 int err = 0;

 if (!cpu_possible(cpu)) {
  pr_err("can't online cpu %d because it is not configured as may-hotadd at boot time\n",
         cpu);
  pr_err("please check additional_cpus= boot parameter\n");
  return -EINVAL;
 }

 err = try_online_node(cpu_to_node(cpu));
 if (err)
  return err;

 cpu_maps_update_begin();

 if (cpu_hotplug_disabled) {
  err = -EBUSY;
  goto out;
 }

 err = _cpu_up(cpu, 0, target);
out:
 cpu_maps_update_done();
 return err;
}

int cpu_up(unsigned int cpu)
{
 return do_cpu_up(cpu, CPUHP_ONLINE);
}
EXPORT_SYMBOL_GPL(cpu_up);

static cpumask_var_t frozen_cpus;

int disable_nonboot_cpus(void)
{
 int cpu, first_cpu, error = 0;

 cpu_maps_update_begin();
 first_cpu = cpumask_first(cpu_online_mask);




 cpumask_clear(frozen_cpus);

 pr_info("Disabling non-boot CPUs ...\n");
 for_each_online_cpu(cpu) {
  if (cpu == first_cpu)
   continue;
  trace_suspend_resume(TPS("CPU_OFF"), cpu, true);
  error = _cpu_down(cpu, 1, CPUHP_OFFLINE);
  trace_suspend_resume(TPS("CPU_OFF"), cpu, false);
  if (!error)
   cpumask_set_cpu(cpu, frozen_cpus);
  else {
   pr_err("Error taking CPU%d down: %d\n", cpu, error);
   break;
  }
 }

 if (!error)
  BUG_ON(num_online_cpus() > 1);
 else
  pr_err("Non-boot CPUs are not disabled\n");






 cpu_hotplug_disabled++;

 cpu_maps_update_done();
 return error;
}

void __weak arch_enable_nonboot_cpus_begin(void)
{
}

void __weak arch_enable_nonboot_cpus_end(void)
{
}

void enable_nonboot_cpus(void)
{
 int cpu, error;


 cpu_maps_update_begin();
 WARN_ON(--cpu_hotplug_disabled < 0);
 if (cpumask_empty(frozen_cpus))
  goto out;

 pr_info("Enabling non-boot CPUs ...\n");

 arch_enable_nonboot_cpus_begin();

 for_each_cpu(cpu, frozen_cpus) {
  trace_suspend_resume(TPS("CPU_ON"), cpu, true);
  error = _cpu_up(cpu, 1, CPUHP_ONLINE);
  trace_suspend_resume(TPS("CPU_ON"), cpu, false);
  if (!error) {
   pr_info("CPU%d is up\n", cpu);
   continue;
  }
  pr_warn("Error taking CPU%d up: %d\n", cpu, error);
 }

 arch_enable_nonboot_cpus_end();

 cpumask_clear(frozen_cpus);
out:
 cpu_maps_update_done();
}

static int __init alloc_frozen_cpus(void)
{
 if (!alloc_cpumask_var(&frozen_cpus, GFP_KERNEL|__GFP_ZERO))
  return -ENOMEM;
 return 0;
}
core_initcall(alloc_frozen_cpus);
static int
cpu_hotplug_pm_callback(struct notifier_block *nb,
   unsigned long action, void *ptr)
{
 switch (action) {

 case PM_SUSPEND_PREPARE:
 case PM_HIBERNATION_PREPARE:
  cpu_hotplug_disable();
  break;

 case PM_POST_SUSPEND:
 case PM_POST_HIBERNATION:
  cpu_hotplug_enable();
  break;

 default:
  return NOTIFY_DONE;
 }

 return NOTIFY_OK;
}


static int __init cpu_hotplug_pm_sync_init(void)
{





 pm_notifier(cpu_hotplug_pm_callback, 0);
 return 0;
}
core_initcall(cpu_hotplug_pm_sync_init);




static struct cpuhp_step cpuhp_bp_states[] = {
 [CPUHP_OFFLINE] = {
  .name = "offline",
  .startup = NULL,
  .teardown = NULL,
 },
 [CPUHP_CREATE_THREADS]= {
  .name = "threads:create",
  .startup = smpboot_create_threads,
  .teardown = NULL,
  .cant_stop = true,
 },




 [CPUHP_NOTIFY_PREPARE] = {
  .name = "notify:prepare",
  .startup = notify_prepare,
  .teardown = notify_dead,
  .skip_onerr = true,
  .cant_stop = true,
 },

 [CPUHP_BRINGUP_CPU] = {
  .name = "cpu:bringup",
  .startup = bringup_cpu,
  .teardown = NULL,
  .cant_stop = true,
 },




 [CPUHP_TEARDOWN_CPU] = {
  .name = "cpu:teardown",
  .startup = NULL,
  .teardown = takedown_cpu,
  .cant_stop = true,
 },
};


static struct cpuhp_step cpuhp_ap_states[] = {

 [CPUHP_AP_IDLE_DEAD] = {
  .name = "idle:dead",
 },




 [CPUHP_AP_OFFLINE] = {
  .name = "ap:offline",
  .cant_stop = true,
 },

 [CPUHP_AP_SCHED_STARTING] = {
  .name = "sched:starting",
  .startup = sched_cpu_starting,
  .teardown = sched_cpu_dying,
 },





 [CPUHP_AP_NOTIFY_STARTING] = {
  .name = "notify:starting",
  .startup = notify_starting,
  .teardown = notify_dying,
  .skip_onerr = true,
  .cant_stop = true,
 },


 [CPUHP_AP_ONLINE] = {
  .name = "ap:online",
 },

 [CPUHP_AP_SMPBOOT_THREADS] = {
  .name = "smpboot:threads",
  .startup = smpboot_unpark_threads,
  .teardown = NULL,
 },




 [CPUHP_AP_NOTIFY_ONLINE] = {
  .name = "notify:online",
  .startup = notify_online,
  .teardown = notify_down_prepare,
  .skip_onerr = true,
 },





 [CPUHP_AP_ACTIVE] = {
  .name = "sched:active",
  .startup = sched_cpu_activate,
  .teardown = sched_cpu_deactivate,
 },


 [CPUHP_ONLINE] = {
  .name = "online",
  .startup = NULL,
  .teardown = NULL,
 },
};


static int cpuhp_cb_check(enum cpuhp_state state)
{
 if (state <= CPUHP_OFFLINE || state >= CPUHP_ONLINE)
  return -EINVAL;
 return 0;
}

static bool cpuhp_is_ap_state(enum cpuhp_state state)
{




 return state > CPUHP_BRINGUP_CPU && state != CPUHP_TEARDOWN_CPU;
}

static struct cpuhp_step *cpuhp_get_step(enum cpuhp_state state)
{
 struct cpuhp_step *sp;

 sp = cpuhp_is_ap_state(state) ? cpuhp_ap_states : cpuhp_bp_states;
 return sp + state;
}

static void cpuhp_store_callbacks(enum cpuhp_state state,
      const char *name,
      int (*startup)(unsigned int cpu),
      int (*teardown)(unsigned int cpu))
{

 struct cpuhp_step *sp;

 mutex_lock(&cpuhp_state_mutex);
 sp = cpuhp_get_step(state);
 sp->startup = startup;
 sp->teardown = teardown;
 sp->name = name;
 mutex_unlock(&cpuhp_state_mutex);
}

static void *cpuhp_get_teardown_cb(enum cpuhp_state state)
{
 return cpuhp_get_step(state)->teardown;
}





static int cpuhp_issue_call(int cpu, enum cpuhp_state state,
       int (*cb)(unsigned int), bool bringup)
{
 int ret;

 if (!cb)
  return 0;




 if (cpuhp_is_ap_state(state))
  ret = cpuhp_invoke_ap_callback(cpu, state, cb);
 else
  ret = cpuhp_invoke_callback(cpu, state, cb);
 ret = cpuhp_invoke_callback(cpu, state, cb);
 BUG_ON(ret && !bringup);
 return ret;
}






static void cpuhp_rollback_install(int failedcpu, enum cpuhp_state state,
       int (*teardown)(unsigned int cpu))
{
 int cpu;

 if (!teardown)
  return;


 for_each_present_cpu(cpu) {
  struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
  int cpustate = st->state;

  if (cpu >= failedcpu)
   break;


  if (cpustate >= state)
   cpuhp_issue_call(cpu, state, teardown, false);
 }
}






static int cpuhp_reserve_state(enum cpuhp_state state)
{
 enum cpuhp_state i;

 mutex_lock(&cpuhp_state_mutex);
 for (i = CPUHP_AP_ONLINE_DYN; i <= CPUHP_AP_ONLINE_DYN_END; i++) {
  if (cpuhp_ap_states[i].name)
   continue;

  cpuhp_ap_states[i].name = "Reserved";
  mutex_unlock(&cpuhp_state_mutex);
  return i;
 }
 mutex_unlock(&cpuhp_state_mutex);
 WARN(1, "No more dynamic states available for CPU hotplug\n");
 return -ENOSPC;
}
int __cpuhp_setup_state(enum cpuhp_state state,
   const char *name, bool invoke,
   int (*startup)(unsigned int cpu),
   int (*teardown)(unsigned int cpu))
{
 int cpu, ret = 0;
 int dyn_state = 0;

 if (cpuhp_cb_check(state) || !name)
  return -EINVAL;

 get_online_cpus();


 if (state == CPUHP_AP_ONLINE_DYN) {
  dyn_state = 1;
  ret = cpuhp_reserve_state(state);
  if (ret < 0)
   goto out;
  state = ret;
 }

 cpuhp_store_callbacks(state, name, startup, teardown);

 if (!invoke || !startup)
  goto out;





 for_each_present_cpu(cpu) {
  struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
  int cpustate = st->state;

  if (cpustate < state)
   continue;

  ret = cpuhp_issue_call(cpu, state, startup, true);
  if (ret) {
   cpuhp_rollback_install(cpu, state, teardown);
   cpuhp_store_callbacks(state, NULL, NULL, NULL);
   goto out;
  }
 }
out:
 put_online_cpus();
 if (!ret && dyn_state)
  return state;
 return ret;
}
EXPORT_SYMBOL(__cpuhp_setup_state);
void __cpuhp_remove_state(enum cpuhp_state state, bool invoke)
{
 int (*teardown)(unsigned int cpu) = cpuhp_get_teardown_cb(state);
 int cpu;

 BUG_ON(cpuhp_cb_check(state));

 get_online_cpus();

 if (!invoke || !teardown)
  goto remove;






 for_each_present_cpu(cpu) {
  struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
  int cpustate = st->state;

  if (cpustate >= state)
   cpuhp_issue_call(cpu, state, teardown, false);
 }
remove:
 cpuhp_store_callbacks(state, NULL, NULL, NULL);
 put_online_cpus();
}
EXPORT_SYMBOL(__cpuhp_remove_state);

static ssize_t show_cpuhp_state(struct device *dev,
    struct device_attribute *attr, char *buf)
{
 struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);

 return sprintf(buf, "%d\n", st->state);
}
static DEVICE_ATTR(state, 0444, show_cpuhp_state, NULL);

static ssize_t write_cpuhp_target(struct device *dev,
      struct device_attribute *attr,
      const char *buf, size_t count)
{
 struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);
 struct cpuhp_step *sp;
 int target, ret;

 ret = kstrtoint(buf, 10, &target);
 if (ret)
  return ret;

 if (target < CPUHP_OFFLINE || target > CPUHP_ONLINE)
  return -EINVAL;
 if (target != CPUHP_OFFLINE && target != CPUHP_ONLINE)
  return -EINVAL;

 ret = lock_device_hotplug_sysfs();
 if (ret)
  return ret;

 mutex_lock(&cpuhp_state_mutex);
 sp = cpuhp_get_step(target);
 ret = !sp->name || sp->cant_stop ? -EINVAL : 0;
 mutex_unlock(&cpuhp_state_mutex);
 if (ret)
  return ret;

 if (st->state < target)
  ret = do_cpu_up(dev->id, target);
 else
  ret = do_cpu_down(dev->id, target);

 unlock_device_hotplug();
 return ret ? ret : count;
}

static ssize_t show_cpuhp_target(struct device *dev,
     struct device_attribute *attr, char *buf)
{
 struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);

 return sprintf(buf, "%d\n", st->target);
}
static DEVICE_ATTR(target, 0644, show_cpuhp_target, write_cpuhp_target);

static struct attribute *cpuhp_cpu_attrs[] = {
 &dev_attr_state.attr,
 &dev_attr_target.attr,
 NULL
};

static struct attribute_group cpuhp_cpu_attr_group = {
 .attrs = cpuhp_cpu_attrs,
 .name = "hotplug",
 NULL
};

static ssize_t show_cpuhp_states(struct device *dev,
     struct device_attribute *attr, char *buf)
{
 ssize_t cur, res = 0;
 int i;

 mutex_lock(&cpuhp_state_mutex);
 for (i = CPUHP_OFFLINE; i <= CPUHP_ONLINE; i++) {
  struct cpuhp_step *sp = cpuhp_get_step(i);

  if (sp->name) {
   cur = sprintf(buf, "%3d: %s\n", i, sp->name);
   buf += cur;
   res += cur;
  }
 }
 mutex_unlock(&cpuhp_state_mutex);
 return res;
}
static DEVICE_ATTR(states, 0444, show_cpuhp_states, NULL);

static struct attribute *cpuhp_cpu_root_attrs[] = {
 &dev_attr_states.attr,
 NULL
};

static struct attribute_group cpuhp_cpu_root_attr_group = {
 .attrs = cpuhp_cpu_root_attrs,
 .name = "hotplug",
 NULL
};

static int __init cpuhp_sysfs_init(void)
{
 int cpu, ret;

 ret = sysfs_create_group(&cpu_subsys.dev_root->kobj,
     &cpuhp_cpu_root_attr_group);
 if (ret)
  return ret;

 for_each_possible_cpu(cpu) {
  struct device *dev = get_cpu_device(cpu);

  if (!dev)
   continue;
  ret = sysfs_create_group(&dev->kobj, &cpuhp_cpu_attr_group);
  if (ret)
   return ret;
 }
 return 0;
}
device_initcall(cpuhp_sysfs_init);

const unsigned long cpu_bit_bitmap[BITS_PER_LONG+1][BITS_TO_LONGS(NR_CPUS)] = {

 MASK_DECLARE_8(0), MASK_DECLARE_8(8),
 MASK_DECLARE_8(16), MASK_DECLARE_8(24),
 MASK_DECLARE_8(32), MASK_DECLARE_8(40),
 MASK_DECLARE_8(48), MASK_DECLARE_8(56),
};
EXPORT_SYMBOL_GPL(cpu_bit_bitmap);

const DECLARE_BITMAP(cpu_all_bits, NR_CPUS) = CPU_BITS_ALL;
EXPORT_SYMBOL(cpu_all_bits);

struct cpumask __cpu_possible_mask __read_mostly
 = {CPU_BITS_ALL};
struct cpumask __cpu_possible_mask __read_mostly;
EXPORT_SYMBOL(__cpu_possible_mask);

struct cpumask __cpu_online_mask __read_mostly;
EXPORT_SYMBOL(__cpu_online_mask);

struct cpumask __cpu_present_mask __read_mostly;
EXPORT_SYMBOL(__cpu_present_mask);

struct cpumask __cpu_active_mask __read_mostly;
EXPORT_SYMBOL(__cpu_active_mask);

void init_cpu_present(const struct cpumask *src)
{
 cpumask_copy(&__cpu_present_mask, src);
}

void init_cpu_possible(const struct cpumask *src)
{
 cpumask_copy(&__cpu_possible_mask, src);
}

void init_cpu_online(const struct cpumask *src)
{
 cpumask_copy(&__cpu_online_mask, src);
}




void __init boot_cpu_init(void)
{
 int cpu = smp_processor_id();


 set_cpu_online(cpu, true);
 set_cpu_active(cpu, true);
 set_cpu_present(cpu, true);
 set_cpu_possible(cpu, true);
}




void __init boot_cpu_state_init(void)
{
 per_cpu_ptr(&cpuhp_state, smp_processor_id())->state = CPUHP_ONLINE;
}


static bool migrate_one_irq(struct irq_desc *desc)
{
 struct irq_data *d = irq_desc_get_irq_data(desc);
 const struct cpumask *affinity = d->common->affinity;
 struct irq_chip *c;
 bool ret = false;





 if (irqd_is_per_cpu(d) ||
     !cpumask_test_cpu(smp_processor_id(), affinity))
  return false;

 if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {
  affinity = cpu_online_mask;
  ret = true;
 }

 c = irq_data_get_irq_chip(d);
 if (!c->irq_set_affinity) {
  pr_debug("IRQ%u: unable to set affinity\n", d->irq);
 } else {
  int r = irq_do_set_affinity(d, affinity, false);
  if (r)
   pr_warn_ratelimited("IRQ%u: set affinity failed(%d).\n",
         d->irq, r);
 }

 return ret;
}
void irq_migrate_all_off_this_cpu(void)
{
 unsigned int irq;
 struct irq_desc *desc;
 unsigned long flags;

 local_irq_save(flags);

 for_each_active_irq(irq) {
  bool affinity_broken;

  desc = irq_to_desc(irq);
  raw_spin_lock(&desc->lock);
  affinity_broken = migrate_one_irq(desc);
  raw_spin_unlock(&desc->lock);

  if (affinity_broken)
   pr_warn_ratelimited("IRQ%u no longer affine to CPU%u\n",
         irq, smp_processor_id());
 }

 local_irq_restore(flags);
}

static DEFINE_RWLOCK(cpu_pm_notifier_lock);
static RAW_NOTIFIER_HEAD(cpu_pm_notifier_chain);

static int cpu_pm_notify(enum cpu_pm_event event, int nr_to_call, int *nr_calls)
{
 int ret;

 ret = __raw_notifier_call_chain(&cpu_pm_notifier_chain, event, NULL,
  nr_to_call, nr_calls);

 return notifier_to_errno(ret);
}
int cpu_pm_register_notifier(struct notifier_block *nb)
{
 unsigned long flags;
 int ret;

 write_lock_irqsave(&cpu_pm_notifier_lock, flags);
 ret = raw_notifier_chain_register(&cpu_pm_notifier_chain, nb);
 write_unlock_irqrestore(&cpu_pm_notifier_lock, flags);

 return ret;
}
EXPORT_SYMBOL_GPL(cpu_pm_register_notifier);
int cpu_pm_unregister_notifier(struct notifier_block *nb)
{
 unsigned long flags;
 int ret;

 write_lock_irqsave(&cpu_pm_notifier_lock, flags);
 ret = raw_notifier_chain_unregister(&cpu_pm_notifier_chain, nb);
 write_unlock_irqrestore(&cpu_pm_notifier_lock, flags);

 return ret;
}
EXPORT_SYMBOL_GPL(cpu_pm_unregister_notifier);
int cpu_pm_enter(void)
{
 int nr_calls;
 int ret = 0;

 read_lock(&cpu_pm_notifier_lock);
 ret = cpu_pm_notify(CPU_PM_ENTER, -1, &nr_calls);
 if (ret)




  cpu_pm_notify(CPU_PM_ENTER_FAILED, nr_calls - 1, NULL);
 read_unlock(&cpu_pm_notifier_lock);

 return ret;
}
EXPORT_SYMBOL_GPL(cpu_pm_enter);
int cpu_pm_exit(void)
{
 int ret;

 read_lock(&cpu_pm_notifier_lock);
 ret = cpu_pm_notify(CPU_PM_EXIT, -1, NULL);
 read_unlock(&cpu_pm_notifier_lock);

 return ret;
}
EXPORT_SYMBOL_GPL(cpu_pm_exit);
int cpu_cluster_pm_enter(void)
{
 int nr_calls;
 int ret = 0;

 read_lock(&cpu_pm_notifier_lock);
 ret = cpu_pm_notify(CPU_CLUSTER_PM_ENTER, -1, &nr_calls);
 if (ret)




  cpu_pm_notify(CPU_CLUSTER_PM_ENTER_FAILED, nr_calls - 1, NULL);
 read_unlock(&cpu_pm_notifier_lock);

 return ret;
}
EXPORT_SYMBOL_GPL(cpu_cluster_pm_enter);
int cpu_cluster_pm_exit(void)
{
 int ret;

 read_lock(&cpu_pm_notifier_lock);
 ret = cpu_pm_notify(CPU_CLUSTER_PM_EXIT, -1, NULL);
 read_unlock(&cpu_pm_notifier_lock);

 return ret;
}
EXPORT_SYMBOL_GPL(cpu_cluster_pm_exit);

static int cpu_pm_suspend(void)
{
 int ret;

 ret = cpu_pm_enter();
 if (ret)
  return ret;

 ret = cpu_cluster_pm_enter();
 return ret;
}

static void cpu_pm_resume(void)
{
 cpu_cluster_pm_exit();
 cpu_pm_exit();
}

static struct syscore_ops cpu_pm_syscore_ops = {
 .suspend = cpu_pm_suspend,
 .resume = cpu_pm_resume,
};

static int cpu_pm_init(void)
{
 register_syscore_ops(&cpu_pm_syscore_ops);
 return 0;
}
core_initcall(cpu_pm_init);


DEFINE_STATIC_KEY_FALSE(cpusets_enabled_key);



struct fmeter {
 int cnt;
 int val;
 time64_t time;
 spinlock_t lock;
};

struct cpuset {
 struct cgroup_subsys_state css;

 unsigned long flags;
 cpumask_var_t cpus_allowed;
 nodemask_t mems_allowed;


 cpumask_var_t effective_cpus;
 nodemask_t effective_mems;
 nodemask_t old_mems_allowed;

 struct fmeter fmeter;





 int attach_in_progress;


 int pn;


 int relax_domain_level;
};

static inline struct cpuset *css_cs(struct cgroup_subsys_state *css)
{
 return css ? container_of(css, struct cpuset, css) : NULL;
}


static inline struct cpuset *task_cs(struct task_struct *task)
{
 return css_cs(task_css(task, cpuset_cgrp_id));
}

static inline struct cpuset *parent_cs(struct cpuset *cs)
{
 return css_cs(cs->css.parent);
}

static inline bool task_has_mempolicy(struct task_struct *task)
{
 return task->mempolicy;
}
static inline bool task_has_mempolicy(struct task_struct *task)
{
 return false;
}



typedef enum {
 CS_ONLINE,
 CS_CPU_EXCLUSIVE,
 CS_MEM_EXCLUSIVE,
 CS_MEM_HARDWALL,
 CS_MEMORY_MIGRATE,
 CS_SCHED_LOAD_BALANCE,
 CS_SPREAD_PAGE,
 CS_SPREAD_SLAB,
} cpuset_flagbits_t;


static inline bool is_cpuset_online(const struct cpuset *cs)
{
 return test_bit(CS_ONLINE, &cs->flags);
}

static inline int is_cpu_exclusive(const struct cpuset *cs)
{
 return test_bit(CS_CPU_EXCLUSIVE, &cs->flags);
}

static inline int is_mem_exclusive(const struct cpuset *cs)
{
 return test_bit(CS_MEM_EXCLUSIVE, &cs->flags);
}

static inline int is_mem_hardwall(const struct cpuset *cs)
{
 return test_bit(CS_MEM_HARDWALL, &cs->flags);
}

static inline int is_sched_load_balance(const struct cpuset *cs)
{
 return test_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);
}

static inline int is_memory_migrate(const struct cpuset *cs)
{
 return test_bit(CS_MEMORY_MIGRATE, &cs->flags);
}

static inline int is_spread_page(const struct cpuset *cs)
{
 return test_bit(CS_SPREAD_PAGE, &cs->flags);
}

static inline int is_spread_slab(const struct cpuset *cs)
{
 return test_bit(CS_SPREAD_SLAB, &cs->flags);
}

static struct cpuset top_cpuset = {
 .flags = ((1 << CS_ONLINE) | (1 << CS_CPU_EXCLUSIVE) |
    (1 << CS_MEM_EXCLUSIVE)),
};
 css_for_each_child((pos_css), &(parent_cs)->css) \
  if (is_cpuset_online(((child_cs) = css_cs((pos_css)))))
 css_for_each_descendant_pre((pos_css), &(root_cs)->css) \
  if (is_cpuset_online(((des_cs) = css_cs((pos_css)))))
static DEFINE_MUTEX(cpuset_mutex);
static DEFINE_SPINLOCK(callback_lock);

static struct workqueue_struct *cpuset_migrate_mm_wq;




static void cpuset_hotplug_workfn(struct work_struct *work);
static DECLARE_WORK(cpuset_hotplug_work, cpuset_hotplug_workfn);

static DECLARE_WAIT_QUEUE_HEAD(cpuset_attach_wq);






static struct dentry *cpuset_mount(struct file_system_type *fs_type,
    int flags, const char *unused_dev_name, void *data)
{
 struct file_system_type *cgroup_fs = get_fs_type("cgroup");
 struct dentry *ret = ERR_PTR(-ENODEV);
 if (cgroup_fs) {
  char mountopts[] =
   "cpuset,noprefix,"
   "release_agent=/sbin/cpuset_release_agent";
  ret = cgroup_fs->mount(cgroup_fs, flags,
        unused_dev_name, mountopts);
  put_filesystem(cgroup_fs);
 }
 return ret;
}

static struct file_system_type cpuset_fs_type = {
 .name = "cpuset",
 .mount = cpuset_mount,
};
static void guarantee_online_cpus(struct cpuset *cs, struct cpumask *pmask)
{
 while (!cpumask_intersects(cs->effective_cpus, cpu_online_mask))
  cs = parent_cs(cs);
 cpumask_and(pmask, cs->effective_cpus, cpu_online_mask);
}
static void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)
{
 while (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))
  cs = parent_cs(cs);
 nodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);
}






static void cpuset_update_task_spread_flag(struct cpuset *cs,
     struct task_struct *tsk)
{
 if (is_spread_page(cs))
  task_set_spread_page(tsk);
 else
  task_clear_spread_page(tsk);

 if (is_spread_slab(cs))
  task_set_spread_slab(tsk);
 else
  task_clear_spread_slab(tsk);
}
static int is_cpuset_subset(const struct cpuset *p, const struct cpuset *q)
{
 return cpumask_subset(p->cpus_allowed, q->cpus_allowed) &&
  nodes_subset(p->mems_allowed, q->mems_allowed) &&
  is_cpu_exclusive(p) <= is_cpu_exclusive(q) &&
  is_mem_exclusive(p) <= is_mem_exclusive(q);
}





static struct cpuset *alloc_trial_cpuset(struct cpuset *cs)
{
 struct cpuset *trial;

 trial = kmemdup(cs, sizeof(*cs), GFP_KERNEL);
 if (!trial)
  return NULL;

 if (!alloc_cpumask_var(&trial->cpus_allowed, GFP_KERNEL))
  goto free_cs;
 if (!alloc_cpumask_var(&trial->effective_cpus, GFP_KERNEL))
  goto free_cpus;

 cpumask_copy(trial->cpus_allowed, cs->cpus_allowed);
 cpumask_copy(trial->effective_cpus, cs->effective_cpus);
 return trial;

free_cpus:
 free_cpumask_var(trial->cpus_allowed);
free_cs:
 kfree(trial);
 return NULL;
}





static void free_trial_cpuset(struct cpuset *trial)
{
 free_cpumask_var(trial->effective_cpus);
 free_cpumask_var(trial->cpus_allowed);
 kfree(trial);
}
static int validate_change(struct cpuset *cur, struct cpuset *trial)
{
 struct cgroup_subsys_state *css;
 struct cpuset *c, *par;
 int ret;

 rcu_read_lock();


 ret = -EBUSY;
 cpuset_for_each_child(c, css, cur)
  if (!is_cpuset_subset(c, trial))
   goto out;


 ret = 0;
 if (cur == &top_cpuset)
  goto out;

 par = parent_cs(cur);


 ret = -EACCES;
 if (!cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&
     !is_cpuset_subset(trial, par))
  goto out;





 ret = -EINVAL;
 cpuset_for_each_child(c, css, par) {
  if ((is_cpu_exclusive(trial) || is_cpu_exclusive(c)) &&
      c != cur &&
      cpumask_intersects(trial->cpus_allowed, c->cpus_allowed))
   goto out;
  if ((is_mem_exclusive(trial) || is_mem_exclusive(c)) &&
      c != cur &&
      nodes_intersects(trial->mems_allowed, c->mems_allowed))
   goto out;
 }





 ret = -ENOSPC;
 if ((cgroup_is_populated(cur->css.cgroup) || cur->attach_in_progress)) {
  if (!cpumask_empty(cur->cpus_allowed) &&
      cpumask_empty(trial->cpus_allowed))
   goto out;
  if (!nodes_empty(cur->mems_allowed) &&
      nodes_empty(trial->mems_allowed))
   goto out;
 }





 ret = -EBUSY;
 if (is_cpu_exclusive(cur) &&
     !cpuset_cpumask_can_shrink(cur->cpus_allowed,
           trial->cpus_allowed))
  goto out;

 ret = 0;
out:
 rcu_read_unlock();
 return ret;
}





static int cpusets_overlap(struct cpuset *a, struct cpuset *b)
{
 return cpumask_intersects(a->effective_cpus, b->effective_cpus);
}

static void
update_domain_attr(struct sched_domain_attr *dattr, struct cpuset *c)
{
 if (dattr->relax_domain_level < c->relax_domain_level)
  dattr->relax_domain_level = c->relax_domain_level;
 return;
}

static void update_domain_attr_tree(struct sched_domain_attr *dattr,
        struct cpuset *root_cs)
{
 struct cpuset *cp;
 struct cgroup_subsys_state *pos_css;

 rcu_read_lock();
 cpuset_for_each_descendant_pre(cp, pos_css, root_cs) {

  if (cpumask_empty(cp->cpus_allowed)) {
   pos_css = css_rightmost_descendant(pos_css);
   continue;
  }

  if (is_sched_load_balance(cp))
   update_domain_attr(dattr, cp);
 }
 rcu_read_unlock();
}
static int generate_sched_domains(cpumask_var_t **domains,
   struct sched_domain_attr **attributes)
{
 struct cpuset *cp;
 struct cpuset **csa;
 int csn;
 int i, j, k;
 cpumask_var_t *doms;
 cpumask_var_t non_isolated_cpus;
 struct sched_domain_attr *dattr;
 int ndoms = 0;
 int nslot;
 struct cgroup_subsys_state *pos_css;

 doms = NULL;
 dattr = NULL;
 csa = NULL;

 if (!alloc_cpumask_var(&non_isolated_cpus, GFP_KERNEL))
  goto done;
 cpumask_andnot(non_isolated_cpus, cpu_possible_mask, cpu_isolated_map);


 if (is_sched_load_balance(&top_cpuset)) {
  ndoms = 1;
  doms = alloc_sched_domains(ndoms);
  if (!doms)
   goto done;

  dattr = kmalloc(sizeof(struct sched_domain_attr), GFP_KERNEL);
  if (dattr) {
   *dattr = SD_ATTR_INIT;
   update_domain_attr_tree(dattr, &top_cpuset);
  }
  cpumask_and(doms[0], top_cpuset.effective_cpus,
         non_isolated_cpus);

  goto done;
 }

 csa = kmalloc(nr_cpusets() * sizeof(cp), GFP_KERNEL);
 if (!csa)
  goto done;
 csn = 0;

 rcu_read_lock();
 cpuset_for_each_descendant_pre(cp, pos_css, &top_cpuset) {
  if (cp == &top_cpuset)
   continue;
  if (!cpumask_empty(cp->cpus_allowed) &&
      !(is_sched_load_balance(cp) &&
        cpumask_intersects(cp->cpus_allowed, non_isolated_cpus)))
   continue;

  if (is_sched_load_balance(cp))
   csa[csn++] = cp;


  pos_css = css_rightmost_descendant(pos_css);
 }
 rcu_read_unlock();

 for (i = 0; i < csn; i++)
  csa[i]->pn = i;
 ndoms = csn;

restart:

 for (i = 0; i < csn; i++) {
  struct cpuset *a = csa[i];
  int apn = a->pn;

  for (j = 0; j < csn; j++) {
   struct cpuset *b = csa[j];
   int bpn = b->pn;

   if (apn != bpn && cpusets_overlap(a, b)) {
    for (k = 0; k < csn; k++) {
     struct cpuset *c = csa[k];

     if (c->pn == bpn)
      c->pn = apn;
    }
    ndoms--;
    goto restart;
   }
  }
 }





 doms = alloc_sched_domains(ndoms);
 if (!doms)
  goto done;





 dattr = kmalloc(ndoms * sizeof(struct sched_domain_attr), GFP_KERNEL);

 for (nslot = 0, i = 0; i < csn; i++) {
  struct cpuset *a = csa[i];
  struct cpumask *dp;
  int apn = a->pn;

  if (apn < 0) {

   continue;
  }

  dp = doms[nslot];

  if (nslot == ndoms) {
   static int warnings = 10;
   if (warnings) {
    pr_warn("rebuild_sched_domains confused: nslot %d, ndoms %d, csn %d, i %d, apn %d\n",
     nslot, ndoms, csn, i, apn);
    warnings--;
   }
   continue;
  }

  cpumask_clear(dp);
  if (dattr)
   *(dattr + nslot) = SD_ATTR_INIT;
  for (j = i; j < csn; j++) {
   struct cpuset *b = csa[j];

   if (apn == b->pn) {
    cpumask_or(dp, dp, b->effective_cpus);
    cpumask_and(dp, dp, non_isolated_cpus);
    if (dattr)
     update_domain_attr_tree(dattr + nslot, b);


    b->pn = -1;
   }
  }
  nslot++;
 }
 BUG_ON(nslot != ndoms);

done:
 free_cpumask_var(non_isolated_cpus);
 kfree(csa);





 if (doms == NULL)
  ndoms = 1;

 *domains = doms;
 *attributes = dattr;
 return ndoms;
}
static void rebuild_sched_domains_locked(void)
{
 struct sched_domain_attr *attr;
 cpumask_var_t *doms;
 int ndoms;

 lockdep_assert_held(&cpuset_mutex);
 get_online_cpus();






 if (!cpumask_equal(top_cpuset.effective_cpus, cpu_active_mask))
  goto out;


 ndoms = generate_sched_domains(&doms, &attr);


 partition_sched_domains(ndoms, doms, attr);
out:
 put_online_cpus();
}
static void rebuild_sched_domains_locked(void)
{
}

void rebuild_sched_domains(void)
{
 mutex_lock(&cpuset_mutex);
 rebuild_sched_domains_locked();
 mutex_unlock(&cpuset_mutex);
}
static void update_tasks_cpumask(struct cpuset *cs)
{
 struct css_task_iter it;
 struct task_struct *task;

 css_task_iter_start(&cs->css, &it);
 while ((task = css_task_iter_next(&it)))
  set_cpus_allowed_ptr(task, cs->effective_cpus);
 css_task_iter_end(&it);
}
static void update_cpumasks_hier(struct cpuset *cs, struct cpumask *new_cpus)
{
 struct cpuset *cp;
 struct cgroup_subsys_state *pos_css;
 bool need_rebuild_sched_domains = false;

 rcu_read_lock();
 cpuset_for_each_descendant_pre(cp, pos_css, cs) {
  struct cpuset *parent = parent_cs(cp);

  cpumask_and(new_cpus, cp->cpus_allowed, parent->effective_cpus);





  if (cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&
      cpumask_empty(new_cpus))
   cpumask_copy(new_cpus, parent->effective_cpus);


  if (cpumask_equal(new_cpus, cp->effective_cpus)) {
   pos_css = css_rightmost_descendant(pos_css);
   continue;
  }

  if (!css_tryget_online(&cp->css))
   continue;
  rcu_read_unlock();

  spin_lock_irq(&callback_lock);
  cpumask_copy(cp->effective_cpus, new_cpus);
  spin_unlock_irq(&callback_lock);

  WARN_ON(!cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&
   !cpumask_equal(cp->cpus_allowed, cp->effective_cpus));

  update_tasks_cpumask(cp);





  if (!cpumask_empty(cp->cpus_allowed) &&
      is_sched_load_balance(cp))
   need_rebuild_sched_domains = true;

  rcu_read_lock();
  css_put(&cp->css);
 }
 rcu_read_unlock();

 if (need_rebuild_sched_domains)
  rebuild_sched_domains_locked();
}







static int update_cpumask(struct cpuset *cs, struct cpuset *trialcs,
     const char *buf)
{
 int retval;


 if (cs == &top_cpuset)
  return -EACCES;







 if (!*buf) {
  cpumask_clear(trialcs->cpus_allowed);
 } else {
  retval = cpulist_parse(buf, trialcs->cpus_allowed);
  if (retval < 0)
   return retval;

  if (!cpumask_subset(trialcs->cpus_allowed,
        top_cpuset.cpus_allowed))
   return -EINVAL;
 }


 if (cpumask_equal(cs->cpus_allowed, trialcs->cpus_allowed))
  return 0;

 retval = validate_change(cs, trialcs);
 if (retval < 0)
  return retval;

 spin_lock_irq(&callback_lock);
 cpumask_copy(cs->cpus_allowed, trialcs->cpus_allowed);
 spin_unlock_irq(&callback_lock);


 update_cpumasks_hier(cs, trialcs->cpus_allowed);
 return 0;
}
struct cpuset_migrate_mm_work {
 struct work_struct work;
 struct mm_struct *mm;
 nodemask_t from;
 nodemask_t to;
};

static void cpuset_migrate_mm_workfn(struct work_struct *work)
{
 struct cpuset_migrate_mm_work *mwork =
  container_of(work, struct cpuset_migrate_mm_work, work);


 do_migrate_pages(mwork->mm, &mwork->from, &mwork->to, MPOL_MF_MOVE_ALL);
 mmput(mwork->mm);
 kfree(mwork);
}

static void cpuset_migrate_mm(struct mm_struct *mm, const nodemask_t *from,
       const nodemask_t *to)
{
 struct cpuset_migrate_mm_work *mwork;

 mwork = kzalloc(sizeof(*mwork), GFP_KERNEL);
 if (mwork) {
  mwork->mm = mm;
  mwork->from = *from;
  mwork->to = *to;
  INIT_WORK(&mwork->work, cpuset_migrate_mm_workfn);
  queue_work(cpuset_migrate_mm_wq, &mwork->work);
 } else {
  mmput(mm);
 }
}

static void cpuset_post_attach(void)
{
 flush_workqueue(cpuset_migrate_mm_wq);
}
static void cpuset_change_task_nodemask(struct task_struct *tsk,
     nodemask_t *newmems)
{
 bool need_loop;





 if (unlikely(test_thread_flag(TIF_MEMDIE)))
  return;
 if (current->flags & PF_EXITING)
  return;

 task_lock(tsk);






 need_loop = task_has_mempolicy(tsk) ||
   !nodes_intersects(*newmems, tsk->mems_allowed);

 if (need_loop) {
  local_irq_disable();
  write_seqcount_begin(&tsk->mems_allowed_seq);
 }

 nodes_or(tsk->mems_allowed, tsk->mems_allowed, *newmems);
 mpol_rebind_task(tsk, newmems, MPOL_REBIND_STEP1);

 mpol_rebind_task(tsk, newmems, MPOL_REBIND_STEP2);
 tsk->mems_allowed = *newmems;

 if (need_loop) {
  write_seqcount_end(&tsk->mems_allowed_seq);
  local_irq_enable();
 }

 task_unlock(tsk);
}

static void *cpuset_being_rebound;
static void update_tasks_nodemask(struct cpuset *cs)
{
 static nodemask_t newmems;
 struct css_task_iter it;
 struct task_struct *task;

 cpuset_being_rebound = cs;

 guarantee_online_mems(cs, &newmems);
 css_task_iter_start(&cs->css, &it);
 while ((task = css_task_iter_next(&it))) {
  struct mm_struct *mm;
  bool migrate;

  cpuset_change_task_nodemask(task, &newmems);

  mm = get_task_mm(task);
  if (!mm)
   continue;

  migrate = is_memory_migrate(cs);

  mpol_rebind_mm(mm, &cs->mems_allowed);
  if (migrate)
   cpuset_migrate_mm(mm, &cs->old_mems_allowed, &newmems);
  else
   mmput(mm);
 }
 css_task_iter_end(&it);





 cs->old_mems_allowed = newmems;


 cpuset_being_rebound = NULL;
}
static void update_nodemasks_hier(struct cpuset *cs, nodemask_t *new_mems)
{
 struct cpuset *cp;
 struct cgroup_subsys_state *pos_css;

 rcu_read_lock();
 cpuset_for_each_descendant_pre(cp, pos_css, cs) {
  struct cpuset *parent = parent_cs(cp);

  nodes_and(*new_mems, cp->mems_allowed, parent->effective_mems);





  if (cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&
      nodes_empty(*new_mems))
   *new_mems = parent->effective_mems;


  if (nodes_equal(*new_mems, cp->effective_mems)) {
   pos_css = css_rightmost_descendant(pos_css);
   continue;
  }

  if (!css_tryget_online(&cp->css))
   continue;
  rcu_read_unlock();

  spin_lock_irq(&callback_lock);
  cp->effective_mems = *new_mems;
  spin_unlock_irq(&callback_lock);

  WARN_ON(!cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&
   !nodes_equal(cp->mems_allowed, cp->effective_mems));

  update_tasks_nodemask(cp);

  rcu_read_lock();
  css_put(&cp->css);
 }
 rcu_read_unlock();
}
static int update_nodemask(struct cpuset *cs, struct cpuset *trialcs,
      const char *buf)
{
 int retval;





 if (cs == &top_cpuset) {
  retval = -EACCES;
  goto done;
 }







 if (!*buf) {
  nodes_clear(trialcs->mems_allowed);
 } else {
  retval = nodelist_parse(buf, trialcs->mems_allowed);
  if (retval < 0)
   goto done;

  if (!nodes_subset(trialcs->mems_allowed,
      top_cpuset.mems_allowed)) {
   retval = -EINVAL;
   goto done;
  }
 }

 if (nodes_equal(cs->mems_allowed, trialcs->mems_allowed)) {
  retval = 0;
  goto done;
 }
 retval = validate_change(cs, trialcs);
 if (retval < 0)
  goto done;

 spin_lock_irq(&callback_lock);
 cs->mems_allowed = trialcs->mems_allowed;
 spin_unlock_irq(&callback_lock);


 update_nodemasks_hier(cs, &trialcs->mems_allowed);
done:
 return retval;
}

int current_cpuset_is_being_rebound(void)
{
 int ret;

 rcu_read_lock();
 ret = task_cs(current) == cpuset_being_rebound;
 rcu_read_unlock();

 return ret;
}

static int update_relax_domain_level(struct cpuset *cs, s64 val)
{
 if (val < -1 || val >= sched_domain_level_max)
  return -EINVAL;

 if (val != cs->relax_domain_level) {
  cs->relax_domain_level = val;
  if (!cpumask_empty(cs->cpus_allowed) &&
      is_sched_load_balance(cs))
   rebuild_sched_domains_locked();
 }

 return 0;
}
static void update_tasks_flags(struct cpuset *cs)
{
 struct css_task_iter it;
 struct task_struct *task;

 css_task_iter_start(&cs->css, &it);
 while ((task = css_task_iter_next(&it)))
  cpuset_update_task_spread_flag(cs, task);
 css_task_iter_end(&it);
}
static int update_flag(cpuset_flagbits_t bit, struct cpuset *cs,
         int turning_on)
{
 struct cpuset *trialcs;
 int balance_flag_changed;
 int spread_flag_changed;
 int err;

 trialcs = alloc_trial_cpuset(cs);
 if (!trialcs)
  return -ENOMEM;

 if (turning_on)
  set_bit(bit, &trialcs->flags);
 else
  clear_bit(bit, &trialcs->flags);

 err = validate_change(cs, trialcs);
 if (err < 0)
  goto out;

 balance_flag_changed = (is_sched_load_balance(cs) !=
    is_sched_load_balance(trialcs));

 spread_flag_changed = ((is_spread_slab(cs) != is_spread_slab(trialcs))
   || (is_spread_page(cs) != is_spread_page(trialcs)));

 spin_lock_irq(&callback_lock);
 cs->flags = trialcs->flags;
 spin_unlock_irq(&callback_lock);

 if (!cpumask_empty(trialcs->cpus_allowed) && balance_flag_changed)
  rebuild_sched_domains_locked();

 if (spread_flag_changed)
  update_tasks_flags(cs);
out:
 free_trial_cpuset(trialcs);
 return err;
}


static void fmeter_init(struct fmeter *fmp)
{
 fmp->cnt = 0;
 fmp->val = 0;
 fmp->time = 0;
 spin_lock_init(&fmp->lock);
}


static void fmeter_update(struct fmeter *fmp)
{
 time64_t now;
 u32 ticks;

 now = ktime_get_seconds();
 ticks = now - fmp->time;

 if (ticks == 0)
  return;

 ticks = min(FM_MAXTICKS, ticks);
 while (ticks-- > 0)
  fmp->val = (FM_COEF * fmp->val) / FM_SCALE;
 fmp->time = now;

 fmp->val += ((FM_SCALE - FM_COEF) * fmp->cnt) / FM_SCALE;
 fmp->cnt = 0;
}


static void fmeter_markevent(struct fmeter *fmp)
{
 spin_lock(&fmp->lock);
 fmeter_update(fmp);
 fmp->cnt = min(FM_MAXCNT, fmp->cnt + FM_SCALE);
 spin_unlock(&fmp->lock);
}


static int fmeter_getrate(struct fmeter *fmp)
{
 int val;

 spin_lock(&fmp->lock);
 fmeter_update(fmp);
 val = fmp->val;
 spin_unlock(&fmp->lock);
 return val;
}

static struct cpuset *cpuset_attach_old_cs;


static int cpuset_can_attach(struct cgroup_taskset *tset)
{
 struct cgroup_subsys_state *css;
 struct cpuset *cs;
 struct task_struct *task;
 int ret;


 cpuset_attach_old_cs = task_cs(cgroup_taskset_first(tset, &css));
 cs = css_cs(css);

 mutex_lock(&cpuset_mutex);


 ret = -ENOSPC;
 if (!cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&
     (cpumask_empty(cs->cpus_allowed) || nodes_empty(cs->mems_allowed)))
  goto out_unlock;

 cgroup_taskset_for_each(task, css, tset) {
  ret = task_can_attach(task, cs->cpus_allowed);
  if (ret)
   goto out_unlock;
  ret = security_task_setscheduler(task);
  if (ret)
   goto out_unlock;
 }





 cs->attach_in_progress++;
 ret = 0;
out_unlock:
 mutex_unlock(&cpuset_mutex);
 return ret;
}

static void cpuset_cancel_attach(struct cgroup_taskset *tset)
{
 struct cgroup_subsys_state *css;
 struct cpuset *cs;

 cgroup_taskset_first(tset, &css);
 cs = css_cs(css);

 mutex_lock(&cpuset_mutex);
 css_cs(css)->attach_in_progress--;
 mutex_unlock(&cpuset_mutex);
}






static cpumask_var_t cpus_attach;

static void cpuset_attach(struct cgroup_taskset *tset)
{

 static nodemask_t cpuset_attach_nodemask_to;
 struct task_struct *task;
 struct task_struct *leader;
 struct cgroup_subsys_state *css;
 struct cpuset *cs;
 struct cpuset *oldcs = cpuset_attach_old_cs;

 cgroup_taskset_first(tset, &css);
 cs = css_cs(css);

 mutex_lock(&cpuset_mutex);


 if (cs == &top_cpuset)
  cpumask_copy(cpus_attach, cpu_possible_mask);
 else
  guarantee_online_cpus(cs, cpus_attach);

 guarantee_online_mems(cs, &cpuset_attach_nodemask_to);

 cgroup_taskset_for_each(task, css, tset) {




  WARN_ON_ONCE(set_cpus_allowed_ptr(task, cpus_attach));

  cpuset_change_task_nodemask(task, &cpuset_attach_nodemask_to);
  cpuset_update_task_spread_flag(cs, task);
 }





 cpuset_attach_nodemask_to = cs->effective_mems;
 cgroup_taskset_for_each_leader(leader, css, tset) {
  struct mm_struct *mm = get_task_mm(leader);

  if (mm) {
   mpol_rebind_mm(mm, &cpuset_attach_nodemask_to);
   if (is_memory_migrate(cs))
    cpuset_migrate_mm(mm, &oldcs->old_mems_allowed,
        &cpuset_attach_nodemask_to);
   else
    mmput(mm);
  }
 }

 cs->old_mems_allowed = cpuset_attach_nodemask_to;

 cs->attach_in_progress--;
 if (!cs->attach_in_progress)
  wake_up(&cpuset_attach_wq);

 mutex_unlock(&cpuset_mutex);
}



typedef enum {
 FILE_MEMORY_MIGRATE,
 FILE_CPULIST,
 FILE_MEMLIST,
 FILE_EFFECTIVE_CPULIST,
 FILE_EFFECTIVE_MEMLIST,
 FILE_CPU_EXCLUSIVE,
 FILE_MEM_EXCLUSIVE,
 FILE_MEM_HARDWALL,
 FILE_SCHED_LOAD_BALANCE,
 FILE_SCHED_RELAX_DOMAIN_LEVEL,
 FILE_MEMORY_PRESSURE_ENABLED,
 FILE_MEMORY_PRESSURE,
 FILE_SPREAD_PAGE,
 FILE_SPREAD_SLAB,
} cpuset_filetype_t;

static int cpuset_write_u64(struct cgroup_subsys_state *css, struct cftype *cft,
       u64 val)
{
 struct cpuset *cs = css_cs(css);
 cpuset_filetype_t type = cft->private;
 int retval = 0;

 mutex_lock(&cpuset_mutex);
 if (!is_cpuset_online(cs)) {
  retval = -ENODEV;
  goto out_unlock;
 }

 switch (type) {
 case FILE_CPU_EXCLUSIVE:
  retval = update_flag(CS_CPU_EXCLUSIVE, cs, val);
  break;
 case FILE_MEM_EXCLUSIVE:
  retval = update_flag(CS_MEM_EXCLUSIVE, cs, val);
  break;
 case FILE_MEM_HARDWALL:
  retval = update_flag(CS_MEM_HARDWALL, cs, val);
  break;
 case FILE_SCHED_LOAD_BALANCE:
  retval = update_flag(CS_SCHED_LOAD_BALANCE, cs, val);
  break;
 case FILE_MEMORY_MIGRATE:
  retval = update_flag(CS_MEMORY_MIGRATE, cs, val);
  break;
 case FILE_MEMORY_PRESSURE_ENABLED:
  cpuset_memory_pressure_enabled = !!val;
  break;
 case FILE_SPREAD_PAGE:
  retval = update_flag(CS_SPREAD_PAGE, cs, val);
  break;
 case FILE_SPREAD_SLAB:
  retval = update_flag(CS_SPREAD_SLAB, cs, val);
  break;
 default:
  retval = -EINVAL;
  break;
 }
out_unlock:
 mutex_unlock(&cpuset_mutex);
 return retval;
}

static int cpuset_write_s64(struct cgroup_subsys_state *css, struct cftype *cft,
       s64 val)
{
 struct cpuset *cs = css_cs(css);
 cpuset_filetype_t type = cft->private;
 int retval = -ENODEV;

 mutex_lock(&cpuset_mutex);
 if (!is_cpuset_online(cs))
  goto out_unlock;

 switch (type) {
 case FILE_SCHED_RELAX_DOMAIN_LEVEL:
  retval = update_relax_domain_level(cs, val);
  break;
 default:
  retval = -EINVAL;
  break;
 }
out_unlock:
 mutex_unlock(&cpuset_mutex);
 return retval;
}




static ssize_t cpuset_write_resmask(struct kernfs_open_file *of,
        char *buf, size_t nbytes, loff_t off)
{
 struct cpuset *cs = css_cs(of_css(of));
 struct cpuset *trialcs;
 int retval = -ENODEV;

 buf = strstrip(buf);
 css_get(&cs->css);
 kernfs_break_active_protection(of->kn);
 flush_work(&cpuset_hotplug_work);

 mutex_lock(&cpuset_mutex);
 if (!is_cpuset_online(cs))
  goto out_unlock;

 trialcs = alloc_trial_cpuset(cs);
 if (!trialcs) {
  retval = -ENOMEM;
  goto out_unlock;
 }

 switch (of_cft(of)->private) {
 case FILE_CPULIST:
  retval = update_cpumask(cs, trialcs, buf);
  break;
 case FILE_MEMLIST:
  retval = update_nodemask(cs, trialcs, buf);
  break;
 default:
  retval = -EINVAL;
  break;
 }

 free_trial_cpuset(trialcs);
out_unlock:
 mutex_unlock(&cpuset_mutex);
 kernfs_unbreak_active_protection(of->kn);
 css_put(&cs->css);
 flush_workqueue(cpuset_migrate_mm_wq);
 return retval ?: nbytes;
}
static int cpuset_common_seq_show(struct seq_file *sf, void *v)
{
 struct cpuset *cs = css_cs(seq_css(sf));
 cpuset_filetype_t type = seq_cft(sf)->private;
 int ret = 0;

 spin_lock_irq(&callback_lock);

 switch (type) {
 case FILE_CPULIST:
  seq_printf(sf, "%*pbl\n", cpumask_pr_args(cs->cpus_allowed));
  break;
 case FILE_MEMLIST:
  seq_printf(sf, "%*pbl\n", nodemask_pr_args(&cs->mems_allowed));
  break;
 case FILE_EFFECTIVE_CPULIST:
  seq_printf(sf, "%*pbl\n", cpumask_pr_args(cs->effective_cpus));
  break;
 case FILE_EFFECTIVE_MEMLIST:
  seq_printf(sf, "%*pbl\n", nodemask_pr_args(&cs->effective_mems));
  break;
 default:
  ret = -EINVAL;
 }

 spin_unlock_irq(&callback_lock);
 return ret;
}

static u64 cpuset_read_u64(struct cgroup_subsys_state *css, struct cftype *cft)
{
 struct cpuset *cs = css_cs(css);
 cpuset_filetype_t type = cft->private;
 switch (type) {
 case FILE_CPU_EXCLUSIVE:
  return is_cpu_exclusive(cs);
 case FILE_MEM_EXCLUSIVE:
  return is_mem_exclusive(cs);
 case FILE_MEM_HARDWALL:
  return is_mem_hardwall(cs);
 case FILE_SCHED_LOAD_BALANCE:
  return is_sched_load_balance(cs);
 case FILE_MEMORY_MIGRATE:
  return is_memory_migrate(cs);
 case FILE_MEMORY_PRESSURE_ENABLED:
  return cpuset_memory_pressure_enabled;
 case FILE_MEMORY_PRESSURE:
  return fmeter_getrate(&cs->fmeter);
 case FILE_SPREAD_PAGE:
  return is_spread_page(cs);
 case FILE_SPREAD_SLAB:
  return is_spread_slab(cs);
 default:
  BUG();
 }


 return 0;
}

static s64 cpuset_read_s64(struct cgroup_subsys_state *css, struct cftype *cft)
{
 struct cpuset *cs = css_cs(css);
 cpuset_filetype_t type = cft->private;
 switch (type) {
 case FILE_SCHED_RELAX_DOMAIN_LEVEL:
  return cs->relax_domain_level;
 default:
  BUG();
 }


 return 0;
}






static struct cftype files[] = {
 {
  .name = "cpus",
  .seq_show = cpuset_common_seq_show,
  .write = cpuset_write_resmask,
  .max_write_len = (100U + 6 * NR_CPUS),
  .private = FILE_CPULIST,
 },

 {
  .name = "mems",
  .seq_show = cpuset_common_seq_show,
  .write = cpuset_write_resmask,
  .max_write_len = (100U + 6 * MAX_NUMNODES),
  .private = FILE_MEMLIST,
 },

 {
  .name = "effective_cpus",
  .seq_show = cpuset_common_seq_show,
  .private = FILE_EFFECTIVE_CPULIST,
 },

 {
  .name = "effective_mems",
  .seq_show = cpuset_common_seq_show,
  .private = FILE_EFFECTIVE_MEMLIST,
 },

 {
  .name = "cpu_exclusive",
  .read_u64 = cpuset_read_u64,
  .write_u64 = cpuset_write_u64,
  .private = FILE_CPU_EXCLUSIVE,
 },

 {
  .name = "mem_exclusive",
  .read_u64 = cpuset_read_u64,
  .write_u64 = cpuset_write_u64,
  .private = FILE_MEM_EXCLUSIVE,
 },

 {
  .name = "mem_hardwall",
  .read_u64 = cpuset_read_u64,
  .write_u64 = cpuset_write_u64,
  .private = FILE_MEM_HARDWALL,
 },

 {
  .name = "sched_load_balance",
  .read_u64 = cpuset_read_u64,
  .write_u64 = cpuset_write_u64,
  .private = FILE_SCHED_LOAD_BALANCE,
 },

 {
  .name = "sched_relax_domain_level",
  .read_s64 = cpuset_read_s64,
  .write_s64 = cpuset_write_s64,
  .private = FILE_SCHED_RELAX_DOMAIN_LEVEL,
 },

 {
  .name = "memory_migrate",
  .read_u64 = cpuset_read_u64,
  .write_u64 = cpuset_write_u64,
  .private = FILE_MEMORY_MIGRATE,
 },

 {
  .name = "memory_pressure",
  .read_u64 = cpuset_read_u64,
 },

 {
  .name = "memory_spread_page",
  .read_u64 = cpuset_read_u64,
  .write_u64 = cpuset_write_u64,
  .private = FILE_SPREAD_PAGE,
 },

 {
  .name = "memory_spread_slab",
  .read_u64 = cpuset_read_u64,
  .write_u64 = cpuset_write_u64,
  .private = FILE_SPREAD_SLAB,
 },

 {
  .name = "memory_pressure_enabled",
  .flags = CFTYPE_ONLY_ON_ROOT,
  .read_u64 = cpuset_read_u64,
  .write_u64 = cpuset_write_u64,
  .private = FILE_MEMORY_PRESSURE_ENABLED,
 },

 { }
};






static struct cgroup_subsys_state *
cpuset_css_alloc(struct cgroup_subsys_state *parent_css)
{
 struct cpuset *cs;

 if (!parent_css)
  return &top_cpuset.css;

 cs = kzalloc(sizeof(*cs), GFP_KERNEL);
 if (!cs)
  return ERR_PTR(-ENOMEM);
 if (!alloc_cpumask_var(&cs->cpus_allowed, GFP_KERNEL))
  goto free_cs;
 if (!alloc_cpumask_var(&cs->effective_cpus, GFP_KERNEL))
  goto free_cpus;

 set_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);
 cpumask_clear(cs->cpus_allowed);
 nodes_clear(cs->mems_allowed);
 cpumask_clear(cs->effective_cpus);
 nodes_clear(cs->effective_mems);
 fmeter_init(&cs->fmeter);
 cs->relax_domain_level = -1;

 return &cs->css;

free_cpus:
 free_cpumask_var(cs->cpus_allowed);
free_cs:
 kfree(cs);
 return ERR_PTR(-ENOMEM);
}

static int cpuset_css_online(struct cgroup_subsys_state *css)
{
 struct cpuset *cs = css_cs(css);
 struct cpuset *parent = parent_cs(cs);
 struct cpuset *tmp_cs;
 struct cgroup_subsys_state *pos_css;

 if (!parent)
  return 0;

 mutex_lock(&cpuset_mutex);

 set_bit(CS_ONLINE, &cs->flags);
 if (is_spread_page(parent))
  set_bit(CS_SPREAD_PAGE, &cs->flags);
 if (is_spread_slab(parent))
  set_bit(CS_SPREAD_SLAB, &cs->flags);

 cpuset_inc();

 spin_lock_irq(&callback_lock);
 if (cgroup_subsys_on_dfl(cpuset_cgrp_subsys)) {
  cpumask_copy(cs->effective_cpus, parent->effective_cpus);
  cs->effective_mems = parent->effective_mems;
 }
 spin_unlock_irq(&callback_lock);

 if (!test_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags))
  goto out_unlock;
 rcu_read_lock();
 cpuset_for_each_child(tmp_cs, pos_css, parent) {
  if (is_mem_exclusive(tmp_cs) || is_cpu_exclusive(tmp_cs)) {
   rcu_read_unlock();
   goto out_unlock;
  }
 }
 rcu_read_unlock();

 spin_lock_irq(&callback_lock);
 cs->mems_allowed = parent->mems_allowed;
 cs->effective_mems = parent->mems_allowed;
 cpumask_copy(cs->cpus_allowed, parent->cpus_allowed);
 cpumask_copy(cs->effective_cpus, parent->cpus_allowed);
 spin_unlock_irq(&callback_lock);
out_unlock:
 mutex_unlock(&cpuset_mutex);
 return 0;
}







static void cpuset_css_offline(struct cgroup_subsys_state *css)
{
 struct cpuset *cs = css_cs(css);

 mutex_lock(&cpuset_mutex);

 if (is_sched_load_balance(cs))
  update_flag(CS_SCHED_LOAD_BALANCE, cs, 0);

 cpuset_dec();
 clear_bit(CS_ONLINE, &cs->flags);

 mutex_unlock(&cpuset_mutex);
}

static void cpuset_css_free(struct cgroup_subsys_state *css)
{
 struct cpuset *cs = css_cs(css);

 free_cpumask_var(cs->effective_cpus);
 free_cpumask_var(cs->cpus_allowed);
 kfree(cs);
}

static void cpuset_bind(struct cgroup_subsys_state *root_css)
{
 mutex_lock(&cpuset_mutex);
 spin_lock_irq(&callback_lock);

 if (cgroup_subsys_on_dfl(cpuset_cgrp_subsys)) {
  cpumask_copy(top_cpuset.cpus_allowed, cpu_possible_mask);
  top_cpuset.mems_allowed = node_possible_map;
 } else {
  cpumask_copy(top_cpuset.cpus_allowed,
        top_cpuset.effective_cpus);
  top_cpuset.mems_allowed = top_cpuset.effective_mems;
 }

 spin_unlock_irq(&callback_lock);
 mutex_unlock(&cpuset_mutex);
}

struct cgroup_subsys cpuset_cgrp_subsys = {
 .css_alloc = cpuset_css_alloc,
 .css_online = cpuset_css_online,
 .css_offline = cpuset_css_offline,
 .css_free = cpuset_css_free,
 .can_attach = cpuset_can_attach,
 .cancel_attach = cpuset_cancel_attach,
 .attach = cpuset_attach,
 .post_attach = cpuset_post_attach,
 .bind = cpuset_bind,
 .legacy_cftypes = files,
 .early_init = true,
};







int __init cpuset_init(void)
{
 int err = 0;

 if (!alloc_cpumask_var(&top_cpuset.cpus_allowed, GFP_KERNEL))
  BUG();
 if (!alloc_cpumask_var(&top_cpuset.effective_cpus, GFP_KERNEL))
  BUG();

 cpumask_setall(top_cpuset.cpus_allowed);
 nodes_setall(top_cpuset.mems_allowed);
 cpumask_setall(top_cpuset.effective_cpus);
 nodes_setall(top_cpuset.effective_mems);

 fmeter_init(&top_cpuset.fmeter);
 set_bit(CS_SCHED_LOAD_BALANCE, &top_cpuset.flags);
 top_cpuset.relax_domain_level = -1;

 err = register_filesystem(&cpuset_fs_type);
 if (err < 0)
  return err;

 if (!alloc_cpumask_var(&cpus_attach, GFP_KERNEL))
  BUG();

 return 0;
}
static void remove_tasks_in_empty_cpuset(struct cpuset *cs)
{
 struct cpuset *parent;





 parent = parent_cs(cs);
 while (cpumask_empty(parent->cpus_allowed) ||
   nodes_empty(parent->mems_allowed))
  parent = parent_cs(parent);

 if (cgroup_transfer_tasks(parent->css.cgroup, cs->css.cgroup)) {
  pr_err("cpuset: failed to transfer tasks out of empty cpuset ");
  pr_cont_cgroup_name(cs->css.cgroup);
  pr_cont("\n");
 }
}

static void
hotplug_update_tasks_legacy(struct cpuset *cs,
       struct cpumask *new_cpus, nodemask_t *new_mems,
       bool cpus_updated, bool mems_updated)
{
 bool is_empty;

 spin_lock_irq(&callback_lock);
 cpumask_copy(cs->cpus_allowed, new_cpus);
 cpumask_copy(cs->effective_cpus, new_cpus);
 cs->mems_allowed = *new_mems;
 cs->effective_mems = *new_mems;
 spin_unlock_irq(&callback_lock);





 if (cpus_updated && !cpumask_empty(cs->cpus_allowed))
  update_tasks_cpumask(cs);
 if (mems_updated && !nodes_empty(cs->mems_allowed))
  update_tasks_nodemask(cs);

 is_empty = cpumask_empty(cs->cpus_allowed) ||
     nodes_empty(cs->mems_allowed);

 mutex_unlock(&cpuset_mutex);






 if (is_empty)
  remove_tasks_in_empty_cpuset(cs);

 mutex_lock(&cpuset_mutex);
}

static void
hotplug_update_tasks(struct cpuset *cs,
       struct cpumask *new_cpus, nodemask_t *new_mems,
       bool cpus_updated, bool mems_updated)
{
 if (cpumask_empty(new_cpus))
  cpumask_copy(new_cpus, parent_cs(cs)->effective_cpus);
 if (nodes_empty(*new_mems))
  *new_mems = parent_cs(cs)->effective_mems;

 spin_lock_irq(&callback_lock);
 cpumask_copy(cs->effective_cpus, new_cpus);
 cs->effective_mems = *new_mems;
 spin_unlock_irq(&callback_lock);

 if (cpus_updated)
  update_tasks_cpumask(cs);
 if (mems_updated)
  update_tasks_nodemask(cs);
}
static void cpuset_hotplug_update_tasks(struct cpuset *cs)
{
 static cpumask_t new_cpus;
 static nodemask_t new_mems;
 bool cpus_updated;
 bool mems_updated;
retry:
 wait_event(cpuset_attach_wq, cs->attach_in_progress == 0);

 mutex_lock(&cpuset_mutex);





 if (cs->attach_in_progress) {
  mutex_unlock(&cpuset_mutex);
  goto retry;
 }

 cpumask_and(&new_cpus, cs->cpus_allowed, parent_cs(cs)->effective_cpus);
 nodes_and(new_mems, cs->mems_allowed, parent_cs(cs)->effective_mems);

 cpus_updated = !cpumask_equal(&new_cpus, cs->effective_cpus);
 mems_updated = !nodes_equal(new_mems, cs->effective_mems);

 if (cgroup_subsys_on_dfl(cpuset_cgrp_subsys))
  hotplug_update_tasks(cs, &new_cpus, &new_mems,
         cpus_updated, mems_updated);
 else
  hotplug_update_tasks_legacy(cs, &new_cpus, &new_mems,
         cpus_updated, mems_updated);

 mutex_unlock(&cpuset_mutex);
}
static void cpuset_hotplug_workfn(struct work_struct *work)
{
 static cpumask_t new_cpus;
 static nodemask_t new_mems;
 bool cpus_updated, mems_updated;
 bool on_dfl = cgroup_subsys_on_dfl(cpuset_cgrp_subsys);

 mutex_lock(&cpuset_mutex);


 cpumask_copy(&new_cpus, cpu_active_mask);
 new_mems = node_states[N_MEMORY];

 cpus_updated = !cpumask_equal(top_cpuset.effective_cpus, &new_cpus);
 mems_updated = !nodes_equal(top_cpuset.effective_mems, new_mems);


 if (cpus_updated) {
  spin_lock_irq(&callback_lock);
  if (!on_dfl)
   cpumask_copy(top_cpuset.cpus_allowed, &new_cpus);
  cpumask_copy(top_cpuset.effective_cpus, &new_cpus);
  spin_unlock_irq(&callback_lock);

 }


 if (mems_updated) {
  spin_lock_irq(&callback_lock);
  if (!on_dfl)
   top_cpuset.mems_allowed = new_mems;
  top_cpuset.effective_mems = new_mems;
  spin_unlock_irq(&callback_lock);
  update_tasks_nodemask(&top_cpuset);
 }

 mutex_unlock(&cpuset_mutex);


 if (cpus_updated || mems_updated) {
  struct cpuset *cs;
  struct cgroup_subsys_state *pos_css;

  rcu_read_lock();
  cpuset_for_each_descendant_pre(cs, pos_css, &top_cpuset) {
   if (cs == &top_cpuset || !css_tryget_online(&cs->css))
    continue;
   rcu_read_unlock();

   cpuset_hotplug_update_tasks(cs);

   rcu_read_lock();
   css_put(&cs->css);
  }
  rcu_read_unlock();
 }


 if (cpus_updated)
  rebuild_sched_domains();
}

void cpuset_update_active_cpus(bool cpu_online)
{
 partition_sched_domains(1, NULL, NULL);
 schedule_work(&cpuset_hotplug_work);
}






static int cpuset_track_online_nodes(struct notifier_block *self,
    unsigned long action, void *arg)
{
 schedule_work(&cpuset_hotplug_work);
 return NOTIFY_OK;
}

static struct notifier_block cpuset_track_online_nodes_nb = {
 .notifier_call = cpuset_track_online_nodes,
 .priority = 10,
};






void __init cpuset_init_smp(void)
{
 cpumask_copy(top_cpuset.cpus_allowed, cpu_active_mask);
 top_cpuset.mems_allowed = node_states[N_MEMORY];
 top_cpuset.old_mems_allowed = top_cpuset.mems_allowed;

 cpumask_copy(top_cpuset.effective_cpus, cpu_active_mask);
 top_cpuset.effective_mems = node_states[N_MEMORY];

 register_hotmemory_notifier(&cpuset_track_online_nodes_nb);

 cpuset_migrate_mm_wq = alloc_ordered_workqueue("cpuset_migrate_mm", 0);
 BUG_ON(!cpuset_migrate_mm_wq);
}
void cpuset_cpus_allowed(struct task_struct *tsk, struct cpumask *pmask)
{
 unsigned long flags;

 spin_lock_irqsave(&callback_lock, flags);
 rcu_read_lock();
 guarantee_online_cpus(task_cs(tsk), pmask);
 rcu_read_unlock();
 spin_unlock_irqrestore(&callback_lock, flags);
}

void cpuset_cpus_allowed_fallback(struct task_struct *tsk)
{
 rcu_read_lock();
 do_set_cpus_allowed(tsk, task_cs(tsk)->effective_cpus);
 rcu_read_unlock();
}

void __init cpuset_init_current_mems_allowed(void)
{
 nodes_setall(current->mems_allowed);
}
nodemask_t cpuset_mems_allowed(struct task_struct *tsk)
{
 nodemask_t mask;
 unsigned long flags;

 spin_lock_irqsave(&callback_lock, flags);
 rcu_read_lock();
 guarantee_online_mems(task_cs(tsk), &mask);
 rcu_read_unlock();
 spin_unlock_irqrestore(&callback_lock, flags);

 return mask;
}







int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask)
{
 return nodes_intersects(*nodemask, current->mems_allowed);
}







static struct cpuset *nearest_hardwall_ancestor(struct cpuset *cs)
{
 while (!(is_mem_exclusive(cs) || is_mem_hardwall(cs)) && parent_cs(cs))
  cs = parent_cs(cs);
 return cs;
}
bool __cpuset_node_allowed(int node, gfp_t gfp_mask)
{
 struct cpuset *cs;
 int allowed;
 unsigned long flags;

 if (in_interrupt())
  return true;
 if (node_isset(node, current->mems_allowed))
  return true;




 if (unlikely(test_thread_flag(TIF_MEMDIE)))
  return true;
 if (gfp_mask & __GFP_HARDWALL)
  return false;

 if (current->flags & PF_EXITING)
  return true;


 spin_lock_irqsave(&callback_lock, flags);

 rcu_read_lock();
 cs = nearest_hardwall_ancestor(task_cs(current));
 allowed = node_isset(node, cs->mems_allowed);
 rcu_read_unlock();

 spin_unlock_irqrestore(&callback_lock, flags);
 return allowed;
}
static int cpuset_spread_node(int *rotor)
{
 return *rotor = next_node_in(*rotor, current->mems_allowed);
}

int cpuset_mem_spread_node(void)
{
 if (current->cpuset_mem_spread_rotor == NUMA_NO_NODE)
  current->cpuset_mem_spread_rotor =
   node_random(&current->mems_allowed);

 return cpuset_spread_node(&current->cpuset_mem_spread_rotor);
}

int cpuset_slab_spread_node(void)
{
 if (current->cpuset_slab_spread_rotor == NUMA_NO_NODE)
  current->cpuset_slab_spread_rotor =
   node_random(&current->mems_allowed);

 return cpuset_spread_node(&current->cpuset_slab_spread_rotor);
}

EXPORT_SYMBOL_GPL(cpuset_mem_spread_node);
int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,
       const struct task_struct *tsk2)
{
 return nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);
}







void cpuset_print_current_mems_allowed(void)
{
 struct cgroup *cgrp;

 rcu_read_lock();

 cgrp = task_cs(current)->css.cgroup;
 pr_info("%s cpuset=", current->comm);
 pr_cont_cgroup_name(cgrp);
 pr_cont(" mems_allowed=%*pbl\n",
  nodemask_pr_args(&current->mems_allowed));

 rcu_read_unlock();
}







int cpuset_memory_pressure_enabled __read_mostly;
void __cpuset_memory_pressure_bump(void)
{
 rcu_read_lock();
 fmeter_markevent(&task_cs(current)->fmeter);
 rcu_read_unlock();
}

int proc_cpuset_show(struct seq_file *m, struct pid_namespace *ns,
       struct pid *pid, struct task_struct *tsk)
{
 char *buf, *p;
 struct cgroup_subsys_state *css;
 int retval;

 retval = -ENOMEM;
 buf = kmalloc(PATH_MAX, GFP_KERNEL);
 if (!buf)
  goto out;

 retval = -ENAMETOOLONG;
 css = task_get_css(tsk, cpuset_cgrp_id);
 p = cgroup_path_ns(css->cgroup, buf, PATH_MAX,
      current->nsproxy->cgroup_ns);
 css_put(css);
 if (!p)
  goto out_free;
 seq_puts(m, p);
 seq_putc(m, '\n');
 retval = 0;
out_free:
 kfree(buf);
out:
 return retval;
}


void cpuset_task_status_allowed(struct seq_file *m, struct task_struct *task)
{
 seq_printf(m, "Mems_allowed:\t%*pb\n",
     nodemask_pr_args(&task->mems_allowed));
 seq_printf(m, "Mems_allowed_list:\t%*pbl\n",
     nodemask_pr_args(&task->mems_allowed));
}





unsigned long saved_max_pfn;
unsigned long long elfcorehdr_addr = ELFCORE_ADDR_MAX;
EXPORT_SYMBOL_GPL(elfcorehdr_addr);




unsigned long long elfcorehdr_size;







static int __init setup_elfcorehdr(char *arg)
{
 char *end;
 if (!arg)
  return -EINVAL;
 elfcorehdr_addr = memparse(arg, &end);
 if (*end == '@') {
  elfcorehdr_size = elfcorehdr_addr;
  elfcorehdr_addr = memparse(end + 1, &end);
 }
 return end > arg ? 0 : -EINVAL;
}
early_param("elfcorehdr", setup_elfcorehdr);

 printk("[%-5.5s%5u] " FMT "\n", \
        current->comm, current->pid, ##__VA_ARGS__)
do { \
 if (0) \
  no_printk("[%-5.5s%5u] " FMT "\n", \
     current->comm, current->pid, ##__VA_ARGS__); \
} while (0)

static struct kmem_cache *cred_jar;


struct group_info init_groups = { .usage = ATOMIC_INIT(2) };




struct cred init_cred = {
 .usage = ATOMIC_INIT(4),
 .subscribers = ATOMIC_INIT(2),
 .magic = CRED_MAGIC,
 .uid = GLOBAL_ROOT_UID,
 .gid = GLOBAL_ROOT_GID,
 .suid = GLOBAL_ROOT_UID,
 .sgid = GLOBAL_ROOT_GID,
 .euid = GLOBAL_ROOT_UID,
 .egid = GLOBAL_ROOT_GID,
 .fsuid = GLOBAL_ROOT_UID,
 .fsgid = GLOBAL_ROOT_GID,
 .securebits = SECUREBITS_DEFAULT,
 .cap_inheritable = CAP_EMPTY_SET,
 .cap_permitted = CAP_FULL_SET,
 .cap_effective = CAP_FULL_SET,
 .cap_bset = CAP_FULL_SET,
 .user = INIT_USER,
 .user_ns = &init_user_ns,
 .group_info = &init_groups,
};

static inline void set_cred_subscribers(struct cred *cred, int n)
{
 atomic_set(&cred->subscribers, n);
}

static inline int read_cred_subscribers(const struct cred *cred)
{
 return atomic_read(&cred->subscribers);
 return 0;
}

static inline void alter_cred_subscribers(const struct cred *_cred, int n)
{
 struct cred *cred = (struct cred *) _cred;

 atomic_add(n, &cred->subscribers);
}




static void put_cred_rcu(struct rcu_head *rcu)
{
 struct cred *cred = container_of(rcu, struct cred, rcu);

 kdebug("put_cred_rcu(%p)", cred);

 if (cred->magic != CRED_MAGIC_DEAD ||
     atomic_read(&cred->usage) != 0 ||
     read_cred_subscribers(cred) != 0)
  panic("CRED: put_cred_rcu() sees %p with"
        " mag %x, put %p, usage %d, subscr %d\n",
        cred, cred->magic, cred->put_addr,
        atomic_read(&cred->usage),
        read_cred_subscribers(cred));
 if (atomic_read(&cred->usage) != 0)
  panic("CRED: put_cred_rcu() sees %p with usage %d\n",
        cred, atomic_read(&cred->usage));

 security_cred_free(cred);
 key_put(cred->session_keyring);
 key_put(cred->process_keyring);
 key_put(cred->thread_keyring);
 key_put(cred->request_key_auth);
 if (cred->group_info)
  put_group_info(cred->group_info);
 free_uid(cred->user);
 put_user_ns(cred->user_ns);
 kmem_cache_free(cred_jar, cred);
}







void __put_cred(struct cred *cred)
{
 kdebug("__put_cred(%p{%d,%d})", cred,
        atomic_read(&cred->usage),
        read_cred_subscribers(cred));

 BUG_ON(atomic_read(&cred->usage) != 0);
 BUG_ON(read_cred_subscribers(cred) != 0);
 cred->magic = CRED_MAGIC_DEAD;
 cred->put_addr = __builtin_return_address(0);
 BUG_ON(cred == current->cred);
 BUG_ON(cred == current->real_cred);

 call_rcu(&cred->rcu, put_cred_rcu);
}
EXPORT_SYMBOL(__put_cred);




void exit_creds(struct task_struct *tsk)
{
 struct cred *cred;

 kdebug("exit_creds(%u,%p,%p,{%d,%d})", tsk->pid, tsk->real_cred, tsk->cred,
        atomic_read(&tsk->cred->usage),
        read_cred_subscribers(tsk->cred));

 cred = (struct cred *) tsk->real_cred;
 tsk->real_cred = NULL;
 validate_creds(cred);
 alter_cred_subscribers(cred, -1);
 put_cred(cred);

 cred = (struct cred *) tsk->cred;
 tsk->cred = NULL;
 validate_creds(cred);
 alter_cred_subscribers(cred, -1);
 put_cred(cred);
}
const struct cred *get_task_cred(struct task_struct *task)
{
 const struct cred *cred;

 rcu_read_lock();

 do {
  cred = __task_cred((task));
  BUG_ON(!cred);
 } while (!atomic_inc_not_zero(&((struct cred *)cred)->usage));

 rcu_read_unlock();
 return cred;
}





struct cred *cred_alloc_blank(void)
{
 struct cred *new;

 new = kmem_cache_zalloc(cred_jar, GFP_KERNEL);
 if (!new)
  return NULL;

 atomic_set(&new->usage, 1);
 new->magic = CRED_MAGIC;

 if (security_cred_alloc_blank(new, GFP_KERNEL) < 0)
  goto error;

 return new;

error:
 abort_creds(new);
 return NULL;
}
struct cred *prepare_creds(void)
{
 struct task_struct *task = current;
 const struct cred *old;
 struct cred *new;

 validate_process_creds();

 new = kmem_cache_alloc(cred_jar, GFP_KERNEL);
 if (!new)
  return NULL;

 kdebug("prepare_creds() alloc %p", new);

 old = task->cred;
 memcpy(new, old, sizeof(struct cred));

 atomic_set(&new->usage, 1);
 set_cred_subscribers(new, 0);
 get_group_info(new->group_info);
 get_uid(new->user);
 get_user_ns(new->user_ns);

 key_get(new->session_keyring);
 key_get(new->process_keyring);
 key_get(new->thread_keyring);
 key_get(new->request_key_auth);

 new->security = NULL;

 if (security_prepare_creds(new, old, GFP_KERNEL) < 0)
  goto error;
 validate_creds(new);
 return new;

error:
 abort_creds(new);
 return NULL;
}
EXPORT_SYMBOL(prepare_creds);





struct cred *prepare_exec_creds(void)
{
 struct cred *new;

 new = prepare_creds();
 if (!new)
  return new;


 key_put(new->thread_keyring);
 new->thread_keyring = NULL;


 key_put(new->process_keyring);
 new->process_keyring = NULL;

 return new;
}
int copy_creds(struct task_struct *p, unsigned long clone_flags)
{
 struct cred *new;
 int ret;

 if (
  !p->cred->thread_keyring &&
  clone_flags & CLONE_THREAD
     ) {
  p->real_cred = get_cred(p->cred);
  get_cred(p->cred);
  alter_cred_subscribers(p->cred, 2);
  kdebug("share_creds(%p{%d,%d})",
         p->cred, atomic_read(&p->cred->usage),
         read_cred_subscribers(p->cred));
  atomic_inc(&p->cred->user->processes);
  return 0;
 }

 new = prepare_creds();
 if (!new)
  return -ENOMEM;

 if (clone_flags & CLONE_NEWUSER) {
  ret = create_user_ns(new);
  if (ret < 0)
   goto error_put;
 }



 if (new->thread_keyring) {
  key_put(new->thread_keyring);
  new->thread_keyring = NULL;
  if (clone_flags & CLONE_THREAD)
   install_thread_keyring_to_cred(new);
 }




 if (!(clone_flags & CLONE_THREAD)) {
  key_put(new->process_keyring);
  new->process_keyring = NULL;
 }

 atomic_inc(&new->user->processes);
 p->cred = p->real_cred = get_cred(new);
 alter_cred_subscribers(new, 2);
 validate_creds(new);
 return 0;

error_put:
 put_cred(new);
 return ret;
}

static bool cred_cap_issubset(const struct cred *set, const struct cred *subset)
{
 const struct user_namespace *set_ns = set->user_ns;
 const struct user_namespace *subset_ns = subset->user_ns;




 if (set_ns == subset_ns)
  return cap_issubset(subset->cap_permitted, set->cap_permitted);






 for (;subset_ns != &init_user_ns; subset_ns = subset_ns->parent) {
  if ((set_ns == subset_ns->parent) &&
      uid_eq(subset_ns->owner, set->euid))
   return true;
 }

 return false;
}
int commit_creds(struct cred *new)
{
 struct task_struct *task = current;
 const struct cred *old = task->real_cred;

 kdebug("commit_creds(%p{%d,%d})", new,
        atomic_read(&new->usage),
        read_cred_subscribers(new));

 BUG_ON(task->cred != old);
 BUG_ON(read_cred_subscribers(old) < 2);
 validate_creds(old);
 validate_creds(new);
 BUG_ON(atomic_read(&new->usage) < 1);

 get_cred(new);


 if (!uid_eq(old->euid, new->euid) ||
     !gid_eq(old->egid, new->egid) ||
     !uid_eq(old->fsuid, new->fsuid) ||
     !gid_eq(old->fsgid, new->fsgid) ||
     !cred_cap_issubset(old, new)) {
  if (task->mm)
   set_dumpable(task->mm, suid_dumpable);
  task->pdeath_signal = 0;
  smp_wmb();
 }


 if (!uid_eq(new->fsuid, old->fsuid))
  key_fsuid_changed(task);
 if (!gid_eq(new->fsgid, old->fsgid))
  key_fsgid_changed(task);





 alter_cred_subscribers(new, 2);
 if (new->user != old->user)
  atomic_inc(&new->user->processes);
 rcu_assign_pointer(task->real_cred, new);
 rcu_assign_pointer(task->cred, new);
 if (new->user != old->user)
  atomic_dec(&old->user->processes);
 alter_cred_subscribers(old, -2);


 if (!uid_eq(new->uid, old->uid) ||
     !uid_eq(new->euid, old->euid) ||
     !uid_eq(new->suid, old->suid) ||
     !uid_eq(new->fsuid, old->fsuid))
  proc_id_connector(task, PROC_EVENT_UID);

 if (!gid_eq(new->gid, old->gid) ||
     !gid_eq(new->egid, old->egid) ||
     !gid_eq(new->sgid, old->sgid) ||
     !gid_eq(new->fsgid, old->fsgid))
  proc_id_connector(task, PROC_EVENT_GID);


 put_cred(old);
 put_cred(old);
 return 0;
}
EXPORT_SYMBOL(commit_creds);
void abort_creds(struct cred *new)
{
 kdebug("abort_creds(%p{%d,%d})", new,
        atomic_read(&new->usage),
        read_cred_subscribers(new));

 BUG_ON(read_cred_subscribers(new) != 0);
 BUG_ON(atomic_read(&new->usage) < 1);
 put_cred(new);
}
EXPORT_SYMBOL(abort_creds);
const struct cred *override_creds(const struct cred *new)
{
 const struct cred *old = current->cred;

 kdebug("override_creds(%p{%d,%d})", new,
        atomic_read(&new->usage),
        read_cred_subscribers(new));

 validate_creds(old);
 validate_creds(new);
 get_cred(new);
 alter_cred_subscribers(new, 1);
 rcu_assign_pointer(current->cred, new);
 alter_cred_subscribers(old, -1);

 kdebug("override_creds() = %p{%d,%d}", old,
        atomic_read(&old->usage),
        read_cred_subscribers(old));
 return old;
}
EXPORT_SYMBOL(override_creds);
void revert_creds(const struct cred *old)
{
 const struct cred *override = current->cred;

 kdebug("revert_creds(%p{%d,%d})", old,
        atomic_read(&old->usage),
        read_cred_subscribers(old));

 validate_creds(old);
 validate_creds(override);
 alter_cred_subscribers(old, 1);
 rcu_assign_pointer(current->cred, old);
 alter_cred_subscribers(override, -1);
 put_cred(override);
}
EXPORT_SYMBOL(revert_creds);




void __init cred_init(void)
{

 cred_jar = kmem_cache_create("cred_jar", sizeof(struct cred), 0,
   SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT, NULL);
}
struct cred *prepare_kernel_cred(struct task_struct *daemon)
{
 const struct cred *old;
 struct cred *new;

 new = kmem_cache_alloc(cred_jar, GFP_KERNEL);
 if (!new)
  return NULL;

 kdebug("prepare_kernel_cred() alloc %p", new);

 if (daemon)
  old = get_task_cred(daemon);
 else
  old = get_cred(&init_cred);

 validate_creds(old);

 *new = *old;
 atomic_set(&new->usage, 1);
 set_cred_subscribers(new, 0);
 get_uid(new->user);
 get_user_ns(new->user_ns);
 get_group_info(new->group_info);

 new->session_keyring = NULL;
 new->process_keyring = NULL;
 new->thread_keyring = NULL;
 new->request_key_auth = NULL;
 new->jit_keyring = KEY_REQKEY_DEFL_THREAD_KEYRING;

 new->security = NULL;
 if (security_prepare_creds(new, old, GFP_KERNEL) < 0)
  goto error;

 put_cred(old);
 validate_creds(new);
 return new;

error:
 put_cred(new);
 put_cred(old);
 return NULL;
}
EXPORT_SYMBOL(prepare_kernel_cred);
int set_security_override(struct cred *new, u32 secid)
{
 return security_kernel_act_as(new, secid);
}
EXPORT_SYMBOL(set_security_override);
int set_security_override_from_ctx(struct cred *new, const char *secctx)
{
 u32 secid;
 int ret;

 ret = security_secctx_to_secid(secctx, strlen(secctx), &secid);
 if (ret < 0)
  return ret;

 return set_security_override(new, secid);
}
EXPORT_SYMBOL(set_security_override_from_ctx);
int set_create_files_as(struct cred *new, struct inode *inode)
{
 new->fsuid = inode->i_uid;
 new->fsgid = inode->i_gid;
 return security_kernel_create_files_as(new, inode);
}
EXPORT_SYMBOL(set_create_files_as);


bool creds_are_invalid(const struct cred *cred)
{
 if (cred->magic != CRED_MAGIC)
  return true;




 if (selinux_is_enabled() && cred->security) {
  if ((unsigned long) cred->security < PAGE_SIZE)
   return true;
  if ((*(u32 *)cred->security & 0xffffff00) ==
      (POISON_FREE << 24 | POISON_FREE << 16 | POISON_FREE << 8))
   return true;
 }
 return false;
}
EXPORT_SYMBOL(creds_are_invalid);




static void dump_invalid_creds(const struct cred *cred, const char *label,
          const struct task_struct *tsk)
{
 printk(KERN_ERR "CRED: %s credentials: %p %s%s%s\n",
        label, cred,
        cred == &init_cred ? "[init]" : "",
        cred == tsk->real_cred ? "[real]" : "",
        cred == tsk->cred ? "[eff]" : "");
 printk(KERN_ERR "CRED: ->magic=%x, put_addr=%p\n",
        cred->magic, cred->put_addr);
 printk(KERN_ERR "CRED: ->usage=%d, subscr=%d\n",
        atomic_read(&cred->usage),
        read_cred_subscribers(cred));
 printk(KERN_ERR "CRED: ->*uid = { %d,%d,%d,%d }\n",
  from_kuid_munged(&init_user_ns, cred->uid),
  from_kuid_munged(&init_user_ns, cred->euid),
  from_kuid_munged(&init_user_ns, cred->suid),
  from_kuid_munged(&init_user_ns, cred->fsuid));
 printk(KERN_ERR "CRED: ->*gid = { %d,%d,%d,%d }\n",
  from_kgid_munged(&init_user_ns, cred->gid),
  from_kgid_munged(&init_user_ns, cred->egid),
  from_kgid_munged(&init_user_ns, cred->sgid),
  from_kgid_munged(&init_user_ns, cred->fsgid));
 printk(KERN_ERR "CRED: ->security is %p\n", cred->security);
 if ((unsigned long) cred->security >= PAGE_SIZE &&
     (((unsigned long) cred->security & 0xffffff00) !=
      (POISON_FREE << 24 | POISON_FREE << 16 | POISON_FREE << 8)))
  printk(KERN_ERR "CRED: ->security {%x, %x}\n",
         ((u32*)cred->security)[0],
         ((u32*)cred->security)[1]);
}




void __invalid_creds(const struct cred *cred, const char *file, unsigned line)
{
 printk(KERN_ERR "CRED: Invalid credentials\n");
 printk(KERN_ERR "CRED: At %s:%u\n", file, line);
 dump_invalid_creds(cred, "Specified", current);
 BUG();
}
EXPORT_SYMBOL(__invalid_creds);




void __validate_process_creds(struct task_struct *tsk,
         const char *file, unsigned line)
{
 if (tsk->cred == tsk->real_cred) {
  if (unlikely(read_cred_subscribers(tsk->cred) < 2 ||
        creds_are_invalid(tsk->cred)))
   goto invalid_creds;
 } else {
  if (unlikely(read_cred_subscribers(tsk->real_cred) < 1 ||
        read_cred_subscribers(tsk->cred) < 1 ||
        creds_are_invalid(tsk->real_cred) ||
        creds_are_invalid(tsk->cred)))
   goto invalid_creds;
 }
 return;

invalid_creds:
 printk(KERN_ERR "CRED: Invalid process credentials\n");
 printk(KERN_ERR "CRED: At %s:%u\n", file, line);

 dump_invalid_creds(tsk->real_cred, "Real", tsk);
 if (tsk->cred != tsk->real_cred)
  dump_invalid_creds(tsk->cred, "Effective", tsk);
 else
  printk(KERN_ERR "CRED: Effective creds == Real creds\n");
 BUG();
}
EXPORT_SYMBOL(__validate_process_creds);




void validate_creds_for_do_exit(struct task_struct *tsk)
{
 kdebug("validate_creds_for_do_exit(%p,%p{%d,%d})",
        tsk->real_cred, tsk->cred,
        atomic_read(&tsk->cred->usage),
        read_cred_subscribers(tsk->cred));

 __validate_process_creds(tsk, __FILE__, __LINE__);
}





static int kgdb_break_asap;

struct debuggerinfo_struct kgdb_info[NR_CPUS];




int kgdb_connected;
EXPORT_SYMBOL_GPL(kgdb_connected);


int kgdb_io_module_registered;


static int exception_level;

struct kgdb_io *dbg_io_ops;
static DEFINE_SPINLOCK(kgdb_registration_lock);


static int kgdbreboot;

static int kgdb_con_registered;

static int kgdb_use_con;

bool dbg_is_early = true;

int dbg_switch_cpu;


int dbg_kdb_mode = 1;

static int __init opt_kgdb_con(char *str)
{
 kgdb_use_con = 1;
 return 0;
}

early_param("kgdbcon", opt_kgdb_con);

module_param(kgdb_use_con, int, 0644);
module_param(kgdbreboot, int, 0644);





static struct kgdb_bkpt kgdb_break[KGDB_MAX_BREAKPOINTS] = {
 [0 ... KGDB_MAX_BREAKPOINTS-1] = { .state = BP_UNDEFINED }
};




atomic_t kgdb_active = ATOMIC_INIT(-1);
EXPORT_SYMBOL_GPL(kgdb_active);
static DEFINE_RAW_SPINLOCK(dbg_master_lock);
static DEFINE_RAW_SPINLOCK(dbg_slave_lock);





static atomic_t masters_in_kgdb;
static atomic_t slaves_in_kgdb;
static atomic_t kgdb_break_tasklet_var;
atomic_t kgdb_setting_breakpoint;

struct task_struct *kgdb_usethread;
struct task_struct *kgdb_contthread;

int kgdb_single_step;
static pid_t kgdb_sstep_pid;


atomic_t kgdb_cpu_doing_single_step = ATOMIC_INIT(-1);
static int kgdb_do_roundup = 1;

static int __init opt_nokgdbroundup(char *str)
{
 kgdb_do_roundup = 0;

 return 0;
}

early_param("nokgdbroundup", opt_nokgdbroundup);
int __weak kgdb_arch_set_breakpoint(struct kgdb_bkpt *bpt)
{
 int err;

 err = probe_kernel_read(bpt->saved_instr, (char *)bpt->bpt_addr,
    BREAK_INSTR_SIZE);
 if (err)
  return err;
 err = probe_kernel_write((char *)bpt->bpt_addr,
     arch_kgdb_ops.gdb_bpt_instr, BREAK_INSTR_SIZE);
 return err;
}

int __weak kgdb_arch_remove_breakpoint(struct kgdb_bkpt *bpt)
{
 return probe_kernel_write((char *)bpt->bpt_addr,
      (char *)bpt->saved_instr, BREAK_INSTR_SIZE);
}

int __weak kgdb_validate_break_address(unsigned long addr)
{
 struct kgdb_bkpt tmp;
 int err;





 tmp.bpt_addr = addr;
 err = kgdb_arch_set_breakpoint(&tmp);
 if (err)
  return err;
 err = kgdb_arch_remove_breakpoint(&tmp);
 if (err)
  pr_err("Critical breakpoint error, kernel memory destroyed at: %lx\n",
         addr);
 return err;
}

unsigned long __weak kgdb_arch_pc(int exception, struct pt_regs *regs)
{
 return instruction_pointer(regs);
}

int __weak kgdb_arch_init(void)
{
 return 0;
}

int __weak kgdb_skipexception(int exception, struct pt_regs *regs)
{
 return 0;
}





static void kgdb_flush_swbreak_addr(unsigned long addr)
{
 if (!CACHE_FLUSH_IS_SAFE)
  return;

 if (current->mm) {
  int i;

  for (i = 0; i < VMACACHE_SIZE; i++) {
   if (!current->vmacache[i])
    continue;
   flush_cache_range(current->vmacache[i],
       addr, addr + BREAK_INSTR_SIZE);
  }
 }


 flush_icache_range(addr, addr + BREAK_INSTR_SIZE);
}




int dbg_activate_sw_breakpoints(void)
{
 int error;
 int ret = 0;
 int i;

 for (i = 0; i < KGDB_MAX_BREAKPOINTS; i++) {
  if (kgdb_break[i].state != BP_SET)
   continue;

  error = kgdb_arch_set_breakpoint(&kgdb_break[i]);
  if (error) {
   ret = error;
   pr_info("BP install failed: %lx\n",
    kgdb_break[i].bpt_addr);
   continue;
  }

  kgdb_flush_swbreak_addr(kgdb_break[i].bpt_addr);
  kgdb_break[i].state = BP_ACTIVE;
 }
 return ret;
}

int dbg_set_sw_break(unsigned long addr)
{
 int err = kgdb_validate_break_address(addr);
 int breakno = -1;
 int i;

 if (err)
  return err;

 for (i = 0; i < KGDB_MAX_BREAKPOINTS; i++) {
  if ((kgdb_break[i].state == BP_SET) &&
     (kgdb_break[i].bpt_addr == addr))
   return -EEXIST;
 }
 for (i = 0; i < KGDB_MAX_BREAKPOINTS; i++) {
  if (kgdb_break[i].state == BP_REMOVED &&
     kgdb_break[i].bpt_addr == addr) {
   breakno = i;
   break;
  }
 }

 if (breakno == -1) {
  for (i = 0; i < KGDB_MAX_BREAKPOINTS; i++) {
   if (kgdb_break[i].state == BP_UNDEFINED) {
    breakno = i;
    break;
   }
  }
 }

 if (breakno == -1)
  return -E2BIG;

 kgdb_break[breakno].state = BP_SET;
 kgdb_break[breakno].type = BP_BREAKPOINT;
 kgdb_break[breakno].bpt_addr = addr;

 return 0;
}

int dbg_deactivate_sw_breakpoints(void)
{
 int error;
 int ret = 0;
 int i;

 for (i = 0; i < KGDB_MAX_BREAKPOINTS; i++) {
  if (kgdb_break[i].state != BP_ACTIVE)
   continue;
  error = kgdb_arch_remove_breakpoint(&kgdb_break[i]);
  if (error) {
   pr_info("BP remove failed: %lx\n",
    kgdb_break[i].bpt_addr);
   ret = error;
  }

  kgdb_flush_swbreak_addr(kgdb_break[i].bpt_addr);
  kgdb_break[i].state = BP_SET;
 }
 return ret;
}

int dbg_remove_sw_break(unsigned long addr)
{
 int i;

 for (i = 0; i < KGDB_MAX_BREAKPOINTS; i++) {
  if ((kgdb_break[i].state == BP_SET) &&
    (kgdb_break[i].bpt_addr == addr)) {
   kgdb_break[i].state = BP_REMOVED;
   return 0;
  }
 }
 return -ENOENT;
}

int kgdb_isremovedbreak(unsigned long addr)
{
 int i;

 for (i = 0; i < KGDB_MAX_BREAKPOINTS; i++) {
  if ((kgdb_break[i].state == BP_REMOVED) &&
     (kgdb_break[i].bpt_addr == addr))
   return 1;
 }
 return 0;
}

int dbg_remove_all_break(void)
{
 int error;
 int i;


 for (i = 0; i < KGDB_MAX_BREAKPOINTS; i++) {
  if (kgdb_break[i].state != BP_ACTIVE)
   goto setundefined;
  error = kgdb_arch_remove_breakpoint(&kgdb_break[i]);
  if (error)
   pr_err("breakpoint remove failed: %lx\n",
          kgdb_break[i].bpt_addr);
setundefined:
  kgdb_break[i].state = BP_UNDEFINED;
 }


 if (arch_kgdb_ops.remove_all_hw_break)
  arch_kgdb_ops.remove_all_hw_break();

 return 0;
}
static int kgdb_io_ready(int print_wait)
{
 if (!dbg_io_ops)
  return 0;
 if (kgdb_connected)
  return 1;
 if (atomic_read(&kgdb_setting_breakpoint))
  return 1;
 if (print_wait) {
  if (!dbg_kdb_mode)
   pr_crit("waiting... or $3#33 for KDB\n");
  pr_crit("Waiting for remote debugger\n");
 }
 return 1;
}

static int kgdb_reenter_check(struct kgdb_state *ks)
{
 unsigned long addr;

 if (atomic_read(&kgdb_active) != raw_smp_processor_id())
  return 0;


 exception_level++;
 addr = kgdb_arch_pc(ks->ex_vector, ks->linux_regs);
 dbg_deactivate_sw_breakpoints();







 if (dbg_remove_sw_break(addr) == 0) {
  exception_level = 0;
  kgdb_skipexception(ks->ex_vector, ks->linux_regs);
  dbg_activate_sw_breakpoints();
  pr_crit("re-enter error: breakpoint removed %lx\n", addr);
  WARN_ON_ONCE(1);

  return 1;
 }
 dbg_remove_all_break();
 kgdb_skipexception(ks->ex_vector, ks->linux_regs);

 if (exception_level > 1) {
  dump_stack();
  panic("Recursive entry to debugger");
 }

 pr_crit("re-enter exception: ALL breakpoints killed\n");

 return 0;
 dump_stack();
 panic("Recursive entry to debugger");

 return 1;
}

static void dbg_touch_watchdogs(void)
{
 touch_softlockup_watchdog_sync();
 clocksource_touch_watchdog();
 rcu_cpu_stall_reset();
}

static int kgdb_cpu_enter(struct kgdb_state *ks, struct pt_regs *regs,
  int exception_state)
{
 unsigned long flags;
 int sstep_tries = 100;
 int error;
 int cpu;
 int trace_on = 0;
 int online_cpus = num_online_cpus();
 u64 time_left;

 kgdb_info[ks->cpu].enter_kgdb++;
 kgdb_info[ks->cpu].exception_state |= exception_state;

 if (exception_state == DCPU_WANT_MASTER)
  atomic_inc(&masters_in_kgdb);
 else
  atomic_inc(&slaves_in_kgdb);

 if (arch_kgdb_ops.disable_hw_break)
  arch_kgdb_ops.disable_hw_break(regs);

acquirelock:




 local_irq_save(flags);

 cpu = ks->cpu;
 kgdb_info[cpu].debuggerinfo = regs;
 kgdb_info[cpu].task = current;
 kgdb_info[cpu].ret_state = 0;
 kgdb_info[cpu].irq_depth = hardirq_count() >> HARDIRQ_SHIFT;


 smp_mb();

 if (exception_level == 1) {
  if (raw_spin_trylock(&dbg_master_lock))
   atomic_xchg(&kgdb_active, cpu);
  goto cpu_master_loop;
 }





 while (1) {
cpu_loop:
  if (kgdb_info[cpu].exception_state & DCPU_NEXT_MASTER) {
   kgdb_info[cpu].exception_state &= ~DCPU_NEXT_MASTER;
   goto cpu_master_loop;
  } else if (kgdb_info[cpu].exception_state & DCPU_WANT_MASTER) {
   if (raw_spin_trylock(&dbg_master_lock)) {
    atomic_xchg(&kgdb_active, cpu);
    break;
   }
  } else if (kgdb_info[cpu].exception_state & DCPU_IS_SLAVE) {
   if (!raw_spin_is_locked(&dbg_slave_lock))
    goto return_normal;
  } else {
return_normal:



   if (arch_kgdb_ops.correct_hw_break)
    arch_kgdb_ops.correct_hw_break();
   if (trace_on)
    tracing_on();
   kgdb_info[cpu].exception_state &=
    ~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);
   kgdb_info[cpu].enter_kgdb--;
   smp_mb__before_atomic();
   atomic_dec(&slaves_in_kgdb);
   dbg_touch_watchdogs();
   local_irq_restore(flags);
   return 0;
  }
  cpu_relax();
 }







 if (atomic_read(&kgdb_cpu_doing_single_step) != -1 &&
     (kgdb_info[cpu].task &&
      kgdb_info[cpu].task->pid != kgdb_sstep_pid) && --sstep_tries) {
  atomic_set(&kgdb_active, -1);
  raw_spin_unlock(&dbg_master_lock);
  dbg_touch_watchdogs();
  local_irq_restore(flags);

  goto acquirelock;
 }

 if (!kgdb_io_ready(1)) {
  kgdb_info[cpu].ret_state = 1;
  goto kgdb_restore;
 }




 if (kgdb_skipexception(ks->ex_vector, ks->linux_regs))
  goto kgdb_restore;


 if (dbg_io_ops->pre_exception)
  dbg_io_ops->pre_exception();





 if (!kgdb_single_step)
  raw_spin_lock(&dbg_slave_lock);


 if (ks->send_ready)
  atomic_set(ks->send_ready, 1);


 else if ((!kgdb_single_step) && kgdb_do_roundup)
  kgdb_roundup_cpus(flags);




 time_left = loops_per_jiffy * HZ;
 while (kgdb_do_roundup && --time_left &&
        (atomic_read(&masters_in_kgdb) + atomic_read(&slaves_in_kgdb)) !=
     online_cpus)
  cpu_relax();
 if (!time_left)
  pr_crit("Timed out waiting for secondary CPUs.\n");





 dbg_deactivate_sw_breakpoints();
 kgdb_single_step = 0;
 kgdb_contthread = current;
 exception_level = 0;
 trace_on = tracing_is_on();
 if (trace_on)
  tracing_off();

 while (1) {
cpu_master_loop:
  if (dbg_kdb_mode) {
   kgdb_connected = 1;
   error = kdb_stub(ks);
   if (error == -1)
    continue;
   kgdb_connected = 0;
  } else {
   error = gdb_serial_stub(ks);
  }

  if (error == DBG_PASS_EVENT) {
   dbg_kdb_mode = !dbg_kdb_mode;
  } else if (error == DBG_SWITCH_CPU_EVENT) {
   kgdb_info[dbg_switch_cpu].exception_state |=
    DCPU_NEXT_MASTER;
   goto cpu_loop;
  } else {
   kgdb_info[cpu].ret_state = error;
   break;
  }
 }


 if (dbg_io_ops->post_exception)
  dbg_io_ops->post_exception();

 if (!kgdb_single_step) {
  raw_spin_unlock(&dbg_slave_lock);

  while (kgdb_do_roundup && atomic_read(&slaves_in_kgdb))
   cpu_relax();
 }

kgdb_restore:
 if (atomic_read(&kgdb_cpu_doing_single_step) != -1) {
  int sstep_cpu = atomic_read(&kgdb_cpu_doing_single_step);
  if (kgdb_info[sstep_cpu].task)
   kgdb_sstep_pid = kgdb_info[sstep_cpu].task->pid;
  else
   kgdb_sstep_pid = 0;
 }
 if (arch_kgdb_ops.correct_hw_break)
  arch_kgdb_ops.correct_hw_break();
 if (trace_on)
  tracing_on();

 kgdb_info[cpu].exception_state &=
  ~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);
 kgdb_info[cpu].enter_kgdb--;
 smp_mb__before_atomic();
 atomic_dec(&masters_in_kgdb);

 atomic_set(&kgdb_active, -1);
 raw_spin_unlock(&dbg_master_lock);
 dbg_touch_watchdogs();
 local_irq_restore(flags);

 return kgdb_info[cpu].ret_state;
}
int
kgdb_handle_exception(int evector, int signo, int ecode, struct pt_regs *regs)
{
 struct kgdb_state kgdb_var;
 struct kgdb_state *ks = &kgdb_var;
 int ret = 0;

 if (arch_kgdb_ops.enable_nmi)
  arch_kgdb_ops.enable_nmi(0);






 if (signo != SIGTRAP && panic_timeout)
  return 1;

 memset(ks, 0, sizeof(struct kgdb_state));
 ks->cpu = raw_smp_processor_id();
 ks->ex_vector = evector;
 ks->signo = signo;
 ks->err_code = ecode;
 ks->linux_regs = regs;

 if (kgdb_reenter_check(ks))
  goto out;
 if (kgdb_info[ks->cpu].enter_kgdb != 0)
  goto out;

 ret = kgdb_cpu_enter(ks, regs, DCPU_WANT_MASTER);
out:
 if (arch_kgdb_ops.enable_nmi)
  arch_kgdb_ops.enable_nmi(1);
 return ret;
}







static int module_event(struct notifier_block *self, unsigned long val,
 void *data)
{
 return 0;
}

static struct notifier_block dbg_module_load_nb = {
 .notifier_call = module_event,
};

int kgdb_nmicallback(int cpu, void *regs)
{
 struct kgdb_state kgdb_var;
 struct kgdb_state *ks = &kgdb_var;

 memset(ks, 0, sizeof(struct kgdb_state));
 ks->cpu = cpu;
 ks->linux_regs = regs;

 if (kgdb_info[ks->cpu].enter_kgdb == 0 &&
   raw_spin_is_locked(&dbg_master_lock)) {
  kgdb_cpu_enter(ks, regs, DCPU_IS_SLAVE);
  return 0;
 }
 return 1;
}

int kgdb_nmicallin(int cpu, int trapnr, void *regs, int err_code,
       atomic_t *send_ready)
{
 if (!kgdb_io_ready(0) || !send_ready)
  return 1;

 if (kgdb_info[cpu].enter_kgdb == 0) {
  struct kgdb_state kgdb_var;
  struct kgdb_state *ks = &kgdb_var;

  memset(ks, 0, sizeof(struct kgdb_state));
  ks->cpu = cpu;
  ks->ex_vector = trapnr;
  ks->signo = SIGTRAP;
  ks->err_code = err_code;
  ks->linux_regs = regs;
  ks->send_ready = send_ready;
  kgdb_cpu_enter(ks, regs, DCPU_WANT_MASTER);
  return 0;
 }
 return 1;
}

static void kgdb_console_write(struct console *co, const char *s,
   unsigned count)
{
 unsigned long flags;



 if (!kgdb_connected || atomic_read(&kgdb_active) != -1 || dbg_kdb_mode)
  return;

 local_irq_save(flags);
 gdbstub_msg_write(s, count);
 local_irq_restore(flags);
}

static struct console kgdbcons = {
 .name = "kgdb",
 .write = kgdb_console_write,
 .flags = CON_PRINTBUFFER | CON_ENABLED,
 .index = -1,
};

static void sysrq_handle_dbg(int key)
{
 if (!dbg_io_ops) {
  pr_crit("ERROR: No KGDB I/O module available\n");
  return;
 }
 if (!kgdb_connected) {
  if (!dbg_kdb_mode)
   pr_crit("KGDB or $3#33 for KDB\n");
  pr_crit("Entering KGDB\n");
 }

 kgdb_breakpoint();
}

static struct sysrq_key_op sysrq_dbg_op = {
 .handler = sysrq_handle_dbg,
 .help_msg = "debug(g)",
 .action_msg = "DEBUG",
};

static int kgdb_panic_event(struct notifier_block *self,
       unsigned long val,
       void *data)
{






 if (panic_timeout)
  return NOTIFY_DONE;

 if (dbg_kdb_mode)
  kdb_printf("PANIC: %s\n", (char *)data);
 kgdb_breakpoint();
 return NOTIFY_DONE;
}

static struct notifier_block kgdb_panic_event_nb = {
       .notifier_call = kgdb_panic_event,
       .priority = INT_MAX,
};

void __weak kgdb_arch_late(void)
{
}

void __init dbg_late_init(void)
{
 dbg_is_early = false;
 if (kgdb_io_module_registered)
  kgdb_arch_late();
 kdb_init(KDB_INIT_FULL);
}

static int
dbg_notify_reboot(struct notifier_block *this, unsigned long code, void *x)
{






 switch (kgdbreboot) {
 case 1:
  kgdb_breakpoint();
 case -1:
  goto done;
 }
 if (!dbg_kdb_mode)
  gdbstub_exit(code);
done:
 return NOTIFY_DONE;
}

static struct notifier_block dbg_reboot_notifier = {
 .notifier_call = dbg_notify_reboot,
 .next = NULL,
 .priority = INT_MAX,
};

static void kgdb_register_callbacks(void)
{
 if (!kgdb_io_module_registered) {
  kgdb_io_module_registered = 1;
  kgdb_arch_init();
  if (!dbg_is_early)
   kgdb_arch_late();
  register_module_notifier(&dbg_module_load_nb);
  register_reboot_notifier(&dbg_reboot_notifier);
  atomic_notifier_chain_register(&panic_notifier_list,
            &kgdb_panic_event_nb);
  register_sysrq_key('g', &sysrq_dbg_op);
  if (kgdb_use_con && !kgdb_con_registered) {
   register_console(&kgdbcons);
   kgdb_con_registered = 1;
  }
 }
}

static void kgdb_unregister_callbacks(void)
{





 if (kgdb_io_module_registered) {
  kgdb_io_module_registered = 0;
  unregister_reboot_notifier(&dbg_reboot_notifier);
  unregister_module_notifier(&dbg_module_load_nb);
  atomic_notifier_chain_unregister(&panic_notifier_list,
            &kgdb_panic_event_nb);
  kgdb_arch_exit();
  unregister_sysrq_key('g', &sysrq_dbg_op);
  if (kgdb_con_registered) {
   unregister_console(&kgdbcons);
   kgdb_con_registered = 0;
  }
 }
}







static void kgdb_tasklet_bpt(unsigned long ing)
{
 kgdb_breakpoint();
 atomic_set(&kgdb_break_tasklet_var, 0);
}

static DECLARE_TASKLET(kgdb_tasklet_breakpoint, kgdb_tasklet_bpt, 0);

void kgdb_schedule_breakpoint(void)
{
 if (atomic_read(&kgdb_break_tasklet_var) ||
  atomic_read(&kgdb_active) != -1 ||
  atomic_read(&kgdb_setting_breakpoint))
  return;
 atomic_inc(&kgdb_break_tasklet_var);
 tasklet_schedule(&kgdb_tasklet_breakpoint);
}
EXPORT_SYMBOL_GPL(kgdb_schedule_breakpoint);

static void kgdb_initial_breakpoint(void)
{
 kgdb_break_asap = 0;

 pr_crit("Waiting for connection from remote gdb...\n");
 kgdb_breakpoint();
}







int kgdb_register_io_module(struct kgdb_io *new_dbg_io_ops)
{
 int err;

 spin_lock(&kgdb_registration_lock);

 if (dbg_io_ops) {
  spin_unlock(&kgdb_registration_lock);

  pr_err("Another I/O driver is already registered with KGDB\n");
  return -EBUSY;
 }

 if (new_dbg_io_ops->init) {
  err = new_dbg_io_ops->init();
  if (err) {
   spin_unlock(&kgdb_registration_lock);
   return err;
  }
 }

 dbg_io_ops = new_dbg_io_ops;

 spin_unlock(&kgdb_registration_lock);

 pr_info("Registered I/O driver %s\n", new_dbg_io_ops->name);


 kgdb_register_callbacks();

 if (kgdb_break_asap)
  kgdb_initial_breakpoint();

 return 0;
}
EXPORT_SYMBOL_GPL(kgdb_register_io_module);







void kgdb_unregister_io_module(struct kgdb_io *old_dbg_io_ops)
{
 BUG_ON(kgdb_connected);





 kgdb_unregister_callbacks();

 spin_lock(&kgdb_registration_lock);

 WARN_ON_ONCE(dbg_io_ops != old_dbg_io_ops);
 dbg_io_ops = NULL;

 spin_unlock(&kgdb_registration_lock);

 pr_info("Unregistered I/O driver %s, debugger disabled\n",
  old_dbg_io_ops->name);
}
EXPORT_SYMBOL_GPL(kgdb_unregister_io_module);

int dbg_io_get_char(void)
{
 int ret = dbg_io_ops->read_char();
 if (ret == NO_POLL_CHAR)
  return -1;
 if (!dbg_kdb_mode)
  return ret;
 if (ret == 127)
  return 8;
 return ret;
}
noinline void kgdb_breakpoint(void)
{
 atomic_inc(&kgdb_setting_breakpoint);
 wmb();
 arch_kgdb_breakpoint();
 wmb();
 atomic_dec(&kgdb_setting_breakpoint);
}
EXPORT_SYMBOL_GPL(kgdb_breakpoint);

static int __init opt_kgdb_wait(char *str)
{
 kgdb_break_asap = 1;

 kdb_init(KDB_INIT_EARLY);
 if (kgdb_io_module_registered)
  kgdb_initial_breakpoint();

 return 0;
}

early_param("kgdbwait", opt_kgdb_wait);

int delayacct_on __read_mostly = 1;
EXPORT_SYMBOL_GPL(delayacct_on);
struct kmem_cache *delayacct_cache;

static int __init delayacct_setup_disable(char *str)
{
 delayacct_on = 0;
 return 1;
}
__setup("nodelayacct", delayacct_setup_disable);

void delayacct_init(void)
{
 delayacct_cache = KMEM_CACHE(task_delay_info, SLAB_PANIC|SLAB_ACCOUNT);
 delayacct_tsk_init(&init_task);
}

void __delayacct_tsk_init(struct task_struct *tsk)
{
 tsk->delays = kmem_cache_zalloc(delayacct_cache, GFP_KERNEL);
 if (tsk->delays)
  spin_lock_init(&tsk->delays->lock);
}





static void delayacct_end(u64 *start, u64 *total, u32 *count)
{
 s64 ns = ktime_get_ns() - *start;
 unsigned long flags;

 if (ns > 0) {
  spin_lock_irqsave(&current->delays->lock, flags);
  *total += ns;
  (*count)++;
  spin_unlock_irqrestore(&current->delays->lock, flags);
 }
}

void __delayacct_blkio_start(void)
{
 current->delays->blkio_start = ktime_get_ns();
}

void __delayacct_blkio_end(void)
{
 if (current->delays->flags & DELAYACCT_PF_SWAPIN)

  delayacct_end(&current->delays->blkio_start,
   &current->delays->swapin_delay,
   &current->delays->swapin_count);
 else
  delayacct_end(&current->delays->blkio_start,
   &current->delays->blkio_delay,
   &current->delays->blkio_count);
}

int __delayacct_add_tsk(struct taskstats *d, struct task_struct *tsk)
{
 cputime_t utime, stime, stimescaled, utimescaled;
 unsigned long long t2, t3;
 unsigned long flags, t1;
 s64 tmp;

 task_cputime(tsk, &utime, &stime);
 tmp = (s64)d->cpu_run_real_total;
 tmp += cputime_to_nsecs(utime + stime);
 d->cpu_run_real_total = (tmp < (s64)d->cpu_run_real_total) ? 0 : tmp;

 task_cputime_scaled(tsk, &utimescaled, &stimescaled);
 tmp = (s64)d->cpu_scaled_run_real_total;
 tmp += cputime_to_nsecs(utimescaled + stimescaled);
 d->cpu_scaled_run_real_total =
  (tmp < (s64)d->cpu_scaled_run_real_total) ? 0 : tmp;





 t1 = tsk->sched_info.pcount;
 t2 = tsk->sched_info.run_delay;
 t3 = tsk->se.sum_exec_runtime;

 d->cpu_count += t1;

 tmp = (s64)d->cpu_delay_total + t2;
 d->cpu_delay_total = (tmp < (s64)d->cpu_delay_total) ? 0 : tmp;

 tmp = (s64)d->cpu_run_virtual_total + t3;
 d->cpu_run_virtual_total =
  (tmp < (s64)d->cpu_run_virtual_total) ? 0 : tmp;



 spin_lock_irqsave(&tsk->delays->lock, flags);
 tmp = d->blkio_delay_total + tsk->delays->blkio_delay;
 d->blkio_delay_total = (tmp < d->blkio_delay_total) ? 0 : tmp;
 tmp = d->swapin_delay_total + tsk->delays->swapin_delay;
 d->swapin_delay_total = (tmp < d->swapin_delay_total) ? 0 : tmp;
 tmp = d->freepages_delay_total + tsk->delays->freepages_delay;
 d->freepages_delay_total = (tmp < d->freepages_delay_total) ? 0 : tmp;
 d->blkio_count += tsk->delays->blkio_count;
 d->swapin_count += tsk->delays->swapin_count;
 d->freepages_count += tsk->delays->freepages_count;
 spin_unlock_irqrestore(&tsk->delays->lock, flags);

 return 0;
}

__u64 __delayacct_blkio_ticks(struct task_struct *tsk)
{
 __u64 ret;
 unsigned long flags;

 spin_lock_irqsave(&tsk->delays->lock, flags);
 ret = nsec_to_clock_t(tsk->delays->blkio_delay +
    tsk->delays->swapin_delay);
 spin_unlock_irqrestore(&tsk->delays->lock, flags);
 return ret;
}

void __delayacct_freepages_start(void)
{
 current->delays->freepages_start = ktime_get_ns();
}

void __delayacct_freepages_end(void)
{
 delayacct_end(&current->delays->freepages_start,
   &current->delays->freepages_delay,
   &current->delays->freepages_count);
}





struct irq_devres {
 unsigned int irq;
 void *dev_id;
};

static void devm_irq_release(struct device *dev, void *res)
{
 struct irq_devres *this = res;

 free_irq(this->irq, this->dev_id);
}

static int devm_irq_match(struct device *dev, void *res, void *data)
{
 struct irq_devres *this = res, *match = data;

 return this->irq == match->irq && this->dev_id == match->dev_id;
}
int devm_request_threaded_irq(struct device *dev, unsigned int irq,
         irq_handler_t handler, irq_handler_t thread_fn,
         unsigned long irqflags, const char *devname,
         void *dev_id)
{
 struct irq_devres *dr;
 int rc;

 dr = devres_alloc(devm_irq_release, sizeof(struct irq_devres),
     GFP_KERNEL);
 if (!dr)
  return -ENOMEM;

 rc = request_threaded_irq(irq, handler, thread_fn, irqflags, devname,
      dev_id);
 if (rc) {
  devres_free(dr);
  return rc;
 }

 dr->irq = irq;
 dr->dev_id = dev_id;
 devres_add(dev, dr);

 return 0;
}
EXPORT_SYMBOL(devm_request_threaded_irq);
int devm_request_any_context_irq(struct device *dev, unsigned int irq,
         irq_handler_t handler, unsigned long irqflags,
         const char *devname, void *dev_id)
{
 struct irq_devres *dr;
 int rc;

 dr = devres_alloc(devm_irq_release, sizeof(struct irq_devres),
     GFP_KERNEL);
 if (!dr)
  return -ENOMEM;

 rc = request_any_context_irq(irq, handler, irqflags, devname, dev_id);
 if (rc < 0) {
  devres_free(dr);
  return rc;
 }

 dr->irq = irq;
 dr->dev_id = dev_id;
 devres_add(dev, dr);

 return rc;
}
EXPORT_SYMBOL(devm_request_any_context_irq);
void devm_free_irq(struct device *dev, unsigned int irq, void *dev_id)
{
 struct irq_devres match_data = { irq, dev_id };

 WARN_ON(devres_destroy(dev, devm_irq_release, devm_irq_match,
          &match_data));
 free_irq(irq, dev_id);
}
EXPORT_SYMBOL(devm_free_irq);
DEFINE_SPINLOCK(dma_spin_lock);












struct dma_chan {
 int lock;
 const char *device_id;
};

static struct dma_chan dma_chan_busy[MAX_DMA_CHANNELS] = {
 [4] = { 1, "cascade" },
};







int request_dma(unsigned int dmanr, const char * device_id)
{
 if (dmanr >= MAX_DMA_CHANNELS)
  return -EINVAL;

 if (xchg(&dma_chan_busy[dmanr].lock, 1) != 0)
  return -EBUSY;

 dma_chan_busy[dmanr].device_id = device_id;


 return 0;
}





void free_dma(unsigned int dmanr)
{
 if (dmanr >= MAX_DMA_CHANNELS) {
  printk(KERN_WARNING "Trying to free DMA%d\n", dmanr);
  return;
 }

 if (xchg(&dma_chan_busy[dmanr].lock, 0) == 0) {
  printk(KERN_WARNING "Trying to free free DMA%d\n", dmanr);
  return;
 }

}


int request_dma(unsigned int dmanr, const char *device_id)
{
 return -EINVAL;
}

void free_dma(unsigned int dmanr)
{
}



static int proc_dma_show(struct seq_file *m, void *v)
{
 int i;

 for (i = 0 ; i < MAX_DMA_CHANNELS ; i++) {
  if (dma_chan_busy[i].lock) {
   seq_printf(m, "%2d: %s\n", i,
       dma_chan_busy[i].device_id);
  }
 }
 return 0;
}
static int proc_dma_show(struct seq_file *m, void *v)
{
 seq_puts(m, "No DMA\n");
 return 0;
}

static int proc_dma_open(struct inode *inode, struct file *file)
{
 return single_open(file, proc_dma_show, NULL);
}

static const struct file_operations proc_dma_operations = {
 .open = proc_dma_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
};

static int __init proc_dma_init(void)
{
 proc_create("dma", 0, NULL, &proc_dma_operations);
 return 0;
}

__initcall(proc_dma_init);

EXPORT_SYMBOL(request_dma);
EXPORT_SYMBOL(free_dma);
EXPORT_SYMBOL(dma_spin_lock);












static void ack_bad(struct irq_data *data)
{
 struct irq_desc *desc = irq_data_to_desc(data);

 print_irq_desc(data->irq, desc);
 ack_bad_irq(data->irq);
}




static void noop(struct irq_data *data) { }

static unsigned int noop_ret(struct irq_data *data)
{
 return 0;
}




struct irq_chip no_irq_chip = {
 .name = "none",
 .irq_startup = noop_ret,
 .irq_shutdown = noop,
 .irq_enable = noop,
 .irq_disable = noop,
 .irq_ack = ack_bad,
 .flags = IRQCHIP_SKIP_SET_WAKE,
};





struct irq_chip dummy_irq_chip = {
 .name = "dummy",
 .irq_startup = noop_ret,
 .irq_shutdown = noop,
 .irq_enable = noop,
 .irq_disable = noop,
 .irq_ack = noop,
 .irq_mask = noop,
 .irq_unmask = noop,
 .flags = IRQCHIP_SKIP_SET_WAKE,
};
EXPORT_SYMBOL_GPL(dummy_irq_chip);

Elf_Half __weak elf_core_extra_phdrs(void)
{
 return 0;
}

int __weak elf_core_write_extra_phdrs(struct coredump_params *cprm, loff_t offset)
{
 return 1;
}

int __weak elf_core_write_extra_data(struct coredump_params *cprm)
{
 return 1;
}

size_t __weak elf_core_extra_data_size(void)
{
 return 0;
}

static int execdomains_proc_show(struct seq_file *m, void *v)
{
 seq_puts(m, "0-0\tLinux           \t[kernel]\n");
 return 0;
}

static int execdomains_proc_open(struct inode *inode, struct file *file)
{
 return single_open(file, execdomains_proc_show, NULL);
}

static const struct file_operations execdomains_proc_fops = {
 .open = execdomains_proc_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
};

static int __init proc_execdomains_init(void)
{
 proc_create("execdomains", 0, NULL, &execdomains_proc_fops);
 return 0;
}
module_init(proc_execdomains_init);

SYSCALL_DEFINE1(personality, unsigned int, personality)
{
 unsigned int old = current->personality;

 if (personality != 0xffffffff)
  set_personality(personality);

 return old;
}








static void __unhash_process(struct task_struct *p, bool group_dead)
{
 nr_threads--;
 detach_pid(p, PIDTYPE_PID);
 if (group_dead) {
  detach_pid(p, PIDTYPE_PGID);
  detach_pid(p, PIDTYPE_SID);

  list_del_rcu(&p->tasks);
  list_del_init(&p->sibling);
  __this_cpu_dec(process_counts);
 }
 list_del_rcu(&p->thread_group);
 list_del_rcu(&p->thread_node);
}




static void __exit_signal(struct task_struct *tsk)
{
 struct signal_struct *sig = tsk->signal;
 bool group_dead = thread_group_leader(tsk);
 struct sighand_struct *sighand;
 struct tty_struct *uninitialized_var(tty);
 cputime_t utime, stime;

 sighand = rcu_dereference_check(tsk->sighand,
     lockdep_tasklist_lock_is_held());
 spin_lock(&sighand->siglock);

 posix_cpu_timers_exit(tsk);
 if (group_dead) {
  posix_cpu_timers_exit_group(tsk);
  tty = sig->tty;
  sig->tty = NULL;
 } else {





  if (unlikely(has_group_leader_pid(tsk)))
   posix_cpu_timers_exit_group(tsk);





  if (sig->notify_count > 0 && !--sig->notify_count)
   wake_up_process(sig->group_exit_task);

  if (tsk == sig->curr_target)
   sig->curr_target = next_thread(tsk);
 }







 task_cputime(tsk, &utime, &stime);
 write_seqlock(&sig->stats_lock);
 sig->utime += utime;
 sig->stime += stime;
 sig->gtime += task_gtime(tsk);
 sig->min_flt += tsk->min_flt;
 sig->maj_flt += tsk->maj_flt;
 sig->nvcsw += tsk->nvcsw;
 sig->nivcsw += tsk->nivcsw;
 sig->inblock += task_io_get_inblock(tsk);
 sig->oublock += task_io_get_oublock(tsk);
 task_io_accounting_add(&sig->ioac, &tsk->ioac);
 sig->sum_sched_runtime += tsk->se.sum_exec_runtime;
 sig->nr_threads--;
 __unhash_process(tsk, group_dead);
 write_sequnlock(&sig->stats_lock);





 flush_sigqueue(&tsk->pending);
 tsk->sighand = NULL;
 spin_unlock(&sighand->siglock);

 __cleanup_sighand(sighand);
 clear_tsk_thread_flag(tsk, TIF_SIGPENDING);
 if (group_dead) {
  flush_sigqueue(&sig->shared_pending);
  tty_kref_put(tty);
 }
}

static void delayed_put_task_struct(struct rcu_head *rhp)
{
 struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);

 perf_event_delayed_put(tsk);
 trace_sched_process_free(tsk);
 put_task_struct(tsk);
}


void release_task(struct task_struct *p)
{
 struct task_struct *leader;
 int zap_leader;
repeat:


 rcu_read_lock();
 atomic_dec(&__task_cred(p)->user->processes);
 rcu_read_unlock();

 proc_flush_task(p);

 write_lock_irq(&tasklist_lock);
 ptrace_release_task(p);
 __exit_signal(p);






 zap_leader = 0;
 leader = p->group_leader;
 if (leader != p && thread_group_empty(leader)
   && leader->exit_state == EXIT_ZOMBIE) {





  zap_leader = do_notify_parent(leader, leader->exit_signal);
  if (zap_leader)
   leader->exit_state = EXIT_DEAD;
 }

 write_unlock_irq(&tasklist_lock);
 release_thread(p);
 call_rcu(&p->rcu, delayed_put_task_struct);

 p = leader;
 if (unlikely(zap_leader))
  goto repeat;
}
static int will_become_orphaned_pgrp(struct pid *pgrp,
     struct task_struct *ignored_task)
{
 struct task_struct *p;

 do_each_pid_task(pgrp, PIDTYPE_PGID, p) {
  if ((p == ignored_task) ||
      (p->exit_state && thread_group_empty(p)) ||
      is_global_init(p->real_parent))
   continue;

  if (task_pgrp(p->real_parent) != pgrp &&
      task_session(p->real_parent) == task_session(p))
   return 0;
 } while_each_pid_task(pgrp, PIDTYPE_PGID, p);

 return 1;
}

int is_current_pgrp_orphaned(void)
{
 int retval;

 read_lock(&tasklist_lock);
 retval = will_become_orphaned_pgrp(task_pgrp(current), NULL);
 read_unlock(&tasklist_lock);

 return retval;
}

static bool has_stopped_jobs(struct pid *pgrp)
{
 struct task_struct *p;

 do_each_pid_task(pgrp, PIDTYPE_PGID, p) {
  if (p->signal->flags & SIGNAL_STOP_STOPPED)
   return true;
 } while_each_pid_task(pgrp, PIDTYPE_PGID, p);

 return false;
}






static void
kill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)
{
 struct pid *pgrp = task_pgrp(tsk);
 struct task_struct *ignored_task = tsk;

 if (!parent)



  parent = tsk->real_parent;
 else



  ignored_task = NULL;

 if (task_pgrp(parent) != pgrp &&
     task_session(parent) == task_session(tsk) &&
     will_become_orphaned_pgrp(pgrp, ignored_task) &&
     has_stopped_jobs(pgrp)) {
  __kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);
  __kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);
 }
}




void mm_update_next_owner(struct mm_struct *mm)
{
 struct task_struct *c, *g, *p = current;

retry:




 if (mm->owner != p)
  return;





 if (atomic_read(&mm->mm_users) <= 1) {
  mm->owner = NULL;
  return;
 }

 read_lock(&tasklist_lock);



 list_for_each_entry(c, &p->children, sibling) {
  if (c->mm == mm)
   goto assign_new_owner;
 }




 list_for_each_entry(c, &p->real_parent->children, sibling) {
  if (c->mm == mm)
   goto assign_new_owner;
 }




 for_each_process(g) {
  if (g->flags & PF_KTHREAD)
   continue;
  for_each_thread(g, c) {
   if (c->mm == mm)
    goto assign_new_owner;
   if (c->mm)
    break;
  }
 }
 read_unlock(&tasklist_lock);





 mm->owner = NULL;
 return;

assign_new_owner:
 BUG_ON(c == p);
 get_task_struct(c);




 task_lock(c);




 read_unlock(&tasklist_lock);
 if (c->mm != mm) {
  task_unlock(c);
  put_task_struct(c);
  goto retry;
 }
 mm->owner = c;
 task_unlock(c);
 put_task_struct(c);
}





static void exit_mm(struct task_struct *tsk)
{
 struct mm_struct *mm = tsk->mm;
 struct core_state *core_state;

 mm_release(tsk, mm);
 if (!mm)
  return;
 sync_mm_rss(mm);







 down_read(&mm->mmap_sem);
 core_state = mm->core_state;
 if (core_state) {
  struct core_thread self;

  up_read(&mm->mmap_sem);

  self.task = tsk;
  self.next = xchg(&core_state->dumper.next, &self);




  if (atomic_dec_and_test(&core_state->nr_threads))
   complete(&core_state->startup);

  for (;;) {
   set_task_state(tsk, TASK_UNINTERRUPTIBLE);
   if (!self.task)
    break;
   freezable_schedule();
  }
  __set_task_state(tsk, TASK_RUNNING);
  down_read(&mm->mmap_sem);
 }
 atomic_inc(&mm->mm_count);
 BUG_ON(mm != tsk->active_mm);

 task_lock(tsk);
 tsk->mm = NULL;
 up_read(&mm->mmap_sem);
 enter_lazy_tlb(mm, current);
 task_unlock(tsk);
 mm_update_next_owner(mm);
 mmput(mm);
 if (test_thread_flag(TIF_MEMDIE))
  exit_oom_victim(tsk);
}

static struct task_struct *find_alive_thread(struct task_struct *p)
{
 struct task_struct *t;

 for_each_thread(p, t) {
  if (!(t->flags & PF_EXITING))
   return t;
 }
 return NULL;
}

static struct task_struct *find_child_reaper(struct task_struct *father)
 __releases(&tasklist_lock)
 __acquires(&tasklist_lock)
{
 struct pid_namespace *pid_ns = task_active_pid_ns(father);
 struct task_struct *reaper = pid_ns->child_reaper;

 if (likely(reaper != father))
  return reaper;

 reaper = find_alive_thread(father);
 if (reaper) {
  pid_ns->child_reaper = reaper;
  return reaper;
 }

 write_unlock_irq(&tasklist_lock);
 if (unlikely(pid_ns == &init_pid_ns)) {
  panic("Attempted to kill init! exitcode=0x%08x\n",
   father->signal->group_exit_code ?: father->exit_code);
 }
 zap_pid_ns_processes(pid_ns);
 write_lock_irq(&tasklist_lock);

 return father;
}
static struct task_struct *find_new_reaper(struct task_struct *father,
        struct task_struct *child_reaper)
{
 struct task_struct *thread, *reaper;

 thread = find_alive_thread(father);
 if (thread)
  return thread;

 if (father->signal->has_child_subreaper) {





  for (reaper = father;
       !same_thread_group(reaper, child_reaper);
       reaper = reaper->real_parent) {

   if (reaper == &init_task)
    break;
   if (!reaper->signal->is_child_subreaper)
    continue;
   thread = find_alive_thread(reaper);
   if (thread)
    return thread;
  }
 }

 return child_reaper;
}




static void reparent_leader(struct task_struct *father, struct task_struct *p,
    struct list_head *dead)
{
 if (unlikely(p->exit_state == EXIT_DEAD))
  return;


 p->exit_signal = SIGCHLD;


 if (!p->ptrace &&
     p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {
  if (do_notify_parent(p, p->exit_signal)) {
   p->exit_state = EXIT_DEAD;
   list_add(&p->ptrace_entry, dead);
  }
 }

 kill_orphaned_pgrp(p, father);
}
static void forget_original_parent(struct task_struct *father,
     struct list_head *dead)
{
 struct task_struct *p, *t, *reaper;

 if (unlikely(!list_empty(&father->ptraced)))
  exit_ptrace(father, dead);


 reaper = find_child_reaper(father);
 if (list_empty(&father->children))
  return;

 reaper = find_new_reaper(father, reaper);
 list_for_each_entry(p, &father->children, sibling) {
  for_each_thread(p, t) {
   t->real_parent = reaper;
   BUG_ON((!t->ptrace) != (t->parent == father));
   if (likely(!t->ptrace))
    t->parent = t->real_parent;
   if (t->pdeath_signal)
    group_send_sig_info(t->pdeath_signal,
          SEND_SIG_NOINFO, t);
  }




  if (!same_thread_group(reaper, father))
   reparent_leader(father, p, dead);
 }
 list_splice_tail_init(&father->children, &reaper->children);
}





static void exit_notify(struct task_struct *tsk, int group_dead)
{
 bool autoreap;
 struct task_struct *p, *n;
 LIST_HEAD(dead);

 write_lock_irq(&tasklist_lock);
 forget_original_parent(tsk, &dead);

 if (group_dead)
  kill_orphaned_pgrp(tsk->group_leader, NULL);

 if (unlikely(tsk->ptrace)) {
  int sig = thread_group_leader(tsk) &&
    thread_group_empty(tsk) &&
    !ptrace_reparented(tsk) ?
   tsk->exit_signal : SIGCHLD;
  autoreap = do_notify_parent(tsk, sig);
 } else if (thread_group_leader(tsk)) {
  autoreap = thread_group_empty(tsk) &&
   do_notify_parent(tsk, tsk->exit_signal);
 } else {
  autoreap = true;
 }

 tsk->exit_state = autoreap ? EXIT_DEAD : EXIT_ZOMBIE;
 if (tsk->exit_state == EXIT_DEAD)
  list_add(&tsk->ptrace_entry, &dead);


 if (unlikely(tsk->signal->notify_count < 0))
  wake_up_process(tsk->signal->group_exit_task);
 write_unlock_irq(&tasklist_lock);

 list_for_each_entry_safe(p, n, &dead, ptrace_entry) {
  list_del_init(&p->ptrace_entry);
  release_task(p);
 }
}

static void check_stack_usage(void)
{
 static DEFINE_SPINLOCK(low_water_lock);
 static int lowest_to_date = THREAD_SIZE;
 unsigned long free;

 free = stack_not_used(current);

 if (free >= lowest_to_date)
  return;

 spin_lock(&low_water_lock);
 if (free < lowest_to_date) {
  pr_warn("%s (%d) used greatest stack depth: %lu bytes left\n",
   current->comm, task_pid_nr(current), free);
  lowest_to_date = free;
 }
 spin_unlock(&low_water_lock);
}
static inline void check_stack_usage(void) {}

void do_exit(long code)
{
 struct task_struct *tsk = current;
 int group_dead;
 TASKS_RCU(int tasks_rcu_i);

 profile_task_exit(tsk);
 kcov_task_exit(tsk);

 WARN_ON(blk_needs_flush_plug(tsk));

 if (unlikely(in_interrupt()))
  panic("Aiee, killing interrupt handler!");
 if (unlikely(!tsk->pid))
  panic("Attempted to kill the idle task!");
 set_fs(USER_DS);

 ptrace_event(PTRACE_EVENT_EXIT, code);

 validate_creds_for_do_exit(tsk);





 if (unlikely(tsk->flags & PF_EXITING)) {
  pr_alert("Fixing recursive fault but reboot is needed!\n");
  tsk->flags |= PF_EXITPIDONE;
  set_current_state(TASK_UNINTERRUPTIBLE);
  schedule();
 }

 exit_signals(tsk);




 smp_mb();
 raw_spin_unlock_wait(&tsk->pi_lock);

 if (unlikely(in_atomic())) {
  pr_info("note: %s[%d] exited with preempt_count %d\n",
   current->comm, task_pid_nr(current),
   preempt_count());
  preempt_count_set(PREEMPT_ENABLED);
 }


 if (tsk->mm)
  sync_mm_rss(tsk->mm);
 acct_update_integrals(tsk);
 group_dead = atomic_dec_and_test(&tsk->signal->live);
 if (group_dead) {
  hrtimer_cancel(&tsk->signal->real_timer);
  exit_itimers(tsk->signal);
  if (tsk->mm)
   setmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);
 }
 acct_collect(code, group_dead);
 if (group_dead)
  tty_audit_exit();
 audit_free(tsk);

 tsk->exit_code = code;
 taskstats_exit(tsk, group_dead);

 exit_mm(tsk);

 if (group_dead)
  acct_process();
 trace_sched_process_exit(tsk);

 exit_sem(tsk);
 exit_shm(tsk);
 exit_files(tsk);
 exit_fs(tsk);
 if (group_dead)
  disassociate_ctty(1);
 exit_task_namespaces(tsk);
 exit_task_work(tsk);
 exit_thread(tsk);







 perf_event_exit_task(tsk);

 cgroup_exit(tsk);




 flush_ptrace_hw_breakpoint(tsk);

 TASKS_RCU(preempt_disable());
 TASKS_RCU(tasks_rcu_i = __srcu_read_lock(&tasks_rcu_exit_srcu));
 TASKS_RCU(preempt_enable());
 exit_notify(tsk, group_dead);
 proc_exit_connector(tsk);
 task_lock(tsk);
 mpol_put(tsk->mempolicy);
 tsk->mempolicy = NULL;
 task_unlock(tsk);
 if (unlikely(current->pi_state_cache))
  kfree(current->pi_state_cache);



 debug_check_no_locks_held();





 tsk->flags |= PF_EXITPIDONE;

 if (tsk->io_context)
  exit_io_context(tsk);

 if (tsk->splice_pipe)
  free_pipe_info(tsk->splice_pipe);

 if (tsk->task_frag.page)
  put_page(tsk->task_frag.page);

 validate_creds_for_do_exit(tsk);

 check_stack_usage();
 preempt_disable();
 if (tsk->nr_dirtied)
  __this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);
 exit_rcu();
 TASKS_RCU(__srcu_read_unlock(&tasks_rcu_exit_srcu, tasks_rcu_i));
 smp_mb();
 raw_spin_unlock_wait(&tsk->pi_lock);


 tsk->state = TASK_DEAD;
 tsk->flags |= PF_NOFREEZE;
 schedule();
 BUG();

 for (;;)
  cpu_relax();
}
EXPORT_SYMBOL_GPL(do_exit);

void complete_and_exit(struct completion *comp, long code)
{
 if (comp)
  complete(comp);

 do_exit(code);
}
EXPORT_SYMBOL(complete_and_exit);

SYSCALL_DEFINE1(exit, int, error_code)
{
 do_exit((error_code&0xff)<<8);
}





void
do_group_exit(int exit_code)
{
 struct signal_struct *sig = current->signal;

 BUG_ON(exit_code & 0x80);

 if (signal_group_exit(sig))
  exit_code = sig->group_exit_code;
 else if (!thread_group_empty(current)) {
  struct sighand_struct *const sighand = current->sighand;

  spin_lock_irq(&sighand->siglock);
  if (signal_group_exit(sig))

   exit_code = sig->group_exit_code;
  else {
   sig->group_exit_code = exit_code;
   sig->flags = SIGNAL_GROUP_EXIT;
   zap_other_threads(current);
  }
  spin_unlock_irq(&sighand->siglock);
 }

 do_exit(exit_code);

}






SYSCALL_DEFINE1(exit_group, int, error_code)
{
 do_group_exit((error_code & 0xff) << 8);

 return 0;
}

struct wait_opts {
 enum pid_type wo_type;
 int wo_flags;
 struct pid *wo_pid;

 struct siginfo __user *wo_info;
 int __user *wo_stat;
 struct rusage __user *wo_rusage;

 wait_queue_t child_wait;
 int notask_error;
};

static inline
struct pid *task_pid_type(struct task_struct *task, enum pid_type type)
{
 if (type != PIDTYPE_PID)
  task = task->group_leader;
 return task->pids[type].pid;
}

static int eligible_pid(struct wait_opts *wo, struct task_struct *p)
{
 return wo->wo_type == PIDTYPE_MAX ||
  task_pid_type(p, wo->wo_type) == wo->wo_pid;
}

static int
eligible_child(struct wait_opts *wo, bool ptrace, struct task_struct *p)
{
 if (!eligible_pid(wo, p))
  return 0;





 if (ptrace || (wo->wo_flags & __WALL))
  return 1;
 if ((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))
  return 0;

 return 1;
}

static int wait_noreap_copyout(struct wait_opts *wo, struct task_struct *p,
    pid_t pid, uid_t uid, int why, int status)
{
 struct siginfo __user *infop;
 int retval = wo->wo_rusage
  ? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;

 put_task_struct(p);
 infop = wo->wo_info;
 if (infop) {
  if (!retval)
   retval = put_user(SIGCHLD, &infop->si_signo);
  if (!retval)
   retval = put_user(0, &infop->si_errno);
  if (!retval)
   retval = put_user((short)why, &infop->si_code);
  if (!retval)
   retval = put_user(pid, &infop->si_pid);
  if (!retval)
   retval = put_user(uid, &infop->si_uid);
  if (!retval)
   retval = put_user(status, &infop->si_status);
 }
 if (!retval)
  retval = pid;
 return retval;
}







static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
{
 int state, retval, status;
 pid_t pid = task_pid_vnr(p);
 uid_t uid = from_kuid_munged(current_user_ns(), task_uid(p));
 struct siginfo __user *infop;

 if (!likely(wo->wo_flags & WEXITED))
  return 0;

 if (unlikely(wo->wo_flags & WNOWAIT)) {
  int exit_code = p->exit_code;
  int why;

  get_task_struct(p);
  read_unlock(&tasklist_lock);
  sched_annotate_sleep();

  if ((exit_code & 0x7f) == 0) {
   why = CLD_EXITED;
   status = exit_code >> 8;
  } else {
   why = (exit_code & 0x80) ? CLD_DUMPED : CLD_KILLED;
   status = exit_code & 0x7f;
  }
  return wait_noreap_copyout(wo, p, pid, uid, why, status);
 }



 state = (ptrace_reparented(p) && thread_group_leader(p)) ?
  EXIT_TRACE : EXIT_DEAD;
 if (cmpxchg(&p->exit_state, EXIT_ZOMBIE, state) != EXIT_ZOMBIE)
  return 0;



 read_unlock(&tasklist_lock);
 sched_annotate_sleep();




 if (state == EXIT_DEAD && thread_group_leader(p)) {
  struct signal_struct *sig = p->signal;
  struct signal_struct *psig = current->signal;
  unsigned long maxrss;
  cputime_t tgutime, tgstime;
  thread_group_cputime_adjusted(p, &tgutime, &tgstime);
  spin_lock_irq(&current->sighand->siglock);
  write_seqlock(&psig->stats_lock);
  psig->cutime += tgutime + sig->cutime;
  psig->cstime += tgstime + sig->cstime;
  psig->cgtime += task_gtime(p) + sig->gtime + sig->cgtime;
  psig->cmin_flt +=
   p->min_flt + sig->min_flt + sig->cmin_flt;
  psig->cmaj_flt +=
   p->maj_flt + sig->maj_flt + sig->cmaj_flt;
  psig->cnvcsw +=
   p->nvcsw + sig->nvcsw + sig->cnvcsw;
  psig->cnivcsw +=
   p->nivcsw + sig->nivcsw + sig->cnivcsw;
  psig->cinblock +=
   task_io_get_inblock(p) +
   sig->inblock + sig->cinblock;
  psig->coublock +=
   task_io_get_oublock(p) +
   sig->oublock + sig->coublock;
  maxrss = max(sig->maxrss, sig->cmaxrss);
  if (psig->cmaxrss < maxrss)
   psig->cmaxrss = maxrss;
  task_io_accounting_add(&psig->ioac, &p->ioac);
  task_io_accounting_add(&psig->ioac, &sig->ioac);
  write_sequnlock(&psig->stats_lock);
  spin_unlock_irq(&current->sighand->siglock);
 }

 retval = wo->wo_rusage
  ? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
 status = (p->signal->flags & SIGNAL_GROUP_EXIT)
  ? p->signal->group_exit_code : p->exit_code;
 if (!retval && wo->wo_stat)
  retval = put_user(status, wo->wo_stat);

 infop = wo->wo_info;
 if (!retval && infop)
  retval = put_user(SIGCHLD, &infop->si_signo);
 if (!retval && infop)
  retval = put_user(0, &infop->si_errno);
 if (!retval && infop) {
  int why;

  if ((status & 0x7f) == 0) {
   why = CLD_EXITED;
   status >>= 8;
  } else {
   why = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;
   status &= 0x7f;
  }
  retval = put_user((short)why, &infop->si_code);
  if (!retval)
   retval = put_user(status, &infop->si_status);
 }
 if (!retval && infop)
  retval = put_user(pid, &infop->si_pid);
 if (!retval && infop)
  retval = put_user(uid, &infop->si_uid);
 if (!retval)
  retval = pid;

 if (state == EXIT_TRACE) {
  write_lock_irq(&tasklist_lock);

  ptrace_unlink(p);


  state = EXIT_ZOMBIE;
  if (do_notify_parent(p, p->exit_signal))
   state = EXIT_DEAD;
  p->exit_state = state;
  write_unlock_irq(&tasklist_lock);
 }
 if (state == EXIT_DEAD)
  release_task(p);

 return retval;
}

static int *task_stopped_code(struct task_struct *p, bool ptrace)
{
 if (ptrace) {
  if (task_is_traced(p) && !(p->jobctl & JOBCTL_LISTENING))
   return &p->exit_code;
 } else {
  if (p->signal->flags & SIGNAL_STOP_STOPPED)
   return &p->signal->group_exit_code;
 }
 return NULL;
}
static int wait_task_stopped(struct wait_opts *wo,
    int ptrace, struct task_struct *p)
{
 struct siginfo __user *infop;
 int retval, exit_code, *p_code, why;
 uid_t uid = 0;
 pid_t pid;




 if (!ptrace && !(wo->wo_flags & WUNTRACED))
  return 0;

 if (!task_stopped_code(p, ptrace))
  return 0;

 exit_code = 0;
 spin_lock_irq(&p->sighand->siglock);

 p_code = task_stopped_code(p, ptrace);
 if (unlikely(!p_code))
  goto unlock_sig;

 exit_code = *p_code;
 if (!exit_code)
  goto unlock_sig;

 if (!unlikely(wo->wo_flags & WNOWAIT))
  *p_code = 0;

 uid = from_kuid_munged(current_user_ns(), task_uid(p));
unlock_sig:
 spin_unlock_irq(&p->sighand->siglock);
 if (!exit_code)
  return 0;
 get_task_struct(p);
 pid = task_pid_vnr(p);
 why = ptrace ? CLD_TRAPPED : CLD_STOPPED;
 read_unlock(&tasklist_lock);
 sched_annotate_sleep();

 if (unlikely(wo->wo_flags & WNOWAIT))
  return wait_noreap_copyout(wo, p, pid, uid, why, exit_code);

 retval = wo->wo_rusage
  ? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
 if (!retval && wo->wo_stat)
  retval = put_user((exit_code << 8) | 0x7f, wo->wo_stat);

 infop = wo->wo_info;
 if (!retval && infop)
  retval = put_user(SIGCHLD, &infop->si_signo);
 if (!retval && infop)
  retval = put_user(0, &infop->si_errno);
 if (!retval && infop)
  retval = put_user((short)why, &infop->si_code);
 if (!retval && infop)
  retval = put_user(exit_code, &infop->si_status);
 if (!retval && infop)
  retval = put_user(pid, &infop->si_pid);
 if (!retval && infop)
  retval = put_user(uid, &infop->si_uid);
 if (!retval)
  retval = pid;
 put_task_struct(p);

 BUG_ON(!retval);
 return retval;
}







static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
{
 int retval;
 pid_t pid;
 uid_t uid;

 if (!unlikely(wo->wo_flags & WCONTINUED))
  return 0;

 if (!(p->signal->flags & SIGNAL_STOP_CONTINUED))
  return 0;

 spin_lock_irq(&p->sighand->siglock);

 if (!(p->signal->flags & SIGNAL_STOP_CONTINUED)) {
  spin_unlock_irq(&p->sighand->siglock);
  return 0;
 }
 if (!unlikely(wo->wo_flags & WNOWAIT))
  p->signal->flags &= ~SIGNAL_STOP_CONTINUED;
 uid = from_kuid_munged(current_user_ns(), task_uid(p));
 spin_unlock_irq(&p->sighand->siglock);

 pid = task_pid_vnr(p);
 get_task_struct(p);
 read_unlock(&tasklist_lock);
 sched_annotate_sleep();

 if (!wo->wo_info) {
  retval = wo->wo_rusage
   ? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
  put_task_struct(p);
  if (!retval && wo->wo_stat)
   retval = put_user(0xffff, wo->wo_stat);
  if (!retval)
   retval = pid;
 } else {
  retval = wait_noreap_copyout(wo, p, pid, uid,
          CLD_CONTINUED, SIGCONT);
  BUG_ON(retval == 0);
 }

 return retval;
}
static int wait_consider_task(struct wait_opts *wo, int ptrace,
    struct task_struct *p)
{





 int exit_state = ACCESS_ONCE(p->exit_state);
 int ret;

 if (unlikely(exit_state == EXIT_DEAD))
  return 0;

 ret = eligible_child(wo, ptrace, p);
 if (!ret)
  return ret;

 ret = security_task_wait(p);
 if (unlikely(ret < 0)) {







  if (wo->notask_error)
   wo->notask_error = ret;
  return 0;
 }

 if (unlikely(exit_state == EXIT_TRACE)) {




  if (likely(!ptrace))
   wo->notask_error = 0;
  return 0;
 }

 if (likely(!ptrace) && unlikely(p->ptrace)) {
  if (!ptrace_reparented(p))
   ptrace = 1;
 }


 if (exit_state == EXIT_ZOMBIE) {

  if (!delay_group_leader(p)) {





   if (unlikely(ptrace) || likely(!p->ptrace))
    return wait_task_zombie(wo, p);
  }
  if (likely(!ptrace) || (wo->wo_flags & (WCONTINUED | WEXITED)))
   wo->notask_error = 0;
 } else {




  wo->notask_error = 0;
 }





 ret = wait_task_stopped(wo, ptrace, p);
 if (ret)
  return ret;






 return wait_task_continued(wo, p);
}
static int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)
{
 struct task_struct *p;

 list_for_each_entry(p, &tsk->children, sibling) {
  int ret = wait_consider_task(wo, 0, p);

  if (ret)
   return ret;
 }

 return 0;
}

static int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)
{
 struct task_struct *p;

 list_for_each_entry(p, &tsk->ptraced, ptrace_entry) {
  int ret = wait_consider_task(wo, 1, p);

  if (ret)
   return ret;
 }

 return 0;
}

static int child_wait_callback(wait_queue_t *wait, unsigned mode,
    int sync, void *key)
{
 struct wait_opts *wo = container_of(wait, struct wait_opts,
      child_wait);
 struct task_struct *p = key;

 if (!eligible_pid(wo, p))
  return 0;

 if ((wo->wo_flags & __WNOTHREAD) && wait->private != p->parent)
  return 0;

 return default_wake_function(wait, mode, sync, key);
}

void __wake_up_parent(struct task_struct *p, struct task_struct *parent)
{
 __wake_up_sync_key(&parent->signal->wait_chldexit,
    TASK_INTERRUPTIBLE, 1, p);
}

static long do_wait(struct wait_opts *wo)
{
 struct task_struct *tsk;
 int retval;

 trace_sched_process_wait(wo->wo_pid);

 init_waitqueue_func_entry(&wo->child_wait, child_wait_callback);
 wo->child_wait.private = current;
 add_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);
repeat:






 wo->notask_error = -ECHILD;
 if ((wo->wo_type < PIDTYPE_MAX) &&
    (!wo->wo_pid || hlist_empty(&wo->wo_pid->tasks[wo->wo_type])))
  goto notask;

 set_current_state(TASK_INTERRUPTIBLE);
 read_lock(&tasklist_lock);
 tsk = current;
 do {
  retval = do_wait_thread(wo, tsk);
  if (retval)
   goto end;

  retval = ptrace_do_wait(wo, tsk);
  if (retval)
   goto end;

  if (wo->wo_flags & __WNOTHREAD)
   break;
 } while_each_thread(current, tsk);
 read_unlock(&tasklist_lock);

notask:
 retval = wo->notask_error;
 if (!retval && !(wo->wo_flags & WNOHANG)) {
  retval = -ERESTARTSYS;
  if (!signal_pending(current)) {
   schedule();
   goto repeat;
  }
 }
end:
 __set_current_state(TASK_RUNNING);
 remove_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);
 return retval;
}

SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
  infop, int, options, struct rusage __user *, ru)
{
 struct wait_opts wo;
 struct pid *pid = NULL;
 enum pid_type type;
 long ret;

 if (options & ~(WNOHANG|WNOWAIT|WEXITED|WSTOPPED|WCONTINUED|
   __WNOTHREAD|__WCLONE|__WALL))
  return -EINVAL;
 if (!(options & (WEXITED|WSTOPPED|WCONTINUED)))
  return -EINVAL;

 switch (which) {
 case P_ALL:
  type = PIDTYPE_MAX;
  break;
 case P_PID:
  type = PIDTYPE_PID;
  if (upid <= 0)
   return -EINVAL;
  break;
 case P_PGID:
  type = PIDTYPE_PGID;
  if (upid <= 0)
   return -EINVAL;
  break;
 default:
  return -EINVAL;
 }

 if (type < PIDTYPE_MAX)
  pid = find_get_pid(upid);

 wo.wo_type = type;
 wo.wo_pid = pid;
 wo.wo_flags = options;
 wo.wo_info = infop;
 wo.wo_stat = NULL;
 wo.wo_rusage = ru;
 ret = do_wait(&wo);

 if (ret > 0) {
  ret = 0;
 } else if (infop) {





  if (!ret)
   ret = put_user(0, &infop->si_signo);
  if (!ret)
   ret = put_user(0, &infop->si_errno);
  if (!ret)
   ret = put_user(0, &infop->si_code);
  if (!ret)
   ret = put_user(0, &infop->si_pid);
  if (!ret)
   ret = put_user(0, &infop->si_uid);
  if (!ret)
   ret = put_user(0, &infop->si_status);
 }

 put_pid(pid);
 return ret;
}

SYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,
  int, options, struct rusage __user *, ru)
{
 struct wait_opts wo;
 struct pid *pid = NULL;
 enum pid_type type;
 long ret;

 if (options & ~(WNOHANG|WUNTRACED|WCONTINUED|
   __WNOTHREAD|__WCLONE|__WALL))
  return -EINVAL;

 if (upid == -1)
  type = PIDTYPE_MAX;
 else if (upid < 0) {
  type = PIDTYPE_PGID;
  pid = find_get_pid(-upid);
 } else if (upid == 0) {
  type = PIDTYPE_PGID;
  pid = get_task_pid(current, PIDTYPE_PGID);
 } else {
  type = PIDTYPE_PID;
  pid = find_get_pid(upid);
 }

 wo.wo_type = type;
 wo.wo_pid = pid;
 wo.wo_flags = options | WEXITED;
 wo.wo_info = NULL;
 wo.wo_stat = stat_addr;
 wo.wo_rusage = ru;
 ret = do_wait(&wo);
 put_pid(pid);

 return ret;
}






SYSCALL_DEFINE3(waitpid, pid_t, pid, int __user *, stat_addr, int, options)
{
 return sys_wait4(pid, stat_addr, options, NULL);
}









DEFINE_MUTEX(text_mutex);

extern struct exception_table_entry __start___ex_table[];
extern struct exception_table_entry __stop___ex_table[];


u32 __initdata __visible main_extable_sort_needed = 1;


void __init sort_main_extable(void)
{
 if (main_extable_sort_needed && __stop___ex_table > __start___ex_table) {
  pr_notice("Sorting __ex_table...\n");
  sort_extable(__start___ex_table, __stop___ex_table);
 }
}


const struct exception_table_entry *search_exception_tables(unsigned long addr)
{
 const struct exception_table_entry *e;

 e = search_extable(__start___ex_table, __stop___ex_table-1, addr);
 if (!e)
  e = search_module_extables(addr);
 return e;
}

static inline int init_kernel_text(unsigned long addr)
{
 if (addr >= (unsigned long)_sinittext &&
     addr < (unsigned long)_einittext)
  return 1;
 return 0;
}

int core_kernel_text(unsigned long addr)
{
 if (addr >= (unsigned long)_stext &&
     addr < (unsigned long)_etext)
  return 1;

 if (system_state == SYSTEM_BOOTING &&
     init_kernel_text(addr))
  return 1;
 return 0;
}
int core_kernel_data(unsigned long addr)
{
 if (addr >= (unsigned long)_sdata &&
     addr < (unsigned long)_edata)
  return 1;
 return 0;
}

int __kernel_text_address(unsigned long addr)
{
 if (core_kernel_text(addr))
  return 1;
 if (is_module_text_address(addr))
  return 1;
 if (is_ftrace_trampoline(addr))
  return 1;
 if (init_kernel_text(addr))
  return 1;
 return 0;
}

int kernel_text_address(unsigned long addr)
{
 if (core_kernel_text(addr))
  return 1;
 if (is_module_text_address(addr))
  return 1;
 return is_ftrace_trampoline(addr);
}
int func_ptr_is_kernel_text(void *ptr)
{
 unsigned long addr;
 addr = (unsigned long) dereference_function_descriptor(ptr);
 if (core_kernel_text(addr))
  return 1;
 return is_module_text_address(addr);
}















unsigned long total_forks;
int nr_threads;

int max_threads;

DEFINE_PER_CPU(unsigned long, process_counts) = 0;

__cacheline_aligned DEFINE_RWLOCK(tasklist_lock);

int lockdep_tasklist_lock_is_held(void)
{
 return lockdep_is_held(&tasklist_lock);
}
EXPORT_SYMBOL_GPL(lockdep_tasklist_lock_is_held);

int nr_processes(void)
{
 int cpu;
 int total = 0;

 for_each_possible_cpu(cpu)
  total += per_cpu(process_counts, cpu);

 return total;
}

void __weak arch_release_task_struct(struct task_struct *tsk)
{
}

static struct kmem_cache *task_struct_cachep;

static inline struct task_struct *alloc_task_struct_node(int node)
{
 return kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL, node);
}

static inline void free_task_struct(struct task_struct *tsk)
{
 kmem_cache_free(task_struct_cachep, tsk);
}

void __weak arch_release_thread_stack(unsigned long *stack)
{
}






static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,
        int node)
{
 struct page *page = alloc_kmem_pages_node(node, THREADINFO_GFP,
        THREAD_SIZE_ORDER);

 if (page)
  memcg_kmem_update_page_stat(page, MEMCG_KERNEL_STACK,
         1 << THREAD_SIZE_ORDER);

 return page ? page_address(page) : NULL;
}

static inline void free_thread_stack(unsigned long *stack)
{
 struct page *page = virt_to_page(stack);

 memcg_kmem_update_page_stat(page, MEMCG_KERNEL_STACK,
        -(1 << THREAD_SIZE_ORDER));
 __free_kmem_pages(page, THREAD_SIZE_ORDER);
}
static struct kmem_cache *thread_stack_cache;

static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,
        int node)
{
 return kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);
}

static void free_thread_stack(unsigned long *stack)
{
 kmem_cache_free(thread_stack_cache, stack);
}

void thread_stack_cache_init(void)
{
 thread_stack_cache = kmem_cache_create("thread_stack", THREAD_SIZE,
           THREAD_SIZE, 0, NULL);
 BUG_ON(thread_stack_cache == NULL);
}


static struct kmem_cache *signal_cachep;


struct kmem_cache *sighand_cachep;


struct kmem_cache *files_cachep;


struct kmem_cache *fs_cachep;


struct kmem_cache *vm_area_cachep;


static struct kmem_cache *mm_cachep;

static void account_kernel_stack(unsigned long *stack, int account)
{
 struct zone *zone = page_zone(virt_to_page(stack));

 mod_zone_page_state(zone, NR_KERNEL_STACK, account);
}

void free_task(struct task_struct *tsk)
{
 account_kernel_stack(tsk->stack, -1);
 arch_release_thread_stack(tsk->stack);
 free_thread_stack(tsk->stack);
 rt_mutex_debug_task_free(tsk);
 ftrace_graph_exit_task(tsk);
 put_seccomp_filter(tsk);
 arch_release_task_struct(tsk);
 free_task_struct(tsk);
}
EXPORT_SYMBOL(free_task);

static inline void free_signal_struct(struct signal_struct *sig)
{
 taskstats_tgid_free(sig);
 sched_autogroup_exit(sig);
 kmem_cache_free(signal_cachep, sig);
}

static inline void put_signal_struct(struct signal_struct *sig)
{
 if (atomic_dec_and_test(&sig->sigcnt))
  free_signal_struct(sig);
}

void __put_task_struct(struct task_struct *tsk)
{
 WARN_ON(!tsk->exit_state);
 WARN_ON(atomic_read(&tsk->usage));
 WARN_ON(tsk == current);

 cgroup_free(tsk);
 task_numa_free(tsk);
 security_task_free(tsk);
 exit_creds(tsk);
 delayacct_tsk_free(tsk);
 put_signal_struct(tsk->signal);

 if (!profile_handoff_task(tsk))
  free_task(tsk);
}
EXPORT_SYMBOL_GPL(__put_task_struct);

void __init __weak arch_task_cache_init(void) { }




static void set_max_threads(unsigned int max_threads_suggested)
{
 u64 threads;





 if (fls64(totalram_pages) + fls64(PAGE_SIZE) > 64)
  threads = MAX_THREADS;
 else
  threads = div64_u64((u64) totalram_pages * (u64) PAGE_SIZE,
        (u64) THREAD_SIZE * 8UL);

 if (threads > max_threads_suggested)
  threads = max_threads_suggested;

 max_threads = clamp_t(u64, threads, MIN_THREADS, MAX_THREADS);
}


int arch_task_struct_size __read_mostly;

void __init fork_init(void)
{

 task_struct_cachep = kmem_cache_create("task_struct",
   arch_task_struct_size, ARCH_MIN_TASKALIGN,
   SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT, NULL);


 arch_task_cache_init();

 set_max_threads(MAX_THREADS);

 init_task.signal->rlim[RLIMIT_NPROC].rlim_cur = max_threads/2;
 init_task.signal->rlim[RLIMIT_NPROC].rlim_max = max_threads/2;
 init_task.signal->rlim[RLIMIT_SIGPENDING] =
  init_task.signal->rlim[RLIMIT_NPROC];
}

int __weak arch_dup_task_struct(struct task_struct *dst,
            struct task_struct *src)
{
 *dst = *src;
 return 0;
}

void set_task_stack_end_magic(struct task_struct *tsk)
{
 unsigned long *stackend;

 stackend = end_of_stack(tsk);
 *stackend = STACK_END_MAGIC;
}

static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
{
 struct task_struct *tsk;
 unsigned long *stack;
 int err;

 if (node == NUMA_NO_NODE)
  node = tsk_fork_get_node(orig);
 tsk = alloc_task_struct_node(node);
 if (!tsk)
  return NULL;

 stack = alloc_thread_stack_node(tsk, node);
 if (!stack)
  goto free_tsk;

 err = arch_dup_task_struct(tsk, orig);
 if (err)
  goto free_stack;

 tsk->stack = stack;






 tsk->seccomp.filter = NULL;

 setup_thread_stack(tsk, orig);
 clear_user_return_notifier(tsk);
 clear_tsk_need_resched(tsk);
 set_task_stack_end_magic(tsk);

 tsk->stack_canary = get_random_int();





 atomic_set(&tsk->usage, 2);
 tsk->btrace_seq = 0;
 tsk->splice_pipe = NULL;
 tsk->task_frag.page = NULL;
 tsk->wake_q.next = NULL;

 account_kernel_stack(stack, 1);

 kcov_task_init(tsk);

 return tsk;

free_stack:
 free_thread_stack(stack);
free_tsk:
 free_task_struct(tsk);
 return NULL;
}

static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
{
 struct vm_area_struct *mpnt, *tmp, *prev, **pprev;
 struct rb_node **rb_link, *rb_parent;
 int retval;
 unsigned long charge;

 uprobe_start_dup_mmap();
 if (down_write_killable(&oldmm->mmap_sem)) {
  retval = -EINTR;
  goto fail_uprobe_end;
 }
 flush_cache_dup_mm(oldmm);
 uprobe_dup_mmap(oldmm, mm);



 down_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);


 RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));

 mm->total_vm = oldmm->total_vm;
 mm->data_vm = oldmm->data_vm;
 mm->exec_vm = oldmm->exec_vm;
 mm->stack_vm = oldmm->stack_vm;

 rb_link = &mm->mm_rb.rb_node;
 rb_parent = NULL;
 pprev = &mm->mmap;
 retval = ksm_fork(mm, oldmm);
 if (retval)
  goto out;
 retval = khugepaged_fork(mm, oldmm);
 if (retval)
  goto out;

 prev = NULL;
 for (mpnt = oldmm->mmap; mpnt; mpnt = mpnt->vm_next) {
  struct file *file;

  if (mpnt->vm_flags & VM_DONTCOPY) {
   vm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt));
   continue;
  }
  charge = 0;
  if (mpnt->vm_flags & VM_ACCOUNT) {
   unsigned long len = vma_pages(mpnt);

   if (security_vm_enough_memory_mm(oldmm, len))
    goto fail_nomem;
   charge = len;
  }
  tmp = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
  if (!tmp)
   goto fail_nomem;
  *tmp = *mpnt;
  INIT_LIST_HEAD(&tmp->anon_vma_chain);
  retval = vma_dup_policy(mpnt, tmp);
  if (retval)
   goto fail_nomem_policy;
  tmp->vm_mm = mm;
  if (anon_vma_fork(tmp, mpnt))
   goto fail_nomem_anon_vma_fork;
  tmp->vm_flags &=
   ~(VM_LOCKED|VM_LOCKONFAULT|VM_UFFD_MISSING|VM_UFFD_WP);
  tmp->vm_next = tmp->vm_prev = NULL;
  tmp->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
  file = tmp->vm_file;
  if (file) {
   struct inode *inode = file_inode(file);
   struct address_space *mapping = file->f_mapping;

   get_file(file);
   if (tmp->vm_flags & VM_DENYWRITE)
    atomic_dec(&inode->i_writecount);
   i_mmap_lock_write(mapping);
   if (tmp->vm_flags & VM_SHARED)
    atomic_inc(&mapping->i_mmap_writable);
   flush_dcache_mmap_lock(mapping);

   vma_interval_tree_insert_after(tmp, mpnt,
     &mapping->i_mmap);
   flush_dcache_mmap_unlock(mapping);
   i_mmap_unlock_write(mapping);
  }






  if (is_vm_hugetlb_page(tmp))
   reset_vma_resv_huge_pages(tmp);




  *pprev = tmp;
  pprev = &tmp->vm_next;
  tmp->vm_prev = prev;
  prev = tmp;

  __vma_link_rb(mm, tmp, rb_link, rb_parent);
  rb_link = &tmp->vm_rb.rb_right;
  rb_parent = &tmp->vm_rb;

  mm->map_count++;
  retval = copy_page_range(mm, oldmm, mpnt);

  if (tmp->vm_ops && tmp->vm_ops->open)
   tmp->vm_ops->open(tmp);

  if (retval)
   goto out;
 }

 arch_dup_mmap(oldmm, mm);
 retval = 0;
out:
 up_write(&mm->mmap_sem);
 flush_tlb_mm(oldmm);
 up_write(&oldmm->mmap_sem);
fail_uprobe_end:
 uprobe_end_dup_mmap();
 return retval;
fail_nomem_anon_vma_fork:
 mpol_put(vma_policy(tmp));
fail_nomem_policy:
 kmem_cache_free(vm_area_cachep, tmp);
fail_nomem:
 retval = -ENOMEM;
 vm_unacct_memory(charge);
 goto out;
}

static inline int mm_alloc_pgd(struct mm_struct *mm)
{
 mm->pgd = pgd_alloc(mm);
 if (unlikely(!mm->pgd))
  return -ENOMEM;
 return 0;
}

static inline void mm_free_pgd(struct mm_struct *mm)
{
 pgd_free(mm, mm->pgd);
}
static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
{
 down_write(&oldmm->mmap_sem);
 RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));
 up_write(&oldmm->mmap_sem);
 return 0;
}

__cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);


static unsigned long default_dump_filter = MMF_DUMP_FILTER_DEFAULT;

static int __init coredump_filter_setup(char *s)
{
 default_dump_filter =
  (simple_strtoul(s, NULL, 0) << MMF_DUMP_FILTER_SHIFT) &
  MMF_DUMP_FILTER_MASK;
 return 1;
}

__setup("coredump_filter=", coredump_filter_setup);


static void mm_init_aio(struct mm_struct *mm)
{
 spin_lock_init(&mm->ioctx_lock);
 mm->ioctx_table = NULL;
}

static void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
{
 mm->owner = p;
}

static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
{
 mm->mmap = NULL;
 mm->mm_rb = RB_ROOT;
 mm->vmacache_seqnum = 0;
 atomic_set(&mm->mm_users, 1);
 atomic_set(&mm->mm_count, 1);
 init_rwsem(&mm->mmap_sem);
 INIT_LIST_HEAD(&mm->mmlist);
 mm->core_state = NULL;
 atomic_long_set(&mm->nr_ptes, 0);
 mm_nr_pmds_init(mm);
 mm->map_count = 0;
 mm->locked_vm = 0;
 mm->pinned_vm = 0;
 memset(&mm->rss_stat, 0, sizeof(mm->rss_stat));
 spin_lock_init(&mm->page_table_lock);
 mm_init_cpumask(mm);
 mm_init_aio(mm);
 mm_init_owner(mm, p);
 mmu_notifier_mm_init(mm);
 clear_tlb_flush_pending(mm);
 mm->pmd_huge_pte = NULL;

 if (current->mm) {
  mm->flags = current->mm->flags & MMF_INIT_MASK;
  mm->def_flags = current->mm->def_flags & VM_INIT_DEF_MASK;
 } else {
  mm->flags = default_dump_filter;
  mm->def_flags = 0;
 }

 if (mm_alloc_pgd(mm))
  goto fail_nopgd;

 if (init_new_context(p, mm))
  goto fail_nocontext;

 return mm;

fail_nocontext:
 mm_free_pgd(mm);
fail_nopgd:
 free_mm(mm);
 return NULL;
}

static void check_mm(struct mm_struct *mm)
{
 int i;

 for (i = 0; i < NR_MM_COUNTERS; i++) {
  long x = atomic_long_read(&mm->rss_stat.count[i]);

  if (unlikely(x))
   printk(KERN_ALERT "BUG: Bad rss-counter state "
       "mm:%p idx:%d val:%ld\n", mm, i, x);
 }

 if (atomic_long_read(&mm->nr_ptes))
  pr_alert("BUG: non-zero nr_ptes on freeing mm: %ld\n",
    atomic_long_read(&mm->nr_ptes));
 if (mm_nr_pmds(mm))
  pr_alert("BUG: non-zero nr_pmds on freeing mm: %ld\n",
    mm_nr_pmds(mm));

 VM_BUG_ON_MM(mm->pmd_huge_pte, mm);
}




struct mm_struct *mm_alloc(void)
{
 struct mm_struct *mm;

 mm = allocate_mm();
 if (!mm)
  return NULL;

 memset(mm, 0, sizeof(*mm));
 return mm_init(mm, current);
}






void __mmdrop(struct mm_struct *mm)
{
 BUG_ON(mm == &init_mm);
 mm_free_pgd(mm);
 destroy_context(mm);
 mmu_notifier_mm_destroy(mm);
 check_mm(mm);
 free_mm(mm);
}
EXPORT_SYMBOL_GPL(__mmdrop);

static inline void __mmput(struct mm_struct *mm)
{
 VM_BUG_ON(atomic_read(&mm->mm_users));

 uprobe_clear_state(mm);
 exit_aio(mm);
 ksm_exit(mm);
 khugepaged_exit(mm);
 exit_mmap(mm);
 set_mm_exe_file(mm, NULL);
 if (!list_empty(&mm->mmlist)) {
  spin_lock(&mmlist_lock);
  list_del(&mm->mmlist);
  spin_unlock(&mmlist_lock);
 }
 if (mm->binfmt)
  module_put(mm->binfmt->module);
 mmdrop(mm);
}




void mmput(struct mm_struct *mm)
{
 might_sleep();

 if (atomic_dec_and_test(&mm->mm_users))
  __mmput(mm);
}
EXPORT_SYMBOL_GPL(mmput);

static void mmput_async_fn(struct work_struct *work)
{
 struct mm_struct *mm = container_of(work, struct mm_struct, async_put_work);
 __mmput(mm);
}

void mmput_async(struct mm_struct *mm)
{
 if (atomic_dec_and_test(&mm->mm_users)) {
  INIT_WORK(&mm->async_put_work, mmput_async_fn);
  schedule_work(&mm->async_put_work);
 }
}
void set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file)
{
 struct file *old_exe_file;






 old_exe_file = rcu_dereference_raw(mm->exe_file);

 if (new_exe_file)
  get_file(new_exe_file);
 rcu_assign_pointer(mm->exe_file, new_exe_file);
 if (old_exe_file)
  fput(old_exe_file);
}







struct file *get_mm_exe_file(struct mm_struct *mm)
{
 struct file *exe_file;

 rcu_read_lock();
 exe_file = rcu_dereference(mm->exe_file);
 if (exe_file && !get_file_rcu(exe_file))
  exe_file = NULL;
 rcu_read_unlock();
 return exe_file;
}
EXPORT_SYMBOL(get_mm_exe_file);
struct mm_struct *get_task_mm(struct task_struct *task)
{
 struct mm_struct *mm;

 task_lock(task);
 mm = task->mm;
 if (mm) {
  if (task->flags & PF_KTHREAD)
   mm = NULL;
  else
   atomic_inc(&mm->mm_users);
 }
 task_unlock(task);
 return mm;
}
EXPORT_SYMBOL_GPL(get_task_mm);

struct mm_struct *mm_access(struct task_struct *task, unsigned int mode)
{
 struct mm_struct *mm;
 int err;

 err = mutex_lock_killable(&task->signal->cred_guard_mutex);
 if (err)
  return ERR_PTR(err);

 mm = get_task_mm(task);
 if (mm && mm != current->mm &&
   !ptrace_may_access(task, mode)) {
  mmput(mm);
  mm = ERR_PTR(-EACCES);
 }
 mutex_unlock(&task->signal->cred_guard_mutex);

 return mm;
}

static void complete_vfork_done(struct task_struct *tsk)
{
 struct completion *vfork;

 task_lock(tsk);
 vfork = tsk->vfork_done;
 if (likely(vfork)) {
  tsk->vfork_done = NULL;
  complete(vfork);
 }
 task_unlock(tsk);
}

static int wait_for_vfork_done(struct task_struct *child,
    struct completion *vfork)
{
 int killed;

 freezer_do_not_count();
 killed = wait_for_completion_killable(vfork);
 freezer_count();

 if (killed) {
  task_lock(child);
  child->vfork_done = NULL;
  task_unlock(child);
 }

 put_task_struct(child);
 return killed;
}
void mm_release(struct task_struct *tsk, struct mm_struct *mm)
{

 if (unlikely(tsk->robust_list)) {
  exit_robust_list(tsk);
  tsk->robust_list = NULL;
 }
 if (unlikely(tsk->compat_robust_list)) {
  compat_exit_robust_list(tsk);
  tsk->compat_robust_list = NULL;
 }
 if (unlikely(!list_empty(&tsk->pi_state_list)))
  exit_pi_state_list(tsk);

 uprobe_free_utask(tsk);


 deactivate_mm(tsk, mm);
 if (tsk->clear_child_tid) {
  if (!(tsk->flags & PF_SIGNALED) &&
      atomic_read(&mm->mm_users) > 1) {




   put_user(0, tsk->clear_child_tid);
   sys_futex(tsk->clear_child_tid, FUTEX_WAKE,
     1, NULL, NULL, 0);
  }
  tsk->clear_child_tid = NULL;
 }





 if (tsk->vfork_done)
  complete_vfork_done(tsk);
}





static struct mm_struct *dup_mm(struct task_struct *tsk)
{
 struct mm_struct *mm, *oldmm = current->mm;
 int err;

 mm = allocate_mm();
 if (!mm)
  goto fail_nomem;

 memcpy(mm, oldmm, sizeof(*mm));

 if (!mm_init(mm, tsk))
  goto fail_nomem;

 err = dup_mmap(mm, oldmm);
 if (err)
  goto free_pt;

 mm->hiwater_rss = get_mm_rss(mm);
 mm->hiwater_vm = mm->total_vm;

 if (mm->binfmt && !try_module_get(mm->binfmt->module))
  goto free_pt;

 return mm;

free_pt:

 mm->binfmt = NULL;
 mmput(mm);

fail_nomem:
 return NULL;
}

static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
{
 struct mm_struct *mm, *oldmm;
 int retval;

 tsk->min_flt = tsk->maj_flt = 0;
 tsk->nvcsw = tsk->nivcsw = 0;
 tsk->last_switch_count = tsk->nvcsw + tsk->nivcsw;

 tsk->mm = NULL;
 tsk->active_mm = NULL;






 oldmm = current->mm;
 if (!oldmm)
  return 0;


 vmacache_flush(tsk);

 if (clone_flags & CLONE_VM) {
  atomic_inc(&oldmm->mm_users);
  mm = oldmm;
  goto good_mm;
 }

 retval = -ENOMEM;
 mm = dup_mm(tsk);
 if (!mm)
  goto fail_nomem;

good_mm:
 tsk->mm = mm;
 tsk->active_mm = mm;
 return 0;

fail_nomem:
 return retval;
}

static int copy_fs(unsigned long clone_flags, struct task_struct *tsk)
{
 struct fs_struct *fs = current->fs;
 if (clone_flags & CLONE_FS) {

  spin_lock(&fs->lock);
  if (fs->in_exec) {
   spin_unlock(&fs->lock);
   return -EAGAIN;
  }
  fs->users++;
  spin_unlock(&fs->lock);
  return 0;
 }
 tsk->fs = copy_fs_struct(fs);
 if (!tsk->fs)
  return -ENOMEM;
 return 0;
}

static int copy_files(unsigned long clone_flags, struct task_struct *tsk)
{
 struct files_struct *oldf, *newf;
 int error = 0;




 oldf = current->files;
 if (!oldf)
  goto out;

 if (clone_flags & CLONE_FILES) {
  atomic_inc(&oldf->count);
  goto out;
 }

 newf = dup_fd(oldf, &error);
 if (!newf)
  goto out;

 tsk->files = newf;
 error = 0;
out:
 return error;
}

static int copy_io(unsigned long clone_flags, struct task_struct *tsk)
{
 struct io_context *ioc = current->io_context;
 struct io_context *new_ioc;

 if (!ioc)
  return 0;



 if (clone_flags & CLONE_IO) {
  ioc_task_link(ioc);
  tsk->io_context = ioc;
 } else if (ioprio_valid(ioc->ioprio)) {
  new_ioc = get_task_io_context(tsk, GFP_KERNEL, NUMA_NO_NODE);
  if (unlikely(!new_ioc))
   return -ENOMEM;

  new_ioc->ioprio = ioc->ioprio;
  put_io_context(new_ioc);
 }
 return 0;
}

static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
{
 struct sighand_struct *sig;

 if (clone_flags & CLONE_SIGHAND) {
  atomic_inc(&current->sighand->count);
  return 0;
 }
 sig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);
 rcu_assign_pointer(tsk->sighand, sig);
 if (!sig)
  return -ENOMEM;

 atomic_set(&sig->count, 1);
 memcpy(sig->action, current->sighand->action, sizeof(sig->action));
 return 0;
}

void __cleanup_sighand(struct sighand_struct *sighand)
{
 if (atomic_dec_and_test(&sighand->count)) {
  signalfd_cleanup(sighand);




  kmem_cache_free(sighand_cachep, sighand);
 }
}




static void posix_cpu_timers_init_group(struct signal_struct *sig)
{
 unsigned long cpu_limit;

 cpu_limit = READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
 if (cpu_limit != RLIM_INFINITY) {
  sig->cputime_expires.prof_exp = secs_to_cputime(cpu_limit);
  sig->cputimer.running = true;
 }


 INIT_LIST_HEAD(&sig->cpu_timers[0]);
 INIT_LIST_HEAD(&sig->cpu_timers[1]);
 INIT_LIST_HEAD(&sig->cpu_timers[2]);
}

static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
{
 struct signal_struct *sig;

 if (clone_flags & CLONE_THREAD)
  return 0;

 sig = kmem_cache_zalloc(signal_cachep, GFP_KERNEL);
 tsk->signal = sig;
 if (!sig)
  return -ENOMEM;

 sig->nr_threads = 1;
 atomic_set(&sig->live, 1);
 atomic_set(&sig->sigcnt, 1);


 sig->thread_head = (struct list_head)LIST_HEAD_INIT(tsk->thread_node);
 tsk->thread_node = (struct list_head)LIST_HEAD_INIT(sig->thread_head);

 init_waitqueue_head(&sig->wait_chldexit);
 sig->curr_target = tsk;
 init_sigpending(&sig->shared_pending);
 INIT_LIST_HEAD(&sig->posix_timers);
 seqlock_init(&sig->stats_lock);
 prev_cputime_init(&sig->prev_cputime);

 hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 sig->real_timer.function = it_real_fn;

 task_lock(current->group_leader);
 memcpy(sig->rlim, current->signal->rlim, sizeof sig->rlim);
 task_unlock(current->group_leader);

 posix_cpu_timers_init_group(sig);

 tty_audit_fork(sig);
 sched_autogroup_fork(sig);

 sig->oom_score_adj = current->signal->oom_score_adj;
 sig->oom_score_adj_min = current->signal->oom_score_adj_min;

 sig->has_child_subreaper = current->signal->has_child_subreaper ||
       current->signal->is_child_subreaper;

 mutex_init(&sig->cred_guard_mutex);

 return 0;
}

static void copy_seccomp(struct task_struct *p)
{






 assert_spin_locked(&current->sighand->siglock);


 get_seccomp_filter(current);
 p->seccomp = current->seccomp;






 if (task_no_new_privs(current))
  task_set_no_new_privs(p);






 if (p->seccomp.mode != SECCOMP_MODE_DISABLED)
  set_tsk_thread_flag(p, TIF_SECCOMP);
}

SYSCALL_DEFINE1(set_tid_address, int __user *, tidptr)
{
 current->clear_child_tid = tidptr;

 return task_pid_vnr(current);
}

static void rt_mutex_init_task(struct task_struct *p)
{
 raw_spin_lock_init(&p->pi_lock);
 p->pi_waiters = RB_ROOT;
 p->pi_waiters_leftmost = NULL;
 p->pi_blocked_on = NULL;
}




static void posix_cpu_timers_init(struct task_struct *tsk)
{
 tsk->cputime_expires.prof_exp = 0;
 tsk->cputime_expires.virt_exp = 0;
 tsk->cputime_expires.sched_exp = 0;
 INIT_LIST_HEAD(&tsk->cpu_timers[0]);
 INIT_LIST_HEAD(&tsk->cpu_timers[1]);
 INIT_LIST_HEAD(&tsk->cpu_timers[2]);
}

static inline void
init_task_pid(struct task_struct *task, enum pid_type type, struct pid *pid)
{
  task->pids[type].pid = pid;
}
static struct task_struct *copy_process(unsigned long clone_flags,
     unsigned long stack_start,
     unsigned long stack_size,
     int __user *child_tidptr,
     struct pid *pid,
     int trace,
     unsigned long tls,
     int node)
{
 int retval;
 struct task_struct *p;

 if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
  return ERR_PTR(-EINVAL);

 if ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))
  return ERR_PTR(-EINVAL);





 if ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))
  return ERR_PTR(-EINVAL);






 if ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))
  return ERR_PTR(-EINVAL);







 if ((clone_flags & CLONE_PARENT) &&
    current->signal->flags & SIGNAL_UNKILLABLE)
  return ERR_PTR(-EINVAL);





 if (clone_flags & CLONE_THREAD) {
  if ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||
      (task_active_pid_ns(current) !=
    current->nsproxy->pid_ns_for_children))
   return ERR_PTR(-EINVAL);
 }

 retval = security_task_create(clone_flags);
 if (retval)
  goto fork_out;

 retval = -ENOMEM;
 p = dup_task_struct(current, node);
 if (!p)
  goto fork_out;

 ftrace_graph_init_task(p);

 rt_mutex_init_task(p);

 DEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);
 DEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);
 retval = -EAGAIN;
 if (atomic_read(&p->real_cred->user->processes) >=
   task_rlimit(p, RLIMIT_NPROC)) {
  if (p->real_cred->user != INIT_USER &&
      !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))
   goto bad_fork_free;
 }
 current->flags &= ~PF_NPROC_EXCEEDED;

 retval = copy_creds(p, clone_flags);
 if (retval < 0)
  goto bad_fork_free;






 retval = -EAGAIN;
 if (nr_threads >= max_threads)
  goto bad_fork_cleanup_count;

 delayacct_tsk_init(p);
 p->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER);
 p->flags |= PF_FORKNOEXEC;
 INIT_LIST_HEAD(&p->children);
 INIT_LIST_HEAD(&p->sibling);
 rcu_copy_process(p);
 p->vfork_done = NULL;
 spin_lock_init(&p->alloc_lock);

 init_sigpending(&p->pending);

 p->utime = p->stime = p->gtime = 0;
 p->utimescaled = p->stimescaled = 0;
 prev_cputime_init(&p->prev_cputime);

 seqcount_init(&p->vtime_seqcount);
 p->vtime_snap = 0;
 p->vtime_snap_whence = VTIME_INACTIVE;

 memset(&p->rss_stat, 0, sizeof(p->rss_stat));

 p->default_timer_slack_ns = current->timer_slack_ns;

 task_io_accounting_init(&p->ioac);
 acct_clear_integrals(p);

 posix_cpu_timers_init(p);

 p->start_time = ktime_get_ns();
 p->real_start_time = ktime_get_boot_ns();
 p->io_context = NULL;
 p->audit_context = NULL;
 threadgroup_change_begin(current);
 cgroup_fork(p);
 p->mempolicy = mpol_dup(p->mempolicy);
 if (IS_ERR(p->mempolicy)) {
  retval = PTR_ERR(p->mempolicy);
  p->mempolicy = NULL;
  goto bad_fork_cleanup_threadgroup_lock;
 }
 p->cpuset_mem_spread_rotor = NUMA_NO_NODE;
 p->cpuset_slab_spread_rotor = NUMA_NO_NODE;
 seqcount_init(&p->mems_allowed_seq);
 p->irq_events = 0;
 p->hardirqs_enabled = 0;
 p->hardirq_enable_ip = 0;
 p->hardirq_enable_event = 0;
 p->hardirq_disable_ip = _THIS_IP_;
 p->hardirq_disable_event = 0;
 p->softirqs_enabled = 1;
 p->softirq_enable_ip = _THIS_IP_;
 p->softirq_enable_event = 0;
 p->softirq_disable_ip = 0;
 p->softirq_disable_event = 0;
 p->hardirq_context = 0;
 p->softirq_context = 0;

 p->pagefault_disabled = 0;

 p->lockdep_depth = 0;
 p->curr_chain_key = 0;
 p->lockdep_recursion = 0;

 p->blocked_on = NULL;
 p->sequential_io = 0;
 p->sequential_io_avg = 0;


 retval = sched_fork(clone_flags, p);
 if (retval)
  goto bad_fork_cleanup_policy;

 retval = perf_event_init_task(p);
 if (retval)
  goto bad_fork_cleanup_policy;
 retval = audit_alloc(p);
 if (retval)
  goto bad_fork_cleanup_perf;

 shm_init_task(p);
 retval = copy_semundo(clone_flags, p);
 if (retval)
  goto bad_fork_cleanup_audit;
 retval = copy_files(clone_flags, p);
 if (retval)
  goto bad_fork_cleanup_semundo;
 retval = copy_fs(clone_flags, p);
 if (retval)
  goto bad_fork_cleanup_files;
 retval = copy_sighand(clone_flags, p);
 if (retval)
  goto bad_fork_cleanup_fs;
 retval = copy_signal(clone_flags, p);
 if (retval)
  goto bad_fork_cleanup_sighand;
 retval = copy_mm(clone_flags, p);
 if (retval)
  goto bad_fork_cleanup_signal;
 retval = copy_namespaces(clone_flags, p);
 if (retval)
  goto bad_fork_cleanup_mm;
 retval = copy_io(clone_flags, p);
 if (retval)
  goto bad_fork_cleanup_namespaces;
 retval = copy_thread_tls(clone_flags, stack_start, stack_size, p, tls);
 if (retval)
  goto bad_fork_cleanup_io;

 if (pid != &init_struct_pid) {
  pid = alloc_pid(p->nsproxy->pid_ns_for_children);
  if (IS_ERR(pid)) {
   retval = PTR_ERR(pid);
   goto bad_fork_cleanup_thread;
  }
 }

 p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;



 p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;
 p->plug = NULL;
 p->robust_list = NULL;
 p->compat_robust_list = NULL;
 INIT_LIST_HEAD(&p->pi_state_list);
 p->pi_state_cache = NULL;



 if ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)
  sas_ss_reset(p);





 user_disable_single_step(p);
 clear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);
 clear_tsk_thread_flag(p, TIF_SYSCALL_EMU);
 clear_all_latency_tracing(p);


 p->pid = pid_nr(pid);
 if (clone_flags & CLONE_THREAD) {
  p->exit_signal = -1;
  p->group_leader = current->group_leader;
  p->tgid = current->tgid;
 } else {
  if (clone_flags & CLONE_PARENT)
   p->exit_signal = current->group_leader->exit_signal;
  else
   p->exit_signal = (clone_flags & CSIGNAL);
  p->group_leader = p;
  p->tgid = p->pid;
 }

 p->nr_dirtied = 0;
 p->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);
 p->dirty_paused_when = 0;

 p->pdeath_signal = 0;
 INIT_LIST_HEAD(&p->thread_group);
 p->task_works = NULL;







 retval = cgroup_can_fork(p);
 if (retval)
  goto bad_fork_free_pid;





 write_lock_irq(&tasklist_lock);


 if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {
  p->real_parent = current->real_parent;
  p->parent_exec_id = current->parent_exec_id;
 } else {
  p->real_parent = current;
  p->parent_exec_id = current->self_exec_id;
 }

 spin_lock(&current->sighand->siglock);





 copy_seccomp(p);
 recalc_sigpending();
 if (signal_pending(current)) {
  spin_unlock(&current->sighand->siglock);
  write_unlock_irq(&tasklist_lock);
  retval = -ERESTARTNOINTR;
  goto bad_fork_cancel_cgroup;
 }

 if (likely(p->pid)) {
  ptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);

  init_task_pid(p, PIDTYPE_PID, pid);
  if (thread_group_leader(p)) {
   init_task_pid(p, PIDTYPE_PGID, task_pgrp(current));
   init_task_pid(p, PIDTYPE_SID, task_session(current));

   if (is_child_reaper(pid)) {
    ns_of_pid(pid)->child_reaper = p;
    p->signal->flags |= SIGNAL_UNKILLABLE;
   }

   p->signal->leader_pid = pid;
   p->signal->tty = tty_kref_get(current->signal->tty);
   list_add_tail(&p->sibling, &p->real_parent->children);
   list_add_tail_rcu(&p->tasks, &init_task.tasks);
   attach_pid(p, PIDTYPE_PGID);
   attach_pid(p, PIDTYPE_SID);
   __this_cpu_inc(process_counts);
  } else {
   current->signal->nr_threads++;
   atomic_inc(&current->signal->live);
   atomic_inc(&current->signal->sigcnt);
   list_add_tail_rcu(&p->thread_group,
       &p->group_leader->thread_group);
   list_add_tail_rcu(&p->thread_node,
       &p->signal->thread_head);
  }
  attach_pid(p, PIDTYPE_PID);
  nr_threads++;
 }

 total_forks++;
 spin_unlock(&current->sighand->siglock);
 syscall_tracepoint_update(p);
 write_unlock_irq(&tasklist_lock);

 proc_fork_connector(p);
 cgroup_post_fork(p);
 threadgroup_change_end(current);
 perf_event_fork(p);

 trace_task_newtask(p, clone_flags);
 uprobe_copy_process(p, clone_flags);

 return p;

bad_fork_cancel_cgroup:
 cgroup_cancel_fork(p);
bad_fork_free_pid:
 if (pid != &init_struct_pid)
  free_pid(pid);
bad_fork_cleanup_thread:
 exit_thread(p);
bad_fork_cleanup_io:
 if (p->io_context)
  exit_io_context(p);
bad_fork_cleanup_namespaces:
 exit_task_namespaces(p);
bad_fork_cleanup_mm:
 if (p->mm)
  mmput(p->mm);
bad_fork_cleanup_signal:
 if (!(clone_flags & CLONE_THREAD))
  free_signal_struct(p->signal);
bad_fork_cleanup_sighand:
 __cleanup_sighand(p->sighand);
bad_fork_cleanup_fs:
 exit_fs(p);
bad_fork_cleanup_files:
 exit_files(p);
bad_fork_cleanup_semundo:
 exit_sem(p);
bad_fork_cleanup_audit:
 audit_free(p);
bad_fork_cleanup_perf:
 perf_event_free_task(p);
bad_fork_cleanup_policy:
 mpol_put(p->mempolicy);
bad_fork_cleanup_threadgroup_lock:
 threadgroup_change_end(current);
 delayacct_tsk_free(p);
bad_fork_cleanup_count:
 atomic_dec(&p->cred->user->processes);
 exit_creds(p);
bad_fork_free:
 free_task(p);
fork_out:
 return ERR_PTR(retval);
}

static inline void init_idle_pids(struct pid_link *links)
{
 enum pid_type type;

 for (type = PIDTYPE_PID; type < PIDTYPE_MAX; ++type) {
  INIT_HLIST_NODE(&links[type].node);
  links[type].pid = &init_struct_pid;
 }
}

struct task_struct *fork_idle(int cpu)
{
 struct task_struct *task;
 task = copy_process(CLONE_VM, 0, 0, NULL, &init_struct_pid, 0, 0,
       cpu_to_node(cpu));
 if (!IS_ERR(task)) {
  init_idle_pids(task->pids);
  init_idle(task, cpu);
 }

 return task;
}







long _do_fork(unsigned long clone_flags,
       unsigned long stack_start,
       unsigned long stack_size,
       int __user *parent_tidptr,
       int __user *child_tidptr,
       unsigned long tls)
{
 struct task_struct *p;
 int trace = 0;
 long nr;







 if (!(clone_flags & CLONE_UNTRACED)) {
  if (clone_flags & CLONE_VFORK)
   trace = PTRACE_EVENT_VFORK;
  else if ((clone_flags & CSIGNAL) != SIGCHLD)
   trace = PTRACE_EVENT_CLONE;
  else
   trace = PTRACE_EVENT_FORK;

  if (likely(!ptrace_event_enabled(current, trace)))
   trace = 0;
 }

 p = copy_process(clone_flags, stack_start, stack_size,
    child_tidptr, NULL, trace, tls, NUMA_NO_NODE);




 if (!IS_ERR(p)) {
  struct completion vfork;
  struct pid *pid;

  trace_sched_process_fork(current, p);

  pid = get_task_pid(p, PIDTYPE_PID);
  nr = pid_vnr(pid);

  if (clone_flags & CLONE_PARENT_SETTID)
   put_user(nr, parent_tidptr);

  if (clone_flags & CLONE_VFORK) {
   p->vfork_done = &vfork;
   init_completion(&vfork);
   get_task_struct(p);
  }

  wake_up_new_task(p);


  if (unlikely(trace))
   ptrace_event_pid(trace, pid);

  if (clone_flags & CLONE_VFORK) {
   if (!wait_for_vfork_done(p, &vfork))
    ptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);
  }

  put_pid(pid);
 } else {
  nr = PTR_ERR(p);
 }
 return nr;
}



long do_fork(unsigned long clone_flags,
       unsigned long stack_start,
       unsigned long stack_size,
       int __user *parent_tidptr,
       int __user *child_tidptr)
{
 return _do_fork(clone_flags, stack_start, stack_size,
   parent_tidptr, child_tidptr, 0);
}




pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
{
 return _do_fork(flags|CLONE_VM|CLONE_UNTRACED, (unsigned long)fn,
  (unsigned long)arg, NULL, NULL, 0);
}

SYSCALL_DEFINE0(fork)
{
 return _do_fork(SIGCHLD, 0, 0, NULL, NULL, 0);

 return -EINVAL;
}

SYSCALL_DEFINE0(vfork)
{
 return _do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0,
   0, NULL, NULL, 0);
}

SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
   int __user *, parent_tidptr,
   unsigned long, tls,
   int __user *, child_tidptr)
SYSCALL_DEFINE5(clone, unsigned long, newsp, unsigned long, clone_flags,
   int __user *, parent_tidptr,
   int __user *, child_tidptr,
   unsigned long, tls)
SYSCALL_DEFINE6(clone, unsigned long, clone_flags, unsigned long, newsp,
  int, stack_size,
  int __user *, parent_tidptr,
  int __user *, child_tidptr,
  unsigned long, tls)
SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
   int __user *, parent_tidptr,
   int __user *, child_tidptr,
   unsigned long, tls)
{
 return _do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr, tls);
}


static void sighand_ctor(void *data)
{
 struct sighand_struct *sighand = data;

 spin_lock_init(&sighand->siglock);
 init_waitqueue_head(&sighand->signalfd_wqh);
}

void __init proc_caches_init(void)
{
 sighand_cachep = kmem_cache_create("sighand_cache",
   sizeof(struct sighand_struct), 0,
   SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_DESTROY_BY_RCU|
   SLAB_NOTRACK|SLAB_ACCOUNT, sighand_ctor);
 signal_cachep = kmem_cache_create("signal_cache",
   sizeof(struct signal_struct), 0,
   SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT,
   NULL);
 files_cachep = kmem_cache_create("files_cache",
   sizeof(struct files_struct), 0,
   SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT,
   NULL);
 fs_cachep = kmem_cache_create("fs_cache",
   sizeof(struct fs_struct), 0,
   SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT,
   NULL);







 mm_cachep = kmem_cache_create("mm_struct",
   sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
   SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT,
   NULL);
 vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC|SLAB_ACCOUNT);
 mmap_init();
 nsproxy_cache_init();
}




static int check_unshare_flags(unsigned long unshare_flags)
{
 if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
    CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
    CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET|
    CLONE_NEWUSER|CLONE_NEWPID|CLONE_NEWCGROUP))
  return -EINVAL;






 if (unshare_flags & (CLONE_THREAD | CLONE_SIGHAND | CLONE_VM)) {
  if (!thread_group_empty(current))
   return -EINVAL;
 }
 if (unshare_flags & (CLONE_SIGHAND | CLONE_VM)) {
  if (atomic_read(&current->sighand->count) > 1)
   return -EINVAL;
 }
 if (unshare_flags & CLONE_VM) {
  if (!current_is_single_threaded())
   return -EINVAL;
 }

 return 0;
}




static int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)
{
 struct fs_struct *fs = current->fs;

 if (!(unshare_flags & CLONE_FS) || !fs)
  return 0;


 if (fs->users == 1)
  return 0;

 *new_fsp = copy_fs_struct(fs);
 if (!*new_fsp)
  return -ENOMEM;

 return 0;
}




static int unshare_fd(unsigned long unshare_flags, struct files_struct **new_fdp)
{
 struct files_struct *fd = current->files;
 int error = 0;

 if ((unshare_flags & CLONE_FILES) &&
     (fd && atomic_read(&fd->count) > 1)) {
  *new_fdp = dup_fd(fd, &error);
  if (!*new_fdp)
   return error;
 }

 return 0;
}
SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
{
 struct fs_struct *fs, *new_fs = NULL;
 struct files_struct *fd, *new_fd = NULL;
 struct cred *new_cred = NULL;
 struct nsproxy *new_nsproxy = NULL;
 int do_sysvsem = 0;
 int err;





 if (unshare_flags & CLONE_NEWUSER)
  unshare_flags |= CLONE_THREAD | CLONE_FS;



 if (unshare_flags & CLONE_VM)
  unshare_flags |= CLONE_SIGHAND;



 if (unshare_flags & CLONE_SIGHAND)
  unshare_flags |= CLONE_THREAD;



 if (unshare_flags & CLONE_NEWNS)
  unshare_flags |= CLONE_FS;

 err = check_unshare_flags(unshare_flags);
 if (err)
  goto bad_unshare_out;





 if (unshare_flags & (CLONE_NEWIPC|CLONE_SYSVSEM))
  do_sysvsem = 1;
 err = unshare_fs(unshare_flags, &new_fs);
 if (err)
  goto bad_unshare_out;
 err = unshare_fd(unshare_flags, &new_fd);
 if (err)
  goto bad_unshare_cleanup_fs;
 err = unshare_userns(unshare_flags, &new_cred);
 if (err)
  goto bad_unshare_cleanup_fd;
 err = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy,
      new_cred, new_fs);
 if (err)
  goto bad_unshare_cleanup_cred;

 if (new_fs || new_fd || do_sysvsem || new_cred || new_nsproxy) {
  if (do_sysvsem) {



   exit_sem(current);
  }
  if (unshare_flags & CLONE_NEWIPC) {

   exit_shm(current);
   shm_init_task(current);
  }

  if (new_nsproxy)
   switch_task_namespaces(current, new_nsproxy);

  task_lock(current);

  if (new_fs) {
   fs = current->fs;
   spin_lock(&fs->lock);
   current->fs = new_fs;
   if (--fs->users)
    new_fs = NULL;
   else
    new_fs = fs;
   spin_unlock(&fs->lock);
  }

  if (new_fd) {
   fd = current->files;
   current->files = new_fd;
   new_fd = fd;
  }

  task_unlock(current);

  if (new_cred) {

   commit_creds(new_cred);
   new_cred = NULL;
  }
 }

bad_unshare_cleanup_cred:
 if (new_cred)
  put_cred(new_cred);
bad_unshare_cleanup_fd:
 if (new_fd)
  put_files_struct(new_fd);

bad_unshare_cleanup_fs:
 if (new_fs)
  free_fs_struct(new_fs);

bad_unshare_out:
 return err;
}







int unshare_files(struct files_struct **displaced)
{
 struct task_struct *task = current;
 struct files_struct *copy = NULL;
 int error;

 error = unshare_fd(CLONE_FILES, &copy);
 if (error || !copy) {
  *displaced = NULL;
  return error;
 }
 *displaced = task->files;
 task_lock(task);
 task->files = copy;
 task_unlock(task);
 return 0;
}

int sysctl_max_threads(struct ctl_table *table, int write,
         void __user *buffer, size_t *lenp, loff_t *ppos)
{
 struct ctl_table t;
 int ret;
 int threads = max_threads;
 int min = MIN_THREADS;
 int max = MAX_THREADS;

 t = *table;
 t.data = &threads;
 t.extra1 = &min;
 t.extra2 = &max;

 ret = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);
 if (ret || !write)
  return ret;

 set_max_threads(threads);

 return 0;
}








atomic_t system_freezing_cnt = ATOMIC_INIT(0);
EXPORT_SYMBOL(system_freezing_cnt);


bool pm_freezing;
bool pm_nosig_freezing;





EXPORT_SYMBOL_GPL(pm_freezing);


static DEFINE_SPINLOCK(freezer_lock);
bool freezing_slow_path(struct task_struct *p)
{
 if (p->flags & (PF_NOFREEZE | PF_SUSPEND_TASK))
  return false;

 if (test_thread_flag(TIF_MEMDIE))
  return false;

 if (pm_nosig_freezing || cgroup_freezing(p))
  return true;

 if (pm_freezing && !(p->flags & PF_KTHREAD))
  return true;

 return false;
}
EXPORT_SYMBOL(freezing_slow_path);


bool __refrigerator(bool check_kthr_stop)
{


 bool was_frozen = false;
 long save = current->state;

 pr_debug("%s entered refrigerator\n", current->comm);

 for (;;) {
  set_current_state(TASK_UNINTERRUPTIBLE);

  spin_lock_irq(&freezer_lock);
  current->flags |= PF_FROZEN;
  if (!freezing(current) ||
      (check_kthr_stop && kthread_should_stop()))
   current->flags &= ~PF_FROZEN;
  spin_unlock_irq(&freezer_lock);

  if (!(current->flags & PF_FROZEN))
   break;
  was_frozen = true;
  schedule();
 }

 pr_debug("%s left refrigerator\n", current->comm);






 set_current_state(save);

 return was_frozen;
}
EXPORT_SYMBOL(__refrigerator);

static void fake_signal_wake_up(struct task_struct *p)
{
 unsigned long flags;

 if (lock_task_sighand(p, &flags)) {
  signal_wake_up(p, 0);
  unlock_task_sighand(p, &flags);
 }
}
bool freeze_task(struct task_struct *p)
{
 unsigned long flags;
 if (freezer_should_skip(p))
  return false;

 spin_lock_irqsave(&freezer_lock, flags);
 if (!freezing(p) || frozen(p)) {
  spin_unlock_irqrestore(&freezer_lock, flags);
  return false;
 }

 if (!(p->flags & PF_KTHREAD))
  fake_signal_wake_up(p);
 else
  wake_up_state(p, TASK_INTERRUPTIBLE);

 spin_unlock_irqrestore(&freezer_lock, flags);
 return true;
}

void __thaw_task(struct task_struct *p)
{
 unsigned long flags;

 spin_lock_irqsave(&freezer_lock, flags);
 if (frozen(p))
  wake_up_process(p);
 spin_unlock_irqrestore(&freezer_lock, flags);
}






bool set_freezable(void)
{
 might_sleep();






 spin_lock_irq(&freezer_lock);
 current->flags &= ~PF_NOFREEZE;
 spin_unlock_irq(&freezer_lock);

 return try_to_freeze();
}
EXPORT_SYMBOL(set_freezable);


int __read_mostly futex_cmpxchg_enabled;









struct futex_pi_state {




 struct list_head list;




 struct rt_mutex pi_mutex;

 struct task_struct *owner;
 atomic_t refcount;

 union futex_key key;
};
struct futex_q {
 struct plist_node list;

 struct task_struct *task;
 spinlock_t *lock_ptr;
 union futex_key key;
 struct futex_pi_state *pi_state;
 struct rt_mutex_waiter *rt_waiter;
 union futex_key *requeue_pi_key;
 u32 bitset;
};

static const struct futex_q futex_q_init = {

 .key = FUTEX_KEY_INIT,
 .bitset = FUTEX_BITSET_MATCH_ANY
};






struct futex_hash_bucket {
 atomic_t waiters;
 spinlock_t lock;
 struct plist_head chain;
} ____cacheline_aligned_in_smp;






static struct {
 struct futex_hash_bucket *queues;
 unsigned long hashsize;
} __futex_data __read_mostly __aligned(2*sizeof(long));






static struct {
 struct fault_attr attr;

 bool ignore_private;
} fail_futex = {
 .attr = FAULT_ATTR_INITIALIZER,
 .ignore_private = false,
};

static int __init setup_fail_futex(char *str)
{
 return setup_fault_attr(&fail_futex.attr, str);
}
__setup("fail_futex=", setup_fail_futex);

static bool should_fail_futex(bool fshared)
{
 if (fail_futex.ignore_private && !fshared)
  return false;

 return should_fail(&fail_futex.attr, 1);
}


static int __init fail_futex_debugfs(void)
{
 umode_t mode = S_IFREG | S_IRUSR | S_IWUSR;
 struct dentry *dir;

 dir = fault_create_debugfs_attr("fail_futex", NULL,
     &fail_futex.attr);
 if (IS_ERR(dir))
  return PTR_ERR(dir);

 if (!debugfs_create_bool("ignore-private", mode, dir,
     &fail_futex.ignore_private)) {
  debugfs_remove_recursive(dir);
  return -ENOMEM;
 }

 return 0;
}

late_initcall(fail_futex_debugfs);


static inline bool should_fail_futex(bool fshared)
{
 return false;
}

static inline void futex_get_mm(union futex_key *key)
{
 atomic_inc(&key->private.mm->mm_count);





 smp_mb__after_atomic();
}




static inline void hb_waiters_inc(struct futex_hash_bucket *hb)
{
 atomic_inc(&hb->waiters);



 smp_mb__after_atomic();
}





static inline void hb_waiters_dec(struct futex_hash_bucket *hb)
{
 atomic_dec(&hb->waiters);
}

static inline int hb_waiters_pending(struct futex_hash_bucket *hb)
{
 return atomic_read(&hb->waiters);
 return 1;
}




static struct futex_hash_bucket *hash_futex(union futex_key *key)
{
 u32 hash = jhash2((u32*)&key->both.word,
     (sizeof(key->both.word)+sizeof(key->both.ptr))/4,
     key->both.offset);
 return &futex_queues[hash & (futex_hashsize - 1)];
}




static inline int match_futex(union futex_key *key1, union futex_key *key2)
{
 return (key1 && key2
  && key1->both.word == key2->both.word
  && key1->both.ptr == key2->both.ptr
  && key1->both.offset == key2->both.offset);
}






static void get_futex_key_refs(union futex_key *key)
{
 if (!key->both.ptr)
  return;

 switch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {
 case FUT_OFF_INODE:
  ihold(key->shared.inode);
  break;
 case FUT_OFF_MMSHARED:
  futex_get_mm(key);
  break;
 default:





  smp_mb();
 }
}







static void drop_futex_key_refs(union futex_key *key)
{
 if (!key->both.ptr) {

  WARN_ON_ONCE(1);
  return;
 }

 switch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {
 case FUT_OFF_INODE:
  iput(key->shared.inode);
  break;
 case FUT_OFF_MMSHARED:
  mmdrop(key->private.mm);
  break;
 }
}
static int
get_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)
{
 unsigned long address = (unsigned long)uaddr;
 struct mm_struct *mm = current->mm;
 struct page *page, *tail;
 struct address_space *mapping;
 int err, ro = 0;




 key->both.offset = address % PAGE_SIZE;
 if (unlikely((address % sizeof(u32)) != 0))
  return -EINVAL;
 address -= key->both.offset;

 if (unlikely(!access_ok(rw, uaddr, sizeof(u32))))
  return -EFAULT;

 if (unlikely(should_fail_futex(fshared)))
  return -EFAULT;
 if (!fshared) {
  key->private.mm = mm;
  key->private.address = address;
  get_futex_key_refs(key);
  return 0;
 }

again:

 if (unlikely(should_fail_futex(fshared)))
  return -EFAULT;

 err = get_user_pages_fast(address, 1, 1, &page);




 if (err == -EFAULT && rw == VERIFY_READ) {
  err = get_user_pages_fast(address, 1, 0, &page);
  ro = 1;
 }
 if (err < 0)
  return err;
 else
  err = 0;
 tail = page;
 page = compound_head(page);
 mapping = READ_ONCE(page->mapping);
 if (unlikely(!mapping)) {
  int shmem_swizzled;






  lock_page(page);
  shmem_swizzled = PageSwapCache(page) || page->mapping;
  unlock_page(page);
  put_page(page);

  if (shmem_swizzled)
   goto again;

  return -EFAULT;
 }
 if (PageAnon(page)) {




  if (unlikely(should_fail_futex(fshared)) || ro) {
   err = -EFAULT;
   goto out;
  }

  key->both.offset |= FUT_OFF_MMSHARED;
  key->private.mm = mm;
  key->private.address = address;

  get_futex_key_refs(key);

 } else {
  struct inode *inode;
  rcu_read_lock();

  if (READ_ONCE(page->mapping) != mapping) {
   rcu_read_unlock();
   put_page(page);

   goto again;
  }

  inode = READ_ONCE(mapping->host);
  if (!inode) {
   rcu_read_unlock();
   put_page(page);

   goto again;
  }
  if (WARN_ON_ONCE(!atomic_inc_not_zero(&inode->i_count))) {
   rcu_read_unlock();
   put_page(page);

   goto again;
  }


  if (WARN_ON_ONCE(inode->i_mapping != mapping)) {
   err = -EFAULT;
   rcu_read_unlock();
   iput(inode);

   goto out;
  }

  key->both.offset |= FUT_OFF_INODE;
  key->shared.inode = inode;
  key->shared.pgoff = basepage_index(tail);
  rcu_read_unlock();
 }

out:
 put_page(page);
 return err;
}

static inline void put_futex_key(union futex_key *key)
{
 drop_futex_key_refs(key);
}
static int fault_in_user_writeable(u32 __user *uaddr)
{
 struct mm_struct *mm = current->mm;
 int ret;

 down_read(&mm->mmap_sem);
 ret = fixup_user_fault(current, mm, (unsigned long)uaddr,
          FAULT_FLAG_WRITE, NULL);
 up_read(&mm->mmap_sem);

 return ret < 0 ? ret : 0;
}
static struct futex_q *futex_top_waiter(struct futex_hash_bucket *hb,
     union futex_key *key)
{
 struct futex_q *this;

 plist_for_each_entry(this, &hb->chain, list) {
  if (match_futex(&this->key, key))
   return this;
 }
 return NULL;
}

static int cmpxchg_futex_value_locked(u32 *curval, u32 __user *uaddr,
          u32 uval, u32 newval)
{
 int ret;

 pagefault_disable();
 ret = futex_atomic_cmpxchg_inatomic(curval, uaddr, uval, newval);
 pagefault_enable();

 return ret;
}

static int get_futex_value_locked(u32 *dest, u32 __user *from)
{
 int ret;

 pagefault_disable();
 ret = __get_user(*dest, from);
 pagefault_enable();

 return ret ? -EFAULT : 0;
}





static int refill_pi_state_cache(void)
{
 struct futex_pi_state *pi_state;

 if (likely(current->pi_state_cache))
  return 0;

 pi_state = kzalloc(sizeof(*pi_state), GFP_KERNEL);

 if (!pi_state)
  return -ENOMEM;

 INIT_LIST_HEAD(&pi_state->list);

 pi_state->owner = NULL;
 atomic_set(&pi_state->refcount, 1);
 pi_state->key = FUTEX_KEY_INIT;

 current->pi_state_cache = pi_state;

 return 0;
}

static struct futex_pi_state * alloc_pi_state(void)
{
 struct futex_pi_state *pi_state = current->pi_state_cache;

 WARN_ON(!pi_state);
 current->pi_state_cache = NULL;

 return pi_state;
}







static void put_pi_state(struct futex_pi_state *pi_state)
{
 if (!pi_state)
  return;

 if (!atomic_dec_and_test(&pi_state->refcount))
  return;





 if (pi_state->owner) {
  raw_spin_lock_irq(&pi_state->owner->pi_lock);
  list_del_init(&pi_state->list);
  raw_spin_unlock_irq(&pi_state->owner->pi_lock);

  rt_mutex_proxy_unlock(&pi_state->pi_mutex, pi_state->owner);
 }

 if (current->pi_state_cache)
  kfree(pi_state);
 else {





  pi_state->owner = NULL;
  atomic_set(&pi_state->refcount, 1);
  current->pi_state_cache = pi_state;
 }
}





static struct task_struct * futex_find_get_task(pid_t pid)
{
 struct task_struct *p;

 rcu_read_lock();
 p = find_task_by_vpid(pid);
 if (p)
  get_task_struct(p);

 rcu_read_unlock();

 return p;
}






void exit_pi_state_list(struct task_struct *curr)
{
 struct list_head *next, *head = &curr->pi_state_list;
 struct futex_pi_state *pi_state;
 struct futex_hash_bucket *hb;
 union futex_key key = FUTEX_KEY_INIT;

 if (!futex_cmpxchg_enabled)
  return;





 raw_spin_lock_irq(&curr->pi_lock);
 while (!list_empty(head)) {

  next = head->next;
  pi_state = list_entry(next, struct futex_pi_state, list);
  key = pi_state->key;
  hb = hash_futex(&key);
  raw_spin_unlock_irq(&curr->pi_lock);

  spin_lock(&hb->lock);

  raw_spin_lock_irq(&curr->pi_lock);




  if (head->next != next) {
   spin_unlock(&hb->lock);
   continue;
  }

  WARN_ON(pi_state->owner != curr);
  WARN_ON(list_empty(&pi_state->list));
  list_del_init(&pi_state->list);
  pi_state->owner = NULL;
  raw_spin_unlock_irq(&curr->pi_lock);

  rt_mutex_unlock(&pi_state->pi_mutex);

  spin_unlock(&hb->lock);

  raw_spin_lock_irq(&curr->pi_lock);
 }
 raw_spin_unlock_irq(&curr->pi_lock);
}
static int attach_to_pi_state(u32 uval, struct futex_pi_state *pi_state,
         struct futex_pi_state **ps)
{
 pid_t pid = uval & FUTEX_TID_MASK;




 if (unlikely(!pi_state))
  return -EINVAL;

 WARN_ON(!atomic_read(&pi_state->refcount));




 if (uval & FUTEX_OWNER_DIED) {





  if (!pi_state->owner) {




   if (pid)
    return -EINVAL;



   goto out_state;
  }
  if (!pid)
   goto out_state;
 } else {




  if (!pi_state->owner)
   return -EINVAL;
 }






 if (pid != task_pid_vnr(pi_state->owner))
  return -EINVAL;
out_state:
 atomic_inc(&pi_state->refcount);
 *ps = pi_state;
 return 0;
}





static int attach_to_pi_owner(u32 uval, union futex_key *key,
         struct futex_pi_state **ps)
{
 pid_t pid = uval & FUTEX_TID_MASK;
 struct futex_pi_state *pi_state;
 struct task_struct *p;





 if (!pid)
  return -ESRCH;
 p = futex_find_get_task(pid);
 if (!p)
  return -ESRCH;

 if (unlikely(p->flags & PF_KTHREAD)) {
  put_task_struct(p);
  return -EPERM;
 }







 raw_spin_lock_irq(&p->pi_lock);
 if (unlikely(p->flags & PF_EXITING)) {





  int ret = (p->flags & PF_EXITPIDONE) ? -ESRCH : -EAGAIN;

  raw_spin_unlock_irq(&p->pi_lock);
  put_task_struct(p);
  return ret;
 }




 pi_state = alloc_pi_state();





 rt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);


 pi_state->key = *key;

 WARN_ON(!list_empty(&pi_state->list));
 list_add(&pi_state->list, &p->pi_state_list);
 pi_state->owner = p;
 raw_spin_unlock_irq(&p->pi_lock);

 put_task_struct(p);

 *ps = pi_state;

 return 0;
}

static int lookup_pi_state(u32 uval, struct futex_hash_bucket *hb,
      union futex_key *key, struct futex_pi_state **ps)
{
 struct futex_q *match = futex_top_waiter(hb, key);





 if (match)
  return attach_to_pi_state(uval, match->pi_state, ps);





 return attach_to_pi_owner(uval, key, ps);
}

static int lock_pi_update_atomic(u32 __user *uaddr, u32 uval, u32 newval)
{
 u32 uninitialized_var(curval);

 if (unlikely(should_fail_futex(true)))
  return -EFAULT;

 if (unlikely(cmpxchg_futex_value_locked(&curval, uaddr, uval, newval)))
  return -EFAULT;


 return curval != uval ? -EAGAIN : 0;
}
static int futex_lock_pi_atomic(u32 __user *uaddr, struct futex_hash_bucket *hb,
    union futex_key *key,
    struct futex_pi_state **ps,
    struct task_struct *task, int set_waiters)
{
 u32 uval, newval, vpid = task_pid_vnr(task);
 struct futex_q *match;
 int ret;





 if (get_futex_value_locked(&uval, uaddr))
  return -EFAULT;

 if (unlikely(should_fail_futex(true)))
  return -EFAULT;




 if ((unlikely((uval & FUTEX_TID_MASK) == vpid)))
  return -EDEADLK;

 if ((unlikely(should_fail_futex(true))))
  return -EDEADLK;





 match = futex_top_waiter(hb, key);
 if (match)
  return attach_to_pi_state(uval, match->pi_state, ps);







 if (!(uval & FUTEX_TID_MASK)) {




  newval = uval & FUTEX_OWNER_DIED;
  newval |= vpid;


  if (set_waiters)
   newval |= FUTEX_WAITERS;

  ret = lock_pi_update_atomic(uaddr, uval, newval);

  return ret < 0 ? ret : 1;
 }






 newval = uval | FUTEX_WAITERS;
 ret = lock_pi_update_atomic(uaddr, uval, newval);
 if (ret)
  return ret;





 return attach_to_pi_owner(uval, key, ps);
}







static void __unqueue_futex(struct futex_q *q)
{
 struct futex_hash_bucket *hb;

 if (WARN_ON_SMP(!q->lock_ptr || !spin_is_locked(q->lock_ptr))
     || WARN_ON(plist_node_empty(&q->list)))
  return;

 hb = container_of(q->lock_ptr, struct futex_hash_bucket, lock);
 plist_del(&q->list, &hb->chain);
 hb_waiters_dec(hb);
}







static void mark_wake_futex(struct wake_q_head *wake_q, struct futex_q *q)
{
 struct task_struct *p = q->task;

 if (WARN(q->pi_state || q->rt_waiter, "refusing to wake PI futex\n"))
  return;





 wake_q_add(wake_q, p);
 __unqueue_futex(q);






 smp_wmb();
 q->lock_ptr = NULL;
}

static int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_q *this,
    struct futex_hash_bucket *hb)
{
 struct task_struct *new_owner;
 struct futex_pi_state *pi_state = this->pi_state;
 u32 uninitialized_var(curval), newval;
 WAKE_Q(wake_q);
 bool deboost;
 int ret = 0;

 if (!pi_state)
  return -EINVAL;





 if (pi_state->owner != current)
  return -EINVAL;

 raw_spin_lock_irq(&pi_state->pi_mutex.wait_lock);
 new_owner = rt_mutex_next_owner(&pi_state->pi_mutex);






 if (!new_owner)
  new_owner = this->task;






 newval = FUTEX_WAITERS | task_pid_vnr(new_owner);

 if (unlikely(should_fail_futex(true)))
  ret = -EFAULT;

 if (cmpxchg_futex_value_locked(&curval, uaddr, uval, newval)) {
  ret = -EFAULT;
 } else if (curval != uval) {






  if ((FUTEX_TID_MASK & curval) == uval)
   ret = -EAGAIN;
  else
   ret = -EINVAL;
 }
 if (ret) {
  raw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);
  return ret;
 }

 raw_spin_lock(&pi_state->owner->pi_lock);
 WARN_ON(list_empty(&pi_state->list));
 list_del_init(&pi_state->list);
 raw_spin_unlock(&pi_state->owner->pi_lock);

 raw_spin_lock(&new_owner->pi_lock);
 WARN_ON(!list_empty(&pi_state->list));
 list_add(&pi_state->list, &new_owner->pi_state_list);
 pi_state->owner = new_owner;
 raw_spin_unlock(&new_owner->pi_lock);

 raw_spin_unlock_irq(&pi_state->pi_mutex.wait_lock);

 deboost = rt_mutex_futex_unlock(&pi_state->pi_mutex, &wake_q);







 spin_unlock(&hb->lock);
 wake_up_q(&wake_q);
 if (deboost)
  rt_mutex_adjust_prio(current);

 return 0;
}




static inline void
double_lock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)
{
 if (hb1 <= hb2) {
  spin_lock(&hb1->lock);
  if (hb1 < hb2)
   spin_lock_nested(&hb2->lock, SINGLE_DEPTH_NESTING);
 } else {
  spin_lock(&hb2->lock);
  spin_lock_nested(&hb1->lock, SINGLE_DEPTH_NESTING);
 }
}

static inline void
double_unlock_hb(struct futex_hash_bucket *hb1, struct futex_hash_bucket *hb2)
{
 spin_unlock(&hb1->lock);
 if (hb1 != hb2)
  spin_unlock(&hb2->lock);
}




static int
futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)
{
 struct futex_hash_bucket *hb;
 struct futex_q *this, *next;
 union futex_key key = FUTEX_KEY_INIT;
 int ret;
 WAKE_Q(wake_q);

 if (!bitset)
  return -EINVAL;

 ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_READ);
 if (unlikely(ret != 0))
  goto out;

 hb = hash_futex(&key);


 if (!hb_waiters_pending(hb))
  goto out_put_key;

 spin_lock(&hb->lock);

 plist_for_each_entry_safe(this, next, &hb->chain, list) {
  if (match_futex (&this->key, &key)) {
   if (this->pi_state || this->rt_waiter) {
    ret = -EINVAL;
    break;
   }


   if (!(this->bitset & bitset))
    continue;

   mark_wake_futex(&wake_q, this);
   if (++ret >= nr_wake)
    break;
  }
 }

 spin_unlock(&hb->lock);
 wake_up_q(&wake_q);
out_put_key:
 put_futex_key(&key);
out:
 return ret;
}





static int
futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,
       int nr_wake, int nr_wake2, int op)
{
 union futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;
 struct futex_hash_bucket *hb1, *hb2;
 struct futex_q *this, *next;
 int ret, op_ret;
 WAKE_Q(wake_q);

retry:
 ret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);
 if (unlikely(ret != 0))
  goto out;
 ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);
 if (unlikely(ret != 0))
  goto out_put_key1;

 hb1 = hash_futex(&key1);
 hb2 = hash_futex(&key2);

retry_private:
 double_lock_hb(hb1, hb2);
 op_ret = futex_atomic_op_inuser(op, uaddr2);
 if (unlikely(op_ret < 0)) {

  double_unlock_hb(hb1, hb2);





  ret = op_ret;
  goto out_put_keys;

  if (unlikely(op_ret != -EFAULT)) {
   ret = op_ret;
   goto out_put_keys;
  }

  ret = fault_in_user_writeable(uaddr2);
  if (ret)
   goto out_put_keys;

  if (!(flags & FLAGS_SHARED))
   goto retry_private;

  put_futex_key(&key2);
  put_futex_key(&key1);
  goto retry;
 }

 plist_for_each_entry_safe(this, next, &hb1->chain, list) {
  if (match_futex (&this->key, &key1)) {
   if (this->pi_state || this->rt_waiter) {
    ret = -EINVAL;
    goto out_unlock;
   }
   mark_wake_futex(&wake_q, this);
   if (++ret >= nr_wake)
    break;
  }
 }

 if (op_ret > 0) {
  op_ret = 0;
  plist_for_each_entry_safe(this, next, &hb2->chain, list) {
   if (match_futex (&this->key, &key2)) {
    if (this->pi_state || this->rt_waiter) {
     ret = -EINVAL;
     goto out_unlock;
    }
    mark_wake_futex(&wake_q, this);
    if (++op_ret >= nr_wake2)
     break;
   }
  }
  ret += op_ret;
 }

out_unlock:
 double_unlock_hb(hb1, hb2);
 wake_up_q(&wake_q);
out_put_keys:
 put_futex_key(&key2);
out_put_key1:
 put_futex_key(&key1);
out:
 return ret;
}
static inline
void requeue_futex(struct futex_q *q, struct futex_hash_bucket *hb1,
     struct futex_hash_bucket *hb2, union futex_key *key2)
{





 if (likely(&hb1->chain != &hb2->chain)) {
  plist_del(&q->list, &hb1->chain);
  hb_waiters_dec(hb1);
  hb_waiters_inc(hb2);
  plist_add(&q->list, &hb2->chain);
  q->lock_ptr = &hb2->lock;
 }
 get_futex_key_refs(key2);
 q->key = *key2;
}
static inline
void requeue_pi_wake_futex(struct futex_q *q, union futex_key *key,
      struct futex_hash_bucket *hb)
{
 get_futex_key_refs(key);
 q->key = *key;

 __unqueue_futex(q);

 WARN_ON(!q->rt_waiter);
 q->rt_waiter = NULL;

 q->lock_ptr = &hb->lock;

 wake_up_state(q->task, TASK_NORMAL);
}
static int futex_proxy_trylock_atomic(u32 __user *pifutex,
     struct futex_hash_bucket *hb1,
     struct futex_hash_bucket *hb2,
     union futex_key *key1, union futex_key *key2,
     struct futex_pi_state **ps, int set_waiters)
{
 struct futex_q *top_waiter = NULL;
 u32 curval;
 int ret, vpid;

 if (get_futex_value_locked(&curval, pifutex))
  return -EFAULT;

 if (unlikely(should_fail_futex(true)))
  return -EFAULT;
 top_waiter = futex_top_waiter(hb1, key1);


 if (!top_waiter)
  return 0;


 if (!match_futex(top_waiter->requeue_pi_key, key2))
  return -EINVAL;






 vpid = task_pid_vnr(top_waiter->task);
 ret = futex_lock_pi_atomic(pifutex, hb2, key2, ps, top_waiter->task,
       set_waiters);
 if (ret == 1) {
  requeue_pi_wake_futex(top_waiter, key2, hb2);
  return vpid;
 }
 return ret;
}
static int futex_requeue(u32 __user *uaddr1, unsigned int flags,
    u32 __user *uaddr2, int nr_wake, int nr_requeue,
    u32 *cmpval, int requeue_pi)
{
 union futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;
 int drop_count = 0, task_count = 0, ret;
 struct futex_pi_state *pi_state = NULL;
 struct futex_hash_bucket *hb1, *hb2;
 struct futex_q *this, *next;
 WAKE_Q(wake_q);

 if (requeue_pi) {




  if (uaddr1 == uaddr2)
   return -EINVAL;





  if (refill_pi_state_cache())
   return -ENOMEM;
  if (nr_wake != 1)
   return -EINVAL;
 }

retry:
 ret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);
 if (unlikely(ret != 0))
  goto out;
 ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2,
       requeue_pi ? VERIFY_WRITE : VERIFY_READ);
 if (unlikely(ret != 0))
  goto out_put_key1;





 if (requeue_pi && match_futex(&key1, &key2)) {
  ret = -EINVAL;
  goto out_put_keys;
 }

 hb1 = hash_futex(&key1);
 hb2 = hash_futex(&key2);

retry_private:
 hb_waiters_inc(hb2);
 double_lock_hb(hb1, hb2);

 if (likely(cmpval != NULL)) {
  u32 curval;

  ret = get_futex_value_locked(&curval, uaddr1);

  if (unlikely(ret)) {
   double_unlock_hb(hb1, hb2);
   hb_waiters_dec(hb2);

   ret = get_user(curval, uaddr1);
   if (ret)
    goto out_put_keys;

   if (!(flags & FLAGS_SHARED))
    goto retry_private;

   put_futex_key(&key2);
   put_futex_key(&key1);
   goto retry;
  }
  if (curval != *cmpval) {
   ret = -EAGAIN;
   goto out_unlock;
  }
 }

 if (requeue_pi && (task_count - nr_wake < nr_requeue)) {






  ret = futex_proxy_trylock_atomic(uaddr2, hb1, hb2, &key1,
       &key2, &pi_state, nr_requeue);
  if (ret > 0) {
   WARN_ON(pi_state);
   drop_count++;
   task_count++;
   ret = lookup_pi_state(ret, hb2, &key2, &pi_state);
  }

  switch (ret) {
  case 0:

   break;


  case -EFAULT:
   double_unlock_hb(hb1, hb2);
   hb_waiters_dec(hb2);
   put_futex_key(&key2);
   put_futex_key(&key1);
   ret = fault_in_user_writeable(uaddr2);
   if (!ret)
    goto retry;
   goto out;
  case -EAGAIN:






   double_unlock_hb(hb1, hb2);
   hb_waiters_dec(hb2);
   put_futex_key(&key2);
   put_futex_key(&key1);
   cond_resched();
   goto retry;
  default:
   goto out_unlock;
  }
 }

 plist_for_each_entry_safe(this, next, &hb1->chain, list) {
  if (task_count - nr_wake >= nr_requeue)
   break;

  if (!match_futex(&this->key, &key1))
   continue;
  if ((requeue_pi && !this->rt_waiter) ||
      (!requeue_pi && this->rt_waiter) ||
      this->pi_state) {
   ret = -EINVAL;
   break;
  }






  if (++task_count <= nr_wake && !requeue_pi) {
   mark_wake_futex(&wake_q, this);
   continue;
  }


  if (requeue_pi && !match_futex(this->requeue_pi_key, &key2)) {
   ret = -EINVAL;
   break;
  }





  if (requeue_pi) {





   atomic_inc(&pi_state->refcount);
   this->pi_state = pi_state;
   ret = rt_mutex_start_proxy_lock(&pi_state->pi_mutex,
       this->rt_waiter,
       this->task);
   if (ret == 1) {
    requeue_pi_wake_futex(this, &key2, hb2);
    drop_count++;
    continue;
   } else if (ret) {
    this->pi_state = NULL;
    put_pi_state(pi_state);




    break;
   }
  }
  requeue_futex(this, hb1, hb2, &key2);
  drop_count++;
 }






 put_pi_state(pi_state);

out_unlock:
 double_unlock_hb(hb1, hb2);
 wake_up_q(&wake_q);
 hb_waiters_dec(hb2);







 while (--drop_count >= 0)
  drop_futex_key_refs(&key1);

out_put_keys:
 put_futex_key(&key2);
out_put_key1:
 put_futex_key(&key1);
out:
 return ret ? ret : task_count;
}


static inline struct futex_hash_bucket *queue_lock(struct futex_q *q)
 __acquires(&hb->lock)
{
 struct futex_hash_bucket *hb;

 hb = hash_futex(&q->key);
 hb_waiters_inc(hb);

 q->lock_ptr = &hb->lock;

 spin_lock(&hb->lock);
 return hb;
}

static inline void
queue_unlock(struct futex_hash_bucket *hb)
 __releases(&hb->lock)
{
 spin_unlock(&hb->lock);
 hb_waiters_dec(hb);
}
static inline void queue_me(struct futex_q *q, struct futex_hash_bucket *hb)
 __releases(&hb->lock)
{
 int prio;
 prio = min(current->normal_prio, MAX_RT_PRIO);

 plist_node_init(&q->list, prio);
 plist_add(&q->list, &hb->chain);
 q->task = current;
 spin_unlock(&hb->lock);
}
static int unqueue_me(struct futex_q *q)
{
 spinlock_t *lock_ptr;
 int ret = 0;


retry:





 lock_ptr = READ_ONCE(q->lock_ptr);
 if (lock_ptr != NULL) {
  spin_lock(lock_ptr);
  if (unlikely(lock_ptr != q->lock_ptr)) {
   spin_unlock(lock_ptr);
   goto retry;
  }
  __unqueue_futex(q);

  BUG_ON(q->pi_state);

  spin_unlock(lock_ptr);
  ret = 1;
 }

 drop_futex_key_refs(&q->key);
 return ret;
}






static void unqueue_me_pi(struct futex_q *q)
 __releases(q->lock_ptr)
{
 __unqueue_futex(q);

 BUG_ON(!q->pi_state);
 put_pi_state(q->pi_state);
 q->pi_state = NULL;

 spin_unlock(q->lock_ptr);
}







static int fixup_pi_state_owner(u32 __user *uaddr, struct futex_q *q,
    struct task_struct *newowner)
{
 u32 newtid = task_pid_vnr(newowner) | FUTEX_WAITERS;
 struct futex_pi_state *pi_state = q->pi_state;
 struct task_struct *oldowner = pi_state->owner;
 u32 uval, uninitialized_var(curval), newval;
 int ret;


 if (!pi_state->owner)
  newtid |= FUTEX_OWNER_DIED;
retry:
 if (get_futex_value_locked(&uval, uaddr))
  goto handle_fault;

 while (1) {
  newval = (uval & FUTEX_OWNER_DIED) | newtid;

  if (cmpxchg_futex_value_locked(&curval, uaddr, uval, newval))
   goto handle_fault;
  if (curval == uval)
   break;
  uval = curval;
 }





 if (pi_state->owner != NULL) {
  raw_spin_lock_irq(&pi_state->owner->pi_lock);
  WARN_ON(list_empty(&pi_state->list));
  list_del_init(&pi_state->list);
  raw_spin_unlock_irq(&pi_state->owner->pi_lock);
 }

 pi_state->owner = newowner;

 raw_spin_lock_irq(&newowner->pi_lock);
 WARN_ON(!list_empty(&pi_state->list));
 list_add(&pi_state->list, &newowner->pi_state_list);
 raw_spin_unlock_irq(&newowner->pi_lock);
 return 0;
handle_fault:
 spin_unlock(q->lock_ptr);

 ret = fault_in_user_writeable(uaddr);

 spin_lock(q->lock_ptr);




 if (pi_state->owner != oldowner)
  return 0;

 if (ret)
  return ret;

 goto retry;
}

static long futex_wait_restart(struct restart_block *restart);
static int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)
{
 struct task_struct *owner;
 int ret = 0;

 if (locked) {




  if (q->pi_state->owner != current)
   ret = fixup_pi_state_owner(uaddr, q, current);
  goto out;
 }





 if (q->pi_state->owner == current) {





  if (rt_mutex_trylock(&q->pi_state->pi_mutex)) {
   locked = 1;
   goto out;
  }






  raw_spin_lock_irq(&q->pi_state->pi_mutex.wait_lock);
  owner = rt_mutex_owner(&q->pi_state->pi_mutex);
  if (!owner)
   owner = rt_mutex_next_owner(&q->pi_state->pi_mutex);
  raw_spin_unlock_irq(&q->pi_state->pi_mutex.wait_lock);
  ret = fixup_pi_state_owner(uaddr, q, owner);
  goto out;
 }





 if (rt_mutex_owner(&q->pi_state->pi_mutex) == current)
  printk(KERN_ERR "fixup_owner: ret = %d pi-mutex: %p "
    "pi-state %p\n", ret,
    q->pi_state->pi_mutex.owner,
    q->pi_state->owner);

out:
 return ret ? ret : locked;
}







static void futex_wait_queue_me(struct futex_hash_bucket *hb, struct futex_q *q,
    struct hrtimer_sleeper *timeout)
{






 set_current_state(TASK_INTERRUPTIBLE);
 queue_me(q, hb);


 if (timeout)
  hrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);





 if (likely(!plist_node_empty(&q->list))) {





  if (!timeout || timeout->task)
   freezable_schedule();
 }
 __set_current_state(TASK_RUNNING);
}
static int futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,
      struct futex_q *q, struct futex_hash_bucket **hb)
{
 u32 uval;
 int ret;
retry:
 ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q->key, VERIFY_READ);
 if (unlikely(ret != 0))
  return ret;

retry_private:
 *hb = queue_lock(q);

 ret = get_futex_value_locked(&uval, uaddr);

 if (ret) {
  queue_unlock(*hb);

  ret = get_user(uval, uaddr);
  if (ret)
   goto out;

  if (!(flags & FLAGS_SHARED))
   goto retry_private;

  put_futex_key(&q->key);
  goto retry;
 }

 if (uval != val) {
  queue_unlock(*hb);
  ret = -EWOULDBLOCK;
 }

out:
 if (ret)
  put_futex_key(&q->key);
 return ret;
}

static int futex_wait(u32 __user *uaddr, unsigned int flags, u32 val,
        ktime_t *abs_time, u32 bitset)
{
 struct hrtimer_sleeper timeout, *to = NULL;
 struct restart_block *restart;
 struct futex_hash_bucket *hb;
 struct futex_q q = futex_q_init;
 int ret;

 if (!bitset)
  return -EINVAL;
 q.bitset = bitset;

 if (abs_time) {
  to = &timeout;

  hrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?
          CLOCK_REALTIME : CLOCK_MONOTONIC,
          HRTIMER_MODE_ABS);
  hrtimer_init_sleeper(to, current);
  hrtimer_set_expires_range_ns(&to->timer, *abs_time,
          current->timer_slack_ns);
 }

retry:




 ret = futex_wait_setup(uaddr, val, flags, &q, &hb);
 if (ret)
  goto out;


 futex_wait_queue_me(hb, &q, to);


 ret = 0;

 if (!unqueue_me(&q))
  goto out;
 ret = -ETIMEDOUT;
 if (to && !to->task)
  goto out;





 if (!signal_pending(current))
  goto retry;

 ret = -ERESTARTSYS;
 if (!abs_time)
  goto out;

 restart = &current->restart_block;
 restart->fn = futex_wait_restart;
 restart->futex.uaddr = uaddr;
 restart->futex.val = val;
 restart->futex.time = abs_time->tv64;
 restart->futex.bitset = bitset;
 restart->futex.flags = flags | FLAGS_HAS_TIMEOUT;

 ret = -ERESTART_RESTARTBLOCK;

out:
 if (to) {
  hrtimer_cancel(&to->timer);
  destroy_hrtimer_on_stack(&to->timer);
 }
 return ret;
}


static long futex_wait_restart(struct restart_block *restart)
{
 u32 __user *uaddr = restart->futex.uaddr;
 ktime_t t, *tp = NULL;

 if (restart->futex.flags & FLAGS_HAS_TIMEOUT) {
  t.tv64 = restart->futex.time;
  tp = &t;
 }
 restart->fn = do_no_restart_syscall;

 return (long)futex_wait(uaddr, restart->futex.flags,
    restart->futex.val, tp, restart->futex.bitset);
}
static int futex_lock_pi(u32 __user *uaddr, unsigned int flags,
    ktime_t *time, int trylock)
{
 struct hrtimer_sleeper timeout, *to = NULL;
 struct futex_hash_bucket *hb;
 struct futex_q q = futex_q_init;
 int res, ret;

 if (refill_pi_state_cache())
  return -ENOMEM;

 if (time) {
  to = &timeout;
  hrtimer_init_on_stack(&to->timer, CLOCK_REALTIME,
          HRTIMER_MODE_ABS);
  hrtimer_init_sleeper(to, current);
  hrtimer_set_expires(&to->timer, *time);
 }

retry:
 ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q.key, VERIFY_WRITE);
 if (unlikely(ret != 0))
  goto out;

retry_private:
 hb = queue_lock(&q);

 ret = futex_lock_pi_atomic(uaddr, hb, &q.key, &q.pi_state, current, 0);
 if (unlikely(ret)) {




  switch (ret) {
  case 1:

   ret = 0;
   goto out_unlock_put_key;
  case -EFAULT:
   goto uaddr_faulted;
  case -EAGAIN:






   queue_unlock(hb);
   put_futex_key(&q.key);
   cond_resched();
   goto retry;
  default:
   goto out_unlock_put_key;
  }
 }




 queue_me(&q, hb);

 WARN_ON(!q.pi_state);



 if (!trylock) {
  ret = rt_mutex_timed_futex_lock(&q.pi_state->pi_mutex, to);
 } else {
  ret = rt_mutex_trylock(&q.pi_state->pi_mutex);

  ret = ret ? 0 : -EWOULDBLOCK;
 }

 spin_lock(q.lock_ptr);




 res = fixup_owner(uaddr, &q, !ret);




 if (res)
  ret = (res < 0) ? res : 0;





 if (ret && (rt_mutex_owner(&q.pi_state->pi_mutex) == current))
  rt_mutex_unlock(&q.pi_state->pi_mutex);


 unqueue_me_pi(&q);

 goto out_put_key;

out_unlock_put_key:
 queue_unlock(hb);

out_put_key:
 put_futex_key(&q.key);
out:
 if (to)
  destroy_hrtimer_on_stack(&to->timer);
 return ret != -EINTR ? ret : -ERESTARTNOINTR;

uaddr_faulted:
 queue_unlock(hb);

 ret = fault_in_user_writeable(uaddr);
 if (ret)
  goto out_put_key;

 if (!(flags & FLAGS_SHARED))
  goto retry_private;

 put_futex_key(&q.key);
 goto retry;
}






static int futex_unlock_pi(u32 __user *uaddr, unsigned int flags)
{
 u32 uninitialized_var(curval), uval, vpid = task_pid_vnr(current);
 union futex_key key = FUTEX_KEY_INIT;
 struct futex_hash_bucket *hb;
 struct futex_q *match;
 int ret;

retry:
 if (get_user(uval, uaddr))
  return -EFAULT;



 if ((uval & FUTEX_TID_MASK) != vpid)
  return -EPERM;

 ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_WRITE);
 if (ret)
  return ret;

 hb = hash_futex(&key);
 spin_lock(&hb->lock);






 match = futex_top_waiter(hb, &key);
 if (match) {
  ret = wake_futex_pi(uaddr, uval, match, hb);




  if (!ret)
   goto out_putkey;




  if (ret == -EFAULT)
   goto pi_faulted;




  if (ret == -EAGAIN) {
   spin_unlock(&hb->lock);
   put_futex_key(&key);
   goto retry;
  }




  goto out_unlock;
 }
 if (cmpxchg_futex_value_locked(&curval, uaddr, uval, 0))
  goto pi_faulted;




 ret = (curval == uval) ? 0 : -EAGAIN;

out_unlock:
 spin_unlock(&hb->lock);
out_putkey:
 put_futex_key(&key);
 return ret;

pi_faulted:
 spin_unlock(&hb->lock);
 put_futex_key(&key);

 ret = fault_in_user_writeable(uaddr);
 if (!ret)
  goto retry;

 return ret;
}
static inline
int handle_early_requeue_pi_wakeup(struct futex_hash_bucket *hb,
       struct futex_q *q, union futex_key *key2,
       struct hrtimer_sleeper *timeout)
{
 int ret = 0;
 if (!match_futex(&q->key, key2)) {
  WARN_ON(q->lock_ptr && (&hb->lock != q->lock_ptr));




  plist_del(&q->list, &hb->chain);
  hb_waiters_dec(hb);


  ret = -EWOULDBLOCK;
  if (timeout && !timeout->task)
   ret = -ETIMEDOUT;
  else if (signal_pending(current))
   ret = -ERESTARTNOINTR;
 }
 return ret;
}
static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
     u32 val, ktime_t *abs_time, u32 bitset,
     u32 __user *uaddr2)
{
 struct hrtimer_sleeper timeout, *to = NULL;
 struct rt_mutex_waiter rt_waiter;
 struct rt_mutex *pi_mutex = NULL;
 struct futex_hash_bucket *hb;
 union futex_key key2 = FUTEX_KEY_INIT;
 struct futex_q q = futex_q_init;
 int res, ret;

 if (uaddr == uaddr2)
  return -EINVAL;

 if (!bitset)
  return -EINVAL;

 if (abs_time) {
  to = &timeout;
  hrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?
          CLOCK_REALTIME : CLOCK_MONOTONIC,
          HRTIMER_MODE_ABS);
  hrtimer_init_sleeper(to, current);
  hrtimer_set_expires_range_ns(&to->timer, *abs_time,
          current->timer_slack_ns);
 }





 debug_rt_mutex_init_waiter(&rt_waiter);
 RB_CLEAR_NODE(&rt_waiter.pi_tree_entry);
 RB_CLEAR_NODE(&rt_waiter.tree_entry);
 rt_waiter.task = NULL;

 ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);
 if (unlikely(ret != 0))
  goto out;

 q.bitset = bitset;
 q.rt_waiter = &rt_waiter;
 q.requeue_pi_key = &key2;





 ret = futex_wait_setup(uaddr, val, flags, &q, &hb);
 if (ret)
  goto out_key2;





 if (match_futex(&q.key, &key2)) {
  queue_unlock(hb);
  ret = -EINVAL;
  goto out_put_keys;
 }


 futex_wait_queue_me(hb, &q, to);

 spin_lock(&hb->lock);
 ret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);
 spin_unlock(&hb->lock);
 if (ret)
  goto out_put_keys;
 if (!q.rt_waiter) {




  if (q.pi_state && (q.pi_state->owner != current)) {
   spin_lock(q.lock_ptr);
   ret = fixup_pi_state_owner(uaddr2, &q, current);




   put_pi_state(q.pi_state);
   spin_unlock(q.lock_ptr);
  }
 } else {





  WARN_ON(!q.pi_state);
  pi_mutex = &q.pi_state->pi_mutex;
  ret = rt_mutex_finish_proxy_lock(pi_mutex, to, &rt_waiter);
  debug_rt_mutex_free_waiter(&rt_waiter);

  spin_lock(q.lock_ptr);




  res = fixup_owner(uaddr2, &q, !ret);




  if (res)
   ret = (res < 0) ? res : 0;


  unqueue_me_pi(&q);
 }





 if (ret == -EFAULT) {
  if (pi_mutex && rt_mutex_owner(pi_mutex) == current)
   rt_mutex_unlock(pi_mutex);
 } else if (ret == -EINTR) {







  ret = -EWOULDBLOCK;
 }

out_put_keys:
 put_futex_key(&q.key);
out_key2:
 put_futex_key(&key2);

out:
 if (to) {
  hrtimer_cancel(&to->timer);
  destroy_hrtimer_on_stack(&to->timer);
 }
 return ret;
}
SYSCALL_DEFINE2(set_robust_list, struct robust_list_head __user *, head,
  size_t, len)
{
 if (!futex_cmpxchg_enabled)
  return -ENOSYS;



 if (unlikely(len != sizeof(*head)))
  return -EINVAL;

 current->robust_list = head;

 return 0;
}







SYSCALL_DEFINE3(get_robust_list, int, pid,
  struct robust_list_head __user * __user *, head_ptr,
  size_t __user *, len_ptr)
{
 struct robust_list_head __user *head;
 unsigned long ret;
 struct task_struct *p;

 if (!futex_cmpxchg_enabled)
  return -ENOSYS;

 rcu_read_lock();

 ret = -ESRCH;
 if (!pid)
  p = current;
 else {
  p = find_task_by_vpid(pid);
  if (!p)
   goto err_unlock;
 }

 ret = -EPERM;
 if (!ptrace_may_access(p, PTRACE_MODE_READ_REALCREDS))
  goto err_unlock;

 head = p->robust_list;
 rcu_read_unlock();

 if (put_user(sizeof(*head), len_ptr))
  return -EFAULT;
 return put_user(head, head_ptr);

err_unlock:
 rcu_read_unlock();

 return ret;
}





int handle_futex_death(u32 __user *uaddr, struct task_struct *curr, int pi)
{
 u32 uval, uninitialized_var(nval), mval;

retry:
 if (get_user(uval, uaddr))
  return -1;

 if ((uval & FUTEX_TID_MASK) == task_pid_vnr(curr)) {
  mval = (uval & FUTEX_WAITERS) | FUTEX_OWNER_DIED;
  if (cmpxchg_futex_value_locked(&nval, uaddr, uval, mval)) {
   if (fault_in_user_writeable(uaddr))
    return -1;
   goto retry;
  }
  if (nval != uval)
   goto retry;





  if (!pi && (uval & FUTEX_WAITERS))
   futex_wake(uaddr, 1, 1, FUTEX_BITSET_MATCH_ANY);
 }
 return 0;
}




static inline int fetch_robust_entry(struct robust_list __user **entry,
         struct robust_list __user * __user *head,
         unsigned int *pi)
{
 unsigned long uentry;

 if (get_user(uentry, (unsigned long __user *)head))
  return -EFAULT;

 *entry = (void __user *)(uentry & ~1UL);
 *pi = uentry & 1;

 return 0;
}







void exit_robust_list(struct task_struct *curr)
{
 struct robust_list_head __user *head = curr->robust_list;
 struct robust_list __user *entry, *next_entry, *pending;
 unsigned int limit = ROBUST_LIST_LIMIT, pi, pip;
 unsigned int uninitialized_var(next_pi);
 unsigned long futex_offset;
 int rc;

 if (!futex_cmpxchg_enabled)
  return;





 if (fetch_robust_entry(&entry, &head->list.next, &pi))
  return;



 if (get_user(futex_offset, &head->futex_offset))
  return;




 if (fetch_robust_entry(&pending, &head->list_op_pending, &pip))
  return;

 next_entry = NULL;
 while (entry != &head->list) {




  rc = fetch_robust_entry(&next_entry, &entry->next, &next_pi);




  if (entry != pending)
   if (handle_futex_death((void __user *)entry + futex_offset,
      curr, pi))
    return;
  if (rc)
   return;
  entry = next_entry;
  pi = next_pi;



  if (!--limit)
   break;

  cond_resched();
 }

 if (pending)
  handle_futex_death((void __user *)pending + futex_offset,
       curr, pip);
}

long do_futex(u32 __user *uaddr, int op, u32 val, ktime_t *timeout,
  u32 __user *uaddr2, u32 val2, u32 val3)
{
 int cmd = op & FUTEX_CMD_MASK;
 unsigned int flags = 0;

 if (!(op & FUTEX_PRIVATE_FLAG))
  flags |= FLAGS_SHARED;

 if (op & FUTEX_CLOCK_REALTIME) {
  flags |= FLAGS_CLOCKRT;
  if (cmd != FUTEX_WAIT && cmd != FUTEX_WAIT_BITSET && \
      cmd != FUTEX_WAIT_REQUEUE_PI)
   return -ENOSYS;
 }

 switch (cmd) {
 case FUTEX_LOCK_PI:
 case FUTEX_UNLOCK_PI:
 case FUTEX_TRYLOCK_PI:
 case FUTEX_WAIT_REQUEUE_PI:
 case FUTEX_CMP_REQUEUE_PI:
  if (!futex_cmpxchg_enabled)
   return -ENOSYS;
 }

 switch (cmd) {
 case FUTEX_WAIT:
  val3 = FUTEX_BITSET_MATCH_ANY;
 case FUTEX_WAIT_BITSET:
  return futex_wait(uaddr, flags, val, timeout, val3);
 case FUTEX_WAKE:
  val3 = FUTEX_BITSET_MATCH_ANY;
 case FUTEX_WAKE_BITSET:
  return futex_wake(uaddr, flags, val, val3);
 case FUTEX_REQUEUE:
  return futex_requeue(uaddr, flags, uaddr2, val, val2, NULL, 0);
 case FUTEX_CMP_REQUEUE:
  return futex_requeue(uaddr, flags, uaddr2, val, val2, &val3, 0);
 case FUTEX_WAKE_OP:
  return futex_wake_op(uaddr, flags, uaddr2, val, val2, val3);
 case FUTEX_LOCK_PI:
  return futex_lock_pi(uaddr, flags, timeout, 0);
 case FUTEX_UNLOCK_PI:
  return futex_unlock_pi(uaddr, flags);
 case FUTEX_TRYLOCK_PI:
  return futex_lock_pi(uaddr, flags, NULL, 1);
 case FUTEX_WAIT_REQUEUE_PI:
  val3 = FUTEX_BITSET_MATCH_ANY;
  return futex_wait_requeue_pi(uaddr, flags, val, timeout, val3,
          uaddr2);
 case FUTEX_CMP_REQUEUE_PI:
  return futex_requeue(uaddr, flags, uaddr2, val, val2, &val3, 1);
 }
 return -ENOSYS;
}


SYSCALL_DEFINE6(futex, u32 __user *, uaddr, int, op, u32, val,
  struct timespec __user *, utime, u32 __user *, uaddr2,
  u32, val3)
{
 struct timespec ts;
 ktime_t t, *tp = NULL;
 u32 val2 = 0;
 int cmd = op & FUTEX_CMD_MASK;

 if (utime && (cmd == FUTEX_WAIT || cmd == FUTEX_LOCK_PI ||
        cmd == FUTEX_WAIT_BITSET ||
        cmd == FUTEX_WAIT_REQUEUE_PI)) {
  if (unlikely(should_fail_futex(!(op & FUTEX_PRIVATE_FLAG))))
   return -EFAULT;
  if (copy_from_user(&ts, utime, sizeof(ts)) != 0)
   return -EFAULT;
  if (!timespec_valid(&ts))
   return -EINVAL;

  t = timespec_to_ktime(ts);
  if (cmd == FUTEX_WAIT)
   t = ktime_add_safe(ktime_get(), t);
  tp = &t;
 }




 if (cmd == FUTEX_REQUEUE || cmd == FUTEX_CMP_REQUEUE ||
     cmd == FUTEX_CMP_REQUEUE_PI || cmd == FUTEX_WAKE_OP)
  val2 = (u32) (unsigned long) utime;

 return do_futex(uaddr, op, val, tp, uaddr2, val2, val3);
}

static void __init futex_detect_cmpxchg(void)
{
 u32 curval;
 if (cmpxchg_futex_value_locked(&curval, NULL, 0, 0) == -EFAULT)
  futex_cmpxchg_enabled = 1;
}

static int __init futex_init(void)
{
 unsigned int futex_shift;
 unsigned long i;

 futex_hashsize = 16;
 futex_hashsize = roundup_pow_of_two(256 * num_possible_cpus());

 futex_queues = alloc_large_system_hash("futex", sizeof(*futex_queues),
            futex_hashsize, 0,
            futex_hashsize < 256 ? HASH_SMALL : 0,
            &futex_shift, NULL,
            futex_hashsize, futex_hashsize);
 futex_hashsize = 1UL << futex_shift;

 futex_detect_cmpxchg();

 for (i = 0; i < futex_hashsize; i++) {
  atomic_set(&futex_queues[i].waiters, 0);
  plist_head_init(&futex_queues[i].chain);
  spin_lock_init(&futex_queues[i].lock);
 }

 return 0;
}
__initcall(futex_init);






static inline int
fetch_robust_entry(compat_uptr_t *uentry, struct robust_list __user **entry,
     compat_uptr_t __user *head, unsigned int *pi)
{
 if (get_user(*uentry, head))
  return -EFAULT;

 *entry = compat_ptr((*uentry) & ~1);
 *pi = (unsigned int)(*uentry) & 1;

 return 0;
}

static void __user *futex_uaddr(struct robust_list __user *entry,
    compat_long_t futex_offset)
{
 compat_uptr_t base = ptr_to_compat(entry);
 void __user *uaddr = compat_ptr(base + futex_offset);

 return uaddr;
}







void compat_exit_robust_list(struct task_struct *curr)
{
 struct compat_robust_list_head __user *head = curr->compat_robust_list;
 struct robust_list __user *entry, *next_entry, *pending;
 unsigned int limit = ROBUST_LIST_LIMIT, pi, pip;
 unsigned int uninitialized_var(next_pi);
 compat_uptr_t uentry, next_uentry, upending;
 compat_long_t futex_offset;
 int rc;

 if (!futex_cmpxchg_enabled)
  return;





 if (fetch_robust_entry(&uentry, &entry, &head->list.next, &pi))
  return;



 if (get_user(futex_offset, &head->futex_offset))
  return;




 if (fetch_robust_entry(&upending, &pending,
          &head->list_op_pending, &pip))
  return;

 next_entry = NULL;
 while (entry != (struct robust_list __user *) &head->list) {




  rc = fetch_robust_entry(&next_uentry, &next_entry,
   (compat_uptr_t __user *)&entry->next, &next_pi);




  if (entry != pending) {
   void __user *uaddr = futex_uaddr(entry, futex_offset);

   if (handle_futex_death(uaddr, curr, pi))
    return;
  }
  if (rc)
   return;
  uentry = next_uentry;
  entry = next_entry;
  pi = next_pi;



  if (!--limit)
   break;

  cond_resched();
 }
 if (pending) {
  void __user *uaddr = futex_uaddr(pending, futex_offset);

  handle_futex_death(uaddr, curr, pip);
 }
}

COMPAT_SYSCALL_DEFINE2(set_robust_list,
  struct compat_robust_list_head __user *, head,
  compat_size_t, len)
{
 if (!futex_cmpxchg_enabled)
  return -ENOSYS;

 if (unlikely(len != sizeof(*head)))
  return -EINVAL;

 current->compat_robust_list = head;

 return 0;
}

COMPAT_SYSCALL_DEFINE3(get_robust_list, int, pid,
   compat_uptr_t __user *, head_ptr,
   compat_size_t __user *, len_ptr)
{
 struct compat_robust_list_head __user *head;
 unsigned long ret;
 struct task_struct *p;

 if (!futex_cmpxchg_enabled)
  return -ENOSYS;

 rcu_read_lock();

 ret = -ESRCH;
 if (!pid)
  p = current;
 else {
  p = find_task_by_vpid(pid);
  if (!p)
   goto err_unlock;
 }

 ret = -EPERM;
 if (!ptrace_may_access(p, PTRACE_MODE_READ_REALCREDS))
  goto err_unlock;

 head = p->compat_robust_list;
 rcu_read_unlock();

 if (put_user(sizeof(*head), len_ptr))
  return -EFAULT;
 return put_user(ptr_to_compat(head), head_ptr);

err_unlock:
 rcu_read_unlock();

 return ret;
}

COMPAT_SYSCALL_DEFINE6(futex, u32 __user *, uaddr, int, op, u32, val,
  struct compat_timespec __user *, utime, u32 __user *, uaddr2,
  u32, val3)
{
 struct timespec ts;
 ktime_t t, *tp = NULL;
 int val2 = 0;
 int cmd = op & FUTEX_CMD_MASK;

 if (utime && (cmd == FUTEX_WAIT || cmd == FUTEX_LOCK_PI ||
        cmd == FUTEX_WAIT_BITSET ||
        cmd == FUTEX_WAIT_REQUEUE_PI)) {
  if (compat_get_timespec(&ts, utime))
   return -EFAULT;
  if (!timespec_valid(&ts))
   return -EINVAL;

  t = timespec_to_ktime(ts);
  if (cmd == FUTEX_WAIT)
   t = ktime_add_safe(ktime_get(), t);
  tp = &t;
 }
 if (cmd == FUTEX_REQUEUE || cmd == FUTEX_CMP_REQUEUE ||
     cmd == FUTEX_CMP_REQUEUE_PI || cmd == FUTEX_WAKE_OP)
  val2 = (int) (unsigned long) utime;

 return do_futex(uaddr, op, val, tp, uaddr2, val2, val3);
}



static char remcom_in_buffer[BUFMAX];
static char remcom_out_buffer[BUFMAX];
static int gdbstub_use_prev_in_buf;
static int gdbstub_prev_in_buf_pos;


static unsigned long gdb_regs[(NUMREGBYTES +
     sizeof(unsigned long) - 1) /
     sizeof(unsigned long)];





static int gdbstub_read_wait(void)
{
 int ret = -1;
 int i;

 if (unlikely(gdbstub_use_prev_in_buf)) {
  if (gdbstub_prev_in_buf_pos < gdbstub_use_prev_in_buf)
   return remcom_in_buffer[gdbstub_prev_in_buf_pos++];
  else
   gdbstub_use_prev_in_buf = 0;
 }


 while (ret < 0)
  for (i = 0; kdb_poll_funcs[i] != NULL; i++) {
   ret = kdb_poll_funcs[i]();
   if (ret > 0)
    break;
  }
 return ret;
}
static int gdbstub_read_wait(void)
{
 int ret = dbg_io_ops->read_char();
 while (ret == NO_POLL_CHAR)
  ret = dbg_io_ops->read_char();
 return ret;
}

static void get_packet(char *buffer)
{
 unsigned char checksum;
 unsigned char xmitcsum;
 int count;
 char ch;

 do {




  while ((ch = (gdbstub_read_wait())) != '$')
                ;

  kgdb_connected = 1;
  checksum = 0;
  xmitcsum = -1;

  count = 0;




  while (count < (BUFMAX - 1)) {
   ch = gdbstub_read_wait();
   if (ch == '#')
    break;
   checksum = checksum + ch;
   buffer[count] = ch;
   count = count + 1;
  }

  if (ch == '#') {
   xmitcsum = hex_to_bin(gdbstub_read_wait()) << 4;
   xmitcsum += hex_to_bin(gdbstub_read_wait());

   if (checksum != xmitcsum)

    dbg_io_ops->write_char('-');
   else

    dbg_io_ops->write_char('+');
   if (dbg_io_ops->flush)
    dbg_io_ops->flush();
  }
  buffer[count] = 0;
 } while (checksum != xmitcsum);
}





static void put_packet(char *buffer)
{
 unsigned char checksum;
 int count;
 char ch;




 while (1) {
  dbg_io_ops->write_char('$');
  checksum = 0;
  count = 0;

  while ((ch = buffer[count])) {
   dbg_io_ops->write_char(ch);
   checksum += ch;
   count++;
  }

  dbg_io_ops->write_char('#');
  dbg_io_ops->write_char(hex_asc_hi(checksum));
  dbg_io_ops->write_char(hex_asc_lo(checksum));
  if (dbg_io_ops->flush)
   dbg_io_ops->flush();


  ch = gdbstub_read_wait();

  if (ch == 3)
   ch = gdbstub_read_wait();


  if (ch == '+')
   return;







  if (ch == '$') {
   dbg_io_ops->write_char('-');
   if (dbg_io_ops->flush)
    dbg_io_ops->flush();
   return;
  }
 }
}

static char gdbmsgbuf[BUFMAX + 1];

void gdbstub_msg_write(const char *s, int len)
{
 char *bufptr;
 int wcount;
 int i;

 if (len == 0)
  len = strlen(s);


 gdbmsgbuf[0] = 'O';


 while (len > 0) {
  bufptr = gdbmsgbuf + 1;


  if ((len << 1) > (BUFMAX - 2))
   wcount = (BUFMAX - 2) >> 1;
  else
   wcount = len;


  for (i = 0; i < wcount; i++)
   bufptr = hex_byte_pack(bufptr, s[i]);
  *bufptr = '\0';


  s += wcount;
  len -= wcount;


  put_packet(gdbmsgbuf);
 }
}






char *kgdb_mem2hex(char *mem, char *buf, int count)
{
 char *tmp;
 int err;





 tmp = buf + count;

 err = probe_kernel_read(tmp, mem, count);
 if (err)
  return NULL;
 while (count > 0) {
  buf = hex_byte_pack(buf, *tmp);
  tmp++;
  count--;
 }
 *buf = 0;

 return buf;
}






int kgdb_hex2mem(char *buf, char *mem, int count)
{
 char *tmp_raw;
 char *tmp_hex;





 tmp_raw = buf + count * 2;

 tmp_hex = tmp_raw - 1;
 while (tmp_hex >= buf) {
  tmp_raw--;
  *tmp_raw = hex_to_bin(*tmp_hex--);
  *tmp_raw |= hex_to_bin(*tmp_hex--) << 4;
 }

 return probe_kernel_write(mem, tmp_raw, count);
}





int kgdb_hex2long(char **ptr, unsigned long *long_val)
{
 int hex_val;
 int num = 0;
 int negate = 0;

 *long_val = 0;

 if (**ptr == '-') {
  negate = 1;
  (*ptr)++;
 }
 while (**ptr) {
  hex_val = hex_to_bin(**ptr);
  if (hex_val < 0)
   break;

  *long_val = (*long_val << 4) | hex_val;
  num++;
  (*ptr)++;
 }

 if (negate)
  *long_val = -*long_val;

 return num;
}






static int kgdb_ebin2mem(char *buf, char *mem, int count)
{
 int size = 0;
 char *c = buf;

 while (count-- > 0) {
  c[size] = *buf++;
  if (c[size] == 0x7d)
   c[size] = *buf++ ^ 0x20;
  size++;
 }

 return probe_kernel_write(mem, c, size);
}

void pt_regs_to_gdb_regs(unsigned long *gdb_regs, struct pt_regs *regs)
{
 int i;
 int idx = 0;
 char *ptr = (char *)gdb_regs;

 for (i = 0; i < DBG_MAX_REG_NUM; i++) {
  dbg_get_reg(i, ptr + idx, regs);
  idx += dbg_reg_def[i].size;
 }
}

void gdb_regs_to_pt_regs(unsigned long *gdb_regs, struct pt_regs *regs)
{
 int i;
 int idx = 0;
 char *ptr = (char *)gdb_regs;

 for (i = 0; i < DBG_MAX_REG_NUM; i++) {
  dbg_set_reg(i, ptr + idx, regs);
  idx += dbg_reg_def[i].size;
 }
}


static int write_mem_msg(int binary)
{
 char *ptr = &remcom_in_buffer[1];
 unsigned long addr;
 unsigned long length;
 int err;

 if (kgdb_hex2long(&ptr, &addr) > 0 && *(ptr++) == ',' &&
     kgdb_hex2long(&ptr, &length) > 0 && *(ptr++) == ':') {
  if (binary)
   err = kgdb_ebin2mem(ptr, (char *)addr, length);
  else
   err = kgdb_hex2mem(ptr, (char *)addr, length);
  if (err)
   return err;
  if (CACHE_FLUSH_IS_SAFE)
   flush_icache_range(addr, addr + length);
  return 0;
 }

 return -EINVAL;
}

static void error_packet(char *pkt, int error)
{
 error = -error;
 pkt[0] = 'E';
 pkt[1] = hex_asc[(error / 10)];
 pkt[2] = hex_asc[(error % 10)];
 pkt[3] = '\0';
}








static char *pack_threadid(char *pkt, unsigned char *id)
{
 unsigned char *limit;
 int lzero = 1;

 limit = id + (BUF_THREAD_ID_SIZE / 2);
 while (id < limit) {
  if (!lzero || *id != 0) {
   pkt = hex_byte_pack(pkt, *id);
   lzero = 0;
  }
  id++;
 }

 if (lzero)
  pkt = hex_byte_pack(pkt, 0);

 return pkt;
}

static void int_to_threadref(unsigned char *id, int value)
{
 put_unaligned_be32(value, id);
}

static struct task_struct *getthread(struct pt_regs *regs, int tid)
{



 if (tid == 0 || tid == -1)
  tid = -atomic_read(&kgdb_active) - 2;
 if (tid < -1 && tid > -NR_CPUS - 2) {
  if (kgdb_info[-tid - 2].task)
   return kgdb_info[-tid - 2].task;
  else
   return idle_task(-tid - 2);
 }
 if (tid <= 0) {
  printk(KERN_ERR "KGDB: Internal thread select error\n");
  dump_stack();
  return NULL;
 }






 return find_task_by_pid_ns(tid, &init_pid_ns);
}






static inline int shadow_pid(int realpid)
{
 if (realpid)
  return realpid;

 return -raw_smp_processor_id() - 2;
}
static void gdb_cmd_status(struct kgdb_state *ks)
{






 dbg_remove_all_break();

 remcom_out_buffer[0] = 'S';
 hex_byte_pack(&remcom_out_buffer[1], ks->signo);
}

static void gdb_get_regs_helper(struct kgdb_state *ks)
{
 struct task_struct *thread;
 void *local_debuggerinfo;
 int i;

 thread = kgdb_usethread;
 if (!thread) {
  thread = kgdb_info[ks->cpu].task;
  local_debuggerinfo = kgdb_info[ks->cpu].debuggerinfo;
 } else {
  local_debuggerinfo = NULL;
  for_each_online_cpu(i) {






   if (thread == kgdb_info[i].task)
    local_debuggerinfo = kgdb_info[i].debuggerinfo;
  }
 }






 if (local_debuggerinfo) {
  pt_regs_to_gdb_regs(gdb_regs, local_debuggerinfo);
 } else {







  sleeping_thread_to_gdb_regs(gdb_regs, thread);
 }
}


static void gdb_cmd_getregs(struct kgdb_state *ks)
{
 gdb_get_regs_helper(ks);
 kgdb_mem2hex((char *)gdb_regs, remcom_out_buffer, NUMREGBYTES);
}


static void gdb_cmd_setregs(struct kgdb_state *ks)
{
 kgdb_hex2mem(&remcom_in_buffer[1], (char *)gdb_regs, NUMREGBYTES);

 if (kgdb_usethread && kgdb_usethread != current) {
  error_packet(remcom_out_buffer, -EINVAL);
 } else {
  gdb_regs_to_pt_regs(gdb_regs, ks->linux_regs);
  strcpy(remcom_out_buffer, "OK");
 }
}


static void gdb_cmd_memread(struct kgdb_state *ks)
{
 char *ptr = &remcom_in_buffer[1];
 unsigned long length;
 unsigned long addr;
 char *err;

 if (kgdb_hex2long(&ptr, &addr) > 0 && *ptr++ == ',' &&
     kgdb_hex2long(&ptr, &length) > 0) {
  err = kgdb_mem2hex((char *)addr, remcom_out_buffer, length);
  if (!err)
   error_packet(remcom_out_buffer, -EINVAL);
 } else {
  error_packet(remcom_out_buffer, -EINVAL);
 }
}


static void gdb_cmd_memwrite(struct kgdb_state *ks)
{
 int err = write_mem_msg(0);

 if (err)
  error_packet(remcom_out_buffer, err);
 else
  strcpy(remcom_out_buffer, "OK");
}

static char *gdb_hex_reg_helper(int regnum, char *out)
{
 int i;
 int offset = 0;

 for (i = 0; i < regnum; i++)
  offset += dbg_reg_def[i].size;
 return kgdb_mem2hex((char *)gdb_regs + offset, out,
       dbg_reg_def[i].size);
}


static void gdb_cmd_reg_get(struct kgdb_state *ks)
{
 unsigned long regnum;
 char *ptr = &remcom_in_buffer[1];

 kgdb_hex2long(&ptr, &regnum);
 if (regnum >= DBG_MAX_REG_NUM) {
  error_packet(remcom_out_buffer, -EINVAL);
  return;
 }
 gdb_get_regs_helper(ks);
 gdb_hex_reg_helper(regnum, remcom_out_buffer);
}


static void gdb_cmd_reg_set(struct kgdb_state *ks)
{
 unsigned long regnum;
 char *ptr = &remcom_in_buffer[1];
 int i = 0;

 kgdb_hex2long(&ptr, &regnum);
 if (*ptr++ != '=' ||
     !(!kgdb_usethread || kgdb_usethread == current) ||
     !dbg_get_reg(regnum, gdb_regs, ks->linux_regs)) {
  error_packet(remcom_out_buffer, -EINVAL);
  return;
 }
 memset(gdb_regs, 0, sizeof(gdb_regs));
 while (i < sizeof(gdb_regs) * 2)
  if (hex_to_bin(ptr[i]) >= 0)
   i++;
  else
   break;
 i = i / 2;
 kgdb_hex2mem(ptr, (char *)gdb_regs, i);
 dbg_set_reg(regnum, gdb_regs, ks->linux_regs);
 strcpy(remcom_out_buffer, "OK");
}


static void gdb_cmd_binwrite(struct kgdb_state *ks)
{
 int err = write_mem_msg(1);

 if (err)
  error_packet(remcom_out_buffer, err);
 else
  strcpy(remcom_out_buffer, "OK");
}


static void gdb_cmd_detachkill(struct kgdb_state *ks)
{
 int error;


 if (remcom_in_buffer[0] == 'D') {
  error = dbg_remove_all_break();
  if (error < 0) {
   error_packet(remcom_out_buffer, error);
  } else {
   strcpy(remcom_out_buffer, "OK");
   kgdb_connected = 0;
  }
  put_packet(remcom_out_buffer);
 } else {




  dbg_remove_all_break();
  kgdb_connected = 0;
 }
}


static int gdb_cmd_reboot(struct kgdb_state *ks)
{

 if (strcmp(remcom_in_buffer, "R0") == 0) {
  printk(KERN_CRIT "Executing emergency reboot\n");
  strcpy(remcom_out_buffer, "OK");
  put_packet(remcom_out_buffer);





  machine_emergency_restart();
  kgdb_connected = 0;

  return 1;
 }
 return 0;
}


static void gdb_cmd_query(struct kgdb_state *ks)
{
 struct task_struct *g;
 struct task_struct *p;
 unsigned char thref[BUF_THREAD_ID_SIZE];
 char *ptr;
 int i;
 int cpu;
 int finished = 0;

 switch (remcom_in_buffer[1]) {
 case 's':
 case 'f':
  if (memcmp(remcom_in_buffer + 2, "ThreadInfo", 10))
   break;

  i = 0;
  remcom_out_buffer[0] = 'm';
  ptr = remcom_out_buffer + 1;
  if (remcom_in_buffer[1] == 'f') {

   for_each_online_cpu(cpu) {
    ks->thr_query = 0;
    int_to_threadref(thref, -cpu - 2);
    ptr = pack_threadid(ptr, thref);
    *(ptr++) = ',';
    i++;
   }
  }

  do_each_thread(g, p) {
   if (i >= ks->thr_query && !finished) {
    int_to_threadref(thref, p->pid);
    ptr = pack_threadid(ptr, thref);
    *(ptr++) = ',';
    ks->thr_query++;
    if (ks->thr_query % KGDB_MAX_THREAD_QUERY == 0)
     finished = 1;
   }
   i++;
  } while_each_thread(g, p);

  *(--ptr) = '\0';
  break;

 case 'C':

  strcpy(remcom_out_buffer, "QC");
  ks->threadid = shadow_pid(current->pid);
  int_to_threadref(thref, ks->threadid);
  pack_threadid(remcom_out_buffer + 2, thref);
  break;
 case 'T':
  if (memcmp(remcom_in_buffer + 1, "ThreadExtraInfo,", 16))
   break;

  ks->threadid = 0;
  ptr = remcom_in_buffer + 17;
  kgdb_hex2long(&ptr, &ks->threadid);
  if (!getthread(ks->linux_regs, ks->threadid)) {
   error_packet(remcom_out_buffer, -EINVAL);
   break;
  }
  if ((int)ks->threadid > 0) {
   kgdb_mem2hex(getthread(ks->linux_regs,
     ks->threadid)->comm,
     remcom_out_buffer, 16);
  } else {
   static char tmpstr[23 + BUF_THREAD_ID_SIZE];

   sprintf(tmpstr, "shadowCPU%d",
     (int)(-ks->threadid - 2));
   kgdb_mem2hex(tmpstr, remcom_out_buffer, strlen(tmpstr));
  }
  break;
 case 'R':
  if (strncmp(remcom_in_buffer, "qRcmd,", 6) == 0) {
   int len = strlen(remcom_in_buffer + 6);

   if ((len % 2) != 0) {
    strcpy(remcom_out_buffer, "E01");
    break;
   }
   kgdb_hex2mem(remcom_in_buffer + 6,
         remcom_out_buffer, len);
   len = len / 2;
   remcom_out_buffer[len++] = 0;

   kdb_common_init_state(ks);
   kdb_parse(remcom_out_buffer);
   kdb_common_deinit_state();

   strcpy(remcom_out_buffer, "OK");
  }
  break;
 }
}


static void gdb_cmd_task(struct kgdb_state *ks)
{
 struct task_struct *thread;
 char *ptr;

 switch (remcom_in_buffer[1]) {
 case 'g':
  ptr = &remcom_in_buffer[2];
  kgdb_hex2long(&ptr, &ks->threadid);
  thread = getthread(ks->linux_regs, ks->threadid);
  if (!thread && ks->threadid > 0) {
   error_packet(remcom_out_buffer, -EINVAL);
   break;
  }
  kgdb_usethread = thread;
  ks->kgdb_usethreadid = ks->threadid;
  strcpy(remcom_out_buffer, "OK");
  break;
 case 'c':
  ptr = &remcom_in_buffer[2];
  kgdb_hex2long(&ptr, &ks->threadid);
  if (!ks->threadid) {
   kgdb_contthread = NULL;
  } else {
   thread = getthread(ks->linux_regs, ks->threadid);
   if (!thread && ks->threadid > 0) {
    error_packet(remcom_out_buffer, -EINVAL);
    break;
   }
   kgdb_contthread = thread;
  }
  strcpy(remcom_out_buffer, "OK");
  break;
 }
}


static void gdb_cmd_thread(struct kgdb_state *ks)
{
 char *ptr = &remcom_in_buffer[1];
 struct task_struct *thread;

 kgdb_hex2long(&ptr, &ks->threadid);
 thread = getthread(ks->linux_regs, ks->threadid);
 if (thread)
  strcpy(remcom_out_buffer, "OK");
 else
  error_packet(remcom_out_buffer, -EINVAL);
}


static void gdb_cmd_break(struct kgdb_state *ks)
{




 char *bpt_type = &remcom_in_buffer[1];
 char *ptr = &remcom_in_buffer[2];
 unsigned long addr;
 unsigned long length;
 int error = 0;

 if (arch_kgdb_ops.set_hw_breakpoint && *bpt_type >= '1') {

  if (*bpt_type > '4')
   return;
 } else {
  if (*bpt_type != '0' && *bpt_type != '1')

   return;
 }





 if (*bpt_type == '1' && !(arch_kgdb_ops.flags & KGDB_HW_BREAKPOINT))

  return;

 if (*(ptr++) != ',') {
  error_packet(remcom_out_buffer, -EINVAL);
  return;
 }
 if (!kgdb_hex2long(&ptr, &addr)) {
  error_packet(remcom_out_buffer, -EINVAL);
  return;
 }
 if (*(ptr++) != ',' ||
  !kgdb_hex2long(&ptr, &length)) {
  error_packet(remcom_out_buffer, -EINVAL);
  return;
 }

 if (remcom_in_buffer[0] == 'Z' && *bpt_type == '0')
  error = dbg_set_sw_break(addr);
 else if (remcom_in_buffer[0] == 'z' && *bpt_type == '0')
  error = dbg_remove_sw_break(addr);
 else if (remcom_in_buffer[0] == 'Z')
  error = arch_kgdb_ops.set_hw_breakpoint(addr,
   (int)length, *bpt_type - '0');
 else if (remcom_in_buffer[0] == 'z')
  error = arch_kgdb_ops.remove_hw_breakpoint(addr,
   (int) length, *bpt_type - '0');

 if (error == 0)
  strcpy(remcom_out_buffer, "OK");
 else
  error_packet(remcom_out_buffer, error);
}


static int gdb_cmd_exception_pass(struct kgdb_state *ks)
{



 if (remcom_in_buffer[1] == '0' && remcom_in_buffer[2] == '9') {

  ks->pass_exception = 1;
  remcom_in_buffer[0] = 'c';

 } else if (remcom_in_buffer[1] == '1' && remcom_in_buffer[2] == '5') {

  ks->pass_exception = 1;
  remcom_in_buffer[0] = 'D';
  dbg_remove_all_break();
  kgdb_connected = 0;
  return 1;

 } else {
  gdbstub_msg_write("KGDB only knows signal 9 (pass)"
   " and 15 (pass and disconnect)\n"
   "Executing a continue without signal passing\n", 0);
  remcom_in_buffer[0] = 'c';
 }


 return -1;
}




int gdb_serial_stub(struct kgdb_state *ks)
{
 int error = 0;
 int tmp;


 memset(remcom_out_buffer, 0, sizeof(remcom_out_buffer));
 kgdb_usethread = kgdb_info[ks->cpu].task;
 ks->kgdb_usethreadid = shadow_pid(kgdb_info[ks->cpu].task->pid);
 ks->pass_exception = 0;

 if (kgdb_connected) {
  unsigned char thref[BUF_THREAD_ID_SIZE];
  char *ptr;


  ptr = remcom_out_buffer;
  *ptr++ = 'T';
  ptr = hex_byte_pack(ptr, ks->signo);
  ptr += strlen(strcpy(ptr, "thread:"));
  int_to_threadref(thref, shadow_pid(current->pid));
  ptr = pack_threadid(ptr, thref);
  *ptr++ = ';';
  put_packet(remcom_out_buffer);
 }

 while (1) {
  error = 0;


  memset(remcom_out_buffer, 0, sizeof(remcom_out_buffer));

  get_packet(remcom_in_buffer);

  switch (remcom_in_buffer[0]) {
  case '?':
   gdb_cmd_status(ks);
   break;
  case 'g':
   gdb_cmd_getregs(ks);
   break;
  case 'G':
   gdb_cmd_setregs(ks);
   break;
  case 'm':
   gdb_cmd_memread(ks);
   break;
  case 'M':
   gdb_cmd_memwrite(ks);
   break;
  case 'p':
   gdb_cmd_reg_get(ks);
   break;
  case 'P':
   gdb_cmd_reg_set(ks);
   break;
  case 'X':
   gdb_cmd_binwrite(ks);
   break;



  case 'D':
  case 'k':
   gdb_cmd_detachkill(ks);
   goto default_handle;
  case 'R':
   if (gdb_cmd_reboot(ks))
    goto default_handle;
   break;
  case 'q':
   gdb_cmd_query(ks);
   break;
  case 'H':
   gdb_cmd_task(ks);
   break;
  case 'T':
   gdb_cmd_thread(ks);
   break;
  case 'z':
  case 'Z':
   gdb_cmd_break(ks);
   break;
  case '3':
   if (remcom_in_buffer[1] == '\0') {
    gdb_cmd_detachkill(ks);
    return DBG_PASS_EVENT;
   }
  case 'C':
   tmp = gdb_cmd_exception_pass(ks);
   if (tmp > 0)
    goto default_handle;
   if (tmp == 0)
    break;

  case 'c':
  case 's':
   if (kgdb_contthread && kgdb_contthread != current) {

    error_packet(remcom_out_buffer, -EINVAL);
    break;
   }
   dbg_activate_sw_breakpoints();

  default:
default_handle:
   error = kgdb_arch_handle_exception(ks->ex_vector,
      ks->signo,
      ks->err_code,
      remcom_in_buffer,
      remcom_out_buffer,
      ks->linux_regs);




   if (error >= 0 || remcom_in_buffer[0] == 'D' ||
       remcom_in_buffer[0] == 'k') {
    error = 0;
    goto kgdb_exit;
   }

  }


  put_packet(remcom_out_buffer);
 }

kgdb_exit:
 if (ks->pass_exception)
  error = 1;
 return error;
}

int gdbstub_state(struct kgdb_state *ks, char *cmd)
{
 int error;

 switch (cmd[0]) {
 case 'e':
  error = kgdb_arch_handle_exception(ks->ex_vector,
         ks->signo,
         ks->err_code,
         remcom_in_buffer,
         remcom_out_buffer,
         ks->linux_regs);
  return error;
 case 's':
 case 'c':
  strcpy(remcom_in_buffer, cmd);
  return 0;
 case '$':
  strcpy(remcom_in_buffer, cmd);
  gdbstub_use_prev_in_buf = strlen(remcom_in_buffer);
  gdbstub_prev_in_buf_pos = 0;
  return 0;
 }
 dbg_io_ops->write_char('+');
 put_packet(remcom_out_buffer);
 return 0;
}





void gdbstub_exit(int status)
{
 unsigned char checksum, ch, buffer[3];
 int loop;

 if (!kgdb_connected)
  return;
 kgdb_connected = 0;

 if (!dbg_io_ops || dbg_kdb_mode)
  return;

 buffer[0] = 'W';
 buffer[1] = hex_asc_hi(status);
 buffer[2] = hex_asc_lo(status);

 dbg_io_ops->write_char('$');
 checksum = 0;

 for (loop = 0; loop < 3; loop++) {
  ch = buffer[loop];
  checksum += ch;
  dbg_io_ops->write_char(ch);
 }

 dbg_io_ops->write_char('#');
 dbg_io_ops->write_char(hex_asc_hi(checksum));
 dbg_io_ops->write_char(hex_asc_lo(checksum));


 if (dbg_io_ops->flush)
  dbg_io_ops->flush();
}







static LIST_HEAD(gc_list);
static DEFINE_RAW_SPINLOCK(gc_lock);





void irq_gc_noop(struct irq_data *d)
{
}
void irq_gc_mask_disable_reg(struct irq_data *d)
{
 struct irq_chip_generic *gc = irq_data_get_irq_chip_data(d);
 struct irq_chip_type *ct = irq_data_get_chip_type(d);
 u32 mask = d->mask;

 irq_gc_lock(gc);
 irq_reg_writel(gc, mask, ct->regs.disable);
 *ct->mask_cache &= ~mask;
 irq_gc_unlock(gc);
}
void irq_gc_mask_set_bit(struct irq_data *d)
{
 struct irq_chip_generic *gc = irq_data_get_irq_chip_data(d);
 struct irq_chip_type *ct = irq_data_get_chip_type(d);
 u32 mask = d->mask;

 irq_gc_lock(gc);
 *ct->mask_cache |= mask;
 irq_reg_writel(gc, *ct->mask_cache, ct->regs.mask);
 irq_gc_unlock(gc);
}
EXPORT_SYMBOL_GPL(irq_gc_mask_set_bit);
void irq_gc_mask_clr_bit(struct irq_data *d)
{
 struct irq_chip_generic *gc = irq_data_get_irq_chip_data(d);
 struct irq_chip_type *ct = irq_data_get_chip_type(d);
 u32 mask = d->mask;

 irq_gc_lock(gc);
 *ct->mask_cache &= ~mask;
 irq_reg_writel(gc, *ct->mask_cache, ct->regs.mask);
 irq_gc_unlock(gc);
}
EXPORT_SYMBOL_GPL(irq_gc_mask_clr_bit);
void irq_gc_unmask_enable_reg(struct irq_data *d)
{
 struct irq_chip_generic *gc = irq_data_get_irq_chip_data(d);
 struct irq_chip_type *ct = irq_data_get_chip_type(d);
 u32 mask = d->mask;

 irq_gc_lock(gc);
 irq_reg_writel(gc, mask, ct->regs.enable);
 *ct->mask_cache |= mask;
 irq_gc_unlock(gc);
}





void irq_gc_ack_set_bit(struct irq_data *d)
{
 struct irq_chip_generic *gc = irq_data_get_irq_chip_data(d);
 struct irq_chip_type *ct = irq_data_get_chip_type(d);
 u32 mask = d->mask;

 irq_gc_lock(gc);
 irq_reg_writel(gc, mask, ct->regs.ack);
 irq_gc_unlock(gc);
}
EXPORT_SYMBOL_GPL(irq_gc_ack_set_bit);





void irq_gc_ack_clr_bit(struct irq_data *d)
{
 struct irq_chip_generic *gc = irq_data_get_irq_chip_data(d);
 struct irq_chip_type *ct = irq_data_get_chip_type(d);
 u32 mask = ~d->mask;

 irq_gc_lock(gc);
 irq_reg_writel(gc, mask, ct->regs.ack);
 irq_gc_unlock(gc);
}





void irq_gc_mask_disable_reg_and_ack(struct irq_data *d)
{
 struct irq_chip_generic *gc = irq_data_get_irq_chip_data(d);
 struct irq_chip_type *ct = irq_data_get_chip_type(d);
 u32 mask = d->mask;

 irq_gc_lock(gc);
 irq_reg_writel(gc, mask, ct->regs.mask);
 irq_reg_writel(gc, mask, ct->regs.ack);
 irq_gc_unlock(gc);
}





void irq_gc_eoi(struct irq_data *d)
{
 struct irq_chip_generic *gc = irq_data_get_irq_chip_data(d);
 struct irq_chip_type *ct = irq_data_get_chip_type(d);
 u32 mask = d->mask;

 irq_gc_lock(gc);
 irq_reg_writel(gc, mask, ct->regs.eoi);
 irq_gc_unlock(gc);
}
int irq_gc_set_wake(struct irq_data *d, unsigned int on)
{
 struct irq_chip_generic *gc = irq_data_get_irq_chip_data(d);
 u32 mask = d->mask;

 if (!(mask & gc->wake_enabled))
  return -EINVAL;

 irq_gc_lock(gc);
 if (on)
  gc->wake_active |= mask;
 else
  gc->wake_active &= ~mask;
 irq_gc_unlock(gc);
 return 0;
}

static u32 irq_readl_be(void __iomem *addr)
{
 return ioread32be(addr);
}

static void irq_writel_be(u32 val, void __iomem *addr)
{
 iowrite32be(val, addr);
}

static void
irq_init_generic_chip(struct irq_chip_generic *gc, const char *name,
        int num_ct, unsigned int irq_base,
        void __iomem *reg_base, irq_flow_handler_t handler)
{
 raw_spin_lock_init(&gc->lock);
 gc->num_ct = num_ct;
 gc->irq_base = irq_base;
 gc->reg_base = reg_base;
 gc->chip_types->chip.name = name;
 gc->chip_types->handler = handler;
}
struct irq_chip_generic *
irq_alloc_generic_chip(const char *name, int num_ct, unsigned int irq_base,
         void __iomem *reg_base, irq_flow_handler_t handler)
{
 struct irq_chip_generic *gc;
 unsigned long sz = sizeof(*gc) + num_ct * sizeof(struct irq_chip_type);

 gc = kzalloc(sz, GFP_KERNEL);
 if (gc) {
  irq_init_generic_chip(gc, name, num_ct, irq_base, reg_base,
          handler);
 }
 return gc;
}
EXPORT_SYMBOL_GPL(irq_alloc_generic_chip);

static void
irq_gc_init_mask_cache(struct irq_chip_generic *gc, enum irq_gc_flags flags)
{
 struct irq_chip_type *ct = gc->chip_types;
 u32 *mskptr = &gc->mask_cache, mskreg = ct->regs.mask;
 int i;

 for (i = 0; i < gc->num_ct; i++) {
  if (flags & IRQ_GC_MASK_CACHE_PER_TYPE) {
   mskptr = &ct[i].mask_cache_priv;
   mskreg = ct[i].regs.mask;
  }
  ct[i].mask_cache = mskptr;
  if (flags & IRQ_GC_INIT_MASK_CACHE)
   *mskptr = irq_reg_readl(gc, mskreg);
 }
}
int irq_alloc_domain_generic_chips(struct irq_domain *d, int irqs_per_chip,
       int num_ct, const char *name,
       irq_flow_handler_t handler,
       unsigned int clr, unsigned int set,
       enum irq_gc_flags gcflags)
{
 struct irq_domain_chip_generic *dgc;
 struct irq_chip_generic *gc;
 int numchips, sz, i;
 unsigned long flags;
 void *tmp;

 if (d->gc)
  return -EBUSY;

 numchips = DIV_ROUND_UP(d->revmap_size, irqs_per_chip);
 if (!numchips)
  return -EINVAL;


 sz = sizeof(*dgc) + numchips * sizeof(gc);
 sz += numchips * (sizeof(*gc) + num_ct * sizeof(struct irq_chip_type));

 tmp = dgc = kzalloc(sz, GFP_KERNEL);
 if (!dgc)
  return -ENOMEM;
 dgc->irqs_per_chip = irqs_per_chip;
 dgc->num_chips = numchips;
 dgc->irq_flags_to_set = set;
 dgc->irq_flags_to_clear = clr;
 dgc->gc_flags = gcflags;
 d->gc = dgc;


 tmp += sizeof(*dgc) + numchips * sizeof(gc);
 for (i = 0; i < numchips; i++) {

  dgc->gc[i] = gc = tmp;
  irq_init_generic_chip(gc, name, num_ct, i * irqs_per_chip,
          NULL, handler);

  gc->domain = d;
  if (gcflags & IRQ_GC_BE_IO) {
   gc->reg_readl = &irq_readl_be;
   gc->reg_writel = &irq_writel_be;
  }

  raw_spin_lock_irqsave(&gc_lock, flags);
  list_add_tail(&gc->list, &gc_list);
  raw_spin_unlock_irqrestore(&gc_lock, flags);

  tmp += sizeof(*gc) + num_ct * sizeof(struct irq_chip_type);
 }
 d->name = name;
 return 0;
}
EXPORT_SYMBOL_GPL(irq_alloc_domain_generic_chips);






struct irq_chip_generic *
irq_get_domain_generic_chip(struct irq_domain *d, unsigned int hw_irq)
{
 struct irq_domain_chip_generic *dgc = d->gc;
 int idx;

 if (!dgc)
  return NULL;
 idx = hw_irq / dgc->irqs_per_chip;
 if (idx >= dgc->num_chips)
  return NULL;
 return dgc->gc[idx];
}
EXPORT_SYMBOL_GPL(irq_get_domain_generic_chip);





static struct lock_class_key irq_nested_lock_class;




int irq_map_generic_chip(struct irq_domain *d, unsigned int virq,
    irq_hw_number_t hw_irq)
{
 struct irq_data *data = irq_domain_get_irq_data(d, virq);
 struct irq_domain_chip_generic *dgc = d->gc;
 struct irq_chip_generic *gc;
 struct irq_chip_type *ct;
 struct irq_chip *chip;
 unsigned long flags;
 int idx;

 if (!d->gc)
  return -ENODEV;

 idx = hw_irq / dgc->irqs_per_chip;
 if (idx >= dgc->num_chips)
  return -EINVAL;
 gc = dgc->gc[idx];

 idx = hw_irq % dgc->irqs_per_chip;

 if (test_bit(idx, &gc->unused))
  return -ENOTSUPP;

 if (test_bit(idx, &gc->installed))
  return -EBUSY;

 ct = gc->chip_types;
 chip = &ct->chip;


 if (!gc->installed) {
  raw_spin_lock_irqsave(&gc->lock, flags);
  irq_gc_init_mask_cache(gc, dgc->gc_flags);
  raw_spin_unlock_irqrestore(&gc->lock, flags);
 }


 set_bit(idx, &gc->installed);

 if (dgc->gc_flags & IRQ_GC_INIT_NESTED_LOCK)
  irq_set_lockdep_class(virq, &irq_nested_lock_class);

 if (chip->irq_calc_mask)
  chip->irq_calc_mask(data);
 else
  data->mask = 1 << idx;

 irq_domain_set_info(d, virq, hw_irq, chip, gc, ct->handler, NULL, NULL);
 irq_modify_status(virq, dgc->irq_flags_to_clear, dgc->irq_flags_to_set);
 return 0;
}
EXPORT_SYMBOL_GPL(irq_map_generic_chip);

struct irq_domain_ops irq_generic_chip_ops = {
 .map = irq_map_generic_chip,
 .xlate = irq_domain_xlate_onetwocell,
};
EXPORT_SYMBOL_GPL(irq_generic_chip_ops);
void irq_setup_generic_chip(struct irq_chip_generic *gc, u32 msk,
       enum irq_gc_flags flags, unsigned int clr,
       unsigned int set)
{
 struct irq_chip_type *ct = gc->chip_types;
 struct irq_chip *chip = &ct->chip;
 unsigned int i;

 raw_spin_lock(&gc_lock);
 list_add_tail(&gc->list, &gc_list);
 raw_spin_unlock(&gc_lock);

 irq_gc_init_mask_cache(gc, flags);

 for (i = gc->irq_base; msk; msk >>= 1, i++) {
  if (!(msk & 0x01))
   continue;

  if (flags & IRQ_GC_INIT_NESTED_LOCK)
   irq_set_lockdep_class(i, &irq_nested_lock_class);

  if (!(flags & IRQ_GC_NO_MASK)) {
   struct irq_data *d = irq_get_irq_data(i);

   if (chip->irq_calc_mask)
    chip->irq_calc_mask(d);
   else
    d->mask = 1 << (i - gc->irq_base);
  }
  irq_set_chip_and_handler(i, chip, ct->handler);
  irq_set_chip_data(i, gc);
  irq_modify_status(i, clr, set);
 }
 gc->irq_cnt = i - gc->irq_base;
}
EXPORT_SYMBOL_GPL(irq_setup_generic_chip);
int irq_setup_alt_chip(struct irq_data *d, unsigned int type)
{
 struct irq_chip_generic *gc = irq_data_get_irq_chip_data(d);
 struct irq_chip_type *ct = gc->chip_types;
 unsigned int i;

 for (i = 0; i < gc->num_ct; i++, ct++) {
  if (ct->type & type) {
   d->chip = &ct->chip;
   irq_data_to_desc(d)->handle_irq = ct->handler;
   return 0;
  }
 }
 return -EINVAL;
}
EXPORT_SYMBOL_GPL(irq_setup_alt_chip);
void irq_remove_generic_chip(struct irq_chip_generic *gc, u32 msk,
        unsigned int clr, unsigned int set)
{
 unsigned int i = gc->irq_base;

 raw_spin_lock(&gc_lock);
 list_del(&gc->list);
 raw_spin_unlock(&gc_lock);

 for (; msk; msk >>= 1, i++) {
  if (!(msk & 0x01))
   continue;


  irq_set_handler(i, NULL);
  irq_set_chip(i, &no_irq_chip);
  irq_set_chip_data(i, NULL);
  irq_modify_status(i, clr, set);
 }
}
EXPORT_SYMBOL_GPL(irq_remove_generic_chip);

static struct irq_data *irq_gc_get_irq_data(struct irq_chip_generic *gc)
{
 unsigned int virq;

 if (!gc->domain)
  return irq_get_irq_data(gc->irq_base);





 if (!gc->installed)
  return NULL;

 virq = irq_find_mapping(gc->domain, gc->irq_base + __ffs(gc->installed));
 return virq ? irq_get_irq_data(virq) : NULL;
}

static int irq_gc_suspend(void)
{
 struct irq_chip_generic *gc;

 list_for_each_entry(gc, &gc_list, list) {
  struct irq_chip_type *ct = gc->chip_types;

  if (ct->chip.irq_suspend) {
   struct irq_data *data = irq_gc_get_irq_data(gc);

   if (data)
    ct->chip.irq_suspend(data);
  }

  if (gc->suspend)
   gc->suspend(gc);
 }
 return 0;
}

static void irq_gc_resume(void)
{
 struct irq_chip_generic *gc;

 list_for_each_entry(gc, &gc_list, list) {
  struct irq_chip_type *ct = gc->chip_types;

  if (gc->resume)
   gc->resume(gc);

  if (ct->chip.irq_resume) {
   struct irq_data *data = irq_gc_get_irq_data(gc);

   if (data)
    ct->chip.irq_resume(data);
  }
 }
}

static void irq_gc_shutdown(void)
{
 struct irq_chip_generic *gc;

 list_for_each_entry(gc, &gc_list, list) {
  struct irq_chip_type *ct = gc->chip_types;

  if (ct->chip.irq_pm_shutdown) {
   struct irq_data *data = irq_gc_get_irq_data(gc);

   if (data)
    ct->chip.irq_pm_shutdown(data);
  }
 }
}

static struct syscore_ops irq_gc_syscore_ops = {
 .suspend = irq_gc_suspend,
 .resume = irq_gc_resume,
 .shutdown = irq_gc_shutdown,
};

static int __init irq_gc_init_ops(void)
{
 register_syscore_ops(&irq_gc_syscore_ops);
 return 0;
}
device_initcall(irq_gc_init_ops);




struct group_info *groups_alloc(int gidsetsize)
{
 struct group_info *group_info;
 int nblocks;
 int i;

 nblocks = (gidsetsize + NGROUPS_PER_BLOCK - 1) / NGROUPS_PER_BLOCK;

 nblocks = nblocks ? : 1;
 group_info = kmalloc(sizeof(*group_info) + nblocks*sizeof(gid_t *), GFP_USER);
 if (!group_info)
  return NULL;
 group_info->ngroups = gidsetsize;
 group_info->nblocks = nblocks;
 atomic_set(&group_info->usage, 1);

 if (gidsetsize <= NGROUPS_SMALL)
  group_info->blocks[0] = group_info->small_block;
 else {
  for (i = 0; i < nblocks; i++) {
   kgid_t *b;
   b = (void *)__get_free_page(GFP_USER);
   if (!b)
    goto out_undo_partial_alloc;
   group_info->blocks[i] = b;
  }
 }
 return group_info;

out_undo_partial_alloc:
 while (--i >= 0) {
  free_page((unsigned long)group_info->blocks[i]);
 }
 kfree(group_info);
 return NULL;
}

EXPORT_SYMBOL(groups_alloc);

void groups_free(struct group_info *group_info)
{
 if (group_info->blocks[0] != group_info->small_block) {
  int i;
  for (i = 0; i < group_info->nblocks; i++)
   free_page((unsigned long)group_info->blocks[i]);
 }
 kfree(group_info);
}

EXPORT_SYMBOL(groups_free);


static int groups_to_user(gid_t __user *grouplist,
     const struct group_info *group_info)
{
 struct user_namespace *user_ns = current_user_ns();
 int i;
 unsigned int count = group_info->ngroups;

 for (i = 0; i < count; i++) {
  gid_t gid;
  gid = from_kgid_munged(user_ns, GROUP_AT(group_info, i));
  if (put_user(gid, grouplist+i))
   return -EFAULT;
 }
 return 0;
}


static int groups_from_user(struct group_info *group_info,
    gid_t __user *grouplist)
{
 struct user_namespace *user_ns = current_user_ns();
 int i;
 unsigned int count = group_info->ngroups;

 for (i = 0; i < count; i++) {
  gid_t gid;
  kgid_t kgid;
  if (get_user(gid, grouplist+i))
   return -EFAULT;

  kgid = make_kgid(user_ns, gid);
  if (!gid_valid(kgid))
   return -EINVAL;

  GROUP_AT(group_info, i) = kgid;
 }
 return 0;
}


static void groups_sort(struct group_info *group_info)
{
 int base, max, stride;
 int gidsetsize = group_info->ngroups;

 for (stride = 1; stride < gidsetsize; stride = 3 * stride + 1)
  ;
 stride /= 3;

 while (stride) {
  max = gidsetsize - stride;
  for (base = 0; base < max; base++) {
   int left = base;
   int right = left + stride;
   kgid_t tmp = GROUP_AT(group_info, right);

   while (left >= 0 && gid_gt(GROUP_AT(group_info, left), tmp)) {
    GROUP_AT(group_info, right) =
        GROUP_AT(group_info, left);
    right = left;
    left -= stride;
   }
   GROUP_AT(group_info, right) = tmp;
  }
  stride /= 3;
 }
}


int groups_search(const struct group_info *group_info, kgid_t grp)
{
 unsigned int left, right;

 if (!group_info)
  return 0;

 left = 0;
 right = group_info->ngroups;
 while (left < right) {
  unsigned int mid = (left+right)/2;
  if (gid_gt(grp, GROUP_AT(group_info, mid)))
   left = mid + 1;
  else if (gid_lt(grp, GROUP_AT(group_info, mid)))
   right = mid;
  else
   return 1;
 }
 return 0;
}






void set_groups(struct cred *new, struct group_info *group_info)
{
 put_group_info(new->group_info);
 groups_sort(group_info);
 get_group_info(group_info);
 new->group_info = group_info;
}

EXPORT_SYMBOL(set_groups);
int set_current_groups(struct group_info *group_info)
{
 struct cred *new;

 new = prepare_creds();
 if (!new)
  return -ENOMEM;

 set_groups(new, group_info);
 return commit_creds(new);
}

EXPORT_SYMBOL(set_current_groups);

SYSCALL_DEFINE2(getgroups, int, gidsetsize, gid_t __user *, grouplist)
{
 const struct cred *cred = current_cred();
 int i;

 if (gidsetsize < 0)
  return -EINVAL;


 i = cred->group_info->ngroups;
 if (gidsetsize) {
  if (i > gidsetsize) {
   i = -EINVAL;
   goto out;
  }
  if (groups_to_user(grouplist, cred->group_info)) {
   i = -EFAULT;
   goto out;
  }
 }
out:
 return i;
}

bool may_setgroups(void)
{
 struct user_namespace *user_ns = current_user_ns();

 return ns_capable(user_ns, CAP_SETGID) &&
  userns_may_setgroups(user_ns);
}






SYSCALL_DEFINE2(setgroups, int, gidsetsize, gid_t __user *, grouplist)
{
 struct group_info *group_info;
 int retval;

 if (!may_setgroups())
  return -EPERM;
 if ((unsigned)gidsetsize > NGROUPS_MAX)
  return -EINVAL;

 group_info = groups_alloc(gidsetsize);
 if (!group_info)
  return -ENOMEM;
 retval = groups_from_user(group_info, grouplist);
 if (retval) {
  put_group_info(group_info);
  return retval;
 }

 retval = set_current_groups(group_info);
 put_group_info(group_info);

 return retval;
}




int in_group_p(kgid_t grp)
{
 const struct cred *cred = current_cred();
 int retval = 1;

 if (!gid_eq(grp, cred->fsgid))
  retval = groups_search(cred->group_info, grp);
 return retval;
}

EXPORT_SYMBOL(in_group_p);

int in_egroup_p(kgid_t grp)
{
 const struct cred *cred = current_cred();
 int retval = 1;

 if (!gid_eq(grp, cred->egid))
  retval = groups_search(cred->group_info, grp);
 return retval;
}

EXPORT_SYMBOL(in_egroup_p);









void handle_bad_irq(struct irq_desc *desc)
{
 unsigned int irq = irq_desc_get_irq(desc);

 print_irq_desc(irq, desc);
 kstat_incr_irqs_this_cpu(desc);
 ack_bad_irq(irq);
}
EXPORT_SYMBOL_GPL(handle_bad_irq);




irqreturn_t no_action(int cpl, void *dev_id)
{
 return IRQ_NONE;
}
EXPORT_SYMBOL_GPL(no_action);

static void warn_no_thread(unsigned int irq, struct irqaction *action)
{
 if (test_and_set_bit(IRQTF_WARNED, &action->thread_flags))
  return;

 printk(KERN_WARNING "IRQ %d device %s returned IRQ_WAKE_THREAD "
        "but no thread function available.", irq, action->name);
}

void __irq_wake_thread(struct irq_desc *desc, struct irqaction *action)
{





 if (action->thread->flags & PF_EXITING)
  return;





 if (test_and_set_bit(IRQTF_RUNTHREAD, &action->thread_flags))
  return;
 desc->threads_oneshot |= action->thread_mask;
 atomic_inc(&desc->threads_active);

 wake_up_process(action->thread);
}

irqreturn_t handle_irq_event_percpu(struct irq_desc *desc)
{
 irqreturn_t retval = IRQ_NONE;
 unsigned int flags = 0, irq = desc->irq_data.irq;
 struct irqaction *action;

 for_each_action_of_desc(desc, action) {
  irqreturn_t res;

  trace_irq_handler_entry(irq, action);
  res = action->handler(irq, action->dev_id);
  trace_irq_handler_exit(irq, action, res);

  if (WARN_ONCE(!irqs_disabled(),"irq %u handler %pF enabled interrupts\n",
         irq, action->handler))
   local_irq_disable();

  switch (res) {
  case IRQ_WAKE_THREAD:




   if (unlikely(!action->thread_fn)) {
    warn_no_thread(irq, action);
    break;
   }

   __irq_wake_thread(desc, action);


  case IRQ_HANDLED:
   flags |= action->flags;
   break;

  default:
   break;
  }

  retval |= res;
 }

 add_interrupt_randomness(irq, flags);

 if (!noirqdebug)
  note_interrupt(desc, retval);
 return retval;
}

irqreturn_t handle_irq_event(struct irq_desc *desc)
{
 irqreturn_t ret;

 desc->istate &= ~IRQS_PENDING;
 irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
 raw_spin_unlock(&desc->lock);

 ret = handle_irq_event_percpu(desc);

 raw_spin_lock(&desc->lock);
 irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
 return ret;
}

struct bucket {
 struct hlist_head head;
 raw_spinlock_t lock;
};

struct bpf_htab {
 struct bpf_map map;
 struct bucket *buckets;
 void *elems;
 struct pcpu_freelist freelist;
 atomic_t count;
 u32 n_buckets;
 u32 elem_size;
};


struct htab_elem {
 union {
  struct hlist_node hash_node;
  struct bpf_htab *htab;
  struct pcpu_freelist_node fnode;
 };
 struct rcu_head rcu;
 u32 hash;
 char key[0] __aligned(8);
};

static inline void htab_elem_set_ptr(struct htab_elem *l, u32 key_size,
         void __percpu *pptr)
{
 *(void __percpu **)(l->key + key_size) = pptr;
}

static inline void __percpu *htab_elem_get_ptr(struct htab_elem *l, u32 key_size)
{
 return *(void __percpu **)(l->key + key_size);
}

static struct htab_elem *get_htab_elem(struct bpf_htab *htab, int i)
{
 return (struct htab_elem *) (htab->elems + i * htab->elem_size);
}

static void htab_free_elems(struct bpf_htab *htab)
{
 int i;

 if (htab->map.map_type != BPF_MAP_TYPE_PERCPU_HASH)
  goto free_elems;

 for (i = 0; i < htab->map.max_entries; i++) {
  void __percpu *pptr;

  pptr = htab_elem_get_ptr(get_htab_elem(htab, i),
      htab->map.key_size);
  free_percpu(pptr);
 }
free_elems:
 vfree(htab->elems);
}

static int prealloc_elems_and_freelist(struct bpf_htab *htab)
{
 int err = -ENOMEM, i;

 htab->elems = vzalloc(htab->elem_size * htab->map.max_entries);
 if (!htab->elems)
  return -ENOMEM;

 if (htab->map.map_type != BPF_MAP_TYPE_PERCPU_HASH)
  goto skip_percpu_elems;

 for (i = 0; i < htab->map.max_entries; i++) {
  u32 size = round_up(htab->map.value_size, 8);
  void __percpu *pptr;

  pptr = __alloc_percpu_gfp(size, 8, GFP_USER | __GFP_NOWARN);
  if (!pptr)
   goto free_elems;
  htab_elem_set_ptr(get_htab_elem(htab, i), htab->map.key_size,
      pptr);
 }

skip_percpu_elems:
 err = pcpu_freelist_init(&htab->freelist);
 if (err)
  goto free_elems;

 pcpu_freelist_populate(&htab->freelist, htab->elems, htab->elem_size,
          htab->map.max_entries);
 return 0;

free_elems:
 htab_free_elems(htab);
 return err;
}


static struct bpf_map *htab_map_alloc(union bpf_attr *attr)
{
 bool percpu = attr->map_type == BPF_MAP_TYPE_PERCPU_HASH;
 struct bpf_htab *htab;
 int err, i;
 u64 cost;

 if (attr->map_flags & ~BPF_F_NO_PREALLOC)

  return ERR_PTR(-EINVAL);

 htab = kzalloc(sizeof(*htab), GFP_USER);
 if (!htab)
  return ERR_PTR(-ENOMEM);


 htab->map.map_type = attr->map_type;
 htab->map.key_size = attr->key_size;
 htab->map.value_size = attr->value_size;
 htab->map.max_entries = attr->max_entries;
 htab->map.map_flags = attr->map_flags;




 err = -EINVAL;
 if (htab->map.max_entries == 0 || htab->map.key_size == 0 ||
     htab->map.value_size == 0)
  goto free_htab;


 htab->n_buckets = roundup_pow_of_two(htab->map.max_entries);

 err = -E2BIG;
 if (htab->map.key_size > MAX_BPF_STACK)



  goto free_htab;

 if (htab->map.value_size >= (1 << (KMALLOC_SHIFT_MAX - 1)) -
     MAX_BPF_STACK - sizeof(struct htab_elem))





  goto free_htab;

 if (percpu && round_up(htab->map.value_size, 8) > PCPU_MIN_UNIT_SIZE)

  goto free_htab;

 htab->elem_size = sizeof(struct htab_elem) +
     round_up(htab->map.key_size, 8);
 if (percpu)
  htab->elem_size += sizeof(void *);
 else
  htab->elem_size += round_up(htab->map.value_size, 8);


 if (htab->n_buckets == 0 ||
     htab->n_buckets > U32_MAX / sizeof(struct bucket))
  goto free_htab;

 cost = (u64) htab->n_buckets * sizeof(struct bucket) +
        (u64) htab->elem_size * htab->map.max_entries;

 if (percpu)
  cost += (u64) round_up(htab->map.value_size, 8) *
   num_possible_cpus() * htab->map.max_entries;

 if (cost >= U32_MAX - PAGE_SIZE)

  goto free_htab;

 htab->map.pages = round_up(cost, PAGE_SIZE) >> PAGE_SHIFT;


 err = bpf_map_precharge_memlock(htab->map.pages);
 if (err)
  goto free_htab;

 err = -ENOMEM;
 htab->buckets = kmalloc_array(htab->n_buckets, sizeof(struct bucket),
          GFP_USER | __GFP_NOWARN);

 if (!htab->buckets) {
  htab->buckets = vmalloc(htab->n_buckets * sizeof(struct bucket));
  if (!htab->buckets)
   goto free_htab;
 }

 for (i = 0; i < htab->n_buckets; i++) {
  INIT_HLIST_HEAD(&htab->buckets[i].head);
  raw_spin_lock_init(&htab->buckets[i].lock);
 }

 if (!(attr->map_flags & BPF_F_NO_PREALLOC)) {
  err = prealloc_elems_and_freelist(htab);
  if (err)
   goto free_buckets;
 }

 return &htab->map;

free_buckets:
 kvfree(htab->buckets);
free_htab:
 kfree(htab);
 return ERR_PTR(err);
}

static inline u32 htab_map_hash(const void *key, u32 key_len)
{
 return jhash(key, key_len, 0);
}

static inline struct bucket *__select_bucket(struct bpf_htab *htab, u32 hash)
{
 return &htab->buckets[hash & (htab->n_buckets - 1)];
}

static inline struct hlist_head *select_bucket(struct bpf_htab *htab, u32 hash)
{
 return &__select_bucket(htab, hash)->head;
}

static struct htab_elem *lookup_elem_raw(struct hlist_head *head, u32 hash,
      void *key, u32 key_size)
{
 struct htab_elem *l;

 hlist_for_each_entry_rcu(l, head, hash_node)
  if (l->hash == hash && !memcmp(&l->key, key, key_size))
   return l;

 return NULL;
}


static void *__htab_map_lookup_elem(struct bpf_map *map, void *key)
{
 struct bpf_htab *htab = container_of(map, struct bpf_htab, map);
 struct hlist_head *head;
 struct htab_elem *l;
 u32 hash, key_size;


 WARN_ON_ONCE(!rcu_read_lock_held());

 key_size = map->key_size;

 hash = htab_map_hash(key, key_size);

 head = select_bucket(htab, hash);

 l = lookup_elem_raw(head, hash, key, key_size);

 return l;
}

static void *htab_map_lookup_elem(struct bpf_map *map, void *key)
{
 struct htab_elem *l = __htab_map_lookup_elem(map, key);

 if (l)
  return l->key + round_up(map->key_size, 8);

 return NULL;
}


static int htab_map_get_next_key(struct bpf_map *map, void *key, void *next_key)
{
 struct bpf_htab *htab = container_of(map, struct bpf_htab, map);
 struct hlist_head *head;
 struct htab_elem *l, *next_l;
 u32 hash, key_size;
 int i;

 WARN_ON_ONCE(!rcu_read_lock_held());

 key_size = map->key_size;

 hash = htab_map_hash(key, key_size);

 head = select_bucket(htab, hash);


 l = lookup_elem_raw(head, hash, key, key_size);

 if (!l) {
  i = 0;
  goto find_first_elem;
 }


 next_l = hlist_entry_safe(rcu_dereference_raw(hlist_next_rcu(&l->hash_node)),
      struct htab_elem, hash_node);

 if (next_l) {

  memcpy(next_key, next_l->key, key_size);
  return 0;
 }


 i = hash & (htab->n_buckets - 1);
 i++;

find_first_elem:

 for (; i < htab->n_buckets; i++) {
  head = select_bucket(htab, i);


  next_l = hlist_entry_safe(rcu_dereference_raw(hlist_first_rcu(head)),
       struct htab_elem, hash_node);
  if (next_l) {

   memcpy(next_key, next_l->key, key_size);
   return 0;
  }
 }


 return -ENOENT;
}

static void htab_elem_free(struct bpf_htab *htab, struct htab_elem *l)
{
 if (htab->map.map_type == BPF_MAP_TYPE_PERCPU_HASH)
  free_percpu(htab_elem_get_ptr(l, htab->map.key_size));
 kfree(l);

}

static void htab_elem_free_rcu(struct rcu_head *head)
{
 struct htab_elem *l = container_of(head, struct htab_elem, rcu);
 struct bpf_htab *htab = l->htab;





 preempt_disable();
 __this_cpu_inc(bpf_prog_active);
 htab_elem_free(htab, l);
 __this_cpu_dec(bpf_prog_active);
 preempt_enable();
}

static void free_htab_elem(struct bpf_htab *htab, struct htab_elem *l)
{
 if (!(htab->map.map_flags & BPF_F_NO_PREALLOC)) {
  pcpu_freelist_push(&htab->freelist, &l->fnode);
 } else {
  atomic_dec(&htab->count);
  l->htab = htab;
  call_rcu(&l->rcu, htab_elem_free_rcu);
 }
}

static struct htab_elem *alloc_htab_elem(struct bpf_htab *htab, void *key,
      void *value, u32 key_size, u32 hash,
      bool percpu, bool onallcpus)
{
 u32 size = htab->map.value_size;
 bool prealloc = !(htab->map.map_flags & BPF_F_NO_PREALLOC);
 struct htab_elem *l_new;
 void __percpu *pptr;

 if (prealloc) {
  l_new = (struct htab_elem *)pcpu_freelist_pop(&htab->freelist);
  if (!l_new)
   return ERR_PTR(-E2BIG);
 } else {
  if (atomic_inc_return(&htab->count) > htab->map.max_entries) {
   atomic_dec(&htab->count);
   return ERR_PTR(-E2BIG);
  }
  l_new = kmalloc(htab->elem_size, GFP_ATOMIC | __GFP_NOWARN);
  if (!l_new)
   return ERR_PTR(-ENOMEM);
 }

 memcpy(l_new->key, key, key_size);
 if (percpu) {

  size = round_up(size, 8);

  if (prealloc) {
   pptr = htab_elem_get_ptr(l_new, key_size);
  } else {

   pptr = __alloc_percpu_gfp(size, 8,
        GFP_ATOMIC | __GFP_NOWARN);
   if (!pptr) {
    kfree(l_new);
    return ERR_PTR(-ENOMEM);
   }
  }

  if (!onallcpus) {

   memcpy(this_cpu_ptr(pptr), value, htab->map.value_size);
  } else {
   int off = 0, cpu;

   for_each_possible_cpu(cpu) {
    bpf_long_memcpy(per_cpu_ptr(pptr, cpu),
      value + off, size);
    off += size;
   }
  }
  if (!prealloc)
   htab_elem_set_ptr(l_new, key_size, pptr);
 } else {
  memcpy(l_new->key + round_up(key_size, 8), value, size);
 }

 l_new->hash = hash;
 return l_new;
}

static int check_flags(struct bpf_htab *htab, struct htab_elem *l_old,
         u64 map_flags)
{
 if (l_old && map_flags == BPF_NOEXIST)

  return -EEXIST;

 if (!l_old && map_flags == BPF_EXIST)

  return -ENOENT;

 return 0;
}


static int htab_map_update_elem(struct bpf_map *map, void *key, void *value,
    u64 map_flags)
{
 struct bpf_htab *htab = container_of(map, struct bpf_htab, map);
 struct htab_elem *l_new = NULL, *l_old;
 struct hlist_head *head;
 unsigned long flags;
 struct bucket *b;
 u32 key_size, hash;
 int ret;

 if (unlikely(map_flags > BPF_EXIST))

  return -EINVAL;

 WARN_ON_ONCE(!rcu_read_lock_held());

 key_size = map->key_size;

 hash = htab_map_hash(key, key_size);

 b = __select_bucket(htab, hash);
 head = &b->head;


 raw_spin_lock_irqsave(&b->lock, flags);

 l_old = lookup_elem_raw(head, hash, key, key_size);

 ret = check_flags(htab, l_old, map_flags);
 if (ret)
  goto err;

 l_new = alloc_htab_elem(htab, key, value, key_size, hash, false, false);
 if (IS_ERR(l_new)) {

  ret = PTR_ERR(l_new);
  goto err;
 }




 hlist_add_head_rcu(&l_new->hash_node, head);
 if (l_old) {
  hlist_del_rcu(&l_old->hash_node);
  free_htab_elem(htab, l_old);
 }
 ret = 0;
err:
 raw_spin_unlock_irqrestore(&b->lock, flags);
 return ret;
}

static int __htab_percpu_map_update_elem(struct bpf_map *map, void *key,
      void *value, u64 map_flags,
      bool onallcpus)
{
 struct bpf_htab *htab = container_of(map, struct bpf_htab, map);
 struct htab_elem *l_new = NULL, *l_old;
 struct hlist_head *head;
 unsigned long flags;
 struct bucket *b;
 u32 key_size, hash;
 int ret;

 if (unlikely(map_flags > BPF_EXIST))

  return -EINVAL;

 WARN_ON_ONCE(!rcu_read_lock_held());

 key_size = map->key_size;

 hash = htab_map_hash(key, key_size);

 b = __select_bucket(htab, hash);
 head = &b->head;


 raw_spin_lock_irqsave(&b->lock, flags);

 l_old = lookup_elem_raw(head, hash, key, key_size);

 ret = check_flags(htab, l_old, map_flags);
 if (ret)
  goto err;

 if (l_old) {
  void __percpu *pptr = htab_elem_get_ptr(l_old, key_size);
  u32 size = htab->map.value_size;


  if (!onallcpus) {
   memcpy(this_cpu_ptr(pptr), value, size);
  } else {
   int off = 0, cpu;

   size = round_up(size, 8);
   for_each_possible_cpu(cpu) {
    bpf_long_memcpy(per_cpu_ptr(pptr, cpu),
      value + off, size);
    off += size;
   }
  }
 } else {
  l_new = alloc_htab_elem(htab, key, value, key_size,
     hash, true, onallcpus);
  if (IS_ERR(l_new)) {
   ret = PTR_ERR(l_new);
   goto err;
  }
  hlist_add_head_rcu(&l_new->hash_node, head);
 }
 ret = 0;
err:
 raw_spin_unlock_irqrestore(&b->lock, flags);
 return ret;
}

static int htab_percpu_map_update_elem(struct bpf_map *map, void *key,
           void *value, u64 map_flags)
{
 return __htab_percpu_map_update_elem(map, key, value, map_flags, false);
}


static int htab_map_delete_elem(struct bpf_map *map, void *key)
{
 struct bpf_htab *htab = container_of(map, struct bpf_htab, map);
 struct hlist_head *head;
 struct bucket *b;
 struct htab_elem *l;
 unsigned long flags;
 u32 hash, key_size;
 int ret = -ENOENT;

 WARN_ON_ONCE(!rcu_read_lock_held());

 key_size = map->key_size;

 hash = htab_map_hash(key, key_size);
 b = __select_bucket(htab, hash);
 head = &b->head;

 raw_spin_lock_irqsave(&b->lock, flags);

 l = lookup_elem_raw(head, hash, key, key_size);

 if (l) {
  hlist_del_rcu(&l->hash_node);
  free_htab_elem(htab, l);
  ret = 0;
 }

 raw_spin_unlock_irqrestore(&b->lock, flags);
 return ret;
}

static void delete_all_elements(struct bpf_htab *htab)
{
 int i;

 for (i = 0; i < htab->n_buckets; i++) {
  struct hlist_head *head = select_bucket(htab, i);
  struct hlist_node *n;
  struct htab_elem *l;

  hlist_for_each_entry_safe(l, n, head, hash_node) {
   hlist_del_rcu(&l->hash_node);
   htab_elem_free(htab, l);
  }
 }
}

static void htab_map_free(struct bpf_map *map)
{
 struct bpf_htab *htab = container_of(map, struct bpf_htab, map);






 synchronize_rcu();




 rcu_barrier();
 if (htab->map.map_flags & BPF_F_NO_PREALLOC) {
  delete_all_elements(htab);
 } else {
  htab_free_elems(htab);
  pcpu_freelist_destroy(&htab->freelist);
 }
 kvfree(htab->buckets);
 kfree(htab);
}

static const struct bpf_map_ops htab_ops = {
 .map_alloc = htab_map_alloc,
 .map_free = htab_map_free,
 .map_get_next_key = htab_map_get_next_key,
 .map_lookup_elem = htab_map_lookup_elem,
 .map_update_elem = htab_map_update_elem,
 .map_delete_elem = htab_map_delete_elem,
};

static struct bpf_map_type_list htab_type __read_mostly = {
 .ops = &htab_ops,
 .type = BPF_MAP_TYPE_HASH,
};


static void *htab_percpu_map_lookup_elem(struct bpf_map *map, void *key)
{
 struct htab_elem *l = __htab_map_lookup_elem(map, key);

 if (l)
  return this_cpu_ptr(htab_elem_get_ptr(l, map->key_size));
 else
  return NULL;
}

int bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value)
{
 struct htab_elem *l;
 void __percpu *pptr;
 int ret = -ENOENT;
 int cpu, off = 0;
 u32 size;





 size = round_up(map->value_size, 8);
 rcu_read_lock();
 l = __htab_map_lookup_elem(map, key);
 if (!l)
  goto out;
 pptr = htab_elem_get_ptr(l, map->key_size);
 for_each_possible_cpu(cpu) {
  bpf_long_memcpy(value + off,
    per_cpu_ptr(pptr, cpu), size);
  off += size;
 }
 ret = 0;
out:
 rcu_read_unlock();
 return ret;
}

int bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,
      u64 map_flags)
{
 int ret;

 rcu_read_lock();
 ret = __htab_percpu_map_update_elem(map, key, value, map_flags, true);
 rcu_read_unlock();

 return ret;
}

static const struct bpf_map_ops htab_percpu_ops = {
 .map_alloc = htab_map_alloc,
 .map_free = htab_map_free,
 .map_get_next_key = htab_map_get_next_key,
 .map_lookup_elem = htab_percpu_map_lookup_elem,
 .map_update_elem = htab_percpu_map_update_elem,
 .map_delete_elem = htab_map_delete_elem,
};

static struct bpf_map_type_list htab_percpu_type __read_mostly = {
 .ops = &htab_percpu_ops,
 .type = BPF_MAP_TYPE_PERCPU_HASH,
};

static int __init register_htab_map(void)
{
 bpf_register_map_type(&htab_type);
 bpf_register_map_type(&htab_percpu_type);
 return 0;
}
late_initcall(register_htab_map);
static u64 bpf_map_lookup_elem(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
{




 struct bpf_map *map = (struct bpf_map *) (unsigned long) r1;
 void *key = (void *) (unsigned long) r2;
 void *value;

 WARN_ON_ONCE(!rcu_read_lock_held());

 value = map->ops->map_lookup_elem(map, key);




 return (unsigned long) value;
}

const struct bpf_func_proto bpf_map_lookup_elem_proto = {
 .func = bpf_map_lookup_elem,
 .gpl_only = false,
 .ret_type = RET_PTR_TO_MAP_VALUE_OR_NULL,
 .arg1_type = ARG_CONST_MAP_PTR,
 .arg2_type = ARG_PTR_TO_MAP_KEY,
};

static u64 bpf_map_update_elem(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
{
 struct bpf_map *map = (struct bpf_map *) (unsigned long) r1;
 void *key = (void *) (unsigned long) r2;
 void *value = (void *) (unsigned long) r3;

 WARN_ON_ONCE(!rcu_read_lock_held());

 return map->ops->map_update_elem(map, key, value, r4);
}

const struct bpf_func_proto bpf_map_update_elem_proto = {
 .func = bpf_map_update_elem,
 .gpl_only = false,
 .ret_type = RET_INTEGER,
 .arg1_type = ARG_CONST_MAP_PTR,
 .arg2_type = ARG_PTR_TO_MAP_KEY,
 .arg3_type = ARG_PTR_TO_MAP_VALUE,
 .arg4_type = ARG_ANYTHING,
};

static u64 bpf_map_delete_elem(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
{
 struct bpf_map *map = (struct bpf_map *) (unsigned long) r1;
 void *key = (void *) (unsigned long) r2;

 WARN_ON_ONCE(!rcu_read_lock_held());

 return map->ops->map_delete_elem(map, key);
}

const struct bpf_func_proto bpf_map_delete_elem_proto = {
 .func = bpf_map_delete_elem,
 .gpl_only = false,
 .ret_type = RET_INTEGER,
 .arg1_type = ARG_CONST_MAP_PTR,
 .arg2_type = ARG_PTR_TO_MAP_KEY,
};

const struct bpf_func_proto bpf_get_prandom_u32_proto = {
 .func = bpf_user_rnd_u32,
 .gpl_only = false,
 .ret_type = RET_INTEGER,
};

static u64 bpf_get_smp_processor_id(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
{
 return raw_smp_processor_id();
}

const struct bpf_func_proto bpf_get_smp_processor_id_proto = {
 .func = bpf_get_smp_processor_id,
 .gpl_only = false,
 .ret_type = RET_INTEGER,
};

static u64 bpf_ktime_get_ns(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
{

 return ktime_get_mono_fast_ns();
}

const struct bpf_func_proto bpf_ktime_get_ns_proto = {
 .func = bpf_ktime_get_ns,
 .gpl_only = true,
 .ret_type = RET_INTEGER,
};

static u64 bpf_get_current_pid_tgid(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
{
 struct task_struct *task = current;

 if (!task)
  return -EINVAL;

 return (u64) task->tgid << 32 | task->pid;
}

const struct bpf_func_proto bpf_get_current_pid_tgid_proto = {
 .func = bpf_get_current_pid_tgid,
 .gpl_only = false,
 .ret_type = RET_INTEGER,
};

static u64 bpf_get_current_uid_gid(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)
{
 struct task_struct *task = current;
 kuid_t uid;
 kgid_t gid;

 if (!task)
  return -EINVAL;

 current_uid_gid(&uid, &gid);
 return (u64) from_kgid(&init_user_ns, gid) << 32 |
  from_kuid(&init_user_ns, uid);
}

const struct bpf_func_proto bpf_get_current_uid_gid_proto = {
 .func = bpf_get_current_uid_gid,
 .gpl_only = false,
 .ret_type = RET_INTEGER,
};

static u64 bpf_get_current_comm(u64 r1, u64 size, u64 r3, u64 r4, u64 r5)
{
 struct task_struct *task = current;
 char *buf = (char *) (long) r1;

 if (unlikely(!task))
  goto err_clear;

 strncpy(buf, task->comm, size);





 buf[size - 1] = 0;
 return 0;
err_clear:
 memset(buf, 0, size);
 return -EINVAL;
}

const struct bpf_func_proto bpf_get_current_comm_proto = {
 .func = bpf_get_current_comm,
 .gpl_only = false,
 .ret_type = RET_INTEGER,
 .arg1_type = ARG_PTR_TO_RAW_STACK,
 .arg2_type = ARG_CONST_STACK_SIZE,
};



static int nocompress;
static int noresume;
static int nohibernate;
static int resume_wait;
static unsigned int resume_delay;
static char resume_file[256] = CONFIG_PM_STD_PARTITION;
dev_t swsusp_resume_device;
sector_t swsusp_resume_block;
__visible int in_suspend __nosavedata;

enum {
 HIBERNATION_INVALID,
 HIBERNATION_PLATFORM,
 HIBERNATION_SHUTDOWN,
 HIBERNATION_REBOOT,
 HIBERNATION_SUSPEND,

 __HIBERNATION_AFTER_LAST
};

static int hibernation_mode = HIBERNATION_SHUTDOWN;

bool freezer_test_done;

static const struct platform_hibernation_ops *hibernation_ops;

bool hibernation_available(void)
{
 return (nohibernate == 0);
}





void hibernation_set_ops(const struct platform_hibernation_ops *ops)
{
 if (ops && !(ops->begin && ops->end && ops->pre_snapshot
     && ops->prepare && ops->finish && ops->enter && ops->pre_restore
     && ops->restore_cleanup && ops->leave)) {
  WARN_ON(1);
  return;
 }
 lock_system_sleep();
 hibernation_ops = ops;
 if (ops)
  hibernation_mode = HIBERNATION_PLATFORM;
 else if (hibernation_mode == HIBERNATION_PLATFORM)
  hibernation_mode = HIBERNATION_SHUTDOWN;

 unlock_system_sleep();
}
EXPORT_SYMBOL_GPL(hibernation_set_ops);

static bool entering_platform_hibernation;

bool system_entering_hibernation(void)
{
 return entering_platform_hibernation;
}
EXPORT_SYMBOL(system_entering_hibernation);

static void hibernation_debug_sleep(void)
{
 printk(KERN_INFO "hibernation debug: Waiting for 5 seconds.\n");
 mdelay(5000);
}

static int hibernation_test(int level)
{
 if (pm_test_level == level) {
  hibernation_debug_sleep();
  return 1;
 }
 return 0;
}
static int hibernation_test(int level) { return 0; }





static int platform_begin(int platform_mode)
{
 return (platform_mode && hibernation_ops) ?
  hibernation_ops->begin() : 0;
}





static void platform_end(int platform_mode)
{
 if (platform_mode && hibernation_ops)
  hibernation_ops->end();
}
static int platform_pre_snapshot(int platform_mode)
{
 return (platform_mode && hibernation_ops) ?
  hibernation_ops->pre_snapshot() : 0;
}
static void platform_leave(int platform_mode)
{
 if (platform_mode && hibernation_ops)
  hibernation_ops->leave();
}
static void platform_finish(int platform_mode)
{
 if (platform_mode && hibernation_ops)
  hibernation_ops->finish();
}
static int platform_pre_restore(int platform_mode)
{
 return (platform_mode && hibernation_ops) ?
  hibernation_ops->pre_restore() : 0;
}
static void platform_restore_cleanup(int platform_mode)
{
 if (platform_mode && hibernation_ops)
  hibernation_ops->restore_cleanup();
}





static void platform_recover(int platform_mode)
{
 if (platform_mode && hibernation_ops && hibernation_ops->recover)
  hibernation_ops->recover();
}
void swsusp_show_speed(ktime_t start, ktime_t stop,
        unsigned nr_pages, char *msg)
{
 ktime_t diff;
 u64 elapsed_centisecs64;
 unsigned int centisecs;
 unsigned int k;
 unsigned int kps;

 diff = ktime_sub(stop, start);
 elapsed_centisecs64 = ktime_divns(diff, 10*NSEC_PER_MSEC);
 centisecs = elapsed_centisecs64;
 if (centisecs == 0)
  centisecs = 1;
 k = nr_pages * (PAGE_SIZE / 1024);
 kps = (k * 100) / centisecs;
 printk(KERN_INFO "PM: %s %u kbytes in %u.%02u seconds (%u.%02u MB/s)\n",
   msg, k,
   centisecs / 100, centisecs % 100,
   kps / 1000, (kps % 1000) / 10);
}
static int create_image(int platform_mode)
{
 int error;

 error = dpm_suspend_end(PMSG_FREEZE);
 if (error) {
  printk(KERN_ERR "PM: Some devices failed to power down, "
   "aborting hibernation\n");
  return error;
 }

 error = platform_pre_snapshot(platform_mode);
 if (error || hibernation_test(TEST_PLATFORM))
  goto Platform_finish;

 error = disable_nonboot_cpus();
 if (error || hibernation_test(TEST_CPUS))
  goto Enable_cpus;

 local_irq_disable();

 error = syscore_suspend();
 if (error) {
  printk(KERN_ERR "PM: Some system devices failed to power down, "
   "aborting hibernation\n");
  goto Enable_irqs;
 }

 if (hibernation_test(TEST_CORE) || pm_wakeup_pending())
  goto Power_up;

 in_suspend = 1;
 save_processor_state();
 trace_suspend_resume(TPS("machine_suspend"), PM_EVENT_HIBERNATE, true);
 error = swsusp_arch_suspend();
 trace_suspend_resume(TPS("machine_suspend"), PM_EVENT_HIBERNATE, false);
 if (error)
  printk(KERN_ERR "PM: Error %d creating hibernation image\n",
   error);

 restore_processor_state();
 if (!in_suspend)
  events_check_enabled = false;

 platform_leave(platform_mode);

 Power_up:
 syscore_resume();

 Enable_irqs:
 local_irq_enable();

 Enable_cpus:
 enable_nonboot_cpus();

 Platform_finish:
 platform_finish(platform_mode);

 dpm_resume_start(in_suspend ?
  (error ? PMSG_RECOVER : PMSG_THAW) : PMSG_RESTORE);

 return error;
}







int hibernation_snapshot(int platform_mode)
{
 pm_message_t msg;
 int error;

 pm_suspend_clear_flags();
 error = platform_begin(platform_mode);
 if (error)
  goto Close;


 error = hibernate_preallocate_memory();
 if (error)
  goto Close;

 error = freeze_kernel_threads();
 if (error)
  goto Cleanup;

 if (hibernation_test(TEST_FREEZER)) {





  freezer_test_done = true;
  goto Thaw;
 }

 error = dpm_prepare(PMSG_FREEZE);
 if (error) {
  dpm_complete(PMSG_RECOVER);
  goto Thaw;
 }

 suspend_console();
 pm_restrict_gfp_mask();

 error = dpm_suspend(PMSG_FREEZE);

 if (error || hibernation_test(TEST_DEVICES))
  platform_recover(platform_mode);
 else
  error = create_image(platform_mode);
 if (error || !in_suspend)
  swsusp_free();

 msg = in_suspend ? (error ? PMSG_RECOVER : PMSG_THAW) : PMSG_RESTORE;
 dpm_resume(msg);

 if (error || !in_suspend)
  pm_restore_gfp_mask();

 resume_console();
 dpm_complete(msg);

 Close:
 platform_end(platform_mode);
 return error;

 Thaw:
 thaw_kernel_threads();
 Cleanup:
 swsusp_free();
 goto Close;
}
static int resume_target_kernel(bool platform_mode)
{
 int error;

 error = dpm_suspend_end(PMSG_QUIESCE);
 if (error) {
  printk(KERN_ERR "PM: Some devices failed to power down, "
   "aborting resume\n");
  return error;
 }

 error = platform_pre_restore(platform_mode);
 if (error)
  goto Cleanup;

 error = disable_nonboot_cpus();
 if (error)
  goto Enable_cpus;

 local_irq_disable();

 error = syscore_suspend();
 if (error)
  goto Enable_irqs;

 save_processor_state();
 error = restore_highmem();
 if (!error) {
  error = swsusp_arch_resume();





  BUG_ON(!error);




  restore_highmem();
 }





 swsusp_free();
 restore_processor_state();
 touch_softlockup_watchdog();

 syscore_resume();

 Enable_irqs:
 local_irq_enable();

 Enable_cpus:
 enable_nonboot_cpus();

 Cleanup:
 platform_restore_cleanup(platform_mode);

 dpm_resume_start(PMSG_RECOVER);

 return error;
}
int hibernation_restore(int platform_mode)
{
 int error;

 pm_prepare_console();
 suspend_console();
 pm_restrict_gfp_mask();
 error = dpm_suspend_start(PMSG_QUIESCE);
 if (!error) {
  error = resume_target_kernel(platform_mode);





  BUG_ON(!error);
 }
 dpm_resume_end(PMSG_RECOVER);
 pm_restore_gfp_mask();
 resume_console();
 pm_restore_console();
 return error;
}




int hibernation_platform_enter(void)
{
 int error;

 if (!hibernation_ops)
  return -ENOSYS;






 error = hibernation_ops->begin();
 if (error)
  goto Close;

 entering_platform_hibernation = true;
 suspend_console();
 error = dpm_suspend_start(PMSG_HIBERNATE);
 if (error) {
  if (hibernation_ops->recover)
   hibernation_ops->recover();
  goto Resume_devices;
 }

 error = dpm_suspend_end(PMSG_HIBERNATE);
 if (error)
  goto Resume_devices;

 error = hibernation_ops->prepare();
 if (error)
  goto Platform_finish;

 error = disable_nonboot_cpus();
 if (error)
  goto Enable_cpus;

 local_irq_disable();
 syscore_suspend();
 if (pm_wakeup_pending()) {
  error = -EAGAIN;
  goto Power_up;
 }

 hibernation_ops->enter();

 while (1);

 Power_up:
 syscore_resume();
 local_irq_enable();

 Enable_cpus:
 enable_nonboot_cpus();

 Platform_finish:
 hibernation_ops->finish();

 dpm_resume_start(PMSG_RESTORE);

 Resume_devices:
 entering_platform_hibernation = false;
 dpm_resume_end(PMSG_RESTORE);
 resume_console();

 Close:
 hibernation_ops->end();

 return error;
}
static void power_down(void)
{
 int error;

 switch (hibernation_mode) {
 case HIBERNATION_REBOOT:
  kernel_restart(NULL);
  break;
 case HIBERNATION_PLATFORM:
  hibernation_platform_enter();
 case HIBERNATION_SHUTDOWN:
  if (pm_power_off)
   kernel_power_off();
  break;
 case HIBERNATION_SUSPEND:
  error = suspend_devices_and_enter(PM_SUSPEND_MEM);
  if (error) {
   if (hibernation_ops)
    hibernation_mode = HIBERNATION_PLATFORM;
   else
    hibernation_mode = HIBERNATION_SHUTDOWN;
   power_down();
  }



  error = swsusp_unmark();
  if (error)
   printk(KERN_ERR "PM: Swap will be unusable! "
                   "Try swapon -a.\n");
  return;
 }
 kernel_halt();




 printk(KERN_CRIT "PM: Please power down manually\n");
 while (1)
  cpu_relax();
}




int hibernate(void)
{
 int error;

 if (!hibernation_available()) {
  pr_debug("PM: Hibernation not available.\n");
  return -EPERM;
 }

 lock_system_sleep();

 if (!atomic_add_unless(&snapshot_device_available, -1, 0)) {
  error = -EBUSY;
  goto Unlock;
 }

 pm_prepare_console();
 error = pm_notifier_call_chain(PM_HIBERNATION_PREPARE);
 if (error)
  goto Exit;

 printk(KERN_INFO "PM: Syncing filesystems ... ");
 sys_sync();
 printk("done.\n");

 error = freeze_processes();
 if (error)
  goto Exit;

 lock_device_hotplug();

 error = create_basic_memory_bitmaps();
 if (error)
  goto Thaw;

 error = hibernation_snapshot(hibernation_mode == HIBERNATION_PLATFORM);
 if (error || freezer_test_done)
  goto Free_bitmaps;

 if (in_suspend) {
  unsigned int flags = 0;

  if (hibernation_mode == HIBERNATION_PLATFORM)
   flags |= SF_PLATFORM_MODE;
  if (nocompress)
   flags |= SF_NOCOMPRESS_MODE;
  else
          flags |= SF_CRC32_MODE;

  pr_debug("PM: writing image.\n");
  error = swsusp_write(flags);
  swsusp_free();
  if (!error)
   power_down();
  in_suspend = 0;
  pm_restore_gfp_mask();
 } else {
  pr_debug("PM: Image restored successfully.\n");
 }

 Free_bitmaps:
 free_basic_memory_bitmaps();
 Thaw:
 unlock_device_hotplug();
 thaw_processes();


 freezer_test_done = false;
 Exit:
 pm_notifier_call_chain(PM_POST_HIBERNATION);
 pm_restore_console();
 atomic_inc(&snapshot_device_available);
 Unlock:
 unlock_system_sleep();
 return error;
}
static int software_resume(void)
{
 int error;
 unsigned int flags;




 if (noresume || !hibernation_available())
  return 0;
 mutex_lock_nested(&pm_mutex, SINGLE_DEPTH_NESTING);

 if (swsusp_resume_device)
  goto Check_image;

 if (!strlen(resume_file)) {
  error = -ENOENT;
  goto Unlock;
 }

 pr_debug("PM: Checking hibernation image partition %s\n", resume_file);

 if (resume_delay) {
  printk(KERN_INFO "Waiting %dsec before reading resume device...\n",
   resume_delay);
  ssleep(resume_delay);
 }


 swsusp_resume_device = name_to_dev_t(resume_file);





 if (isdigit(resume_file[0]) && resume_wait) {
  int partno;
  while (!get_gendisk(swsusp_resume_device, &partno))
   msleep(10);
 }

 if (!swsusp_resume_device) {




  wait_for_device_probe();

  if (resume_wait) {
   while ((swsusp_resume_device = name_to_dev_t(resume_file)) == 0)
    msleep(10);
   async_synchronize_full();
  }

  swsusp_resume_device = name_to_dev_t(resume_file);
  if (!swsusp_resume_device) {
   error = -ENODEV;
   goto Unlock;
  }
 }

 Check_image:
 pr_debug("PM: Hibernation image partition %d:%d present\n",
  MAJOR(swsusp_resume_device), MINOR(swsusp_resume_device));

 pr_debug("PM: Looking for hibernation image.\n");
 error = swsusp_check();
 if (error)
  goto Unlock;


 if (!atomic_add_unless(&snapshot_device_available, -1, 0)) {
  error = -EBUSY;
  swsusp_close(FMODE_READ);
  goto Unlock;
 }

 pm_prepare_console();
 error = pm_notifier_call_chain(PM_RESTORE_PREPARE);
 if (error)
  goto Close_Finish;

 pr_debug("PM: Preparing processes for restore.\n");
 error = freeze_processes();
 if (error)
  goto Close_Finish;

 pr_debug("PM: Loading hibernation image.\n");

 lock_device_hotplug();
 error = create_basic_memory_bitmaps();
 if (error)
  goto Thaw;

 error = swsusp_read(&flags);
 swsusp_close(FMODE_READ);
 if (!error)
  hibernation_restore(flags & SF_PLATFORM_MODE);

 printk(KERN_ERR "PM: Failed to load hibernation image, recovering.\n");
 swsusp_free();
 free_basic_memory_bitmaps();
 Thaw:
 unlock_device_hotplug();
 thaw_processes();
 Finish:
 pm_notifier_call_chain(PM_POST_RESTORE);
 pm_restore_console();
 atomic_inc(&snapshot_device_available);

 Unlock:
 mutex_unlock(&pm_mutex);
 pr_debug("PM: Hibernation image not present or could not be loaded.\n");
 return error;
 Close_Finish:
 swsusp_close(FMODE_READ);
 goto Finish;
}

late_initcall_sync(software_resume);


static const char * const hibernation_modes[] = {
 [HIBERNATION_PLATFORM] = "platform",
 [HIBERNATION_SHUTDOWN] = "shutdown",
 [HIBERNATION_REBOOT] = "reboot",
 [HIBERNATION_SUSPEND] = "suspend",
};
static ssize_t disk_show(struct kobject *kobj, struct kobj_attribute *attr,
    char *buf)
{
 int i;
 char *start = buf;

 if (!hibernation_available())
  return sprintf(buf, "[disabled]\n");

 for (i = HIBERNATION_FIRST; i <= HIBERNATION_MAX; i++) {
  if (!hibernation_modes[i])
   continue;
  switch (i) {
  case HIBERNATION_SHUTDOWN:
  case HIBERNATION_REBOOT:
  case HIBERNATION_SUSPEND:
   break;
  case HIBERNATION_PLATFORM:
   if (hibernation_ops)
    break;

   continue;
  }
  if (i == hibernation_mode)
   buf += sprintf(buf, "[%s] ", hibernation_modes[i]);
  else
   buf += sprintf(buf, "%s ", hibernation_modes[i]);
 }
 buf += sprintf(buf, "\n");
 return buf-start;
}

static ssize_t disk_store(struct kobject *kobj, struct kobj_attribute *attr,
     const char *buf, size_t n)
{
 int error = 0;
 int i;
 int len;
 char *p;
 int mode = HIBERNATION_INVALID;

 if (!hibernation_available())
  return -EPERM;

 p = memchr(buf, '\n', n);
 len = p ? p - buf : n;

 lock_system_sleep();
 for (i = HIBERNATION_FIRST; i <= HIBERNATION_MAX; i++) {
  if (len == strlen(hibernation_modes[i])
      && !strncmp(buf, hibernation_modes[i], len)) {
   mode = i;
   break;
  }
 }
 if (mode != HIBERNATION_INVALID) {
  switch (mode) {
  case HIBERNATION_SHUTDOWN:
  case HIBERNATION_REBOOT:
  case HIBERNATION_SUSPEND:
   hibernation_mode = mode;
   break;
  case HIBERNATION_PLATFORM:
   if (hibernation_ops)
    hibernation_mode = mode;
   else
    error = -EINVAL;
  }
 } else
  error = -EINVAL;

 if (!error)
  pr_debug("PM: Hibernation mode set to '%s'\n",
    hibernation_modes[mode]);
 unlock_system_sleep();
 return error ? error : n;
}

power_attr(disk);

static ssize_t resume_show(struct kobject *kobj, struct kobj_attribute *attr,
      char *buf)
{
 return sprintf(buf,"%d:%d\n", MAJOR(swsusp_resume_device),
         MINOR(swsusp_resume_device));
}

static ssize_t resume_store(struct kobject *kobj, struct kobj_attribute *attr,
       const char *buf, size_t n)
{
 dev_t res;
 int len = n;
 char *name;

 if (len && buf[len-1] == '\n')
  len--;
 name = kstrndup(buf, len, GFP_KERNEL);
 if (!name)
  return -ENOMEM;

 res = name_to_dev_t(name);
 kfree(name);
 if (!res)
  return -EINVAL;

 lock_system_sleep();
 swsusp_resume_device = res;
 unlock_system_sleep();
 printk(KERN_INFO "PM: Starting manual resume from disk\n");
 noresume = 0;
 software_resume();
 return n;
}

power_attr(resume);

static ssize_t image_size_show(struct kobject *kobj, struct kobj_attribute *attr,
          char *buf)
{
 return sprintf(buf, "%lu\n", image_size);
}

static ssize_t image_size_store(struct kobject *kobj, struct kobj_attribute *attr,
    const char *buf, size_t n)
{
 unsigned long size;

 if (sscanf(buf, "%lu", &size) == 1) {
  image_size = size;
  return n;
 }

 return -EINVAL;
}

power_attr(image_size);

static ssize_t reserved_size_show(struct kobject *kobj,
      struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%lu\n", reserved_size);
}

static ssize_t reserved_size_store(struct kobject *kobj,
       struct kobj_attribute *attr,
       const char *buf, size_t n)
{
 unsigned long size;

 if (sscanf(buf, "%lu", &size) == 1) {
  reserved_size = size;
  return n;
 }

 return -EINVAL;
}

power_attr(reserved_size);

static struct attribute * g[] = {
 &disk_attr.attr,
 &resume_attr.attr,
 &image_size_attr.attr,
 &reserved_size_attr.attr,
 NULL,
};


static struct attribute_group attr_group = {
 .attrs = g,
};


static int __init pm_disk_init(void)
{
 return sysfs_create_group(power_kobj, &attr_group);
}

core_initcall(pm_disk_init);


static int __init resume_setup(char *str)
{
 if (noresume)
  return 1;

 strncpy( resume_file, str, 255 );
 return 1;
}

static int __init resume_offset_setup(char *str)
{
 unsigned long long offset;

 if (noresume)
  return 1;

 if (sscanf(str, "%llu", &offset) == 1)
  swsusp_resume_block = offset;

 return 1;
}

static int __init hibernate_setup(char *str)
{
 if (!strncmp(str, "noresume", 8))
  noresume = 1;
 else if (!strncmp(str, "nocompress", 10))
  nocompress = 1;
 else if (!strncmp(str, "no", 2)) {
  noresume = 1;
  nohibernate = 1;
 }
 return 1;
}

static int __init noresume_setup(char *str)
{
 noresume = 1;
 return 1;
}

static int __init resumewait_setup(char *str)
{
 resume_wait = 1;
 return 1;
}

static int __init resumedelay_setup(char *str)
{
 int rc = kstrtouint(str, 0, &resume_delay);

 if (rc)
  return rc;
 return 1;
}

static int __init nohibernate_setup(char *str)
{
 noresume = 1;
 nohibernate = 1;
 return 1;
}

static int __init kaslr_nohibernate_setup(char *str)
{
 return nohibernate_setup(str);
}

static int __init page_poison_nohibernate_setup(char *str)
{





 if (!strcmp(str, "on")) {
  pr_info("Disabling hibernation due to page poisoning\n");
  return nohibernate_setup(str);
 }
 return 1;
}

__setup("noresume", noresume_setup);
__setup("resume_offset=", resume_offset_setup);
__setup("resume=", resume_setup);
__setup("hibernate=", hibernate_setup);
__setup("resumewait", resumewait_setup);
__setup("resumedelay=", resumedelay_setup);
__setup("nohibernate", nohibernate_setup);
__setup("kaslr", kaslr_nohibernate_setup);
__setup("page_poison=", page_poison_nohibernate_setup);



DEFINE_PER_CPU(struct hrtimer_cpu_base, hrtimer_bases) =
{
 .lock = __RAW_SPIN_LOCK_UNLOCKED(hrtimer_bases.lock),
 .seq = SEQCNT_ZERO(hrtimer_bases.seq),
 .clock_base =
 {
  {
   .index = HRTIMER_BASE_MONOTONIC,
   .clockid = CLOCK_MONOTONIC,
   .get_time = &ktime_get,
  },
  {
   .index = HRTIMER_BASE_REALTIME,
   .clockid = CLOCK_REALTIME,
   .get_time = &ktime_get_real,
  },
  {
   .index = HRTIMER_BASE_BOOTTIME,
   .clockid = CLOCK_BOOTTIME,
   .get_time = &ktime_get_boottime,
  },
  {
   .index = HRTIMER_BASE_TAI,
   .clockid = CLOCK_TAI,
   .get_time = &ktime_get_clocktai,
  },
 }
};

static const int hrtimer_clock_to_base_table[MAX_CLOCKS] = {
 [CLOCK_REALTIME] = HRTIMER_BASE_REALTIME,
 [CLOCK_MONOTONIC] = HRTIMER_BASE_MONOTONIC,
 [CLOCK_BOOTTIME] = HRTIMER_BASE_BOOTTIME,
 [CLOCK_TAI] = HRTIMER_BASE_TAI,
};

static inline int hrtimer_clockid_to_base(clockid_t clock_id)
{
 return hrtimer_clock_to_base_table[clock_id];
}











static struct hrtimer_cpu_base migration_cpu_base = {
 .seq = SEQCNT_ZERO(migration_cpu_base),
 .clock_base = { { .cpu_base = &migration_cpu_base, }, },
};

static
struct hrtimer_clock_base *lock_hrtimer_base(const struct hrtimer *timer,
          unsigned long *flags)
{
 struct hrtimer_clock_base *base;

 for (;;) {
  base = timer->base;
  if (likely(base != &migration_base)) {
   raw_spin_lock_irqsave(&base->cpu_base->lock, *flags);
   if (likely(base == timer->base))
    return base;

   raw_spin_unlock_irqrestore(&base->cpu_base->lock, *flags);
  }
  cpu_relax();
 }
}
static int
hrtimer_check_target(struct hrtimer *timer, struct hrtimer_clock_base *new_base)
{
 ktime_t expires;

 if (!new_base->cpu_base->hres_active)
  return 0;

 expires = ktime_sub(hrtimer_get_expires(timer), new_base->offset);
 return expires.tv64 <= new_base->cpu_base->expires_next.tv64;
 return 0;
}

static inline
struct hrtimer_cpu_base *get_target_base(struct hrtimer_cpu_base *base,
      int pinned)
{
 if (pinned || !base->migration_enabled)
  return base;
 return &per_cpu(hrtimer_bases, get_nohz_timer_target());
}
static inline
struct hrtimer_cpu_base *get_target_base(struct hrtimer_cpu_base *base,
      int pinned)
{
 return base;
}
static inline struct hrtimer_clock_base *
switch_hrtimer_base(struct hrtimer *timer, struct hrtimer_clock_base *base,
      int pinned)
{
 struct hrtimer_cpu_base *new_cpu_base, *this_cpu_base;
 struct hrtimer_clock_base *new_base;
 int basenum = base->index;

 this_cpu_base = this_cpu_ptr(&hrtimer_bases);
 new_cpu_base = get_target_base(this_cpu_base, pinned);
again:
 new_base = &new_cpu_base->clock_base[basenum];

 if (base != new_base) {
  if (unlikely(hrtimer_callback_running(timer)))
   return base;


  timer->base = &migration_base;
  raw_spin_unlock(&base->cpu_base->lock);
  raw_spin_lock(&new_base->cpu_base->lock);

  if (new_cpu_base != this_cpu_base &&
      hrtimer_check_target(timer, new_base)) {
   raw_spin_unlock(&new_base->cpu_base->lock);
   raw_spin_lock(&base->cpu_base->lock);
   new_cpu_base = this_cpu_base;
   timer->base = base;
   goto again;
  }
  timer->base = new_base;
 } else {
  if (new_cpu_base != this_cpu_base &&
      hrtimer_check_target(timer, new_base)) {
   new_cpu_base = this_cpu_base;
   goto again;
  }
 }
 return new_base;
}


static inline struct hrtimer_clock_base *
lock_hrtimer_base(const struct hrtimer *timer, unsigned long *flags)
{
 struct hrtimer_clock_base *base = timer->base;

 raw_spin_lock_irqsave(&base->cpu_base->lock, *flags);

 return base;
}










s64 __ktime_divns(const ktime_t kt, s64 div)
{
 int sft = 0;
 s64 dclc;
 u64 tmp;

 dclc = ktime_to_ns(kt);
 tmp = dclc < 0 ? -dclc : dclc;


 while (div >> 32) {
  sft++;
  div >>= 1;
 }
 tmp >>= sft;
 do_div(tmp, (unsigned long) div);
 return dclc < 0 ? -tmp : tmp;
}
EXPORT_SYMBOL_GPL(__ktime_divns);




ktime_t ktime_add_safe(const ktime_t lhs, const ktime_t rhs)
{
 ktime_t res = ktime_add(lhs, rhs);





 if (res.tv64 < 0 || res.tv64 < lhs.tv64 || res.tv64 < rhs.tv64)
  res = ktime_set(KTIME_SEC_MAX, 0);

 return res;
}

EXPORT_SYMBOL_GPL(ktime_add_safe);


static struct debug_obj_descr hrtimer_debug_descr;

static void *hrtimer_debug_hint(void *addr)
{
 return ((struct hrtimer *) addr)->function;
}





static bool hrtimer_fixup_init(void *addr, enum debug_obj_state state)
{
 struct hrtimer *timer = addr;

 switch (state) {
 case ODEBUG_STATE_ACTIVE:
  hrtimer_cancel(timer);
  debug_object_init(timer, &hrtimer_debug_descr);
  return true;
 default:
  return false;
 }
}






static bool hrtimer_fixup_activate(void *addr, enum debug_obj_state state)
{
 switch (state) {
 case ODEBUG_STATE_ACTIVE:
  WARN_ON(1);

 default:
  return false;
 }
}





static bool hrtimer_fixup_free(void *addr, enum debug_obj_state state)
{
 struct hrtimer *timer = addr;

 switch (state) {
 case ODEBUG_STATE_ACTIVE:
  hrtimer_cancel(timer);
  debug_object_free(timer, &hrtimer_debug_descr);
  return true;
 default:
  return false;
 }
}

static struct debug_obj_descr hrtimer_debug_descr = {
 .name = "hrtimer",
 .debug_hint = hrtimer_debug_hint,
 .fixup_init = hrtimer_fixup_init,
 .fixup_activate = hrtimer_fixup_activate,
 .fixup_free = hrtimer_fixup_free,
};

static inline void debug_hrtimer_init(struct hrtimer *timer)
{
 debug_object_init(timer, &hrtimer_debug_descr);
}

static inline void debug_hrtimer_activate(struct hrtimer *timer)
{
 debug_object_activate(timer, &hrtimer_debug_descr);
}

static inline void debug_hrtimer_deactivate(struct hrtimer *timer)
{
 debug_object_deactivate(timer, &hrtimer_debug_descr);
}

static inline void debug_hrtimer_free(struct hrtimer *timer)
{
 debug_object_free(timer, &hrtimer_debug_descr);
}

static void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
      enum hrtimer_mode mode);

void hrtimer_init_on_stack(struct hrtimer *timer, clockid_t clock_id,
      enum hrtimer_mode mode)
{
 debug_object_init_on_stack(timer, &hrtimer_debug_descr);
 __hrtimer_init(timer, clock_id, mode);
}
EXPORT_SYMBOL_GPL(hrtimer_init_on_stack);

void destroy_hrtimer_on_stack(struct hrtimer *timer)
{
 debug_object_free(timer, &hrtimer_debug_descr);
}
EXPORT_SYMBOL_GPL(destroy_hrtimer_on_stack);

static inline void debug_hrtimer_init(struct hrtimer *timer) { }
static inline void debug_hrtimer_activate(struct hrtimer *timer) { }
static inline void debug_hrtimer_deactivate(struct hrtimer *timer) { }

static inline void
debug_init(struct hrtimer *timer, clockid_t clockid,
    enum hrtimer_mode mode)
{
 debug_hrtimer_init(timer);
 trace_hrtimer_init(timer, clockid, mode);
}

static inline void debug_activate(struct hrtimer *timer)
{
 debug_hrtimer_activate(timer);
 trace_hrtimer_start(timer);
}

static inline void debug_deactivate(struct hrtimer *timer)
{
 debug_hrtimer_deactivate(timer);
 trace_hrtimer_cancel(timer);
}

static inline void hrtimer_update_next_timer(struct hrtimer_cpu_base *cpu_base,
          struct hrtimer *timer)
{
 cpu_base->next_timer = timer;
}

static ktime_t __hrtimer_get_next_event(struct hrtimer_cpu_base *cpu_base)
{
 struct hrtimer_clock_base *base = cpu_base->clock_base;
 ktime_t expires, expires_next = { .tv64 = KTIME_MAX };
 unsigned int active = cpu_base->active_bases;

 hrtimer_update_next_timer(cpu_base, NULL);
 for (; active; base++, active >>= 1) {
  struct timerqueue_node *next;
  struct hrtimer *timer;

  if (!(active & 0x01))
   continue;

  next = timerqueue_getnext(&base->active);
  timer = container_of(next, struct hrtimer, node);
  expires = ktime_sub(hrtimer_get_expires(timer), base->offset);
  if (expires.tv64 < expires_next.tv64) {
   expires_next = expires;
   hrtimer_update_next_timer(cpu_base, timer);
  }
 }





 if (expires_next.tv64 < 0)
  expires_next.tv64 = 0;
 return expires_next;
}

static inline ktime_t hrtimer_update_base(struct hrtimer_cpu_base *base)
{
 ktime_t *offs_real = &base->clock_base[HRTIMER_BASE_REALTIME].offset;
 ktime_t *offs_boot = &base->clock_base[HRTIMER_BASE_BOOTTIME].offset;
 ktime_t *offs_tai = &base->clock_base[HRTIMER_BASE_TAI].offset;

 return ktime_get_update_offsets_now(&base->clock_was_set_seq,
         offs_real, offs_boot, offs_tai);
}






static bool hrtimer_hres_enabled __read_mostly = true;
unsigned int hrtimer_resolution __read_mostly = LOW_RES_NSEC;
EXPORT_SYMBOL_GPL(hrtimer_resolution);




static int __init setup_hrtimer_hres(char *str)
{
 return (kstrtobool(str, &hrtimer_hres_enabled) == 0);
}

__setup("highres=", setup_hrtimer_hres);




static inline int hrtimer_is_hres_enabled(void)
{
 return hrtimer_hres_enabled;
}




static inline int __hrtimer_hres_active(struct hrtimer_cpu_base *cpu_base)
{
 return cpu_base->hres_active;
}

static inline int hrtimer_hres_active(void)
{
 return __hrtimer_hres_active(this_cpu_ptr(&hrtimer_bases));
}






static void
hrtimer_force_reprogram(struct hrtimer_cpu_base *cpu_base, int skip_equal)
{
 ktime_t expires_next;

 if (!cpu_base->hres_active)
  return;

 expires_next = __hrtimer_get_next_event(cpu_base);

 if (skip_equal && expires_next.tv64 == cpu_base->expires_next.tv64)
  return;

 cpu_base->expires_next.tv64 = expires_next.tv64;
 if (cpu_base->hang_detected)
  return;

 tick_program_event(cpu_base->expires_next, 1);
}
static void hrtimer_reprogram(struct hrtimer *timer,
         struct hrtimer_clock_base *base)
{
 struct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);
 ktime_t expires = ktime_sub(hrtimer_get_expires(timer), base->offset);

 WARN_ON_ONCE(hrtimer_get_expires_tv64(timer) < 0);





 if (base->cpu_base != cpu_base)
  return;
 if (cpu_base->in_hrtirq)
  return;





 if (expires.tv64 < 0)
  expires.tv64 = 0;

 if (expires.tv64 >= cpu_base->expires_next.tv64)
  return;


 cpu_base->next_timer = timer;







 if (cpu_base->hang_detected)
  return;





 cpu_base->expires_next = expires;
 tick_program_event(expires, 1);
}




static inline void hrtimer_init_hres(struct hrtimer_cpu_base *base)
{
 base->expires_next.tv64 = KTIME_MAX;
 base->hres_active = 0;
}






static void retrigger_next_event(void *arg)
{
 struct hrtimer_cpu_base *base = this_cpu_ptr(&hrtimer_bases);

 if (!base->hres_active)
  return;

 raw_spin_lock(&base->lock);
 hrtimer_update_base(base);
 hrtimer_force_reprogram(base, 0);
 raw_spin_unlock(&base->lock);
}




static void hrtimer_switch_to_hres(void)
{
 struct hrtimer_cpu_base *base = this_cpu_ptr(&hrtimer_bases);

 if (tick_init_highres()) {
  printk(KERN_WARNING "Could not switch to high resolution "
        "mode on CPU %d\n", base->cpu);
  return;
 }
 base->hres_active = 1;
 hrtimer_resolution = HIGH_RES_NSEC;

 tick_setup_sched_timer();

 retrigger_next_event(NULL);
}

static void clock_was_set_work(struct work_struct *work)
{
 clock_was_set();
}

static DECLARE_WORK(hrtimer_work, clock_was_set_work);





void clock_was_set_delayed(void)
{
 schedule_work(&hrtimer_work);
}


static inline int __hrtimer_hres_active(struct hrtimer_cpu_base *b) { return 0; }
static inline int hrtimer_hres_active(void) { return 0; }
static inline int hrtimer_is_hres_enabled(void) { return 0; }
static inline void hrtimer_switch_to_hres(void) { }
static inline void
hrtimer_force_reprogram(struct hrtimer_cpu_base *base, int skip_equal) { }
static inline int hrtimer_reprogram(struct hrtimer *timer,
        struct hrtimer_clock_base *base)
{
 return 0;
}
static inline void hrtimer_init_hres(struct hrtimer_cpu_base *base) { }
static inline void retrigger_next_event(void *arg) { }

void clock_was_set(void)
{

 on_each_cpu(retrigger_next_event, NULL, 1);
 timerfd_clock_was_set();
}







void hrtimers_resume(void)
{
 WARN_ONCE(!irqs_disabled(),
    KERN_INFO "hrtimers_resume() called with IRQs enabled!");


 retrigger_next_event(NULL);

 clock_was_set_delayed();
}

static inline void timer_stats_hrtimer_set_start_info(struct hrtimer *timer)
{
 if (timer->start_site)
  return;
 timer->start_site = __builtin_return_address(0);
 memcpy(timer->start_comm, current->comm, TASK_COMM_LEN);
 timer->start_pid = current->pid;
}

static inline void timer_stats_hrtimer_clear_start_info(struct hrtimer *timer)
{
 timer->start_site = NULL;
}

static inline void timer_stats_account_hrtimer(struct hrtimer *timer)
{
 if (likely(!timer_stats_active))
  return;
 timer_stats_update_stats(timer, timer->start_pid, timer->start_site,
     timer->function, timer->start_comm, 0);
}




static inline
void unlock_hrtimer_base(const struct hrtimer *timer, unsigned long *flags)
{
 raw_spin_unlock_irqrestore(&timer->base->cpu_base->lock, *flags);
}
u64 hrtimer_forward(struct hrtimer *timer, ktime_t now, ktime_t interval)
{
 u64 orun = 1;
 ktime_t delta;

 delta = ktime_sub(now, hrtimer_get_expires(timer));

 if (delta.tv64 < 0)
  return 0;

 if (WARN_ON(timer->state & HRTIMER_STATE_ENQUEUED))
  return 0;

 if (interval.tv64 < hrtimer_resolution)
  interval.tv64 = hrtimer_resolution;

 if (unlikely(delta.tv64 >= interval.tv64)) {
  s64 incr = ktime_to_ns(interval);

  orun = ktime_divns(delta, incr);
  hrtimer_add_expires_ns(timer, incr * orun);
  if (hrtimer_get_expires_tv64(timer) > now.tv64)
   return orun;




  orun++;
 }
 hrtimer_add_expires(timer, interval);

 return orun;
}
EXPORT_SYMBOL_GPL(hrtimer_forward);
static int enqueue_hrtimer(struct hrtimer *timer,
      struct hrtimer_clock_base *base)
{
 debug_activate(timer);

 base->cpu_base->active_bases |= 1 << base->index;

 timer->state = HRTIMER_STATE_ENQUEUED;

 return timerqueue_add(&base->active, &timer->node);
}
static void __remove_hrtimer(struct hrtimer *timer,
        struct hrtimer_clock_base *base,
        u8 newstate, int reprogram)
{
 struct hrtimer_cpu_base *cpu_base = base->cpu_base;
 u8 state = timer->state;

 timer->state = newstate;
 if (!(state & HRTIMER_STATE_ENQUEUED))
  return;

 if (!timerqueue_del(&base->active, &timer->node))
  cpu_base->active_bases &= ~(1 << base->index);

 if (reprogram && timer == cpu_base->next_timer)
  hrtimer_force_reprogram(cpu_base, 1);
}




static inline int
remove_hrtimer(struct hrtimer *timer, struct hrtimer_clock_base *base, bool restart)
{
 if (hrtimer_is_queued(timer)) {
  u8 state = timer->state;
  int reprogram;
  debug_deactivate(timer);
  timer_stats_hrtimer_clear_start_info(timer);
  reprogram = base->cpu_base == this_cpu_ptr(&hrtimer_bases);

  if (!restart)
   state = HRTIMER_STATE_INACTIVE;

  __remove_hrtimer(timer, base, state, reprogram);
  return 1;
 }
 return 0;
}

static inline ktime_t hrtimer_update_lowres(struct hrtimer *timer, ktime_t tim,
         const enum hrtimer_mode mode)
{





 timer->is_rel = mode & HRTIMER_MODE_REL;
 if (timer->is_rel)
  tim = ktime_add_safe(tim, ktime_set(0, hrtimer_resolution));
 return tim;
}
void hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
       u64 delta_ns, const enum hrtimer_mode mode)
{
 struct hrtimer_clock_base *base, *new_base;
 unsigned long flags;
 int leftmost;

 base = lock_hrtimer_base(timer, &flags);


 remove_hrtimer(timer, base, true);

 if (mode & HRTIMER_MODE_REL)
  tim = ktime_add_safe(tim, base->get_time());

 tim = hrtimer_update_lowres(timer, tim, mode);

 hrtimer_set_expires_range_ns(timer, tim, delta_ns);


 new_base = switch_hrtimer_base(timer, base, mode & HRTIMER_MODE_PINNED);

 timer_stats_hrtimer_set_start_info(timer);

 leftmost = enqueue_hrtimer(timer, new_base);
 if (!leftmost)
  goto unlock;

 if (!hrtimer_is_hres_active(timer)) {




  if (new_base->cpu_base->nohz_active)
   wake_up_nohz_cpu(new_base->cpu_base->cpu);
 } else {
  hrtimer_reprogram(timer, new_base);
 }
unlock:
 unlock_hrtimer_base(timer, &flags);
}
EXPORT_SYMBOL_GPL(hrtimer_start_range_ns);
int hrtimer_try_to_cancel(struct hrtimer *timer)
{
 struct hrtimer_clock_base *base;
 unsigned long flags;
 int ret = -1;







 if (!hrtimer_active(timer))
  return 0;

 base = lock_hrtimer_base(timer, &flags);

 if (!hrtimer_callback_running(timer))
  ret = remove_hrtimer(timer, base, false);

 unlock_hrtimer_base(timer, &flags);

 return ret;

}
EXPORT_SYMBOL_GPL(hrtimer_try_to_cancel);
int hrtimer_cancel(struct hrtimer *timer)
{
 for (;;) {
  int ret = hrtimer_try_to_cancel(timer);

  if (ret >= 0)
   return ret;
  cpu_relax();
 }
}
EXPORT_SYMBOL_GPL(hrtimer_cancel);






ktime_t __hrtimer_get_remaining(const struct hrtimer *timer, bool adjust)
{
 unsigned long flags;
 ktime_t rem;

 lock_hrtimer_base(timer, &flags);
 if (IS_ENABLED(CONFIG_TIME_LOW_RES) && adjust)
  rem = hrtimer_expires_remaining_adjusted(timer);
 else
  rem = hrtimer_expires_remaining(timer);
 unlock_hrtimer_base(timer, &flags);

 return rem;
}
EXPORT_SYMBOL_GPL(__hrtimer_get_remaining);






u64 hrtimer_get_next_event(void)
{
 struct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);
 u64 expires = KTIME_MAX;
 unsigned long flags;

 raw_spin_lock_irqsave(&cpu_base->lock, flags);

 if (!__hrtimer_hres_active(cpu_base))
  expires = __hrtimer_get_next_event(cpu_base).tv64;

 raw_spin_unlock_irqrestore(&cpu_base->lock, flags);

 return expires;
}

static void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
      enum hrtimer_mode mode)
{
 struct hrtimer_cpu_base *cpu_base;
 int base;

 memset(timer, 0, sizeof(struct hrtimer));

 cpu_base = raw_cpu_ptr(&hrtimer_bases);

 if (clock_id == CLOCK_REALTIME && mode != HRTIMER_MODE_ABS)
  clock_id = CLOCK_MONOTONIC;

 base = hrtimer_clockid_to_base(clock_id);
 timer->base = &cpu_base->clock_base[base];
 timerqueue_init(&timer->node);

 timer->start_site = NULL;
 timer->start_pid = -1;
 memset(timer->start_comm, 0, TASK_COMM_LEN);
}







void hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
    enum hrtimer_mode mode)
{
 debug_init(timer, clock_id, mode);
 __hrtimer_init(timer, clock_id, mode);
}
EXPORT_SYMBOL_GPL(hrtimer_init);
bool hrtimer_active(const struct hrtimer *timer)
{
 struct hrtimer_cpu_base *cpu_base;
 unsigned int seq;

 do {
  cpu_base = READ_ONCE(timer->base->cpu_base);
  seq = raw_read_seqcount_begin(&cpu_base->seq);

  if (timer->state != HRTIMER_STATE_INACTIVE ||
      cpu_base->running == timer)
   return true;

 } while (read_seqcount_retry(&cpu_base->seq, seq) ||
   cpu_base != READ_ONCE(timer->base->cpu_base));

 return false;
}
EXPORT_SYMBOL_GPL(hrtimer_active);
static void __run_hrtimer(struct hrtimer_cpu_base *cpu_base,
     struct hrtimer_clock_base *base,
     struct hrtimer *timer, ktime_t *now)
{
 enum hrtimer_restart (*fn)(struct hrtimer *);
 int restart;

 lockdep_assert_held(&cpu_base->lock);

 debug_deactivate(timer);
 cpu_base->running = timer;
 raw_write_seqcount_barrier(&cpu_base->seq);

 __remove_hrtimer(timer, base, HRTIMER_STATE_INACTIVE, 0);
 timer_stats_account_hrtimer(timer);
 fn = timer->function;






 if (IS_ENABLED(CONFIG_TIME_LOW_RES))
  timer->is_rel = false;






 raw_spin_unlock(&cpu_base->lock);
 trace_hrtimer_expire_entry(timer, now);
 restart = fn(timer);
 trace_hrtimer_expire_exit(timer);
 raw_spin_lock(&cpu_base->lock);
 if (restart != HRTIMER_NORESTART &&
     !(timer->state & HRTIMER_STATE_ENQUEUED))
  enqueue_hrtimer(timer, base);
 raw_write_seqcount_barrier(&cpu_base->seq);

 WARN_ON_ONCE(cpu_base->running != timer);
 cpu_base->running = NULL;
}

static void __hrtimer_run_queues(struct hrtimer_cpu_base *cpu_base, ktime_t now)
{
 struct hrtimer_clock_base *base = cpu_base->clock_base;
 unsigned int active = cpu_base->active_bases;

 for (; active; base++, active >>= 1) {
  struct timerqueue_node *node;
  ktime_t basenow;

  if (!(active & 0x01))
   continue;

  basenow = ktime_add(now, base->offset);

  while ((node = timerqueue_getnext(&base->active))) {
   struct hrtimer *timer;

   timer = container_of(node, struct hrtimer, node);
   if (basenow.tv64 < hrtimer_get_softexpires_tv64(timer))
    break;

   __run_hrtimer(cpu_base, base, timer, &basenow);
  }
 }
}






void hrtimer_interrupt(struct clock_event_device *dev)
{
 struct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);
 ktime_t expires_next, now, entry_time, delta;
 int retries = 0;

 BUG_ON(!cpu_base->hres_active);
 cpu_base->nr_events++;
 dev->next_event.tv64 = KTIME_MAX;

 raw_spin_lock(&cpu_base->lock);
 entry_time = now = hrtimer_update_base(cpu_base);
retry:
 cpu_base->in_hrtirq = 1;







 cpu_base->expires_next.tv64 = KTIME_MAX;

 __hrtimer_run_queues(cpu_base, now);


 expires_next = __hrtimer_get_next_event(cpu_base);




 cpu_base->expires_next = expires_next;
 cpu_base->in_hrtirq = 0;
 raw_spin_unlock(&cpu_base->lock);


 if (!tick_program_event(expires_next, 0)) {
  cpu_base->hang_detected = 0;
  return;
 }
 raw_spin_lock(&cpu_base->lock);
 now = hrtimer_update_base(cpu_base);
 cpu_base->nr_retries++;
 if (++retries < 3)
  goto retry;






 cpu_base->nr_hangs++;
 cpu_base->hang_detected = 1;
 raw_spin_unlock(&cpu_base->lock);
 delta = ktime_sub(now, entry_time);
 if ((unsigned int)delta.tv64 > cpu_base->max_hang_time)
  cpu_base->max_hang_time = (unsigned int) delta.tv64;




 if (delta.tv64 > 100 * NSEC_PER_MSEC)
  expires_next = ktime_add_ns(now, 100 * NSEC_PER_MSEC);
 else
  expires_next = ktime_add(now, delta);
 tick_program_event(expires_next, 1);
 printk_once(KERN_WARNING "hrtimer: interrupt took %llu ns\n",
      ktime_to_ns(delta));
}





static inline void __hrtimer_peek_ahead_timers(void)
{
 struct tick_device *td;

 if (!hrtimer_hres_active())
  return;

 td = this_cpu_ptr(&tick_cpu_device);
 if (td && td->evtdev)
  hrtimer_interrupt(td->evtdev);
}


static inline void __hrtimer_peek_ahead_timers(void) { }





void hrtimer_run_queues(void)
{
 struct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);
 ktime_t now;

 if (__hrtimer_hres_active(cpu_base))
  return;
 if (tick_check_oneshot_change(!hrtimer_is_hres_enabled())) {
  hrtimer_switch_to_hres();
  return;
 }

 raw_spin_lock(&cpu_base->lock);
 now = hrtimer_update_base(cpu_base);
 __hrtimer_run_queues(cpu_base, now);
 raw_spin_unlock(&cpu_base->lock);
}




static enum hrtimer_restart hrtimer_wakeup(struct hrtimer *timer)
{
 struct hrtimer_sleeper *t =
  container_of(timer, struct hrtimer_sleeper, timer);
 struct task_struct *task = t->task;

 t->task = NULL;
 if (task)
  wake_up_process(task);

 return HRTIMER_NORESTART;
}

void hrtimer_init_sleeper(struct hrtimer_sleeper *sl, struct task_struct *task)
{
 sl->timer.function = hrtimer_wakeup;
 sl->task = task;
}
EXPORT_SYMBOL_GPL(hrtimer_init_sleeper);

static int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode)
{
 hrtimer_init_sleeper(t, current);

 do {
  set_current_state(TASK_INTERRUPTIBLE);
  hrtimer_start_expires(&t->timer, mode);

  if (likely(t->task))
   freezable_schedule();

  hrtimer_cancel(&t->timer);
  mode = HRTIMER_MODE_ABS;

 } while (t->task && !signal_pending(current));

 __set_current_state(TASK_RUNNING);

 return t->task == NULL;
}

static int update_rmtp(struct hrtimer *timer, struct timespec __user *rmtp)
{
 struct timespec rmt;
 ktime_t rem;

 rem = hrtimer_expires_remaining(timer);
 if (rem.tv64 <= 0)
  return 0;
 rmt = ktime_to_timespec(rem);

 if (copy_to_user(rmtp, &rmt, sizeof(*rmtp)))
  return -EFAULT;

 return 1;
}

long __sched hrtimer_nanosleep_restart(struct restart_block *restart)
{
 struct hrtimer_sleeper t;
 struct timespec __user *rmtp;
 int ret = 0;

 hrtimer_init_on_stack(&t.timer, restart->nanosleep.clockid,
    HRTIMER_MODE_ABS);
 hrtimer_set_expires_tv64(&t.timer, restart->nanosleep.expires);

 if (do_nanosleep(&t, HRTIMER_MODE_ABS))
  goto out;

 rmtp = restart->nanosleep.rmtp;
 if (rmtp) {
  ret = update_rmtp(&t.timer, rmtp);
  if (ret <= 0)
   goto out;
 }


 ret = -ERESTART_RESTARTBLOCK;
out:
 destroy_hrtimer_on_stack(&t.timer);
 return ret;
}

long hrtimer_nanosleep(struct timespec *rqtp, struct timespec __user *rmtp,
         const enum hrtimer_mode mode, const clockid_t clockid)
{
 struct restart_block *restart;
 struct hrtimer_sleeper t;
 int ret = 0;
 u64 slack;

 slack = current->timer_slack_ns;
 if (dl_task(current) || rt_task(current))
  slack = 0;

 hrtimer_init_on_stack(&t.timer, clockid, mode);
 hrtimer_set_expires_range_ns(&t.timer, timespec_to_ktime(*rqtp), slack);
 if (do_nanosleep(&t, mode))
  goto out;


 if (mode == HRTIMER_MODE_ABS) {
  ret = -ERESTARTNOHAND;
  goto out;
 }

 if (rmtp) {
  ret = update_rmtp(&t.timer, rmtp);
  if (ret <= 0)
   goto out;
 }

 restart = &current->restart_block;
 restart->fn = hrtimer_nanosleep_restart;
 restart->nanosleep.clockid = t.timer.base->clockid;
 restart->nanosleep.rmtp = rmtp;
 restart->nanosleep.expires = hrtimer_get_expires_tv64(&t.timer);

 ret = -ERESTART_RESTARTBLOCK;
out:
 destroy_hrtimer_on_stack(&t.timer);
 return ret;
}

SYSCALL_DEFINE2(nanosleep, struct timespec __user *, rqtp,
  struct timespec __user *, rmtp)
{
 struct timespec tu;

 if (copy_from_user(&tu, rqtp, sizeof(tu)))
  return -EFAULT;

 if (!timespec_valid(&tu))
  return -EINVAL;

 return hrtimer_nanosleep(&tu, rmtp, HRTIMER_MODE_REL, CLOCK_MONOTONIC);
}




static void init_hrtimers_cpu(int cpu)
{
 struct hrtimer_cpu_base *cpu_base = &per_cpu(hrtimer_bases, cpu);
 int i;

 for (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {
  cpu_base->clock_base[i].cpu_base = cpu_base;
  timerqueue_init_head(&cpu_base->clock_base[i].active);
 }

 cpu_base->cpu = cpu;
 hrtimer_init_hres(cpu_base);
}


static void migrate_hrtimer_list(struct hrtimer_clock_base *old_base,
    struct hrtimer_clock_base *new_base)
{
 struct hrtimer *timer;
 struct timerqueue_node *node;

 while ((node = timerqueue_getnext(&old_base->active))) {
  timer = container_of(node, struct hrtimer, node);
  BUG_ON(hrtimer_callback_running(timer));
  debug_deactivate(timer);






  __remove_hrtimer(timer, old_base, HRTIMER_STATE_ENQUEUED, 0);
  timer->base = new_base;
  enqueue_hrtimer(timer, new_base);
 }
}

static void migrate_hrtimers(int scpu)
{
 struct hrtimer_cpu_base *old_base, *new_base;
 int i;

 BUG_ON(cpu_online(scpu));
 tick_cancel_sched_timer(scpu);

 local_irq_disable();
 old_base = &per_cpu(hrtimer_bases, scpu);
 new_base = this_cpu_ptr(&hrtimer_bases);




 raw_spin_lock(&new_base->lock);
 raw_spin_lock_nested(&old_base->lock, SINGLE_DEPTH_NESTING);

 for (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {
  migrate_hrtimer_list(&old_base->clock_base[i],
         &new_base->clock_base[i]);
 }

 raw_spin_unlock(&old_base->lock);
 raw_spin_unlock(&new_base->lock);


 __hrtimer_peek_ahead_timers();
 local_irq_enable();
}


static int hrtimer_cpu_notify(struct notifier_block *self,
     unsigned long action, void *hcpu)
{
 int scpu = (long)hcpu;

 switch (action) {

 case CPU_UP_PREPARE:
 case CPU_UP_PREPARE_FROZEN:
  init_hrtimers_cpu(scpu);
  break;

 case CPU_DEAD:
 case CPU_DEAD_FROZEN:
  migrate_hrtimers(scpu);
  break;

 default:
  break;
 }

 return NOTIFY_OK;
}

static struct notifier_block hrtimers_nb = {
 .notifier_call = hrtimer_cpu_notify,
};

void __init hrtimers_init(void)
{
 hrtimer_cpu_notify(&hrtimers_nb, (unsigned long)CPU_UP_PREPARE,
     (void *)(long)smp_processor_id());
 register_cpu_notifier(&hrtimers_nb);
}
int __sched
schedule_hrtimeout_range_clock(ktime_t *expires, u64 delta,
          const enum hrtimer_mode mode, int clock)
{
 struct hrtimer_sleeper t;





 if (expires && !expires->tv64) {
  __set_current_state(TASK_RUNNING);
  return 0;
 }




 if (!expires) {
  schedule();
  return -EINTR;
 }

 hrtimer_init_on_stack(&t.timer, clock, mode);
 hrtimer_set_expires_range_ns(&t.timer, *expires, delta);

 hrtimer_init_sleeper(&t, current);

 hrtimer_start_expires(&t.timer, mode);

 if (likely(t.task))
  schedule();

 hrtimer_cancel(&t.timer);
 destroy_hrtimer_on_stack(&t.timer);

 __set_current_state(TASK_RUNNING);

 return !t.task ? 0 : -EINTR;
}
int __sched schedule_hrtimeout_range(ktime_t *expires, u64 delta,
         const enum hrtimer_mode mode)
{
 return schedule_hrtimeout_range_clock(expires, delta, mode,
           CLOCK_MONOTONIC);
}
EXPORT_SYMBOL_GPL(schedule_hrtimeout_range);
int __sched schedule_hrtimeout(ktime_t *expires,
          const enum hrtimer_mode mode)
{
 return schedule_hrtimeout_range(expires, 0, mode);
}
EXPORT_SYMBOL_GPL(schedule_hrtimeout);











int __read_mostly sysctl_hung_task_check_count = PID_MAX_LIMIT;




unsigned long __read_mostly sysctl_hung_task_timeout_secs = CONFIG_DEFAULT_HUNG_TASK_TIMEOUT;

int __read_mostly sysctl_hung_task_warnings = 10;

static int __read_mostly did_panic;

static struct task_struct *watchdog_task;





unsigned int __read_mostly sysctl_hung_task_panic =
    CONFIG_BOOTPARAM_HUNG_TASK_PANIC_VALUE;

static int __init hung_task_panic_setup(char *str)
{
 int rc = kstrtouint(str, 0, &sysctl_hung_task_panic);

 if (rc)
  return rc;
 return 1;
}
__setup("hung_task_panic=", hung_task_panic_setup);

static int
hung_task_panic(struct notifier_block *this, unsigned long event, void *ptr)
{
 did_panic = 1;

 return NOTIFY_DONE;
}

static struct notifier_block panic_block = {
 .notifier_call = hung_task_panic,
};

static void check_hung_task(struct task_struct *t, unsigned long timeout)
{
 unsigned long switch_count = t->nvcsw + t->nivcsw;





 if (unlikely(t->flags & (PF_FROZEN | PF_FREEZER_SKIP)))
     return;






 if (unlikely(!switch_count))
  return;

 if (switch_count != t->last_switch_count) {
  t->last_switch_count = switch_count;
  return;
 }

 trace_sched_process_hang(t);

 if (!sysctl_hung_task_warnings)
  return;

 if (sysctl_hung_task_warnings > 0)
  sysctl_hung_task_warnings--;





 pr_err("INFO: task %s:%d blocked for more than %ld seconds.\n",
  t->comm, t->pid, timeout);
 pr_err("      %s %s %.*s\n",
  print_tainted(), init_utsname()->release,
  (int)strcspn(init_utsname()->version, " "),
  init_utsname()->version);
 pr_err("\"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\""
  " disables this message.\n");
 sched_show_task(t);
 debug_show_held_locks(t);

 touch_nmi_watchdog();

 if (sysctl_hung_task_panic) {
  trigger_all_cpu_backtrace();
  panic("hung_task: blocked tasks");
 }
}
static bool rcu_lock_break(struct task_struct *g, struct task_struct *t)
{
 bool can_cont;

 get_task_struct(g);
 get_task_struct(t);
 rcu_read_unlock();
 cond_resched();
 rcu_read_lock();
 can_cont = pid_alive(g) && pid_alive(t);
 put_task_struct(t);
 put_task_struct(g);

 return can_cont;
}






static void check_hung_uninterruptible_tasks(unsigned long timeout)
{
 int max_count = sysctl_hung_task_check_count;
 int batch_count = HUNG_TASK_BATCHING;
 struct task_struct *g, *t;





 if (test_taint(TAINT_DIE) || did_panic)
  return;

 rcu_read_lock();
 for_each_process_thread(g, t) {
  if (!max_count--)
   goto unlock;
  if (!--batch_count) {
   batch_count = HUNG_TASK_BATCHING;
   if (!rcu_lock_break(g, t))
    goto unlock;
  }

  if (t->state == TASK_UNINTERRUPTIBLE)
   check_hung_task(t, timeout);
 }
 unlock:
 rcu_read_unlock();
}

static long hung_timeout_jiffies(unsigned long last_checked,
     unsigned long timeout)
{

 return timeout ? last_checked - jiffies + timeout * HZ :
  MAX_SCHEDULE_TIMEOUT;
}




int proc_dohung_task_timeout_secs(struct ctl_table *table, int write,
      void __user *buffer,
      size_t *lenp, loff_t *ppos)
{
 int ret;

 ret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);

 if (ret || !write)
  goto out;

 wake_up_process(watchdog_task);

 out:
 return ret;
}

static atomic_t reset_hung_task = ATOMIC_INIT(0);

void reset_hung_task_detector(void)
{
 atomic_set(&reset_hung_task, 1);
}
EXPORT_SYMBOL_GPL(reset_hung_task_detector);




static int watchdog(void *dummy)
{
 unsigned long hung_last_checked = jiffies;

 set_user_nice(current, 0);

 for ( ; ; ) {
  unsigned long timeout = sysctl_hung_task_timeout_secs;
  long t = hung_timeout_jiffies(hung_last_checked, timeout);

  if (t <= 0) {
   if (!atomic_xchg(&reset_hung_task, 0))
    check_hung_uninterruptible_tasks(timeout);
   hung_last_checked = jiffies;
   continue;
  }
  schedule_timeout_interruptible(t);
 }

 return 0;
}

static int __init hung_task_init(void)
{
 atomic_notifier_chain_register(&panic_notifier_list, &panic_block);
 watchdog_task = kthread_run(watchdog, NULL, "khungtaskd");

 return 0;
}
subsys_initcall(hung_task_init);




struct bp_cpuinfo {

 unsigned int cpu_pinned;

 unsigned int *tsk_pinned;

 unsigned int flexible;
};

static DEFINE_PER_CPU(struct bp_cpuinfo, bp_cpuinfo[TYPE_MAX]);
static int nr_slots[TYPE_MAX];

static struct bp_cpuinfo *get_bp_info(int cpu, enum bp_type_idx type)
{
 return per_cpu_ptr(bp_cpuinfo + type, cpu);
}


static LIST_HEAD(bp_task_head);

static int constraints_initialized;


struct bp_busy_slots {
 unsigned int pinned;
 unsigned int flexible;
};


static DEFINE_MUTEX(nr_bp_mutex);

__weak int hw_breakpoint_weight(struct perf_event *bp)
{
 return 1;
}

static inline enum bp_type_idx find_slot_idx(struct perf_event *bp)
{
 if (bp->attr.bp_type & HW_BREAKPOINT_RW)
  return TYPE_DATA;

 return TYPE_INST;
}





static unsigned int max_task_bp_pinned(int cpu, enum bp_type_idx type)
{
 unsigned int *tsk_pinned = get_bp_info(cpu, type)->tsk_pinned;
 int i;

 for (i = nr_slots[type] - 1; i >= 0; i--) {
  if (tsk_pinned[i] > 0)
   return i + 1;
 }

 return 0;
}





static int task_bp_pinned(int cpu, struct perf_event *bp, enum bp_type_idx type)
{
 struct task_struct *tsk = bp->hw.target;
 struct perf_event *iter;
 int count = 0;

 list_for_each_entry(iter, &bp_task_head, hw.bp_list) {
  if (iter->hw.target == tsk &&
      find_slot_idx(iter) == type &&
      (iter->cpu < 0 || cpu == iter->cpu))
   count += hw_breakpoint_weight(iter);
 }

 return count;
}

static const struct cpumask *cpumask_of_bp(struct perf_event *bp)
{
 if (bp->cpu >= 0)
  return cpumask_of(bp->cpu);
 return cpu_possible_mask;
}





static void
fetch_bp_busy_slots(struct bp_busy_slots *slots, struct perf_event *bp,
      enum bp_type_idx type)
{
 const struct cpumask *cpumask = cpumask_of_bp(bp);
 int cpu;

 for_each_cpu(cpu, cpumask) {
  struct bp_cpuinfo *info = get_bp_info(cpu, type);
  int nr;

  nr = info->cpu_pinned;
  if (!bp->hw.target)
   nr += max_task_bp_pinned(cpu, type);
  else
   nr += task_bp_pinned(cpu, bp, type);

  if (nr > slots->pinned)
   slots->pinned = nr;

  nr = info->flexible;
  if (nr > slots->flexible)
   slots->flexible = nr;
 }
}






static void
fetch_this_slot(struct bp_busy_slots *slots, int weight)
{
 slots->pinned += weight;
}




static void toggle_bp_task_slot(struct perf_event *bp, int cpu,
    enum bp_type_idx type, int weight)
{
 unsigned int *tsk_pinned = get_bp_info(cpu, type)->tsk_pinned;
 int old_idx, new_idx;

 old_idx = task_bp_pinned(cpu, bp, type) - 1;
 new_idx = old_idx + weight;

 if (old_idx >= 0)
  tsk_pinned[old_idx]--;
 if (new_idx >= 0)
  tsk_pinned[new_idx]++;
}




static void
toggle_bp_slot(struct perf_event *bp, bool enable, enum bp_type_idx type,
        int weight)
{
 const struct cpumask *cpumask = cpumask_of_bp(bp);
 int cpu;

 if (!enable)
  weight = -weight;


 if (!bp->hw.target) {
  get_bp_info(bp->cpu, type)->cpu_pinned += weight;
  return;
 }


 for_each_cpu(cpu, cpumask)
  toggle_bp_task_slot(bp, cpu, type, weight);

 if (enable)
  list_add_tail(&bp->hw.bp_list, &bp_task_head);
 else
  list_del(&bp->hw.bp_list);
}




__weak void arch_unregister_hw_breakpoint(struct perf_event *bp)
{




}
static int __reserve_bp_slot(struct perf_event *bp)
{
 struct bp_busy_slots slots = {0};
 enum bp_type_idx type;
 int weight;


 if (!constraints_initialized)
  return -ENOMEM;


 if (bp->attr.bp_type == HW_BREAKPOINT_EMPTY ||
     bp->attr.bp_type == HW_BREAKPOINT_INVALID)
  return -EINVAL;

 type = find_slot_idx(bp);
 weight = hw_breakpoint_weight(bp);

 fetch_bp_busy_slots(&slots, bp, type);




 fetch_this_slot(&slots, weight);


 if (slots.pinned + (!!slots.flexible) > nr_slots[type])
  return -ENOSPC;

 toggle_bp_slot(bp, true, type, weight);

 return 0;
}

int reserve_bp_slot(struct perf_event *bp)
{
 int ret;

 mutex_lock(&nr_bp_mutex);

 ret = __reserve_bp_slot(bp);

 mutex_unlock(&nr_bp_mutex);

 return ret;
}

static void __release_bp_slot(struct perf_event *bp)
{
 enum bp_type_idx type;
 int weight;

 type = find_slot_idx(bp);
 weight = hw_breakpoint_weight(bp);
 toggle_bp_slot(bp, false, type, weight);
}

void release_bp_slot(struct perf_event *bp)
{
 mutex_lock(&nr_bp_mutex);

 arch_unregister_hw_breakpoint(bp);
 __release_bp_slot(bp);

 mutex_unlock(&nr_bp_mutex);
}






int dbg_reserve_bp_slot(struct perf_event *bp)
{
 if (mutex_is_locked(&nr_bp_mutex))
  return -1;

 return __reserve_bp_slot(bp);
}

int dbg_release_bp_slot(struct perf_event *bp)
{
 if (mutex_is_locked(&nr_bp_mutex))
  return -1;

 __release_bp_slot(bp);

 return 0;
}

static int validate_hw_breakpoint(struct perf_event *bp)
{
 int ret;

 ret = arch_validate_hwbkpt_settings(bp);
 if (ret)
  return ret;

 if (arch_check_bp_in_kernelspace(bp)) {
  if (bp->attr.exclude_kernel)
   return -EINVAL;




  if (!capable(CAP_SYS_ADMIN))
   return -EPERM;
 }

 return 0;
}

int register_perf_hw_breakpoint(struct perf_event *bp)
{
 int ret;

 ret = reserve_bp_slot(bp);
 if (ret)
  return ret;

 ret = validate_hw_breakpoint(bp);


 if (ret)
  release_bp_slot(bp);

 return ret;
}







struct perf_event *
register_user_hw_breakpoint(struct perf_event_attr *attr,
       perf_overflow_handler_t triggered,
       void *context,
       struct task_struct *tsk)
{
 return perf_event_create_kernel_counter(attr, -1, tsk, triggered,
      context);
}
EXPORT_SYMBOL_GPL(register_user_hw_breakpoint);
int modify_user_hw_breakpoint(struct perf_event *bp, struct perf_event_attr *attr)
{
 u64 old_addr = bp->attr.bp_addr;
 u64 old_len = bp->attr.bp_len;
 int old_type = bp->attr.bp_type;
 int err = 0;







 if (irqs_disabled() && bp->ctx && bp->ctx->task == current)
  perf_event_disable_local(bp);
 else
  perf_event_disable(bp);

 bp->attr.bp_addr = attr->bp_addr;
 bp->attr.bp_type = attr->bp_type;
 bp->attr.bp_len = attr->bp_len;

 if (attr->disabled)
  goto end;

 err = validate_hw_breakpoint(bp);
 if (!err)
  perf_event_enable(bp);

 if (err) {
  bp->attr.bp_addr = old_addr;
  bp->attr.bp_type = old_type;
  bp->attr.bp_len = old_len;
  if (!bp->attr.disabled)
   perf_event_enable(bp);

  return err;
 }

end:
 bp->attr.disabled = attr->disabled;

 return 0;
}
EXPORT_SYMBOL_GPL(modify_user_hw_breakpoint);





void unregister_hw_breakpoint(struct perf_event *bp)
{
 if (!bp)
  return;
 perf_event_release_kernel(bp);
}
EXPORT_SYMBOL_GPL(unregister_hw_breakpoint);
struct perf_event * __percpu *
register_wide_hw_breakpoint(struct perf_event_attr *attr,
       perf_overflow_handler_t triggered,
       void *context)
{
 struct perf_event * __percpu *cpu_events, *bp;
 long err = 0;
 int cpu;

 cpu_events = alloc_percpu(typeof(*cpu_events));
 if (!cpu_events)
  return (void __percpu __force *)ERR_PTR(-ENOMEM);

 get_online_cpus();
 for_each_online_cpu(cpu) {
  bp = perf_event_create_kernel_counter(attr, cpu, NULL,
            triggered, context);
  if (IS_ERR(bp)) {
   err = PTR_ERR(bp);
   break;
  }

  per_cpu(*cpu_events, cpu) = bp;
 }
 put_online_cpus();

 if (likely(!err))
  return cpu_events;

 unregister_wide_hw_breakpoint(cpu_events);
 return (void __percpu __force *)ERR_PTR(err);
}
EXPORT_SYMBOL_GPL(register_wide_hw_breakpoint);





void unregister_wide_hw_breakpoint(struct perf_event * __percpu *cpu_events)
{
 int cpu;

 for_each_possible_cpu(cpu)
  unregister_hw_breakpoint(per_cpu(*cpu_events, cpu));

 free_percpu(cpu_events);
}
EXPORT_SYMBOL_GPL(unregister_wide_hw_breakpoint);

static struct notifier_block hw_breakpoint_exceptions_nb = {
 .notifier_call = hw_breakpoint_exceptions_notify,

 .priority = 0x7fffffff
};

static void bp_perf_event_destroy(struct perf_event *event)
{
 release_bp_slot(event);
}

static int hw_breakpoint_event_init(struct perf_event *bp)
{
 int err;

 if (bp->attr.type != PERF_TYPE_BREAKPOINT)
  return -ENOENT;




 if (has_branch_stack(bp))
  return -EOPNOTSUPP;

 err = register_perf_hw_breakpoint(bp);
 if (err)
  return err;

 bp->destroy = bp_perf_event_destroy;

 return 0;
}

static int hw_breakpoint_add(struct perf_event *bp, int flags)
{
 if (!(flags & PERF_EF_START))
  bp->hw.state = PERF_HES_STOPPED;

 if (is_sampling_event(bp)) {
  bp->hw.last_period = bp->hw.sample_period;
  perf_swevent_set_period(bp);
 }

 return arch_install_hw_breakpoint(bp);
}

static void hw_breakpoint_del(struct perf_event *bp, int flags)
{
 arch_uninstall_hw_breakpoint(bp);
}

static void hw_breakpoint_start(struct perf_event *bp, int flags)
{
 bp->hw.state = 0;
}

static void hw_breakpoint_stop(struct perf_event *bp, int flags)
{
 bp->hw.state = PERF_HES_STOPPED;
}

static struct pmu perf_breakpoint = {
 .task_ctx_nr = perf_sw_context,

 .event_init = hw_breakpoint_event_init,
 .add = hw_breakpoint_add,
 .del = hw_breakpoint_del,
 .start = hw_breakpoint_start,
 .stop = hw_breakpoint_stop,
 .read = hw_breakpoint_pmu_read,
};

int __init init_hw_breakpoint(void)
{
 int cpu, err_cpu;
 int i;

 for (i = 0; i < TYPE_MAX; i++)
  nr_slots[i] = hw_breakpoint_slots(i);

 for_each_possible_cpu(cpu) {
  for (i = 0; i < TYPE_MAX; i++) {
   struct bp_cpuinfo *info = get_bp_info(cpu, i);

   info->tsk_pinned = kcalloc(nr_slots[i], sizeof(int),
       GFP_KERNEL);
   if (!info->tsk_pinned)
    goto err_alloc;
  }
 }

 constraints_initialized = 1;

 perf_pmu_register(&perf_breakpoint, "breakpoint", PERF_TYPE_BREAKPOINT);

 return register_die_notifier(&hw_breakpoint_exceptions_nb);

 err_alloc:
 for_each_possible_cpu(err_cpu) {
  for (i = 0; i < TYPE_MAX; i++)
   kfree(get_bp_info(err_cpu, i)->tsk_pinned);
  if (err_cpu == cpu)
   break;
 }

 return -ENOMEM;
}

enum bpf_type {
 BPF_TYPE_UNSPEC = 0,
 BPF_TYPE_PROG,
 BPF_TYPE_MAP,
};

static void *bpf_any_get(void *raw, enum bpf_type type)
{
 switch (type) {
 case BPF_TYPE_PROG:
  raw = bpf_prog_inc(raw);
  break;
 case BPF_TYPE_MAP:
  raw = bpf_map_inc(raw, true);
  break;
 default:
  WARN_ON_ONCE(1);
  break;
 }

 return raw;
}

static void bpf_any_put(void *raw, enum bpf_type type)
{
 switch (type) {
 case BPF_TYPE_PROG:
  bpf_prog_put(raw);
  break;
 case BPF_TYPE_MAP:
  bpf_map_put_with_uref(raw);
  break;
 default:
  WARN_ON_ONCE(1);
  break;
 }
}

static void *bpf_fd_probe_obj(u32 ufd, enum bpf_type *type)
{
 void *raw;

 *type = BPF_TYPE_MAP;
 raw = bpf_map_get_with_uref(ufd);
 if (IS_ERR(raw)) {
  *type = BPF_TYPE_PROG;
  raw = bpf_prog_get(ufd);
 }

 return raw;
}

static const struct inode_operations bpf_dir_iops;

static const struct inode_operations bpf_prog_iops = { };
static const struct inode_operations bpf_map_iops = { };

static struct inode *bpf_get_inode(struct super_block *sb,
       const struct inode *dir,
       umode_t mode)
{
 struct inode *inode;

 switch (mode & S_IFMT) {
 case S_IFDIR:
 case S_IFREG:
  break;
 default:
  return ERR_PTR(-EINVAL);
 }

 inode = new_inode(sb);
 if (!inode)
  return ERR_PTR(-ENOSPC);

 inode->i_ino = get_next_ino();
 inode->i_atime = CURRENT_TIME;
 inode->i_mtime = inode->i_atime;
 inode->i_ctime = inode->i_atime;

 inode_init_owner(inode, dir, mode);

 return inode;
}

static int bpf_inode_type(const struct inode *inode, enum bpf_type *type)
{
 *type = BPF_TYPE_UNSPEC;
 if (inode->i_op == &bpf_prog_iops)
  *type = BPF_TYPE_PROG;
 else if (inode->i_op == &bpf_map_iops)
  *type = BPF_TYPE_MAP;
 else
  return -EACCES;

 return 0;
}

static int bpf_mkdir(struct inode *dir, struct dentry *dentry, umode_t mode)
{
 struct inode *inode;

 inode = bpf_get_inode(dir->i_sb, dir, mode | S_IFDIR);
 if (IS_ERR(inode))
  return PTR_ERR(inode);

 inode->i_op = &bpf_dir_iops;
 inode->i_fop = &simple_dir_operations;

 inc_nlink(inode);
 inc_nlink(dir);

 d_instantiate(dentry, inode);
 dget(dentry);

 return 0;
}

static int bpf_mkobj_ops(struct inode *dir, struct dentry *dentry,
    umode_t mode, const struct inode_operations *iops)
{
 struct inode *inode;

 inode = bpf_get_inode(dir->i_sb, dir, mode | S_IFREG);
 if (IS_ERR(inode))
  return PTR_ERR(inode);

 inode->i_op = iops;
 inode->i_private = dentry->d_fsdata;

 d_instantiate(dentry, inode);
 dget(dentry);

 return 0;
}

static int bpf_mkobj(struct inode *dir, struct dentry *dentry, umode_t mode,
       dev_t devt)
{
 enum bpf_type type = MINOR(devt);

 if (MAJOR(devt) != UNNAMED_MAJOR || !S_ISREG(mode) ||
     dentry->d_fsdata == NULL)
  return -EPERM;

 switch (type) {
 case BPF_TYPE_PROG:
  return bpf_mkobj_ops(dir, dentry, mode, &bpf_prog_iops);
 case BPF_TYPE_MAP:
  return bpf_mkobj_ops(dir, dentry, mode, &bpf_map_iops);
 default:
  return -EPERM;
 }
}

static struct dentry *
bpf_lookup(struct inode *dir, struct dentry *dentry, unsigned flags)
{
 if (strchr(dentry->d_name.name, '.'))
  return ERR_PTR(-EPERM);
 return simple_lookup(dir, dentry, flags);
}

static const struct inode_operations bpf_dir_iops = {
 .lookup = bpf_lookup,
 .mknod = bpf_mkobj,
 .mkdir = bpf_mkdir,
 .rmdir = simple_rmdir,
 .rename = simple_rename,
 .link = simple_link,
 .unlink = simple_unlink,
};

static int bpf_obj_do_pin(const struct filename *pathname, void *raw,
     enum bpf_type type)
{
 struct dentry *dentry;
 struct inode *dir;
 struct path path;
 umode_t mode;
 dev_t devt;
 int ret;

 dentry = kern_path_create(AT_FDCWD, pathname->name, &path, 0);
 if (IS_ERR(dentry))
  return PTR_ERR(dentry);

 mode = S_IFREG | ((S_IRUSR | S_IWUSR) & ~current_umask());
 devt = MKDEV(UNNAMED_MAJOR, type);

 ret = security_path_mknod(&path, dentry, mode, devt);
 if (ret)
  goto out;

 dir = d_inode(path.dentry);
 if (dir->i_op != &bpf_dir_iops) {
  ret = -EPERM;
  goto out;
 }

 dentry->d_fsdata = raw;
 ret = vfs_mknod(dir, dentry, mode, devt);
 dentry->d_fsdata = NULL;
out:
 done_path_create(&path, dentry);
 return ret;
}

int bpf_obj_pin_user(u32 ufd, const char __user *pathname)
{
 struct filename *pname;
 enum bpf_type type;
 void *raw;
 int ret;

 pname = getname(pathname);
 if (IS_ERR(pname))
  return PTR_ERR(pname);

 raw = bpf_fd_probe_obj(ufd, &type);
 if (IS_ERR(raw)) {
  ret = PTR_ERR(raw);
  goto out;
 }

 ret = bpf_obj_do_pin(pname, raw, type);
 if (ret != 0)
  bpf_any_put(raw, type);
out:
 putname(pname);
 return ret;
}

static void *bpf_obj_do_get(const struct filename *pathname,
       enum bpf_type *type)
{
 struct inode *inode;
 struct path path;
 void *raw;
 int ret;

 ret = kern_path(pathname->name, LOOKUP_FOLLOW, &path);
 if (ret)
  return ERR_PTR(ret);

 inode = d_backing_inode(path.dentry);
 ret = inode_permission(inode, MAY_WRITE);
 if (ret)
  goto out;

 ret = bpf_inode_type(inode, type);
 if (ret)
  goto out;

 raw = bpf_any_get(inode->i_private, *type);
 if (!IS_ERR(raw))
  touch_atime(&path);

 path_put(&path);
 return raw;
out:
 path_put(&path);
 return ERR_PTR(ret);
}

int bpf_obj_get_user(const char __user *pathname)
{
 enum bpf_type type = BPF_TYPE_UNSPEC;
 struct filename *pname;
 int ret = -ENOENT;
 void *raw;

 pname = getname(pathname);
 if (IS_ERR(pname))
  return PTR_ERR(pname);

 raw = bpf_obj_do_get(pname, &type);
 if (IS_ERR(raw)) {
  ret = PTR_ERR(raw);
  goto out;
 }

 if (type == BPF_TYPE_PROG)
  ret = bpf_prog_new_fd(raw);
 else if (type == BPF_TYPE_MAP)
  ret = bpf_map_new_fd(raw);
 else
  goto out;

 if (ret < 0)
  bpf_any_put(raw, type);
out:
 putname(pname);
 return ret;
}

static void bpf_evict_inode(struct inode *inode)
{
 enum bpf_type type;

 truncate_inode_pages_final(&inode->i_data);
 clear_inode(inode);

 if (!bpf_inode_type(inode, &type))
  bpf_any_put(inode->i_private, type);
}

static const struct super_operations bpf_super_ops = {
 .statfs = simple_statfs,
 .drop_inode = generic_delete_inode,
 .evict_inode = bpf_evict_inode,
};

static int bpf_fill_super(struct super_block *sb, void *data, int silent)
{
 static struct tree_descr bpf_rfiles[] = { { "" } };
 struct inode *inode;
 int ret;

 ret = simple_fill_super(sb, BPF_FS_MAGIC, bpf_rfiles);
 if (ret)
  return ret;

 sb->s_op = &bpf_super_ops;

 inode = sb->s_root->d_inode;
 inode->i_op = &bpf_dir_iops;
 inode->i_mode &= ~S_IALLUGO;
 inode->i_mode |= S_ISVTX | S_IRWXUGO;

 return 0;
}

static struct dentry *bpf_mount(struct file_system_type *type, int flags,
    const char *dev_name, void *data)
{
 return mount_nodev(type, flags, data, bpf_fill_super);
}

static struct file_system_type bpf_fs_type = {
 .owner = THIS_MODULE,
 .name = "bpf",
 .mount = bpf_mount,
 .kill_sb = kill_litter_super,
};

MODULE_ALIAS_FS("bpf");

static int __init bpf_init(void)
{
 int ret;

 ret = sysfs_create_mount_point(fs_kobj, "bpf");
 if (ret)
  return ret;

 ret = register_filesystem(&bpf_fs_type);
 if (ret)
  sysfs_remove_mount_point(fs_kobj, "bpf");

 return ret;
}
fs_initcall(bpf_init);

int irq_reserve_ipi(struct irq_domain *domain,
        const struct cpumask *dest)
{
 unsigned int nr_irqs, offset;
 struct irq_data *data;
 int virq, i;

 if (!domain ||!irq_domain_is_ipi(domain)) {
  pr_warn("Reservation on a non IPI domain\n");
  return -EINVAL;
 }

 if (!cpumask_subset(dest, cpu_possible_mask)) {
  pr_warn("Reservation is not in possible_cpu_mask\n");
  return -EINVAL;
 }

 nr_irqs = cpumask_weight(dest);
 if (!nr_irqs) {
  pr_warn("Reservation for empty destination mask\n");
  return -EINVAL;
 }

 if (irq_domain_is_ipi_single(domain)) {






  nr_irqs = 1;
  offset = 0;
 } else {
  unsigned int next;







  offset = cpumask_first(dest);




  next = cpumask_next_zero(offset, dest);
  if (next < nr_cpu_ids)
   next = cpumask_next(next, dest);
  if (next < nr_cpu_ids) {
   pr_warn("Destination mask has holes\n");
   return -EINVAL;
  }
 }

 virq = irq_domain_alloc_descs(-1, nr_irqs, 0, NUMA_NO_NODE);
 if (virq <= 0) {
  pr_warn("Can't reserve IPI, failed to alloc descs\n");
  return -ENOMEM;
 }

 virq = __irq_domain_alloc_irqs(domain, virq, nr_irqs, NUMA_NO_NODE,
           (void *) dest, true);

 if (virq <= 0) {
  pr_warn("Can't reserve IPI, failed to alloc hw irqs\n");
  goto free_descs;
 }

 for (i = 0; i < nr_irqs; i++) {
  data = irq_get_irq_data(virq + i);
  cpumask_copy(data->common->affinity, dest);
  data->common->ipi_offset = offset;
  irq_set_status_flags(virq + i, IRQ_NO_BALANCING);
 }
 return virq;

free_descs:
 irq_free_descs(virq, nr_irqs);
 return -EBUSY;
}
int irq_destroy_ipi(unsigned int irq, const struct cpumask *dest)
{
 struct irq_data *data = irq_get_irq_data(irq);
 struct cpumask *ipimask = data ? irq_data_get_affinity_mask(data) : NULL;
 struct irq_domain *domain;
 unsigned int nr_irqs;

 if (!irq || !data || !ipimask)
  return -EINVAL;

 domain = data->domain;
 if (WARN_ON(domain == NULL))
  return -EINVAL;

 if (!irq_domain_is_ipi(domain)) {
  pr_warn("Trying to destroy a non IPI domain!\n");
  return -EINVAL;
 }

 if (WARN_ON(!cpumask_subset(dest, ipimask)))




  return -EINVAL;

 if (irq_domain_is_ipi_per_cpu(domain)) {
  irq = irq + cpumask_first(dest) - data->common->ipi_offset;
  nr_irqs = cpumask_weight(dest);
 } else {
  nr_irqs = 1;
 }

 irq_domain_free_irqs(irq, nr_irqs);
 return 0;
}
irq_hw_number_t ipi_get_hwirq(unsigned int irq, unsigned int cpu)
{
 struct irq_data *data = irq_get_irq_data(irq);
 struct cpumask *ipimask = data ? irq_data_get_affinity_mask(data) : NULL;

 if (!data || !ipimask || cpu > nr_cpu_ids)
  return INVALID_HWIRQ;

 if (!cpumask_test_cpu(cpu, ipimask))
  return INVALID_HWIRQ;







 if (irq_domain_is_ipi_per_cpu(data->domain))
  data = irq_get_irq_data(irq + cpu - data->common->ipi_offset);

 return data ? irqd_to_hwirq(data) : INVALID_HWIRQ;
}
EXPORT_SYMBOL_GPL(ipi_get_hwirq);

static int ipi_send_verify(struct irq_chip *chip, struct irq_data *data,
      const struct cpumask *dest, unsigned int cpu)
{
 struct cpumask *ipimask = irq_data_get_affinity_mask(data);

 if (!chip || !ipimask)
  return -EINVAL;

 if (!chip->ipi_send_single && !chip->ipi_send_mask)
  return -EINVAL;

 if (cpu > nr_cpu_ids)
  return -EINVAL;

 if (dest) {
  if (!cpumask_subset(dest, ipimask))
   return -EINVAL;
 } else {
  if (!cpumask_test_cpu(cpu, ipimask))
   return -EINVAL;
 }
 return 0;
}
int __ipi_send_single(struct irq_desc *desc, unsigned int cpu)
{
 struct irq_data *data = irq_desc_get_irq_data(desc);
 struct irq_chip *chip = irq_data_get_irq_chip(data);






 if (WARN_ON_ONCE(ipi_send_verify(chip, data, NULL, cpu)))
  return -EINVAL;
 if (!chip->ipi_send_single) {
  chip->ipi_send_mask(data, cpumask_of(cpu));
  return 0;
 }


 if (irq_domain_is_ipi_per_cpu(data->domain) &&
     cpu != data->common->ipi_offset) {

  unsigned irq = data->irq + cpu - data->common->ipi_offset;

  data = irq_get_irq_data(irq);
 }
 chip->ipi_send_single(data, cpu);
 return 0;
}
int __ipi_send_mask(struct irq_desc *desc, const struct cpumask *dest)
{
 struct irq_data *data = irq_desc_get_irq_data(desc);
 struct irq_chip *chip = irq_data_get_irq_chip(data);
 unsigned int cpu;






 if (WARN_ON_ONCE(ipi_send_verify(chip, data, dest, 0)))
  return -EINVAL;
 if (chip->ipi_send_mask) {
  chip->ipi_send_mask(data, dest);
  return 0;
 }

 if (irq_domain_is_ipi_per_cpu(data->domain)) {
  unsigned int base = data->irq;

  for_each_cpu(cpu, dest) {
   unsigned irq = base + cpu - data->common->ipi_offset;

   data = irq_get_irq_data(irq);
   chip->ipi_send_single(data, cpu);
  }
 } else {
  for_each_cpu(cpu, dest)
   chip->ipi_send_single(data, cpu);
 }
 return 0;
}
int ipi_send_single(unsigned int virq, unsigned int cpu)
{
 struct irq_desc *desc = irq_to_desc(virq);
 struct irq_data *data = desc ? irq_desc_get_irq_data(desc) : NULL;
 struct irq_chip *chip = data ? irq_data_get_irq_chip(data) : NULL;

 if (WARN_ON_ONCE(ipi_send_verify(chip, data, NULL, cpu)))
  return -EINVAL;

 return __ipi_send_single(desc, cpu);
}
EXPORT_SYMBOL_GPL(ipi_send_single);
int ipi_send_mask(unsigned int virq, const struct cpumask *dest)
{
 struct irq_desc *desc = irq_to_desc(virq);
 struct irq_data *data = desc ? irq_desc_get_irq_data(desc) : NULL;
 struct irq_chip *chip = data ? irq_data_get_irq_chip(data) : NULL;

 if (WARN_ON_ONCE(ipi_send_verify(chip, data, dest, 0)))
  return -EINVAL;

 return __ipi_send_mask(desc, dest);
}
EXPORT_SYMBOL_GPL(ipi_send_mask);





static struct lock_class_key irq_desc_lock_class;

static int __init irq_affinity_setup(char *str)
{
 zalloc_cpumask_var(&irq_default_affinity, GFP_NOWAIT);
 cpulist_parse(str, irq_default_affinity);




 cpumask_set_cpu(smp_processor_id(), irq_default_affinity);
 return 1;
}
__setup("irqaffinity=", irq_affinity_setup);

static void __init init_irq_default_affinity(void)
{
 if (!irq_default_affinity)
  zalloc_cpumask_var(&irq_default_affinity, GFP_NOWAIT);
 if (cpumask_empty(irq_default_affinity))
  cpumask_setall(irq_default_affinity);
}
static void __init init_irq_default_affinity(void)
{
}

static int alloc_masks(struct irq_desc *desc, gfp_t gfp, int node)
{
 if (!zalloc_cpumask_var_node(&desc->irq_common_data.affinity,
         gfp, node))
  return -ENOMEM;

 if (!zalloc_cpumask_var_node(&desc->pending_mask, gfp, node)) {
  free_cpumask_var(desc->irq_common_data.affinity);
  return -ENOMEM;
 }
 return 0;
}

static void desc_smp_init(struct irq_desc *desc, int node)
{
 cpumask_copy(desc->irq_common_data.affinity, irq_default_affinity);
 cpumask_clear(desc->pending_mask);
 desc->irq_common_data.node = node;
}

static inline int
alloc_masks(struct irq_desc *desc, gfp_t gfp, int node) { return 0; }
static inline void desc_smp_init(struct irq_desc *desc, int node) { }

static void desc_set_defaults(unsigned int irq, struct irq_desc *desc, int node,
  struct module *owner)
{
 int cpu;

 desc->irq_common_data.handler_data = NULL;
 desc->irq_common_data.msi_desc = NULL;

 desc->irq_data.common = &desc->irq_common_data;
 desc->irq_data.irq = irq;
 desc->irq_data.chip = &no_irq_chip;
 desc->irq_data.chip_data = NULL;
 irq_settings_clr_and_set(desc, ~0, _IRQ_DEFAULT_INIT_FLAGS);
 irqd_set(&desc->irq_data, IRQD_IRQ_DISABLED);
 desc->handle_irq = handle_bad_irq;
 desc->depth = 1;
 desc->irq_count = 0;
 desc->irqs_unhandled = 0;
 desc->name = NULL;
 desc->owner = owner;
 for_each_possible_cpu(cpu)
  *per_cpu_ptr(desc->kstat_irqs, cpu) = 0;
 desc_smp_init(desc, node);
}

int nr_irqs = NR_IRQS;
EXPORT_SYMBOL_GPL(nr_irqs);

static DEFINE_MUTEX(sparse_irq_lock);
static DECLARE_BITMAP(allocated_irqs, IRQ_BITMAP_BITS);


static RADIX_TREE(irq_desc_tree, GFP_KERNEL);

static void irq_insert_desc(unsigned int irq, struct irq_desc *desc)
{
 radix_tree_insert(&irq_desc_tree, irq, desc);
}

struct irq_desc *irq_to_desc(unsigned int irq)
{
 return radix_tree_lookup(&irq_desc_tree, irq);
}
EXPORT_SYMBOL(irq_to_desc);

static void delete_irq_desc(unsigned int irq)
{
 radix_tree_delete(&irq_desc_tree, irq);
}

static void free_masks(struct irq_desc *desc)
{
 free_cpumask_var(desc->pending_mask);
 free_cpumask_var(desc->irq_common_data.affinity);
}
static inline void free_masks(struct irq_desc *desc) { }

void irq_lock_sparse(void)
{
 mutex_lock(&sparse_irq_lock);
}

void irq_unlock_sparse(void)
{
 mutex_unlock(&sparse_irq_lock);
}

static struct irq_desc *alloc_desc(int irq, int node, struct module *owner)
{
 struct irq_desc *desc;
 gfp_t gfp = GFP_KERNEL;

 desc = kzalloc_node(sizeof(*desc), gfp, node);
 if (!desc)
  return NULL;

 desc->kstat_irqs = alloc_percpu(unsigned int);
 if (!desc->kstat_irqs)
  goto err_desc;

 if (alloc_masks(desc, gfp, node))
  goto err_kstat;

 raw_spin_lock_init(&desc->lock);
 lockdep_set_class(&desc->lock, &irq_desc_lock_class);
 init_rcu_head(&desc->rcu);

 desc_set_defaults(irq, desc, node, owner);

 return desc;

err_kstat:
 free_percpu(desc->kstat_irqs);
err_desc:
 kfree(desc);
 return NULL;
}

static void delayed_free_desc(struct rcu_head *rhp)
{
 struct irq_desc *desc = container_of(rhp, struct irq_desc, rcu);

 free_masks(desc);
 free_percpu(desc->kstat_irqs);
 kfree(desc);
}

static void free_desc(unsigned int irq)
{
 struct irq_desc *desc = irq_to_desc(irq);

 unregister_irq_proc(irq, desc);







 mutex_lock(&sparse_irq_lock);
 delete_irq_desc(irq);
 mutex_unlock(&sparse_irq_lock);






 call_rcu(&desc->rcu, delayed_free_desc);
}

static int alloc_descs(unsigned int start, unsigned int cnt, int node,
         struct module *owner)
{
 struct irq_desc *desc;
 int i;

 for (i = 0; i < cnt; i++) {
  desc = alloc_desc(start + i, node, owner);
  if (!desc)
   goto err;
  mutex_lock(&sparse_irq_lock);
  irq_insert_desc(start + i, desc);
  mutex_unlock(&sparse_irq_lock);
 }
 return start;

err:
 for (i--; i >= 0; i--)
  free_desc(start + i);

 mutex_lock(&sparse_irq_lock);
 bitmap_clear(allocated_irqs, start, cnt);
 mutex_unlock(&sparse_irq_lock);
 return -ENOMEM;
}

static int irq_expand_nr_irqs(unsigned int nr)
{
 if (nr > IRQ_BITMAP_BITS)
  return -ENOMEM;
 nr_irqs = nr;
 return 0;
}

int __init early_irq_init(void)
{
 int i, initcnt, node = first_online_node;
 struct irq_desc *desc;

 init_irq_default_affinity();


 initcnt = arch_probe_nr_irqs();
 printk(KERN_INFO "NR_IRQS:%d nr_irqs:%d %d\n", NR_IRQS, nr_irqs, initcnt);

 if (WARN_ON(nr_irqs > IRQ_BITMAP_BITS))
  nr_irqs = IRQ_BITMAP_BITS;

 if (WARN_ON(initcnt > IRQ_BITMAP_BITS))
  initcnt = IRQ_BITMAP_BITS;

 if (initcnt > nr_irqs)
  nr_irqs = initcnt;

 for (i = 0; i < initcnt; i++) {
  desc = alloc_desc(i, node, NULL);
  set_bit(i, allocated_irqs);
  irq_insert_desc(i, desc);
 }
 return arch_early_irq_init();
}


struct irq_desc irq_desc[NR_IRQS] __cacheline_aligned_in_smp = {
 [0 ... NR_IRQS-1] = {
  .handle_irq = handle_bad_irq,
  .depth = 1,
  .lock = __RAW_SPIN_LOCK_UNLOCKED(irq_desc->lock),
 }
};

int __init early_irq_init(void)
{
 int count, i, node = first_online_node;
 struct irq_desc *desc;

 init_irq_default_affinity();

 printk(KERN_INFO "NR_IRQS:%d\n", NR_IRQS);

 desc = irq_desc;
 count = ARRAY_SIZE(irq_desc);

 for (i = 0; i < count; i++) {
  desc[i].kstat_irqs = alloc_percpu(unsigned int);
  alloc_masks(&desc[i], GFP_KERNEL, node);
  raw_spin_lock_init(&desc[i].lock);
  lockdep_set_class(&desc[i].lock, &irq_desc_lock_class);
  desc_set_defaults(i, &desc[i], node, NULL);
 }
 return arch_early_irq_init();
}

struct irq_desc *irq_to_desc(unsigned int irq)
{
 return (irq < NR_IRQS) ? irq_desc + irq : NULL;
}
EXPORT_SYMBOL(irq_to_desc);

static void free_desc(unsigned int irq)
{
 struct irq_desc *desc = irq_to_desc(irq);
 unsigned long flags;

 raw_spin_lock_irqsave(&desc->lock, flags);
 desc_set_defaults(irq, desc, irq_desc_get_node(desc), NULL);
 raw_spin_unlock_irqrestore(&desc->lock, flags);
}

static inline int alloc_descs(unsigned int start, unsigned int cnt, int node,
         struct module *owner)
{
 u32 i;

 for (i = 0; i < cnt; i++) {
  struct irq_desc *desc = irq_to_desc(start + i);

  desc->owner = owner;
 }
 return start;
}

static int irq_expand_nr_irqs(unsigned int nr)
{
 return -ENOMEM;
}

void irq_mark_irq(unsigned int irq)
{
 mutex_lock(&sparse_irq_lock);
 bitmap_set(allocated_irqs, irq, 1);
 mutex_unlock(&sparse_irq_lock);
}

void irq_init_desc(unsigned int irq)
{
 free_desc(irq);
}







int generic_handle_irq(unsigned int irq)
{
 struct irq_desc *desc = irq_to_desc(irq);

 if (!desc)
  return -EINVAL;
 generic_handle_irq_desc(desc);
 return 0;
}
EXPORT_SYMBOL_GPL(generic_handle_irq);

int __handle_domain_irq(struct irq_domain *domain, unsigned int hwirq,
   bool lookup, struct pt_regs *regs)
{
 struct pt_regs *old_regs = set_irq_regs(regs);
 unsigned int irq = hwirq;
 int ret = 0;

 irq_enter();

 if (lookup)
  irq = irq_find_mapping(domain, hwirq);





 if (unlikely(!irq || irq >= nr_irqs)) {
  ack_bad_irq(irq);
  ret = -EINVAL;
 } else {
  generic_handle_irq(irq);
 }

 irq_exit();
 set_irq_regs(old_regs);
 return ret;
}
void irq_free_descs(unsigned int from, unsigned int cnt)
{
 int i;

 if (from >= nr_irqs || (from + cnt) > nr_irqs)
  return;

 for (i = 0; i < cnt; i++)
  free_desc(from + i);

 mutex_lock(&sparse_irq_lock);
 bitmap_clear(allocated_irqs, from, cnt);
 mutex_unlock(&sparse_irq_lock);
}
EXPORT_SYMBOL_GPL(irq_free_descs);
int __ref
__irq_alloc_descs(int irq, unsigned int from, unsigned int cnt, int node,
    struct module *owner)
{
 int start, ret;

 if (!cnt)
  return -EINVAL;

 if (irq >= 0) {
  if (from > irq)
   return -EINVAL;
  from = irq;
 } else {





  from = arch_dynirq_lower_bound(from);
 }

 mutex_lock(&sparse_irq_lock);

 start = bitmap_find_next_zero_area(allocated_irqs, IRQ_BITMAP_BITS,
        from, cnt, 0);
 ret = -EEXIST;
 if (irq >=0 && start != irq)
  goto err;

 if (start + cnt > nr_irqs) {
  ret = irq_expand_nr_irqs(start + cnt);
  if (ret)
   goto err;
 }

 bitmap_set(allocated_irqs, start, cnt);
 mutex_unlock(&sparse_irq_lock);
 return alloc_descs(start, cnt, node, owner);

err:
 mutex_unlock(&sparse_irq_lock);
 return ret;
}
EXPORT_SYMBOL_GPL(__irq_alloc_descs);








unsigned int irq_alloc_hwirqs(int cnt, int node)
{
 int i, irq = __irq_alloc_descs(-1, 0, cnt, node, NULL);

 if (irq < 0)
  return 0;

 for (i = irq; cnt > 0; i++, cnt--) {
  if (arch_setup_hwirq(i, node))
   goto err;
  irq_clear_status_flags(i, _IRQ_NOREQUEST);
 }
 return irq;

err:
 for (i--; i >= irq; i--) {
  irq_set_status_flags(i, _IRQ_NOREQUEST | _IRQ_NOPROBE);
  arch_teardown_hwirq(i);
 }
 irq_free_descs(irq, cnt);
 return 0;
}
EXPORT_SYMBOL_GPL(irq_alloc_hwirqs);







void irq_free_hwirqs(unsigned int from, int cnt)
{
 int i, j;

 for (i = from, j = cnt; j > 0; i++, j--) {
  irq_set_status_flags(i, _IRQ_NOREQUEST | _IRQ_NOPROBE);
  arch_teardown_hwirq(i);
 }
 irq_free_descs(from, cnt);
}
EXPORT_SYMBOL_GPL(irq_free_hwirqs);







unsigned int irq_get_next_irq(unsigned int offset)
{
 return find_next_bit(allocated_irqs, nr_irqs, offset);
}

struct irq_desc *
__irq_get_desc_lock(unsigned int irq, unsigned long *flags, bool bus,
      unsigned int check)
{
 struct irq_desc *desc = irq_to_desc(irq);

 if (desc) {
  if (check & _IRQ_DESC_CHECK) {
   if ((check & _IRQ_DESC_PERCPU) &&
       !irq_settings_is_per_cpu_devid(desc))
    return NULL;

   if (!(check & _IRQ_DESC_PERCPU) &&
       irq_settings_is_per_cpu_devid(desc))
    return NULL;
  }

  if (bus)
   chip_bus_lock(desc);
  raw_spin_lock_irqsave(&desc->lock, *flags);
 }
 return desc;
}

void __irq_put_desc_unlock(struct irq_desc *desc, unsigned long flags, bool bus)
{
 raw_spin_unlock_irqrestore(&desc->lock, flags);
 if (bus)
  chip_bus_sync_unlock(desc);
}

int irq_set_percpu_devid_partition(unsigned int irq,
       const struct cpumask *affinity)
{
 struct irq_desc *desc = irq_to_desc(irq);

 if (!desc)
  return -EINVAL;

 if (desc->percpu_enabled)
  return -EINVAL;

 desc->percpu_enabled = kzalloc(sizeof(*desc->percpu_enabled), GFP_KERNEL);

 if (!desc->percpu_enabled)
  return -ENOMEM;

 if (affinity)
  desc->percpu_affinity = affinity;
 else
  desc->percpu_affinity = cpu_possible_mask;

 irq_set_percpu_devid_flags(irq);
 return 0;
}

int irq_set_percpu_devid(unsigned int irq)
{
 return irq_set_percpu_devid_partition(irq, NULL);
}

int irq_get_percpu_devid_partition(unsigned int irq, struct cpumask *affinity)
{
 struct irq_desc *desc = irq_to_desc(irq);

 if (!desc || !desc->percpu_enabled)
  return -EINVAL;

 if (affinity)
  cpumask_copy(affinity, desc->percpu_affinity);

 return 0;
}

void kstat_incr_irq_this_cpu(unsigned int irq)
{
 kstat_incr_irqs_this_cpu(irq_to_desc(irq));
}
unsigned int kstat_irqs_cpu(unsigned int irq, int cpu)
{
 struct irq_desc *desc = irq_to_desc(irq);

 return desc && desc->kstat_irqs ?
   *per_cpu_ptr(desc->kstat_irqs, cpu) : 0;
}
unsigned int kstat_irqs(unsigned int irq)
{
 struct irq_desc *desc = irq_to_desc(irq);
 int cpu;
 unsigned int sum = 0;

 if (!desc || !desc->kstat_irqs)
  return 0;
 for_each_possible_cpu(cpu)
  sum += *per_cpu_ptr(desc->kstat_irqs, cpu);
 return sum;
}
unsigned int kstat_irqs_usr(unsigned int irq)
{
 unsigned int sum;

 irq_lock_sparse();
 sum = kstat_irqs(irq);
 irq_unlock_sparse();
 return sum;
}


static LIST_HEAD(irq_domain_list);
static DEFINE_MUTEX(irq_domain_mutex);

static DEFINE_MUTEX(revmap_trees_mutex);
static struct irq_domain *irq_default_domain;

static void irq_domain_check_hierarchy(struct irq_domain *domain);

struct irqchip_fwid {
 struct fwnode_handle fwnode;
 char *name;
 void *data;
};
struct fwnode_handle *irq_domain_alloc_fwnode(void *data)
{
 struct irqchip_fwid *fwid;
 char *name;

 fwid = kzalloc(sizeof(*fwid), GFP_KERNEL);
 name = kasprintf(GFP_KERNEL, "irqchip@%p", data);

 if (!fwid || !name) {
  kfree(fwid);
  kfree(name);
  return NULL;
 }

 fwid->name = name;
 fwid->data = data;
 fwid->fwnode.type = FWNODE_IRQCHIP;
 return &fwid->fwnode;
}
EXPORT_SYMBOL_GPL(irq_domain_alloc_fwnode);






void irq_domain_free_fwnode(struct fwnode_handle *fwnode)
{
 struct irqchip_fwid *fwid;

 if (WARN_ON(!is_fwnode_irqchip(fwnode)))
  return;

 fwid = container_of(fwnode, struct irqchip_fwid, fwnode);
 kfree(fwid->name);
 kfree(fwid);
}
EXPORT_SYMBOL_GPL(irq_domain_free_fwnode);
struct irq_domain *__irq_domain_add(struct fwnode_handle *fwnode, int size,
        irq_hw_number_t hwirq_max, int direct_max,
        const struct irq_domain_ops *ops,
        void *host_data)
{
 struct irq_domain *domain;
 struct device_node *of_node;

 of_node = to_of_node(fwnode);

 domain = kzalloc_node(sizeof(*domain) + (sizeof(unsigned int) * size),
         GFP_KERNEL, of_node_to_nid(of_node));
 if (WARN_ON(!domain))
  return NULL;

 of_node_get(of_node);


 INIT_RADIX_TREE(&domain->revmap_tree, GFP_KERNEL);
 domain->ops = ops;
 domain->host_data = host_data;
 domain->fwnode = fwnode;
 domain->hwirq_max = hwirq_max;
 domain->revmap_size = size;
 domain->revmap_direct_max_irq = direct_max;
 irq_domain_check_hierarchy(domain);

 mutex_lock(&irq_domain_mutex);
 list_add(&domain->link, &irq_domain_list);
 mutex_unlock(&irq_domain_mutex);

 pr_debug("Added domain %s\n", domain->name);
 return domain;
}
EXPORT_SYMBOL_GPL(__irq_domain_add);
void irq_domain_remove(struct irq_domain *domain)
{
 mutex_lock(&irq_domain_mutex);

 WARN_ON(!radix_tree_empty(&domain->revmap_tree));

 list_del(&domain->link);




 if (unlikely(irq_default_domain == domain))
  irq_set_default_host(NULL);

 mutex_unlock(&irq_domain_mutex);

 pr_debug("Removed domain %s\n", domain->name);

 of_node_put(irq_domain_get_of_node(domain));
 kfree(domain);
}
EXPORT_SYMBOL_GPL(irq_domain_remove);
struct irq_domain *irq_domain_add_simple(struct device_node *of_node,
      unsigned int size,
      unsigned int first_irq,
      const struct irq_domain_ops *ops,
      void *host_data)
{
 struct irq_domain *domain;

 domain = __irq_domain_add(of_node_to_fwnode(of_node), size, size, 0, ops, host_data);
 if (!domain)
  return NULL;

 if (first_irq > 0) {
  if (IS_ENABLED(CONFIG_SPARSE_IRQ)) {

   int rc = irq_alloc_descs(first_irq, first_irq, size,
       of_node_to_nid(of_node));
   if (rc < 0)
    pr_info("Cannot allocate irq_descs @ IRQ%d, assuming pre-allocated\n",
     first_irq);
  }
  irq_domain_associate_many(domain, first_irq, 0, size);
 }

 return domain;
}
EXPORT_SYMBOL_GPL(irq_domain_add_simple);
struct irq_domain *irq_domain_add_legacy(struct device_node *of_node,
      unsigned int size,
      unsigned int first_irq,
      irq_hw_number_t first_hwirq,
      const struct irq_domain_ops *ops,
      void *host_data)
{
 struct irq_domain *domain;

 domain = __irq_domain_add(of_node_to_fwnode(of_node), first_hwirq + size,
      first_hwirq + size, 0, ops, host_data);
 if (domain)
  irq_domain_associate_many(domain, first_irq, first_hwirq, size);

 return domain;
}
EXPORT_SYMBOL_GPL(irq_domain_add_legacy);






struct irq_domain *irq_find_matching_fwspec(struct irq_fwspec *fwspec,
         enum irq_domain_bus_token bus_token)
{
 struct irq_domain *h, *found = NULL;
 struct fwnode_handle *fwnode = fwspec->fwnode;
 int rc;
 mutex_lock(&irq_domain_mutex);
 list_for_each_entry(h, &irq_domain_list, link) {
  if (h->ops->select && fwspec->param_count)
   rc = h->ops->select(h, fwspec, bus_token);
  else if (h->ops->match)
   rc = h->ops->match(h, to_of_node(fwnode), bus_token);
  else
   rc = ((fwnode != NULL) && (h->fwnode == fwnode) &&
         ((bus_token == DOMAIN_BUS_ANY) ||
          (h->bus_token == bus_token)));

  if (rc) {
   found = h;
   break;
  }
 }
 mutex_unlock(&irq_domain_mutex);
 return found;
}
EXPORT_SYMBOL_GPL(irq_find_matching_fwspec);
void irq_set_default_host(struct irq_domain *domain)
{
 pr_debug("Default domain set to @0x%p\n", domain);

 irq_default_domain = domain;
}
EXPORT_SYMBOL_GPL(irq_set_default_host);

void irq_domain_disassociate(struct irq_domain *domain, unsigned int irq)
{
 struct irq_data *irq_data = irq_get_irq_data(irq);
 irq_hw_number_t hwirq;

 if (WARN(!irq_data || irq_data->domain != domain,
   "virq%i doesn't exist; cannot disassociate\n", irq))
  return;

 hwirq = irq_data->hwirq;
 irq_set_status_flags(irq, IRQ_NOREQUEST);


 irq_set_chip_and_handler(irq, NULL, NULL);


 synchronize_irq(irq);


 if (domain->ops->unmap)
  domain->ops->unmap(domain, irq);
 smp_mb();

 irq_data->domain = NULL;
 irq_data->hwirq = 0;


 if (hwirq < domain->revmap_size) {
  domain->linear_revmap[hwirq] = 0;
 } else {
  mutex_lock(&revmap_trees_mutex);
  radix_tree_delete(&domain->revmap_tree, hwirq);
  mutex_unlock(&revmap_trees_mutex);
 }
}

int irq_domain_associate(struct irq_domain *domain, unsigned int virq,
    irq_hw_number_t hwirq)
{
 struct irq_data *irq_data = irq_get_irq_data(virq);
 int ret;

 if (WARN(hwirq >= domain->hwirq_max,
   "error: hwirq 0x%x is too large for %s\n", (int)hwirq, domain->name))
  return -EINVAL;
 if (WARN(!irq_data, "error: virq%i is not allocated", virq))
  return -EINVAL;
 if (WARN(irq_data->domain, "error: virq%i is already associated", virq))
  return -EINVAL;

 mutex_lock(&irq_domain_mutex);
 irq_data->hwirq = hwirq;
 irq_data->domain = domain;
 if (domain->ops->map) {
  ret = domain->ops->map(domain, virq, hwirq);
  if (ret != 0) {





   if (ret != -EPERM) {
    pr_info("%s didn't like hwirq-0x%lx to VIRQ%i mapping (rc=%d)\n",
           domain->name, hwirq, virq, ret);
   }
   irq_data->domain = NULL;
   irq_data->hwirq = 0;
   mutex_unlock(&irq_domain_mutex);
   return ret;
  }


  if (!domain->name && irq_data->chip)
   domain->name = irq_data->chip->name;
 }

 if (hwirq < domain->revmap_size) {
  domain->linear_revmap[hwirq] = virq;
 } else {
  mutex_lock(&revmap_trees_mutex);
  radix_tree_insert(&domain->revmap_tree, hwirq, irq_data);
  mutex_unlock(&revmap_trees_mutex);
 }
 mutex_unlock(&irq_domain_mutex);

 irq_clear_status_flags(virq, IRQ_NOREQUEST);

 return 0;
}
EXPORT_SYMBOL_GPL(irq_domain_associate);

void irq_domain_associate_many(struct irq_domain *domain, unsigned int irq_base,
          irq_hw_number_t hwirq_base, int count)
{
 struct device_node *of_node;
 int i;

 of_node = irq_domain_get_of_node(domain);
 pr_debug("%s(%s, irqbase=%i, hwbase=%i, count=%i)\n", __func__,
  of_node_full_name(of_node), irq_base, (int)hwirq_base, count);

 for (i = 0; i < count; i++) {
  irq_domain_associate(domain, irq_base + i, hwirq_base + i);
 }
}
EXPORT_SYMBOL_GPL(irq_domain_associate_many);
unsigned int irq_create_direct_mapping(struct irq_domain *domain)
{
 struct device_node *of_node;
 unsigned int virq;

 if (domain == NULL)
  domain = irq_default_domain;

 of_node = irq_domain_get_of_node(domain);
 virq = irq_alloc_desc_from(1, of_node_to_nid(of_node));
 if (!virq) {
  pr_debug("create_direct virq allocation failed\n");
  return 0;
 }
 if (virq >= domain->revmap_direct_max_irq) {
  pr_err("ERROR: no free irqs available below %i maximum\n",
   domain->revmap_direct_max_irq);
  irq_free_desc(virq);
  return 0;
 }
 pr_debug("create_direct obtained virq %d\n", virq);

 if (irq_domain_associate(domain, virq, virq)) {
  irq_free_desc(virq);
  return 0;
 }

 return virq;
}
EXPORT_SYMBOL_GPL(irq_create_direct_mapping);
unsigned int irq_create_mapping(struct irq_domain *domain,
    irq_hw_number_t hwirq)
{
 struct device_node *of_node;
 int virq;

 pr_debug("irq_create_mapping(0x%p, 0x%lx)\n", domain, hwirq);


 if (domain == NULL)
  domain = irq_default_domain;
 if (domain == NULL) {
  WARN(1, "%s(, %lx) called with NULL domain\n", __func__, hwirq);
  return 0;
 }
 pr_debug("-> using domain @%p\n", domain);

 of_node = irq_domain_get_of_node(domain);


 virq = irq_find_mapping(domain, hwirq);
 if (virq) {
  pr_debug("-> existing mapping on virq %d\n", virq);
  return virq;
 }


 virq = irq_domain_alloc_descs(-1, 1, hwirq, of_node_to_nid(of_node));
 if (virq <= 0) {
  pr_debug("-> virq allocation failed\n");
  return 0;
 }

 if (irq_domain_associate(domain, virq, hwirq)) {
  irq_free_desc(virq);
  return 0;
 }

 pr_debug("irq %lu on domain %s mapped to virtual irq %u\n",
  hwirq, of_node_full_name(of_node), virq);

 return virq;
}
EXPORT_SYMBOL_GPL(irq_create_mapping);
int irq_create_strict_mappings(struct irq_domain *domain, unsigned int irq_base,
          irq_hw_number_t hwirq_base, int count)
{
 struct device_node *of_node;
 int ret;

 of_node = irq_domain_get_of_node(domain);
 ret = irq_alloc_descs(irq_base, irq_base, count,
         of_node_to_nid(of_node));
 if (unlikely(ret < 0))
  return ret;

 irq_domain_associate_many(domain, irq_base, hwirq_base, count);
 return 0;
}
EXPORT_SYMBOL_GPL(irq_create_strict_mappings);

static int irq_domain_translate(struct irq_domain *d,
    struct irq_fwspec *fwspec,
    irq_hw_number_t *hwirq, unsigned int *type)
{
 if (d->ops->translate)
  return d->ops->translate(d, fwspec, hwirq, type);
 if (d->ops->xlate)
  return d->ops->xlate(d, to_of_node(fwspec->fwnode),
         fwspec->param, fwspec->param_count,
         hwirq, type);


 *hwirq = fwspec->param[0];
 return 0;
}

static void of_phandle_args_to_fwspec(struct of_phandle_args *irq_data,
          struct irq_fwspec *fwspec)
{
 int i;

 fwspec->fwnode = irq_data->np ? &irq_data->np->fwnode : NULL;
 fwspec->param_count = irq_data->args_count;

 for (i = 0; i < irq_data->args_count; i++)
  fwspec->param[i] = irq_data->args[i];
}

unsigned int irq_create_fwspec_mapping(struct irq_fwspec *fwspec)
{
 struct irq_domain *domain;
 irq_hw_number_t hwirq;
 unsigned int type = IRQ_TYPE_NONE;
 int virq;

 if (fwspec->fwnode) {
  domain = irq_find_matching_fwspec(fwspec, DOMAIN_BUS_WIRED);
  if (!domain)
   domain = irq_find_matching_fwspec(fwspec, DOMAIN_BUS_ANY);
 } else {
  domain = irq_default_domain;
 }

 if (!domain) {
  pr_warn("no irq domain found for %s !\n",
   of_node_full_name(to_of_node(fwspec->fwnode)));
  return 0;
 }

 if (irq_domain_translate(domain, fwspec, &hwirq, &type))
  return 0;

 if (irq_domain_is_hierarchy(domain)) {




  virq = irq_find_mapping(domain, hwirq);
  if (virq)
   return virq;

  virq = irq_domain_alloc_irqs(domain, 1, NUMA_NO_NODE, fwspec);
  if (virq <= 0)
   return 0;
 } else {

  virq = irq_create_mapping(domain, hwirq);
  if (!virq)
   return virq;
 }


 if (type != IRQ_TYPE_NONE &&
     type != irq_get_trigger_type(virq))
  irq_set_irq_type(virq, type);
 return virq;
}
EXPORT_SYMBOL_GPL(irq_create_fwspec_mapping);

unsigned int irq_create_of_mapping(struct of_phandle_args *irq_data)
{
 struct irq_fwspec fwspec;

 of_phandle_args_to_fwspec(irq_data, &fwspec);
 return irq_create_fwspec_mapping(&fwspec);
}
EXPORT_SYMBOL_GPL(irq_create_of_mapping);





void irq_dispose_mapping(unsigned int virq)
{
 struct irq_data *irq_data = irq_get_irq_data(virq);
 struct irq_domain *domain;

 if (!virq || !irq_data)
  return;

 domain = irq_data->domain;
 if (WARN_ON(domain == NULL))
  return;

 irq_domain_disassociate(domain, virq);
 irq_free_desc(virq);
}
EXPORT_SYMBOL_GPL(irq_dispose_mapping);






unsigned int irq_find_mapping(struct irq_domain *domain,
         irq_hw_number_t hwirq)
{
 struct irq_data *data;


 if (domain == NULL)
  domain = irq_default_domain;
 if (domain == NULL)
  return 0;

 if (hwirq < domain->revmap_direct_max_irq) {
  data = irq_domain_get_irq_data(domain, hwirq);
  if (data && data->hwirq == hwirq)
   return hwirq;
 }


 if (hwirq < domain->revmap_size)
  return domain->linear_revmap[hwirq];

 rcu_read_lock();
 data = radix_tree_lookup(&domain->revmap_tree, hwirq);
 rcu_read_unlock();
 return data ? data->irq : 0;
}
EXPORT_SYMBOL_GPL(irq_find_mapping);

static int virq_debug_show(struct seq_file *m, void *private)
{
 unsigned long flags;
 struct irq_desc *desc;
 struct irq_domain *domain;
 struct radix_tree_iter iter;
 void *data, **slot;
 int i;

 seq_printf(m, " %-16s  %-6s  %-10s  %-10s  %s\n",
     "name", "mapped", "linear-max", "direct-max", "devtree-node");
 mutex_lock(&irq_domain_mutex);
 list_for_each_entry(domain, &irq_domain_list, link) {
  struct device_node *of_node;
  int count = 0;
  of_node = irq_domain_get_of_node(domain);
  radix_tree_for_each_slot(slot, &domain->revmap_tree, &iter, 0)
   count++;
  seq_printf(m, "%c%-16s  %6u  %10u  %10u  %s\n",
      domain == irq_default_domain ? '*' : ' ', domain->name,
      domain->revmap_size + count, domain->revmap_size,
      domain->revmap_direct_max_irq,
      of_node ? of_node_full_name(of_node) : "");
 }
 mutex_unlock(&irq_domain_mutex);

 seq_printf(m, "%-5s  %-7s  %-15s  %-*s  %6s  %-14s  %s\n", "irq", "hwirq",
        "chip name", (int)(2 * sizeof(void *) + 2), "chip data",
        "active", "type", "domain");

 for (i = 1; i < nr_irqs; i++) {
  desc = irq_to_desc(i);
  if (!desc)
   continue;

  raw_spin_lock_irqsave(&desc->lock, flags);
  domain = desc->irq_data.domain;

  if (domain) {
   struct irq_chip *chip;
   int hwirq = desc->irq_data.hwirq;
   bool direct;

   seq_printf(m, "%5d  ", i);
   seq_printf(m, "0x%05x  ", hwirq);

   chip = irq_desc_get_chip(desc);
   seq_printf(m, "%-15s  ", (chip && chip->name) ? chip->name : "none");

   data = irq_desc_get_chip_data(desc);
   seq_printf(m, data ? "0x%p  " : "  %p  ", data);

   seq_printf(m, "   %c    ", (desc->action && desc->action->handler) ? '*' : ' ');
   direct = (i == hwirq) && (i < domain->revmap_direct_max_irq);
   seq_printf(m, "%6s%-8s  ",
       (hwirq < domain->revmap_size) ? "LINEAR" : "RADIX",
       direct ? "(DIRECT)" : "");
   seq_printf(m, "%s\n", desc->irq_data.domain->name);
  }

  raw_spin_unlock_irqrestore(&desc->lock, flags);
 }

 return 0;
}

static int virq_debug_open(struct inode *inode, struct file *file)
{
 return single_open(file, virq_debug_show, inode->i_private);
}

static const struct file_operations virq_debug_fops = {
 .open = virq_debug_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
};

static int __init irq_debugfs_init(void)
{
 if (debugfs_create_file("irq_domain_mapping", S_IRUGO, NULL,
     NULL, &virq_debug_fops) == NULL)
  return -ENOMEM;

 return 0;
}
__initcall(irq_debugfs_init);







int irq_domain_xlate_onecell(struct irq_domain *d, struct device_node *ctrlr,
        const u32 *intspec, unsigned int intsize,
        unsigned long *out_hwirq, unsigned int *out_type)
{
 if (WARN_ON(intsize < 1))
  return -EINVAL;
 *out_hwirq = intspec[0];
 *out_type = IRQ_TYPE_NONE;
 return 0;
}
EXPORT_SYMBOL_GPL(irq_domain_xlate_onecell);
int irq_domain_xlate_twocell(struct irq_domain *d, struct device_node *ctrlr,
   const u32 *intspec, unsigned int intsize,
   irq_hw_number_t *out_hwirq, unsigned int *out_type)
{
 if (WARN_ON(intsize < 2))
  return -EINVAL;
 *out_hwirq = intspec[0];
 *out_type = intspec[1] & IRQ_TYPE_SENSE_MASK;
 return 0;
}
EXPORT_SYMBOL_GPL(irq_domain_xlate_twocell);
int irq_domain_xlate_onetwocell(struct irq_domain *d,
    struct device_node *ctrlr,
    const u32 *intspec, unsigned int intsize,
    unsigned long *out_hwirq, unsigned int *out_type)
{
 if (WARN_ON(intsize < 1))
  return -EINVAL;
 *out_hwirq = intspec[0];
 *out_type = (intsize > 1) ? intspec[1] : IRQ_TYPE_NONE;
 return 0;
}
EXPORT_SYMBOL_GPL(irq_domain_xlate_onetwocell);

const struct irq_domain_ops irq_domain_simple_ops = {
 .xlate = irq_domain_xlate_onetwocell,
};
EXPORT_SYMBOL_GPL(irq_domain_simple_ops);

int irq_domain_alloc_descs(int virq, unsigned int cnt, irq_hw_number_t hwirq,
      int node)
{
 unsigned int hint;

 if (virq >= 0) {
  virq = irq_alloc_descs(virq, virq, cnt, node);
 } else {
  hint = hwirq % nr_irqs;
  if (hint == 0)
   hint++;
  virq = irq_alloc_descs_from(hint, cnt, node);
  if (virq <= 0 && hint > 1)
   virq = irq_alloc_descs_from(1, cnt, node);
 }

 return virq;
}

struct irq_domain *irq_domain_create_hierarchy(struct irq_domain *parent,
         unsigned int flags,
         unsigned int size,
         struct fwnode_handle *fwnode,
         const struct irq_domain_ops *ops,
         void *host_data)
{
 struct irq_domain *domain;

 if (size)
  domain = irq_domain_create_linear(fwnode, size, ops, host_data);
 else
  domain = irq_domain_create_tree(fwnode, ops, host_data);
 if (domain) {
  domain->parent = parent;
  domain->flags |= flags;
 }

 return domain;
}
EXPORT_SYMBOL_GPL(irq_domain_create_hierarchy);

static void irq_domain_insert_irq(int virq)
{
 struct irq_data *data;

 for (data = irq_get_irq_data(virq); data; data = data->parent_data) {
  struct irq_domain *domain = data->domain;
  irq_hw_number_t hwirq = data->hwirq;

  if (hwirq < domain->revmap_size) {
   domain->linear_revmap[hwirq] = virq;
  } else {
   mutex_lock(&revmap_trees_mutex);
   radix_tree_insert(&domain->revmap_tree, hwirq, data);
   mutex_unlock(&revmap_trees_mutex);
  }


  if (!domain->name && data->chip)
   domain->name = data->chip->name;
 }

 irq_clear_status_flags(virq, IRQ_NOREQUEST);
}

static void irq_domain_remove_irq(int virq)
{
 struct irq_data *data;

 irq_set_status_flags(virq, IRQ_NOREQUEST);
 irq_set_chip_and_handler(virq, NULL, NULL);
 synchronize_irq(virq);
 smp_mb();

 for (data = irq_get_irq_data(virq); data; data = data->parent_data) {
  struct irq_domain *domain = data->domain;
  irq_hw_number_t hwirq = data->hwirq;

  if (hwirq < domain->revmap_size) {
   domain->linear_revmap[hwirq] = 0;
  } else {
   mutex_lock(&revmap_trees_mutex);
   radix_tree_delete(&domain->revmap_tree, hwirq);
   mutex_unlock(&revmap_trees_mutex);
  }
 }
}

static struct irq_data *irq_domain_insert_irq_data(struct irq_domain *domain,
         struct irq_data *child)
{
 struct irq_data *irq_data;

 irq_data = kzalloc_node(sizeof(*irq_data), GFP_KERNEL,
    irq_data_get_node(child));
 if (irq_data) {
  child->parent_data = irq_data;
  irq_data->irq = child->irq;
  irq_data->common = child->common;
  irq_data->domain = domain;
 }

 return irq_data;
}

static void irq_domain_free_irq_data(unsigned int virq, unsigned int nr_irqs)
{
 struct irq_data *irq_data, *tmp;
 int i;

 for (i = 0; i < nr_irqs; i++) {
  irq_data = irq_get_irq_data(virq + i);
  tmp = irq_data->parent_data;
  irq_data->parent_data = NULL;
  irq_data->domain = NULL;

  while (tmp) {
   irq_data = tmp;
   tmp = tmp->parent_data;
   kfree(irq_data);
  }
 }
}

static int irq_domain_alloc_irq_data(struct irq_domain *domain,
         unsigned int virq, unsigned int nr_irqs)
{
 struct irq_data *irq_data;
 struct irq_domain *parent;
 int i;


 for (i = 0; i < nr_irqs; i++) {
  irq_data = irq_get_irq_data(virq + i);
  irq_data->domain = domain;

  for (parent = domain->parent; parent; parent = parent->parent) {
   irq_data = irq_domain_insert_irq_data(parent, irq_data);
   if (!irq_data) {
    irq_domain_free_irq_data(virq, i + 1);
    return -ENOMEM;
   }
  }
 }

 return 0;
}






struct irq_data *irq_domain_get_irq_data(struct irq_domain *domain,
      unsigned int virq)
{
 struct irq_data *irq_data;

 for (irq_data = irq_get_irq_data(virq); irq_data;
      irq_data = irq_data->parent_data)
  if (irq_data->domain == domain)
   return irq_data;

 return NULL;
}
EXPORT_SYMBOL_GPL(irq_domain_get_irq_data);
int irq_domain_set_hwirq_and_chip(struct irq_domain *domain, unsigned int virq,
      irq_hw_number_t hwirq, struct irq_chip *chip,
      void *chip_data)
{
 struct irq_data *irq_data = irq_domain_get_irq_data(domain, virq);

 if (!irq_data)
  return -ENOENT;

 irq_data->hwirq = hwirq;
 irq_data->chip = chip ? chip : &no_irq_chip;
 irq_data->chip_data = chip_data;

 return 0;
}
EXPORT_SYMBOL_GPL(irq_domain_set_hwirq_and_chip);
void irq_domain_set_info(struct irq_domain *domain, unsigned int virq,
    irq_hw_number_t hwirq, struct irq_chip *chip,
    void *chip_data, irq_flow_handler_t handler,
    void *handler_data, const char *handler_name)
{
 irq_domain_set_hwirq_and_chip(domain, virq, hwirq, chip, chip_data);
 __irq_set_handler(virq, handler, 0, handler_name);
 irq_set_handler_data(virq, handler_data);
}
EXPORT_SYMBOL(irq_domain_set_info);





void irq_domain_reset_irq_data(struct irq_data *irq_data)
{
 irq_data->hwirq = 0;
 irq_data->chip = &no_irq_chip;
 irq_data->chip_data = NULL;
}
EXPORT_SYMBOL_GPL(irq_domain_reset_irq_data);







void irq_domain_free_irqs_common(struct irq_domain *domain, unsigned int virq,
     unsigned int nr_irqs)
{
 struct irq_data *irq_data;
 int i;

 for (i = 0; i < nr_irqs; i++) {
  irq_data = irq_domain_get_irq_data(domain, virq + i);
  if (irq_data)
   irq_domain_reset_irq_data(irq_data);
 }
 irq_domain_free_irqs_parent(domain, virq, nr_irqs);
}
EXPORT_SYMBOL_GPL(irq_domain_free_irqs_common);







void irq_domain_free_irqs_top(struct irq_domain *domain, unsigned int virq,
         unsigned int nr_irqs)
{
 int i;

 for (i = 0; i < nr_irqs; i++) {
  irq_set_handler_data(virq + i, NULL);
  irq_set_handler(virq + i, NULL);
 }
 irq_domain_free_irqs_common(domain, virq, nr_irqs);
}

static bool irq_domain_is_auto_recursive(struct irq_domain *domain)
{
 return domain->flags & IRQ_DOMAIN_FLAG_AUTO_RECURSIVE;
}

static void irq_domain_free_irqs_recursive(struct irq_domain *domain,
        unsigned int irq_base,
        unsigned int nr_irqs)
{
 domain->ops->free(domain, irq_base, nr_irqs);
 if (irq_domain_is_auto_recursive(domain)) {
  BUG_ON(!domain->parent);
  irq_domain_free_irqs_recursive(domain->parent, irq_base,
            nr_irqs);
 }
}

int irq_domain_alloc_irqs_recursive(struct irq_domain *domain,
        unsigned int irq_base,
        unsigned int nr_irqs, void *arg)
{
 int ret = 0;
 struct irq_domain *parent = domain->parent;
 bool recursive = irq_domain_is_auto_recursive(domain);

 BUG_ON(recursive && !parent);
 if (recursive)
  ret = irq_domain_alloc_irqs_recursive(parent, irq_base,
            nr_irqs, arg);
 if (ret >= 0)
  ret = domain->ops->alloc(domain, irq_base, nr_irqs, arg);
 if (ret < 0 && recursive)
  irq_domain_free_irqs_recursive(parent, irq_base, nr_irqs);

 return ret;
}
int __irq_domain_alloc_irqs(struct irq_domain *domain, int irq_base,
       unsigned int nr_irqs, int node, void *arg,
       bool realloc)
{
 int i, ret, virq;

 if (domain == NULL) {
  domain = irq_default_domain;
  if (WARN(!domain, "domain is NULL; cannot allocate IRQ\n"))
   return -EINVAL;
 }

 if (!domain->ops->alloc) {
  pr_debug("domain->ops->alloc() is NULL\n");
  return -ENOSYS;
 }

 if (realloc && irq_base >= 0) {
  virq = irq_base;
 } else {
  virq = irq_domain_alloc_descs(irq_base, nr_irqs, 0, node);
  if (virq < 0) {
   pr_debug("cannot allocate IRQ(base %d, count %d)\n",
     irq_base, nr_irqs);
   return virq;
  }
 }

 if (irq_domain_alloc_irq_data(domain, virq, nr_irqs)) {
  pr_debug("cannot allocate memory for IRQ%d\n", virq);
  ret = -ENOMEM;
  goto out_free_desc;
 }

 mutex_lock(&irq_domain_mutex);
 ret = irq_domain_alloc_irqs_recursive(domain, virq, nr_irqs, arg);
 if (ret < 0) {
  mutex_unlock(&irq_domain_mutex);
  goto out_free_irq_data;
 }
 for (i = 0; i < nr_irqs; i++)
  irq_domain_insert_irq(virq + i);
 mutex_unlock(&irq_domain_mutex);

 return virq;

out_free_irq_data:
 irq_domain_free_irq_data(virq, nr_irqs);
out_free_desc:
 irq_free_descs(virq, nr_irqs);
 return ret;
}






void irq_domain_free_irqs(unsigned int virq, unsigned int nr_irqs)
{
 struct irq_data *data = irq_get_irq_data(virq);
 int i;

 if (WARN(!data || !data->domain || !data->domain->ops->free,
   "NULL pointer, cannot free irq\n"))
  return;

 mutex_lock(&irq_domain_mutex);
 for (i = 0; i < nr_irqs; i++)
  irq_domain_remove_irq(virq + i);
 irq_domain_free_irqs_recursive(data->domain, virq, nr_irqs);
 mutex_unlock(&irq_domain_mutex);

 irq_domain_free_irq_data(virq, nr_irqs);
 irq_free_descs(virq, nr_irqs);
}
int irq_domain_alloc_irqs_parent(struct irq_domain *domain,
     unsigned int irq_base, unsigned int nr_irqs,
     void *arg)
{

 if (irq_domain_is_auto_recursive(domain))
  return 0;

 domain = domain->parent;
 if (domain)
  return irq_domain_alloc_irqs_recursive(domain, irq_base,
             nr_irqs, arg);
 return -ENOSYS;
}
EXPORT_SYMBOL_GPL(irq_domain_alloc_irqs_parent);
void irq_domain_free_irqs_parent(struct irq_domain *domain,
     unsigned int irq_base, unsigned int nr_irqs)
{

 if (!irq_domain_is_auto_recursive(domain) && domain->parent)
  irq_domain_free_irqs_recursive(domain->parent, irq_base,
            nr_irqs);
}
EXPORT_SYMBOL_GPL(irq_domain_free_irqs_parent);
void irq_domain_activate_irq(struct irq_data *irq_data)
{
 if (irq_data && irq_data->domain) {
  struct irq_domain *domain = irq_data->domain;

  if (irq_data->parent_data)
   irq_domain_activate_irq(irq_data->parent_data);
  if (domain->ops->activate)
   domain->ops->activate(domain, irq_data);
 }
}
void irq_domain_deactivate_irq(struct irq_data *irq_data)
{
 if (irq_data && irq_data->domain) {
  struct irq_domain *domain = irq_data->domain;

  if (domain->ops->deactivate)
   domain->ops->deactivate(domain, irq_data);
  if (irq_data->parent_data)
   irq_domain_deactivate_irq(irq_data->parent_data);
 }
}

static void irq_domain_check_hierarchy(struct irq_domain *domain)
{

 if (domain->ops->alloc)
  domain->flags |= IRQ_DOMAIN_FLAG_HIERARCHY;
}





struct irq_data *irq_domain_get_irq_data(struct irq_domain *domain,
      unsigned int virq)
{
 struct irq_data *irq_data = irq_get_irq_data(virq);

 return (irq_data && irq_data->domain == domain) ? irq_data : NULL;
}
EXPORT_SYMBOL_GPL(irq_domain_get_irq_data);
void irq_domain_set_info(struct irq_domain *domain, unsigned int virq,
    irq_hw_number_t hwirq, struct irq_chip *chip,
    void *chip_data, irq_flow_handler_t handler,
    void *handler_data, const char *handler_name)
{
 irq_set_chip_and_handler_name(virq, chip, handler, handler_name);
 irq_set_chip_data(virq, chip_data);
 irq_set_handler_data(virq, handler_data);
}

static void irq_domain_check_hierarchy(struct irq_domain *domain)
{
}









static DEFINE_PER_CPU(struct llist_head, raised_list);
static DEFINE_PER_CPU(struct llist_head, lazy_list);




static bool irq_work_claim(struct irq_work *work)
{
 unsigned long flags, oflags, nflags;





 flags = work->flags & ~IRQ_WORK_PENDING;
 for (;;) {
  nflags = flags | IRQ_WORK_FLAGS;
  oflags = cmpxchg(&work->flags, flags, nflags);
  if (oflags == flags)
   break;
  if (oflags & IRQ_WORK_PENDING)
   return false;
  flags = oflags;
  cpu_relax();
 }

 return true;
}

void __weak arch_irq_work_raise(void)
{



}







bool irq_work_queue_on(struct irq_work *work, int cpu)
{

 WARN_ON_ONCE(cpu_is_offline(cpu));


 WARN_ON_ONCE(in_nmi());


 if (!irq_work_claim(work))
  return false;

 if (llist_add(&work->llnode, &per_cpu(raised_list, cpu)))
  arch_send_call_function_single_ipi(cpu);

 return true;
}
EXPORT_SYMBOL_GPL(irq_work_queue_on);


bool irq_work_queue(struct irq_work *work)
{

 if (!irq_work_claim(work))
  return false;


 preempt_disable();


 if (work->flags & IRQ_WORK_LAZY) {
  if (llist_add(&work->llnode, this_cpu_ptr(&lazy_list)) &&
      tick_nohz_tick_stopped())
   arch_irq_work_raise();
 } else {
  if (llist_add(&work->llnode, this_cpu_ptr(&raised_list)))
   arch_irq_work_raise();
 }

 preempt_enable();

 return true;
}
EXPORT_SYMBOL_GPL(irq_work_queue);

bool irq_work_needs_cpu(void)
{
 struct llist_head *raised, *lazy;

 raised = this_cpu_ptr(&raised_list);
 lazy = this_cpu_ptr(&lazy_list);

 if (llist_empty(raised) || arch_irq_work_has_interrupt())
  if (llist_empty(lazy))
   return false;


 WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));

 return true;
}

static void irq_work_run_list(struct llist_head *list)
{
 unsigned long flags;
 struct irq_work *work;
 struct llist_node *llnode;

 BUG_ON(!irqs_disabled());

 if (llist_empty(list))
  return;

 llnode = llist_del_all(list);
 while (llnode != NULL) {
  work = llist_entry(llnode, struct irq_work, llnode);

  llnode = llist_next(llnode);
  flags = work->flags & ~IRQ_WORK_PENDING;
  xchg(&work->flags, flags);

  work->func(work);




  (void)cmpxchg(&work->flags, flags, flags & ~IRQ_WORK_BUSY);
 }
}





void irq_work_run(void)
{
 irq_work_run_list(this_cpu_ptr(&raised_list));
 irq_work_run_list(this_cpu_ptr(&lazy_list));
}
EXPORT_SYMBOL_GPL(irq_work_run);

void irq_work_tick(void)
{
 struct llist_head *raised = this_cpu_ptr(&raised_list);

 if (!llist_empty(raised) && !arch_irq_work_has_interrupt())
  irq_work_run_list(raised);
 irq_work_run_list(this_cpu_ptr(&lazy_list));
}





void irq_work_sync(struct irq_work *work)
{
 WARN_ON_ONCE(irqs_disabled());

 while (work->flags & IRQ_WORK_BUSY)
  cpu_relax();
}
EXPORT_SYMBOL_GPL(irq_work_sync);

static struct timeval itimer_get_remtime(struct hrtimer *timer)
{
 ktime_t rem = __hrtimer_get_remaining(timer, true);






 if (hrtimer_active(timer)) {
  if (rem.tv64 <= 0)
   rem.tv64 = NSEC_PER_USEC;
 } else
  rem.tv64 = 0;

 return ktime_to_timeval(rem);
}

static void get_cpu_itimer(struct task_struct *tsk, unsigned int clock_id,
      struct itimerval *const value)
{
 cputime_t cval, cinterval;
 struct cpu_itimer *it = &tsk->signal->it[clock_id];

 spin_lock_irq(&tsk->sighand->siglock);

 cval = it->expires;
 cinterval = it->incr;
 if (cval) {
  struct task_cputime cputime;
  cputime_t t;

  thread_group_cputimer(tsk, &cputime);
  if (clock_id == CPUCLOCK_PROF)
   t = cputime.utime + cputime.stime;
  else

   t = cputime.utime;

  if (cval < t)

   cval = cputime_one_jiffy;
  else
   cval = cval - t;
 }

 spin_unlock_irq(&tsk->sighand->siglock);

 cputime_to_timeval(cval, &value->it_value);
 cputime_to_timeval(cinterval, &value->it_interval);
}

int do_getitimer(int which, struct itimerval *value)
{
 struct task_struct *tsk = current;

 switch (which) {
 case ITIMER_REAL:
  spin_lock_irq(&tsk->sighand->siglock);
  value->it_value = itimer_get_remtime(&tsk->signal->real_timer);
  value->it_interval =
   ktime_to_timeval(tsk->signal->it_real_incr);
  spin_unlock_irq(&tsk->sighand->siglock);
  break;
 case ITIMER_VIRTUAL:
  get_cpu_itimer(tsk, CPUCLOCK_VIRT, value);
  break;
 case ITIMER_PROF:
  get_cpu_itimer(tsk, CPUCLOCK_PROF, value);
  break;
 default:
  return(-EINVAL);
 }
 return 0;
}

SYSCALL_DEFINE2(getitimer, int, which, struct itimerval __user *, value)
{
 int error = -EFAULT;
 struct itimerval get_buffer;

 if (value) {
  error = do_getitimer(which, &get_buffer);
  if (!error &&
      copy_to_user(value, &get_buffer, sizeof(get_buffer)))
   error = -EFAULT;
 }
 return error;
}





enum hrtimer_restart it_real_fn(struct hrtimer *timer)
{
 struct signal_struct *sig =
  container_of(timer, struct signal_struct, real_timer);

 trace_itimer_expire(ITIMER_REAL, sig->leader_pid, 0);
 kill_pid_info(SIGALRM, SEND_SIG_PRIV, sig->leader_pid);

 return HRTIMER_NORESTART;
}

static inline u32 cputime_sub_ns(cputime_t ct, s64 real_ns)
{
 struct timespec ts;
 s64 cpu_ns;

 cputime_to_timespec(ct, &ts);
 cpu_ns = timespec_to_ns(&ts);

 return (cpu_ns <= real_ns) ? 0 : cpu_ns - real_ns;
}

static void set_cpu_itimer(struct task_struct *tsk, unsigned int clock_id,
      const struct itimerval *const value,
      struct itimerval *const ovalue)
{
 cputime_t cval, nval, cinterval, ninterval;
 s64 ns_ninterval, ns_nval;
 u32 error, incr_error;
 struct cpu_itimer *it = &tsk->signal->it[clock_id];

 nval = timeval_to_cputime(&value->it_value);
 ns_nval = timeval_to_ns(&value->it_value);
 ninterval = timeval_to_cputime(&value->it_interval);
 ns_ninterval = timeval_to_ns(&value->it_interval);

 error = cputime_sub_ns(nval, ns_nval);
 incr_error = cputime_sub_ns(ninterval, ns_ninterval);

 spin_lock_irq(&tsk->sighand->siglock);

 cval = it->expires;
 cinterval = it->incr;
 if (cval || nval) {
  if (nval > 0)
   nval += cputime_one_jiffy;
  set_process_cpu_timer(tsk, clock_id, &nval, &cval);
 }
 it->expires = nval;
 it->incr = ninterval;
 it->error = error;
 it->incr_error = incr_error;
 trace_itimer_state(clock_id == CPUCLOCK_VIRT ?
      ITIMER_VIRTUAL : ITIMER_PROF, value, nval);

 spin_unlock_irq(&tsk->sighand->siglock);

 if (ovalue) {
  cputime_to_timeval(cval, &ovalue->it_value);
  cputime_to_timeval(cinterval, &ovalue->it_interval);
 }
}




 (((t)->tv_sec >= 0) && (((unsigned long) (t)->tv_usec) < USEC_PER_SEC))

int do_setitimer(int which, struct itimerval *value, struct itimerval *ovalue)
{
 struct task_struct *tsk = current;
 struct hrtimer *timer;
 ktime_t expires;




 if (!timeval_valid(&value->it_value) ||
     !timeval_valid(&value->it_interval))
  return -EINVAL;

 switch (which) {
 case ITIMER_REAL:
again:
  spin_lock_irq(&tsk->sighand->siglock);
  timer = &tsk->signal->real_timer;
  if (ovalue) {
   ovalue->it_value = itimer_get_remtime(timer);
   ovalue->it_interval
    = ktime_to_timeval(tsk->signal->it_real_incr);
  }

  if (hrtimer_try_to_cancel(timer) < 0) {
   spin_unlock_irq(&tsk->sighand->siglock);
   goto again;
  }
  expires = timeval_to_ktime(value->it_value);
  if (expires.tv64 != 0) {
   tsk->signal->it_real_incr =
    timeval_to_ktime(value->it_interval);
   hrtimer_start(timer, expires, HRTIMER_MODE_REL);
  } else
   tsk->signal->it_real_incr.tv64 = 0;

  trace_itimer_state(ITIMER_REAL, value, 0);
  spin_unlock_irq(&tsk->sighand->siglock);
  break;
 case ITIMER_VIRTUAL:
  set_cpu_itimer(tsk, CPUCLOCK_VIRT, value, ovalue);
  break;
 case ITIMER_PROF:
  set_cpu_itimer(tsk, CPUCLOCK_PROF, value, ovalue);
  break;
 default:
  return -EINVAL;
 }
 return 0;
}
unsigned int alarm_setitimer(unsigned int seconds)
{
 struct itimerval it_new, it_old;

 if (seconds > INT_MAX)
  seconds = INT_MAX;
 it_new.it_value.tv_sec = seconds;
 it_new.it_value.tv_usec = 0;
 it_new.it_interval.tv_sec = it_new.it_interval.tv_usec = 0;

 do_setitimer(ITIMER_REAL, &it_new, &it_old);





 if ((!it_old.it_value.tv_sec && it_old.it_value.tv_usec) ||
       it_old.it_value.tv_usec >= 500000)
  it_old.it_value.tv_sec++;

 return it_old.it_value.tv_sec;
}

SYSCALL_DEFINE3(setitimer, int, which, struct itimerval __user *, value,
  struct itimerval __user *, ovalue)
{
 struct itimerval set_buffer, get_buffer;
 int error;

 if (value) {
  if(copy_from_user(&set_buffer, value, sizeof(set_buffer)))
   return -EFAULT;
 } else {
  memset(&set_buffer, 0, sizeof(set_buffer));
  printk_once(KERN_WARNING "%s calls setitimer() with new_value NULL pointer."
       " Misfeature support will be removed\n",
       current->comm);
 }

 error = do_setitimer(which, &set_buffer, ovalue ? &get_buffer : NULL);
 if (error || !ovalue)
  return error;

 if (copy_to_user(ovalue, &get_buffer, sizeof(get_buffer)))
  return -EFAULT;
 return 0;
}


static cycle_t jiffies_read(struct clocksource *cs)
{
 return (cycle_t) jiffies;
}

static struct clocksource clocksource_jiffies = {
 .name = "jiffies",
 .rating = 1,
 .read = jiffies_read,
 .mask = CLOCKSOURCE_MASK(32),
 .mult = NSEC_PER_JIFFY << JIFFIES_SHIFT,
 .shift = JIFFIES_SHIFT,
 .max_cycles = 10,
};

__cacheline_aligned_in_smp DEFINE_SEQLOCK(jiffies_lock);

u64 get_jiffies_64(void)
{
 unsigned long seq;
 u64 ret;

 do {
  seq = read_seqbegin(&jiffies_lock);
  ret = jiffies_64;
 } while (read_seqretry(&jiffies_lock, seq));
 return ret;
}
EXPORT_SYMBOL(get_jiffies_64);

EXPORT_SYMBOL(jiffies);

static int __init init_jiffies_clocksource(void)
{
 return __clocksource_register(&clocksource_jiffies);
}

core_initcall(init_jiffies_clocksource);

struct clocksource * __init __weak clocksource_default_clock(void)
{
 return &clocksource_jiffies;
}

struct clocksource refined_jiffies;

int register_refined_jiffies(long cycles_per_second)
{
 u64 nsec_per_tick, shift_hz;
 long cycles_per_tick;



 refined_jiffies = clocksource_jiffies;
 refined_jiffies.name = "refined-jiffies";
 refined_jiffies.rating++;


 cycles_per_tick = (cycles_per_second + HZ/2)/HZ;

 shift_hz = (u64)cycles_per_second << 8;
 shift_hz += cycles_per_tick/2;
 do_div(shift_hz, cycles_per_tick);

 nsec_per_tick = (u64)NSEC_PER_SEC << 8;
 nsec_per_tick += (u32)shift_hz/2;
 do_div(nsec_per_tick, (u32)shift_hz);

 refined_jiffies.mult = ((u32)nsec_per_tick) << JIFFIES_SHIFT;

 __clocksource_register(&refined_jiffies);
 return 0;
}










static DEFINE_MUTEX(jump_label_mutex);

void jump_label_lock(void)
{
 mutex_lock(&jump_label_mutex);
}

void jump_label_unlock(void)
{
 mutex_unlock(&jump_label_mutex);
}

static int jump_label_cmp(const void *a, const void *b)
{
 const struct jump_entry *jea = a;
 const struct jump_entry *jeb = b;

 if (jea->key < jeb->key)
  return -1;

 if (jea->key > jeb->key)
  return 1;

 return 0;
}

static void
jump_label_sort_entries(struct jump_entry *start, struct jump_entry *stop)
{
 unsigned long size;

 size = (((unsigned long)stop - (unsigned long)start)
     / sizeof(struct jump_entry));
 sort(start, size, sizeof(struct jump_entry), jump_label_cmp, NULL);
}

static void jump_label_update(struct static_key *key);

void static_key_slow_inc(struct static_key *key)
{
 int v, v1;

 STATIC_KEY_CHECK_USE();
 for (v = atomic_read(&key->enabled); v > 0; v = v1) {
  v1 = atomic_cmpxchg(&key->enabled, v, v + 1);
  if (likely(v1 == v))
   return;
 }

 jump_label_lock();
 if (atomic_read(&key->enabled) == 0) {
  atomic_set(&key->enabled, -1);
  jump_label_update(key);
  atomic_set(&key->enabled, 1);
 } else {
  atomic_inc(&key->enabled);
 }
 jump_label_unlock();
}
EXPORT_SYMBOL_GPL(static_key_slow_inc);

static void __static_key_slow_dec(struct static_key *key,
  unsigned long rate_limit, struct delayed_work *work)
{







 if (!atomic_dec_and_mutex_lock(&key->enabled, &jump_label_mutex)) {
  WARN(atomic_read(&key->enabled) < 0,
       "jump label: negative count!\n");
  return;
 }

 if (rate_limit) {
  atomic_inc(&key->enabled);
  schedule_delayed_work(work, rate_limit);
 } else {
  jump_label_update(key);
 }
 jump_label_unlock();
}

static void jump_label_update_timeout(struct work_struct *work)
{
 struct static_key_deferred *key =
  container_of(work, struct static_key_deferred, work.work);
 __static_key_slow_dec(&key->key, 0, NULL);
}

void static_key_slow_dec(struct static_key *key)
{
 STATIC_KEY_CHECK_USE();
 __static_key_slow_dec(key, 0, NULL);
}
EXPORT_SYMBOL_GPL(static_key_slow_dec);

void static_key_slow_dec_deferred(struct static_key_deferred *key)
{
 STATIC_KEY_CHECK_USE();
 __static_key_slow_dec(&key->key, key->timeout, &key->work);
}
EXPORT_SYMBOL_GPL(static_key_slow_dec_deferred);

void jump_label_rate_limit(struct static_key_deferred *key,
  unsigned long rl)
{
 STATIC_KEY_CHECK_USE();
 key->timeout = rl;
 INIT_DELAYED_WORK(&key->work, jump_label_update_timeout);
}
EXPORT_SYMBOL_GPL(jump_label_rate_limit);

static int addr_conflict(struct jump_entry *entry, void *start, void *end)
{
 if (entry->code <= (unsigned long)end &&
  entry->code + JUMP_LABEL_NOP_SIZE > (unsigned long)start)
  return 1;

 return 0;
}

static int __jump_label_text_reserved(struct jump_entry *iter_start,
  struct jump_entry *iter_stop, void *start, void *end)
{
 struct jump_entry *iter;

 iter = iter_start;
 while (iter < iter_stop) {
  if (addr_conflict(iter, start, end))
   return 1;
  iter++;
 }

 return 0;
}







void __weak __init_or_module arch_jump_label_transform_static(struct jump_entry *entry,
         enum jump_label_type type)
{
 arch_jump_label_transform(entry, type);
}

static inline struct jump_entry *static_key_entries(struct static_key *key)
{
 return (struct jump_entry *)((unsigned long)key->entries & ~JUMP_TYPE_MASK);
}

static inline bool static_key_type(struct static_key *key)
{
 return (unsigned long)key->entries & JUMP_TYPE_MASK;
}

static inline struct static_key *jump_entry_key(struct jump_entry *entry)
{
 return (struct static_key *)((unsigned long)entry->key & ~1UL);
}

static bool jump_entry_branch(struct jump_entry *entry)
{
 return (unsigned long)entry->key & 1UL;
}

static enum jump_label_type jump_label_type(struct jump_entry *entry)
{
 struct static_key *key = jump_entry_key(entry);
 bool enabled = static_key_enabled(key);
 bool branch = jump_entry_branch(entry);


 return enabled ^ branch;
}

static void __jump_label_update(struct static_key *key,
    struct jump_entry *entry,
    struct jump_entry *stop)
{
 for (; (entry < stop) && (jump_entry_key(entry) == key); entry++) {





  if (entry->code && kernel_text_address(entry->code))
   arch_jump_label_transform(entry, jump_label_type(entry));
 }
}

void __init jump_label_init(void)
{
 struct jump_entry *iter_start = __start___jump_table;
 struct jump_entry *iter_stop = __stop___jump_table;
 struct static_key *key = NULL;
 struct jump_entry *iter;

 jump_label_lock();
 jump_label_sort_entries(iter_start, iter_stop);

 for (iter = iter_start; iter < iter_stop; iter++) {
  struct static_key *iterk;


  if (jump_label_type(iter) == JUMP_LABEL_NOP)
   arch_jump_label_transform_static(iter, JUMP_LABEL_NOP);

  iterk = jump_entry_key(iter);
  if (iterk == key)
   continue;

  key = iterk;



  *((unsigned long *)&key->entries) += (unsigned long)iter;
  key->next = NULL;
 }
 static_key_initialized = true;
 jump_label_unlock();
}


static enum jump_label_type jump_label_init_type(struct jump_entry *entry)
{
 struct static_key *key = jump_entry_key(entry);
 bool type = static_key_type(key);
 bool branch = jump_entry_branch(entry);


 return type ^ branch;
}

struct static_key_mod {
 struct static_key_mod *next;
 struct jump_entry *entries;
 struct module *mod;
};

static int __jump_label_mod_text_reserved(void *start, void *end)
{
 struct module *mod;

 mod = __module_text_address((unsigned long)start);
 if (!mod)
  return 0;

 WARN_ON_ONCE(__module_text_address((unsigned long)end) != mod);

 return __jump_label_text_reserved(mod->jump_entries,
    mod->jump_entries + mod->num_jump_entries,
    start, end);
}

static void __jump_label_mod_update(struct static_key *key)
{
 struct static_key_mod *mod;

 for (mod = key->next; mod; mod = mod->next) {
  struct module *m = mod->mod;

  __jump_label_update(key, mod->entries,
        m->jump_entries + m->num_jump_entries);
 }
}
void jump_label_apply_nops(struct module *mod)
{
 struct jump_entry *iter_start = mod->jump_entries;
 struct jump_entry *iter_stop = iter_start + mod->num_jump_entries;
 struct jump_entry *iter;


 if (iter_start == iter_stop)
  return;

 for (iter = iter_start; iter < iter_stop; iter++) {

  if (jump_label_init_type(iter) == JUMP_LABEL_NOP)
   arch_jump_label_transform_static(iter, JUMP_LABEL_NOP);
 }
}

static int jump_label_add_module(struct module *mod)
{
 struct jump_entry *iter_start = mod->jump_entries;
 struct jump_entry *iter_stop = iter_start + mod->num_jump_entries;
 struct jump_entry *iter;
 struct static_key *key = NULL;
 struct static_key_mod *jlm;


 if (iter_start == iter_stop)
  return 0;

 jump_label_sort_entries(iter_start, iter_stop);

 for (iter = iter_start; iter < iter_stop; iter++) {
  struct static_key *iterk;

  iterk = jump_entry_key(iter);
  if (iterk == key)
   continue;

  key = iterk;
  if (within_module(iter->key, mod)) {



   *((unsigned long *)&key->entries) += (unsigned long)iter;
   key->next = NULL;
   continue;
  }
  jlm = kzalloc(sizeof(struct static_key_mod), GFP_KERNEL);
  if (!jlm)
   return -ENOMEM;
  jlm->mod = mod;
  jlm->entries = iter;
  jlm->next = key->next;
  key->next = jlm;


  if (jump_label_type(iter) != jump_label_init_type(iter))
   __jump_label_update(key, iter, iter_stop);
 }

 return 0;
}

static void jump_label_del_module(struct module *mod)
{
 struct jump_entry *iter_start = mod->jump_entries;
 struct jump_entry *iter_stop = iter_start + mod->num_jump_entries;
 struct jump_entry *iter;
 struct static_key *key = NULL;
 struct static_key_mod *jlm, **prev;

 for (iter = iter_start; iter < iter_stop; iter++) {
  if (jump_entry_key(iter) == key)
   continue;

  key = jump_entry_key(iter);

  if (within_module(iter->key, mod))
   continue;

  prev = &key->next;
  jlm = key->next;

  while (jlm && jlm->mod != mod) {
   prev = &jlm->next;
   jlm = jlm->next;
  }

  if (jlm) {
   *prev = jlm->next;
   kfree(jlm);
  }
 }
}

static void jump_label_invalidate_module_init(struct module *mod)
{
 struct jump_entry *iter_start = mod->jump_entries;
 struct jump_entry *iter_stop = iter_start + mod->num_jump_entries;
 struct jump_entry *iter;

 for (iter = iter_start; iter < iter_stop; iter++) {
  if (within_module_init(iter->code, mod))
   iter->code = 0;
 }
}

static int
jump_label_module_notify(struct notifier_block *self, unsigned long val,
    void *data)
{
 struct module *mod = data;
 int ret = 0;

 switch (val) {
 case MODULE_STATE_COMING:
  jump_label_lock();
  ret = jump_label_add_module(mod);
  if (ret)
   jump_label_del_module(mod);
  jump_label_unlock();
  break;
 case MODULE_STATE_GOING:
  jump_label_lock();
  jump_label_del_module(mod);
  jump_label_unlock();
  break;
 case MODULE_STATE_LIVE:
  jump_label_lock();
  jump_label_invalidate_module_init(mod);
  jump_label_unlock();
  break;
 }

 return notifier_from_errno(ret);
}

struct notifier_block jump_label_module_nb = {
 .notifier_call = jump_label_module_notify,
 .priority = 1,
};

static __init int jump_label_init_module(void)
{
 return register_module_notifier(&jump_label_module_nb);
}
early_initcall(jump_label_init_module);

int jump_label_text_reserved(void *start, void *end)
{
 int ret = __jump_label_text_reserved(__start___jump_table,
   __stop___jump_table, start, end);

 if (ret)
  return ret;

 ret = __jump_label_mod_text_reserved(start, end);
 return ret;
}

static void jump_label_update(struct static_key *key)
{
 struct jump_entry *stop = __stop___jump_table;
 struct jump_entry *entry = static_key_entries(key);
 struct module *mod;

 __jump_label_mod_update(key);

 preempt_disable();
 mod = __module_address((unsigned long)key);
 if (mod)
  stop = mod->jump_entries + mod->num_jump_entries;
 preempt_enable();

 if (entry)
  __jump_label_update(key, entry, stop);
}

static DEFINE_STATIC_KEY_TRUE(sk_true);
static DEFINE_STATIC_KEY_FALSE(sk_false);

static __init int jump_label_test(void)
{
 int i;

 for (i = 0; i < 2; i++) {
  WARN_ON(static_key_enabled(&sk_true.key) != true);
  WARN_ON(static_key_enabled(&sk_false.key) != false);

  WARN_ON(!static_branch_likely(&sk_true));
  WARN_ON(!static_branch_unlikely(&sk_true));
  WARN_ON(static_branch_likely(&sk_false));
  WARN_ON(static_branch_unlikely(&sk_false));

  static_branch_disable(&sk_true);
  static_branch_enable(&sk_false);

  WARN_ON(static_key_enabled(&sk_true.key) == true);
  WARN_ON(static_key_enabled(&sk_false.key) == false);

  WARN_ON(static_branch_likely(&sk_true));
  WARN_ON(static_branch_unlikely(&sk_true));
  WARN_ON(!static_branch_likely(&sk_false));
  WARN_ON(!static_branch_unlikely(&sk_false));

  static_branch_enable(&sk_true);
  static_branch_disable(&sk_false);
 }

 return 0;
}
late_initcall(jump_label_test);








extern const unsigned long kallsyms_addresses[] __weak;
extern const int kallsyms_offsets[] __weak;
extern const u8 kallsyms_names[] __weak;





extern const unsigned long kallsyms_num_syms
__attribute__((weak, section(".rodata")));

extern const unsigned long kallsyms_relative_base
__attribute__((weak, section(".rodata")));

extern const u8 kallsyms_token_table[] __weak;
extern const u16 kallsyms_token_index[] __weak;

extern const unsigned long kallsyms_markers[] __weak;

static inline int is_kernel_inittext(unsigned long addr)
{
 if (addr >= (unsigned long)_sinittext
     && addr <= (unsigned long)_einittext)
  return 1;
 return 0;
}

static inline int is_kernel_text(unsigned long addr)
{
 if ((addr >= (unsigned long)_stext && addr <= (unsigned long)_etext) ||
     arch_is_kernel_text(addr))
  return 1;
 return in_gate_area_no_mm(addr);
}

static inline int is_kernel(unsigned long addr)
{
 if (addr >= (unsigned long)_stext && addr <= (unsigned long)_end)
  return 1;
 return in_gate_area_no_mm(addr);
}

static int is_ksym_addr(unsigned long addr)
{
 if (all_var)
  return is_kernel(addr);

 return is_kernel_text(addr) || is_kernel_inittext(addr);
}






static unsigned int kallsyms_expand_symbol(unsigned int off,
        char *result, size_t maxlen)
{
 int len, skipped_first = 0;
 const u8 *tptr, *data;


 data = &kallsyms_names[off];
 len = *data;
 data++;





 off += len + 1;





 while (len) {
  tptr = &kallsyms_token_table[kallsyms_token_index[*data]];
  data++;
  len--;

  while (*tptr) {
   if (skipped_first) {
    if (maxlen <= 1)
     goto tail;
    *result = *tptr;
    result++;
    maxlen--;
   } else
    skipped_first = 1;
   tptr++;
  }
 }

tail:
 if (maxlen)
  *result = '\0';


 return off;
}





static char kallsyms_get_symbol_type(unsigned int off)
{




 return kallsyms_token_table[kallsyms_token_index[kallsyms_names[off + 1]]];
}






static unsigned int get_symbol_offset(unsigned long pos)
{
 const u8 *name;
 int i;





 name = &kallsyms_names[kallsyms_markers[pos >> 8]];







 for (i = 0; i < (pos & 0xFF); i++)
  name = name + (*name) + 1;

 return name - kallsyms_names;
}

static unsigned long kallsyms_sym_address(int idx)
{
 if (!IS_ENABLED(CONFIG_KALLSYMS_BASE_RELATIVE))
  return kallsyms_addresses[idx];


 if (!IS_ENABLED(CONFIG_KALLSYMS_ABSOLUTE_PERCPU))
  return kallsyms_relative_base + (u32)kallsyms_offsets[idx];


 if (kallsyms_offsets[idx] >= 0)
  return kallsyms_offsets[idx];


 return kallsyms_relative_base - 1 - kallsyms_offsets[idx];
}


unsigned long kallsyms_lookup_name(const char *name)
{
 char namebuf[KSYM_NAME_LEN];
 unsigned long i;
 unsigned int off;

 for (i = 0, off = 0; i < kallsyms_num_syms; i++) {
  off = kallsyms_expand_symbol(off, namebuf, ARRAY_SIZE(namebuf));

  if (strcmp(namebuf, name) == 0)
   return kallsyms_sym_address(i);
 }
 return module_kallsyms_lookup_name(name);
}
EXPORT_SYMBOL_GPL(kallsyms_lookup_name);

int kallsyms_on_each_symbol(int (*fn)(void *, const char *, struct module *,
          unsigned long),
       void *data)
{
 char namebuf[KSYM_NAME_LEN];
 unsigned long i;
 unsigned int off;
 int ret;

 for (i = 0, off = 0; i < kallsyms_num_syms; i++) {
  off = kallsyms_expand_symbol(off, namebuf, ARRAY_SIZE(namebuf));
  ret = fn(data, namebuf, NULL, kallsyms_sym_address(i));
  if (ret != 0)
   return ret;
 }
 return module_kallsyms_on_each_symbol(fn, data);
}
EXPORT_SYMBOL_GPL(kallsyms_on_each_symbol);

static unsigned long get_symbol_pos(unsigned long addr,
        unsigned long *symbolsize,
        unsigned long *offset)
{
 unsigned long symbol_start = 0, symbol_end = 0;
 unsigned long i, low, high, mid;


 if (!IS_ENABLED(CONFIG_KALLSYMS_BASE_RELATIVE))
  BUG_ON(!kallsyms_addresses);
 else
  BUG_ON(!kallsyms_offsets);


 low = 0;
 high = kallsyms_num_syms;

 while (high - low > 1) {
  mid = low + (high - low) / 2;
  if (kallsyms_sym_address(mid) <= addr)
   low = mid;
  else
   high = mid;
 }





 while (low && kallsyms_sym_address(low-1) == kallsyms_sym_address(low))
  --low;

 symbol_start = kallsyms_sym_address(low);


 for (i = low + 1; i < kallsyms_num_syms; i++) {
  if (kallsyms_sym_address(i) > symbol_start) {
   symbol_end = kallsyms_sym_address(i);
   break;
  }
 }


 if (!symbol_end) {
  if (is_kernel_inittext(addr))
   symbol_end = (unsigned long)_einittext;
  else if (all_var)
   symbol_end = (unsigned long)_end;
  else
   symbol_end = (unsigned long)_etext;
 }

 if (symbolsize)
  *symbolsize = symbol_end - symbol_start;
 if (offset)
  *offset = addr - symbol_start;

 return low;
}




int kallsyms_lookup_size_offset(unsigned long addr, unsigned long *symbolsize,
    unsigned long *offset)
{
 char namebuf[KSYM_NAME_LEN];
 if (is_ksym_addr(addr))
  return !!get_symbol_pos(addr, symbolsize, offset);

 return !!module_address_lookup(addr, symbolsize, offset, NULL, namebuf);
}
const char *kallsyms_lookup(unsigned long addr,
       unsigned long *symbolsize,
       unsigned long *offset,
       char **modname, char *namebuf)
{
 namebuf[KSYM_NAME_LEN - 1] = 0;
 namebuf[0] = 0;

 if (is_ksym_addr(addr)) {
  unsigned long pos;

  pos = get_symbol_pos(addr, symbolsize, offset);

  kallsyms_expand_symbol(get_symbol_offset(pos),
           namebuf, KSYM_NAME_LEN);
  if (modname)
   *modname = NULL;
  return namebuf;
 }


 return module_address_lookup(addr, symbolsize, offset, modname,
         namebuf);
}

int lookup_symbol_name(unsigned long addr, char *symname)
{
 symname[0] = '\0';
 symname[KSYM_NAME_LEN - 1] = '\0';

 if (is_ksym_addr(addr)) {
  unsigned long pos;

  pos = get_symbol_pos(addr, NULL, NULL);

  kallsyms_expand_symbol(get_symbol_offset(pos),
           symname, KSYM_NAME_LEN);
  return 0;
 }

 return lookup_module_symbol_name(addr, symname);
}

int lookup_symbol_attrs(unsigned long addr, unsigned long *size,
   unsigned long *offset, char *modname, char *name)
{
 name[0] = '\0';
 name[KSYM_NAME_LEN - 1] = '\0';

 if (is_ksym_addr(addr)) {
  unsigned long pos;

  pos = get_symbol_pos(addr, size, offset);

  kallsyms_expand_symbol(get_symbol_offset(pos),
           name, KSYM_NAME_LEN);
  modname[0] = '\0';
  return 0;
 }

 return lookup_module_symbol_attrs(addr, size, offset, modname, name);
}


static int __sprint_symbol(char *buffer, unsigned long address,
      int symbol_offset, int add_offset)
{
 char *modname;
 const char *name;
 unsigned long offset, size;
 int len;

 address += symbol_offset;
 name = kallsyms_lookup(address, &size, &offset, &modname, buffer);
 if (!name)
  return sprintf(buffer, "0x%lx", address - symbol_offset);

 if (name != buffer)
  strcpy(buffer, name);
 len = strlen(buffer);
 offset -= symbol_offset;

 if (add_offset)
  len += sprintf(buffer + len, "+%#lx/%#lx", offset, size);

 if (modname)
  len += sprintf(buffer + len, " [%s]", modname);

 return len;
}
int sprint_symbol(char *buffer, unsigned long address)
{
 return __sprint_symbol(buffer, address, 0, 1);
}
EXPORT_SYMBOL_GPL(sprint_symbol);
int sprint_symbol_no_offset(char *buffer, unsigned long address)
{
 return __sprint_symbol(buffer, address, 0, 0);
}
EXPORT_SYMBOL_GPL(sprint_symbol_no_offset);
int sprint_backtrace(char *buffer, unsigned long address)
{
 return __sprint_symbol(buffer, address, -1, 1);
}


void __print_symbol(const char *fmt, unsigned long address)
{
 char buffer[KSYM_SYMBOL_LEN];

 sprint_symbol(buffer, address);

 printk(fmt, buffer);
}
EXPORT_SYMBOL(__print_symbol);


struct kallsym_iter {
 loff_t pos;
 unsigned long value;
 unsigned int nameoff;
 char type;
 char name[KSYM_NAME_LEN];
 char module_name[MODULE_NAME_LEN];
 int exported;
};

static int get_ksymbol_mod(struct kallsym_iter *iter)
{
 if (module_get_kallsym(iter->pos - kallsyms_num_syms, &iter->value,
    &iter->type, iter->name, iter->module_name,
    &iter->exported) < 0)
  return 0;
 return 1;
}


static unsigned long get_ksymbol_core(struct kallsym_iter *iter)
{
 unsigned off = iter->nameoff;

 iter->module_name[0] = '\0';
 iter->value = kallsyms_sym_address(iter->pos);

 iter->type = kallsyms_get_symbol_type(off);

 off = kallsyms_expand_symbol(off, iter->name, ARRAY_SIZE(iter->name));

 return off - iter->nameoff;
}

static void reset_iter(struct kallsym_iter *iter, loff_t new_pos)
{
 iter->name[0] = '\0';
 iter->nameoff = get_symbol_offset(new_pos);
 iter->pos = new_pos;
}


static int update_iter(struct kallsym_iter *iter, loff_t pos)
{

 if (pos >= kallsyms_num_syms) {
  iter->pos = pos;
  return get_ksymbol_mod(iter);
 }


 if (pos != iter->pos)
  reset_iter(iter, pos);

 iter->nameoff += get_ksymbol_core(iter);
 iter->pos++;

 return 1;
}

static void *s_next(struct seq_file *m, void *p, loff_t *pos)
{
 (*pos)++;

 if (!update_iter(m->private, *pos))
  return NULL;
 return p;
}

static void *s_start(struct seq_file *m, loff_t *pos)
{
 if (!update_iter(m->private, *pos))
  return NULL;
 return m->private;
}

static void s_stop(struct seq_file *m, void *p)
{
}

static int s_show(struct seq_file *m, void *p)
{
 struct kallsym_iter *iter = m->private;


 if (!iter->name[0])
  return 0;

 if (iter->module_name[0]) {
  char type;





  type = iter->exported ? toupper(iter->type) :
     tolower(iter->type);
  seq_printf(m, "%pK %c %s\t[%s]\n", (void *)iter->value,
      type, iter->name, iter->module_name);
 } else
  seq_printf(m, "%pK %c %s\n", (void *)iter->value,
      iter->type, iter->name);
 return 0;
}

static const struct seq_operations kallsyms_op = {
 .start = s_start,
 .next = s_next,
 .stop = s_stop,
 .show = s_show
};

static int kallsyms_open(struct inode *inode, struct file *file)
{





 struct kallsym_iter *iter;
 iter = __seq_open_private(file, &kallsyms_op, sizeof(*iter));
 if (!iter)
  return -ENOMEM;
 reset_iter(iter, 0);

 return 0;
}

const char *kdb_walk_kallsyms(loff_t *pos)
{
 static struct kallsym_iter kdb_walk_kallsyms_iter;
 if (*pos == 0) {
  memset(&kdb_walk_kallsyms_iter, 0,
         sizeof(kdb_walk_kallsyms_iter));
  reset_iter(&kdb_walk_kallsyms_iter, 0);
 }
 while (1) {
  if (!update_iter(&kdb_walk_kallsyms_iter, *pos))
   return NULL;
  ++*pos;

  if (kdb_walk_kallsyms_iter.name[0])
   return kdb_walk_kallsyms_iter.name;
 }
}

static const struct file_operations kallsyms_operations = {
 .open = kallsyms_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = seq_release_private,
};

static int __init kallsyms_init(void)
{
 proc_create("kallsyms", 0444, NULL, &kallsyms_operations);
 return 0;
}
device_initcall(kallsyms_init);

static unsigned long cookies[KCMP_TYPES][2] __read_mostly;

static long kptr_obfuscate(long v, int type)
{
 return (v ^ cookies[type][0]) * cookies[type][1];
}







static int kcmp_ptr(void *v1, void *v2, enum kcmp_type type)
{
 long t1, t2;

 t1 = kptr_obfuscate((long)v1, type);
 t2 = kptr_obfuscate((long)v2, type);

 return (t1 < t2) | ((t1 > t2) << 1);
}


static struct file *
get_file_raw_ptr(struct task_struct *task, unsigned int idx)
{
 struct file *file = NULL;

 task_lock(task);
 rcu_read_lock();

 if (task->files)
  file = fcheck_files(task->files, idx);

 rcu_read_unlock();
 task_unlock(task);

 return file;
}

static void kcmp_unlock(struct mutex *m1, struct mutex *m2)
{
 if (likely(m2 != m1))
  mutex_unlock(m2);
 mutex_unlock(m1);
}

static int kcmp_lock(struct mutex *m1, struct mutex *m2)
{
 int err;

 if (m2 > m1)
  swap(m1, m2);

 err = mutex_lock_killable(m1);
 if (!err && likely(m1 != m2)) {
  err = mutex_lock_killable_nested(m2, SINGLE_DEPTH_NESTING);
  if (err)
   mutex_unlock(m1);
 }

 return err;
}

SYSCALL_DEFINE5(kcmp, pid_t, pid1, pid_t, pid2, int, type,
  unsigned long, idx1, unsigned long, idx2)
{
 struct task_struct *task1, *task2;
 int ret;

 rcu_read_lock();




 task1 = find_task_by_vpid(pid1);
 task2 = find_task_by_vpid(pid2);
 if (!task1 || !task2)
  goto err_no_task;

 get_task_struct(task1);
 get_task_struct(task2);

 rcu_read_unlock();




 ret = kcmp_lock(&task1->signal->cred_guard_mutex,
   &task2->signal->cred_guard_mutex);
 if (ret)
  goto err;
 if (!ptrace_may_access(task1, PTRACE_MODE_READ_REALCREDS) ||
     !ptrace_may_access(task2, PTRACE_MODE_READ_REALCREDS)) {
  ret = -EPERM;
  goto err_unlock;
 }

 switch (type) {
 case KCMP_FILE: {
  struct file *filp1, *filp2;

  filp1 = get_file_raw_ptr(task1, idx1);
  filp2 = get_file_raw_ptr(task2, idx2);

  if (filp1 && filp2)
   ret = kcmp_ptr(filp1, filp2, KCMP_FILE);
  else
   ret = -EBADF;
  break;
 }
 case KCMP_VM:
  ret = kcmp_ptr(task1->mm, task2->mm, KCMP_VM);
  break;
 case KCMP_FILES:
  ret = kcmp_ptr(task1->files, task2->files, KCMP_FILES);
  break;
 case KCMP_FS:
  ret = kcmp_ptr(task1->fs, task2->fs, KCMP_FS);
  break;
 case KCMP_SIGHAND:
  ret = kcmp_ptr(task1->sighand, task2->sighand, KCMP_SIGHAND);
  break;
 case KCMP_IO:
  ret = kcmp_ptr(task1->io_context, task2->io_context, KCMP_IO);
  break;
 case KCMP_SYSVSEM:
  ret = kcmp_ptr(task1->sysvsem.undo_list,
          task2->sysvsem.undo_list,
          KCMP_SYSVSEM);
  ret = -EOPNOTSUPP;
  break;
 default:
  ret = -EINVAL;
  break;
 }

err_unlock:
 kcmp_unlock(&task1->signal->cred_guard_mutex,
      &task2->signal->cred_guard_mutex);
err:
 put_task_struct(task1);
 put_task_struct(task2);

 return ret;

err_no_task:
 rcu_read_unlock();
 return -ESRCH;
}

static __init int kcmp_cookies_init(void)
{
 int i;

 get_random_bytes(cookies, sizeof(cookies));

 for (i = 0; i < KCMP_TYPES; i++)
  cookies[i][1] |= (~(~0UL >> 1) | 1);

 return 0;
}
arch_initcall(kcmp_cookies_init);

struct kcov {





 atomic_t refcount;

 spinlock_t lock;
 enum kcov_mode mode;

 unsigned size;

 void *area;

 struct task_struct *t;
};





void notrace __sanitizer_cov_trace_pc(void)
{
 struct task_struct *t;
 enum kcov_mode mode;

 t = current;




 if (!t || in_interrupt())
  return;
 mode = READ_ONCE(t->kcov_mode);
 if (mode == KCOV_MODE_TRACE) {
  unsigned long *area;
  unsigned long pos;
  barrier();
  area = t->kcov_area;

  pos = READ_ONCE(area[0]) + 1;
  if (likely(pos < t->kcov_size)) {
   area[pos] = _RET_IP_;
   WRITE_ONCE(area[0], pos);
  }
 }
}
EXPORT_SYMBOL(__sanitizer_cov_trace_pc);

static void kcov_get(struct kcov *kcov)
{
 atomic_inc(&kcov->refcount);
}

static void kcov_put(struct kcov *kcov)
{
 if (atomic_dec_and_test(&kcov->refcount)) {
  vfree(kcov->area);
  kfree(kcov);
 }
}

void kcov_task_init(struct task_struct *t)
{
 t->kcov_mode = KCOV_MODE_DISABLED;
 t->kcov_size = 0;
 t->kcov_area = NULL;
 t->kcov = NULL;
}

void kcov_task_exit(struct task_struct *t)
{
 struct kcov *kcov;

 kcov = t->kcov;
 if (kcov == NULL)
  return;
 spin_lock(&kcov->lock);
 if (WARN_ON(kcov->t != t)) {
  spin_unlock(&kcov->lock);
  return;
 }

 kcov_task_init(t);
 kcov->t = NULL;
 spin_unlock(&kcov->lock);
 kcov_put(kcov);
}

static int kcov_mmap(struct file *filep, struct vm_area_struct *vma)
{
 int res = 0;
 void *area;
 struct kcov *kcov = vma->vm_file->private_data;
 unsigned long size, off;
 struct page *page;

 area = vmalloc_user(vma->vm_end - vma->vm_start);
 if (!area)
  return -ENOMEM;

 spin_lock(&kcov->lock);
 size = kcov->size * sizeof(unsigned long);
 if (kcov->mode == KCOV_MODE_DISABLED || vma->vm_pgoff != 0 ||
     vma->vm_end - vma->vm_start != size) {
  res = -EINVAL;
  goto exit;
 }
 if (!kcov->area) {
  kcov->area = area;
  vma->vm_flags |= VM_DONTEXPAND;
  spin_unlock(&kcov->lock);
  for (off = 0; off < size; off += PAGE_SIZE) {
   page = vmalloc_to_page(kcov->area + off);
   if (vm_insert_page(vma, vma->vm_start + off, page))
    WARN_ONCE(1, "vm_insert_page() failed");
  }
  return 0;
 }
exit:
 spin_unlock(&kcov->lock);
 vfree(area);
 return res;
}

static int kcov_open(struct inode *inode, struct file *filep)
{
 struct kcov *kcov;

 kcov = kzalloc(sizeof(*kcov), GFP_KERNEL);
 if (!kcov)
  return -ENOMEM;
 atomic_set(&kcov->refcount, 1);
 spin_lock_init(&kcov->lock);
 filep->private_data = kcov;
 return nonseekable_open(inode, filep);
}

static int kcov_close(struct inode *inode, struct file *filep)
{
 kcov_put(filep->private_data);
 return 0;
}

static int kcov_ioctl_locked(struct kcov *kcov, unsigned int cmd,
        unsigned long arg)
{
 struct task_struct *t;
 unsigned long size, unused;

 switch (cmd) {
 case KCOV_INIT_TRACE:




  if (kcov->mode != KCOV_MODE_DISABLED)
   return -EBUSY;





  size = arg;
  if (size < 2 || size > INT_MAX / sizeof(unsigned long))
   return -EINVAL;
  kcov->size = size;
  kcov->mode = KCOV_MODE_TRACE;
  return 0;
 case KCOV_ENABLE:







  unused = arg;
  if (unused != 0 || kcov->mode == KCOV_MODE_DISABLED ||
      kcov->area == NULL)
   return -EINVAL;
  if (kcov->t != NULL)
   return -EBUSY;
  t = current;

  t->kcov_size = kcov->size;
  t->kcov_area = kcov->area;

  barrier();
  WRITE_ONCE(t->kcov_mode, kcov->mode);
  t->kcov = kcov;
  kcov->t = t;

  kcov_get(kcov);
  return 0;
 case KCOV_DISABLE:

  unused = arg;
  if (unused != 0 || current->kcov != kcov)
   return -EINVAL;
  t = current;
  if (WARN_ON(kcov->t != t))
   return -EINVAL;
  kcov_task_init(t);
  kcov->t = NULL;
  kcov_put(kcov);
  return 0;
 default:
  return -ENOTTY;
 }
}

static long kcov_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)
{
 struct kcov *kcov;
 int res;

 kcov = filep->private_data;
 spin_lock(&kcov->lock);
 res = kcov_ioctl_locked(kcov, cmd, arg);
 spin_unlock(&kcov->lock);
 return res;
}

static const struct file_operations kcov_fops = {
 .open = kcov_open,
 .unlocked_ioctl = kcov_ioctl,
 .mmap = kcov_mmap,
 .release = kcov_close,
};

static int __init kcov_init(void)
{





 if (!debugfs_create_file_unsafe("kcov", 0600, NULL, NULL, &kcov_fops)) {
  pr_err("failed to create kcov in debugfs\n");
  return -ENOMEM;
 }
 return 0;
}

device_initcall(kcov_init);



static int copy_user_segment_list(struct kimage *image,
      unsigned long nr_segments,
      struct kexec_segment __user *segments)
{
 int ret;
 size_t segment_bytes;


 image->nr_segments = nr_segments;
 segment_bytes = nr_segments * sizeof(*segments);
 ret = copy_from_user(image->segment, segments, segment_bytes);
 if (ret)
  ret = -EFAULT;

 return ret;
}

static int kimage_alloc_init(struct kimage **rimage, unsigned long entry,
        unsigned long nr_segments,
        struct kexec_segment __user *segments,
        unsigned long flags)
{
 int ret;
 struct kimage *image;
 bool kexec_on_panic = flags & KEXEC_ON_CRASH;

 if (kexec_on_panic) {

  if ((entry < crashk_res.start) || (entry > crashk_res.end))
   return -EADDRNOTAVAIL;
 }


 image = do_kimage_alloc_init();
 if (!image)
  return -ENOMEM;

 image->start = entry;

 ret = copy_user_segment_list(image, nr_segments, segments);
 if (ret)
  goto out_free_image;

 if (kexec_on_panic) {

  image->control_page = crashk_res.start;
  image->type = KEXEC_TYPE_CRASH;
 }

 ret = sanity_check_segment_list(image);
 if (ret)
  goto out_free_image;






 ret = -ENOMEM;
 image->control_code_page = kimage_alloc_control_pages(image,
        get_order(KEXEC_CONTROL_PAGE_SIZE));
 if (!image->control_code_page) {
  pr_err("Could not allocate control_code_buffer\n");
  goto out_free_image;
 }

 if (!kexec_on_panic) {
  image->swap_page = kimage_alloc_control_pages(image, 0);
  if (!image->swap_page) {
   pr_err("Could not allocate swap buffer\n");
   goto out_free_control_pages;
  }
 }

 *rimage = image;
 return 0;
out_free_control_pages:
 kimage_free_page_list(&image->control_pages);
out_free_image:
 kfree(image);
 return ret;
}

static int do_kexec_load(unsigned long entry, unsigned long nr_segments,
  struct kexec_segment __user *segments, unsigned long flags)
{
 struct kimage **dest_image, *image;
 unsigned long i;
 int ret;

 if (flags & KEXEC_ON_CRASH) {
  dest_image = &kexec_crash_image;
  if (kexec_crash_image)
   arch_kexec_unprotect_crashkres();
 } else {
  dest_image = &kexec_image;
 }

 if (nr_segments == 0) {

  kimage_free(xchg(dest_image, NULL));
  return 0;
 }
 if (flags & KEXEC_ON_CRASH) {





  kimage_free(xchg(&kexec_crash_image, NULL));
 }

 ret = kimage_alloc_init(&image, entry, nr_segments, segments, flags);
 if (ret)
  return ret;

 if (flags & KEXEC_PRESERVE_CONTEXT)
  image->preserve_context = 1;

 ret = machine_kexec_prepare(image);
 if (ret)
  goto out;

 for (i = 0; i < nr_segments; i++) {
  ret = kimage_load_segment(image, &image->segment[i]);
  if (ret)
   goto out;
 }

 kimage_terminate(image);


 image = xchg(dest_image, image);

out:
 if ((flags & KEXEC_ON_CRASH) && kexec_crash_image)
  arch_kexec_protect_crashkres();

 kimage_free(image);
 return ret;
}
SYSCALL_DEFINE4(kexec_load, unsigned long, entry, unsigned long, nr_segments,
  struct kexec_segment __user *, segments, unsigned long, flags)
{
 int result;


 if (!capable(CAP_SYS_BOOT) || kexec_load_disabled)
  return -EPERM;





 if ((flags & KEXEC_FLAGS) != (flags & ~KEXEC_ARCH_MASK))
  return -EINVAL;


 if (((flags & KEXEC_ARCH_MASK) != KEXEC_ARCH) &&
  ((flags & KEXEC_ARCH_MASK) != KEXEC_ARCH_DEFAULT))
  return -EINVAL;




 if (nr_segments > KEXEC_SEGMENT_MAX)
  return -EINVAL;
 if (!mutex_trylock(&kexec_mutex))
  return -EBUSY;

 result = do_kexec_load(entry, nr_segments, segments, flags);

 mutex_unlock(&kexec_mutex);

 return result;
}

COMPAT_SYSCALL_DEFINE4(kexec_load, compat_ulong_t, entry,
         compat_ulong_t, nr_segments,
         struct compat_kexec_segment __user *, segments,
         compat_ulong_t, flags)
{
 struct compat_kexec_segment in;
 struct kexec_segment out, __user *ksegments;
 unsigned long i, result;




 if ((flags & KEXEC_ARCH_MASK) == KEXEC_ARCH_DEFAULT)
  return -EINVAL;

 if (nr_segments > KEXEC_SEGMENT_MAX)
  return -EINVAL;

 ksegments = compat_alloc_user_space(nr_segments * sizeof(out));
 for (i = 0; i < nr_segments; i++) {
  result = copy_from_user(&in, &segments[i], sizeof(in));
  if (result)
   return -EFAULT;

  out.buf = compat_ptr(in.buf);
  out.bufsz = in.bufsz;
  out.mem = in.mem;
  out.memsz = in.memsz;

  result = copy_to_user(&ksegments[i], &out, sizeof(out));
  if (result)
   return -EFAULT;
 }

 return sys_kexec_load(entry, nr_segments, ksegments, flags);
}




DEFINE_MUTEX(kexec_mutex);


note_buf_t __percpu *crash_notes;


static unsigned char vmcoreinfo_data[VMCOREINFO_BYTES];
u32 vmcoreinfo_note[VMCOREINFO_NOTE_SIZE/4];
size_t vmcoreinfo_size;
size_t vmcoreinfo_max_size = sizeof(vmcoreinfo_data);


bool kexec_in_progress = false;



struct resource crashk_res = {
 .name = "Crash kernel",
 .start = 0,
 .end = 0,
 .flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM,
 .desc = IORES_DESC_CRASH_KERNEL
};
struct resource crashk_low_res = {
 .name = "Crash kernel",
 .start = 0,
 .end = 0,
 .flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM,
 .desc = IORES_DESC_CRASH_KERNEL
};

int kexec_should_crash(struct task_struct *p)
{





 if (crash_kexec_post_notifiers)
  return 0;




 if (in_interrupt() || !p->pid || is_global_init(p) || panic_on_oops)
  return 1;
 return 0;
}

static struct page *kimage_alloc_page(struct kimage *image,
           gfp_t gfp_mask,
           unsigned long dest);

int sanity_check_segment_list(struct kimage *image)
{
 int result, i;
 unsigned long nr_segments = image->nr_segments;
 result = -EADDRNOTAVAIL;
 for (i = 0; i < nr_segments; i++) {
  unsigned long mstart, mend;

  mstart = image->segment[i].mem;
  mend = mstart + image->segment[i].memsz;
  if ((mstart & ~PAGE_MASK) || (mend & ~PAGE_MASK))
   return result;
  if (mend >= KEXEC_DESTINATION_MEMORY_LIMIT)
   return result;
 }






 result = -EINVAL;
 for (i = 0; i < nr_segments; i++) {
  unsigned long mstart, mend;
  unsigned long j;

  mstart = image->segment[i].mem;
  mend = mstart + image->segment[i].memsz;
  for (j = 0; j < i; j++) {
   unsigned long pstart, pend;

   pstart = image->segment[j].mem;
   pend = pstart + image->segment[j].memsz;

   if ((mend > pstart) && (mstart < pend))
    return result;
  }
 }






 result = -EINVAL;
 for (i = 0; i < nr_segments; i++) {
  if (image->segment[i].bufsz > image->segment[i].memsz)
   return result;
 }
 if (image->type == KEXEC_TYPE_CRASH) {
  result = -EADDRNOTAVAIL;
  for (i = 0; i < nr_segments; i++) {
   unsigned long mstart, mend;

   mstart = image->segment[i].mem;
   mend = mstart + image->segment[i].memsz - 1;

   if ((mstart < crashk_res.start) ||
       (mend > crashk_res.end))
    return result;
  }
 }

 return 0;
}

struct kimage *do_kimage_alloc_init(void)
{
 struct kimage *image;


 image = kzalloc(sizeof(*image), GFP_KERNEL);
 if (!image)
  return NULL;

 image->head = 0;
 image->entry = &image->head;
 image->last_entry = &image->head;
 image->control_page = ~0;
 image->type = KEXEC_TYPE_DEFAULT;


 INIT_LIST_HEAD(&image->control_pages);


 INIT_LIST_HEAD(&image->dest_pages);


 INIT_LIST_HEAD(&image->unusable_pages);

 return image;
}

int kimage_is_destination_range(struct kimage *image,
     unsigned long start,
     unsigned long end)
{
 unsigned long i;

 for (i = 0; i < image->nr_segments; i++) {
  unsigned long mstart, mend;

  mstart = image->segment[i].mem;
  mend = mstart + image->segment[i].memsz;
  if ((end > mstart) && (start < mend))
   return 1;
 }

 return 0;
}

static struct page *kimage_alloc_pages(gfp_t gfp_mask, unsigned int order)
{
 struct page *pages;

 pages = alloc_pages(gfp_mask, order);
 if (pages) {
  unsigned int count, i;

  pages->mapping = NULL;
  set_page_private(pages, order);
  count = 1 << order;
  for (i = 0; i < count; i++)
   SetPageReserved(pages + i);
 }

 return pages;
}

static void kimage_free_pages(struct page *page)
{
 unsigned int order, count, i;

 order = page_private(page);
 count = 1 << order;
 for (i = 0; i < count; i++)
  ClearPageReserved(page + i);
 __free_pages(page, order);
}

void kimage_free_page_list(struct list_head *list)
{
 struct page *page, *next;

 list_for_each_entry_safe(page, next, list, lru) {
  list_del(&page->lru);
  kimage_free_pages(page);
 }
}

static struct page *kimage_alloc_normal_control_pages(struct kimage *image,
       unsigned int order)
{
 struct list_head extra_pages;
 struct page *pages;
 unsigned int count;

 count = 1 << order;
 INIT_LIST_HEAD(&extra_pages);




 do {
  unsigned long pfn, epfn, addr, eaddr;

  pages = kimage_alloc_pages(KEXEC_CONTROL_MEMORY_GFP, order);
  if (!pages)
   break;
  pfn = page_to_pfn(pages);
  epfn = pfn + count;
  addr = pfn << PAGE_SHIFT;
  eaddr = epfn << PAGE_SHIFT;
  if ((epfn >= (KEXEC_CONTROL_MEMORY_LIMIT >> PAGE_SHIFT)) ||
         kimage_is_destination_range(image, addr, eaddr)) {
   list_add(&pages->lru, &extra_pages);
   pages = NULL;
  }
 } while (!pages);

 if (pages) {

  list_add(&pages->lru, &image->control_pages);







 }







 kimage_free_page_list(&extra_pages);

 return pages;
}

static struct page *kimage_alloc_crash_control_pages(struct kimage *image,
            unsigned int order)
{
 unsigned long hole_start, hole_end, size;
 struct page *pages;

 pages = NULL;
 size = (1 << order) << PAGE_SHIFT;
 hole_start = (image->control_page + (size - 1)) & ~(size - 1);
 hole_end = hole_start + size - 1;
 while (hole_end <= crashk_res.end) {
  unsigned long i;

  if (hole_end > KEXEC_CRASH_CONTROL_MEMORY_LIMIT)
   break;

  for (i = 0; i < image->nr_segments; i++) {
   unsigned long mstart, mend;

   mstart = image->segment[i].mem;
   mend = mstart + image->segment[i].memsz - 1;
   if ((hole_end >= mstart) && (hole_start <= mend)) {

    hole_start = (mend + (size - 1)) & ~(size - 1);
    hole_end = hole_start + size - 1;
    break;
   }
  }

  if (i == image->nr_segments) {
   pages = pfn_to_page(hole_start >> PAGE_SHIFT);
   image->control_page = hole_end;
   break;
  }
 }

 return pages;
}


struct page *kimage_alloc_control_pages(struct kimage *image,
      unsigned int order)
{
 struct page *pages = NULL;

 switch (image->type) {
 case KEXEC_TYPE_DEFAULT:
  pages = kimage_alloc_normal_control_pages(image, order);
  break;
 case KEXEC_TYPE_CRASH:
  pages = kimage_alloc_crash_control_pages(image, order);
  break;
 }

 return pages;
}

static int kimage_add_entry(struct kimage *image, kimage_entry_t entry)
{
 if (*image->entry != 0)
  image->entry++;

 if (image->entry == image->last_entry) {
  kimage_entry_t *ind_page;
  struct page *page;

  page = kimage_alloc_page(image, GFP_KERNEL, KIMAGE_NO_DEST);
  if (!page)
   return -ENOMEM;

  ind_page = page_address(page);
  *image->entry = virt_to_phys(ind_page) | IND_INDIRECTION;
  image->entry = ind_page;
  image->last_entry = ind_page +
          ((PAGE_SIZE/sizeof(kimage_entry_t)) - 1);
 }
 *image->entry = entry;
 image->entry++;
 *image->entry = 0;

 return 0;
}

static int kimage_set_destination(struct kimage *image,
       unsigned long destination)
{
 int result;

 destination &= PAGE_MASK;
 result = kimage_add_entry(image, destination | IND_DESTINATION);

 return result;
}


static int kimage_add_page(struct kimage *image, unsigned long page)
{
 int result;

 page &= PAGE_MASK;
 result = kimage_add_entry(image, page | IND_SOURCE);

 return result;
}


static void kimage_free_extra_pages(struct kimage *image)
{

 kimage_free_page_list(&image->dest_pages);


 kimage_free_page_list(&image->unusable_pages);

}
void kimage_terminate(struct kimage *image)
{
 if (*image->entry != 0)
  image->entry++;

 *image->entry = IND_DONE;
}

 for (ptr = &image->head; (entry = *ptr) && !(entry & IND_DONE); \
  ptr = (entry & IND_INDIRECTION) ? \
   phys_to_virt((entry & PAGE_MASK)) : ptr + 1)

static void kimage_free_entry(kimage_entry_t entry)
{
 struct page *page;

 page = pfn_to_page(entry >> PAGE_SHIFT);
 kimage_free_pages(page);
}

void kimage_free(struct kimage *image)
{
 kimage_entry_t *ptr, entry;
 kimage_entry_t ind = 0;

 if (!image)
  return;

 kimage_free_extra_pages(image);
 for_each_kimage_entry(image, ptr, entry) {
  if (entry & IND_INDIRECTION) {

   if (ind & IND_INDIRECTION)
    kimage_free_entry(ind);



   ind = entry;
  } else if (entry & IND_SOURCE)
   kimage_free_entry(entry);
 }

 if (ind & IND_INDIRECTION)
  kimage_free_entry(ind);


 machine_kexec_cleanup(image);


 kimage_free_page_list(&image->control_pages);





 if (image->file_mode)
  kimage_file_post_load_cleanup(image);

 kfree(image);
}

static kimage_entry_t *kimage_dst_used(struct kimage *image,
     unsigned long page)
{
 kimage_entry_t *ptr, entry;
 unsigned long destination = 0;

 for_each_kimage_entry(image, ptr, entry) {
  if (entry & IND_DESTINATION)
   destination = entry & PAGE_MASK;
  else if (entry & IND_SOURCE) {
   if (page == destination)
    return ptr;
   destination += PAGE_SIZE;
  }
 }

 return NULL;
}

static struct page *kimage_alloc_page(struct kimage *image,
     gfp_t gfp_mask,
     unsigned long destination)
{
 struct page *page;
 unsigned long addr;





 list_for_each_entry(page, &image->dest_pages, lru) {
  addr = page_to_pfn(page) << PAGE_SHIFT;
  if (addr == destination) {
   list_del(&page->lru);
   return page;
  }
 }
 page = NULL;
 while (1) {
  kimage_entry_t *old;


  page = kimage_alloc_pages(gfp_mask, 0);
  if (!page)
   return NULL;

  if (page_to_pfn(page) >
    (KEXEC_SOURCE_MEMORY_LIMIT >> PAGE_SHIFT)) {
   list_add(&page->lru, &image->unusable_pages);
   continue;
  }
  addr = page_to_pfn(page) << PAGE_SHIFT;


  if (addr == destination)
   break;


  if (!kimage_is_destination_range(image, addr,
        addr + PAGE_SIZE))
   break;






  old = kimage_dst_used(image, addr);
  if (old) {

   unsigned long old_addr;
   struct page *old_page;

   old_addr = *old & PAGE_MASK;
   old_page = pfn_to_page(old_addr >> PAGE_SHIFT);
   copy_highpage(page, old_page);
   *old = addr | (*old & ~PAGE_MASK);





   if (!(gfp_mask & __GFP_HIGHMEM) &&
       PageHighMem(old_page)) {
    kimage_free_pages(old_page);
    continue;
   }
   addr = old_addr;
   page = old_page;
   break;
  }

  list_add(&page->lru, &image->dest_pages);
 }

 return page;
}

static int kimage_load_normal_segment(struct kimage *image,
      struct kexec_segment *segment)
{
 unsigned long maddr;
 size_t ubytes, mbytes;
 int result;
 unsigned char __user *buf = NULL;
 unsigned char *kbuf = NULL;

 result = 0;
 if (image->file_mode)
  kbuf = segment->kbuf;
 else
  buf = segment->buf;
 ubytes = segment->bufsz;
 mbytes = segment->memsz;
 maddr = segment->mem;

 result = kimage_set_destination(image, maddr);
 if (result < 0)
  goto out;

 while (mbytes) {
  struct page *page;
  char *ptr;
  size_t uchunk, mchunk;

  page = kimage_alloc_page(image, GFP_HIGHUSER, maddr);
  if (!page) {
   result = -ENOMEM;
   goto out;
  }
  result = kimage_add_page(image, page_to_pfn(page)
        << PAGE_SHIFT);
  if (result < 0)
   goto out;

  ptr = kmap(page);

  clear_page(ptr);
  ptr += maddr & ~PAGE_MASK;
  mchunk = min_t(size_t, mbytes,
    PAGE_SIZE - (maddr & ~PAGE_MASK));
  uchunk = min(ubytes, mchunk);


  if (image->file_mode)
   memcpy(ptr, kbuf, uchunk);
  else
   result = copy_from_user(ptr, buf, uchunk);
  kunmap(page);
  if (result) {
   result = -EFAULT;
   goto out;
  }
  ubytes -= uchunk;
  maddr += mchunk;
  if (image->file_mode)
   kbuf += mchunk;
  else
   buf += mchunk;
  mbytes -= mchunk;
 }
out:
 return result;
}

static int kimage_load_crash_segment(struct kimage *image,
     struct kexec_segment *segment)
{




 unsigned long maddr;
 size_t ubytes, mbytes;
 int result;
 unsigned char __user *buf = NULL;
 unsigned char *kbuf = NULL;

 result = 0;
 if (image->file_mode)
  kbuf = segment->kbuf;
 else
  buf = segment->buf;
 ubytes = segment->bufsz;
 mbytes = segment->memsz;
 maddr = segment->mem;
 while (mbytes) {
  struct page *page;
  char *ptr;
  size_t uchunk, mchunk;

  page = pfn_to_page(maddr >> PAGE_SHIFT);
  if (!page) {
   result = -ENOMEM;
   goto out;
  }
  ptr = kmap(page);
  ptr += maddr & ~PAGE_MASK;
  mchunk = min_t(size_t, mbytes,
    PAGE_SIZE - (maddr & ~PAGE_MASK));
  uchunk = min(ubytes, mchunk);
  if (mchunk > uchunk) {

   memset(ptr + uchunk, 0, mchunk - uchunk);
  }


  if (image->file_mode)
   memcpy(ptr, kbuf, uchunk);
  else
   result = copy_from_user(ptr, buf, uchunk);
  kexec_flush_icache_page(page);
  kunmap(page);
  if (result) {
   result = -EFAULT;
   goto out;
  }
  ubytes -= uchunk;
  maddr += mchunk;
  if (image->file_mode)
   kbuf += mchunk;
  else
   buf += mchunk;
  mbytes -= mchunk;
 }
out:
 return result;
}

int kimage_load_segment(struct kimage *image,
    struct kexec_segment *segment)
{
 int result = -ENOMEM;

 switch (image->type) {
 case KEXEC_TYPE_DEFAULT:
  result = kimage_load_normal_segment(image, segment);
  break;
 case KEXEC_TYPE_CRASH:
  result = kimage_load_crash_segment(image, segment);
  break;
 }

 return result;
}

struct kimage *kexec_image;
struct kimage *kexec_crash_image;
int kexec_load_disabled;






void __crash_kexec(struct pt_regs *regs)
{
 if (mutex_trylock(&kexec_mutex)) {
  if (kexec_crash_image) {
   struct pt_regs fixed_regs;

   crash_setup_regs(&fixed_regs, regs);
   crash_save_vmcoreinfo();
   machine_crash_shutdown(&fixed_regs);
   machine_kexec(kexec_crash_image);
  }
  mutex_unlock(&kexec_mutex);
 }
}

void crash_kexec(struct pt_regs *regs)
{
 int old_cpu, this_cpu;






 this_cpu = raw_smp_processor_id();
 old_cpu = atomic_cmpxchg(&panic_cpu, PANIC_CPU_INVALID, this_cpu);
 if (old_cpu == PANIC_CPU_INVALID) {

  printk_nmi_flush_on_panic();
  __crash_kexec(regs);





  atomic_set(&panic_cpu, PANIC_CPU_INVALID);
 }
}

size_t crash_get_memory_size(void)
{
 size_t size = 0;

 mutex_lock(&kexec_mutex);
 if (crashk_res.end != crashk_res.start)
  size = resource_size(&crashk_res);
 mutex_unlock(&kexec_mutex);
 return size;
}

void __weak crash_free_reserved_phys_range(unsigned long begin,
        unsigned long end)
{
 unsigned long addr;

 for (addr = begin; addr < end; addr += PAGE_SIZE)
  free_reserved_page(pfn_to_page(addr >> PAGE_SHIFT));
}

int crash_shrink_memory(unsigned long new_size)
{
 int ret = 0;
 unsigned long start, end;
 unsigned long old_size;
 struct resource *ram_res;

 mutex_lock(&kexec_mutex);

 if (kexec_crash_image) {
  ret = -ENOENT;
  goto unlock;
 }
 start = crashk_res.start;
 end = crashk_res.end;
 old_size = (end == 0) ? 0 : end - start + 1;
 if (new_size >= old_size) {
  ret = (new_size == old_size) ? 0 : -EINVAL;
  goto unlock;
 }

 ram_res = kzalloc(sizeof(*ram_res), GFP_KERNEL);
 if (!ram_res) {
  ret = -ENOMEM;
  goto unlock;
 }

 start = roundup(start, KEXEC_CRASH_MEM_ALIGN);
 end = roundup(start + new_size, KEXEC_CRASH_MEM_ALIGN);

 crash_free_reserved_phys_range(end, crashk_res.end);

 if ((start == end) && (crashk_res.parent != NULL))
  release_resource(&crashk_res);

 ram_res->start = end;
 ram_res->end = crashk_res.end;
 ram_res->flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM;
 ram_res->name = "System RAM";

 crashk_res.end = end - 1;

 insert_resource(&iomem_resource, ram_res);

unlock:
 mutex_unlock(&kexec_mutex);
 return ret;
}

static u32 *append_elf_note(u32 *buf, char *name, unsigned type, void *data,
       size_t data_len)
{
 struct elf_note note;

 note.n_namesz = strlen(name) + 1;
 note.n_descsz = data_len;
 note.n_type = type;
 memcpy(buf, &note, sizeof(note));
 buf += (sizeof(note) + 3)/4;
 memcpy(buf, name, note.n_namesz);
 buf += (note.n_namesz + 3)/4;
 memcpy(buf, data, note.n_descsz);
 buf += (note.n_descsz + 3)/4;

 return buf;
}

static void final_note(u32 *buf)
{
 struct elf_note note;

 note.n_namesz = 0;
 note.n_descsz = 0;
 note.n_type = 0;
 memcpy(buf, &note, sizeof(note));
}

void crash_save_cpu(struct pt_regs *regs, int cpu)
{
 struct elf_prstatus prstatus;
 u32 *buf;

 if ((cpu < 0) || (cpu >= nr_cpu_ids))
  return;
 buf = (u32 *)per_cpu_ptr(crash_notes, cpu);
 if (!buf)
  return;
 memset(&prstatus, 0, sizeof(prstatus));
 prstatus.pr_pid = current->pid;
 elf_core_copy_kernel_regs(&prstatus.pr_reg, regs);
 buf = append_elf_note(buf, KEXEC_CORE_NOTE_NAME, NT_PRSTATUS,
         &prstatus, sizeof(prstatus));
 final_note(buf);
}

static int __init crash_notes_memory_init(void)
{

 size_t size, align;
 size = sizeof(note_buf_t);
 align = min(roundup_pow_of_two(sizeof(note_buf_t)), PAGE_SIZE);





 BUILD_BUG_ON(size > PAGE_SIZE);

 crash_notes = __alloc_percpu(size, align);
 if (!crash_notes) {
  pr_warn("Memory allocation for saving cpu register states failed\n");
  return -ENOMEM;
 }
 return 0;
}
subsys_initcall(crash_notes_memory_init);
static int __init parse_crashkernel_mem(char *cmdline,
     unsigned long long system_ram,
     unsigned long long *crash_size,
     unsigned long long *crash_base)
{
 char *cur = cmdline, *tmp;


 do {
  unsigned long long start, end = ULLONG_MAX, size;


  start = memparse(cur, &tmp);
  if (cur == tmp) {
   pr_warn("crashkernel: Memory value expected\n");
   return -EINVAL;
  }
  cur = tmp;
  if (*cur != '-') {
   pr_warn("crashkernel: '-' expected\n");
   return -EINVAL;
  }
  cur++;


  if (*cur != ':') {
   end = memparse(cur, &tmp);
   if (cur == tmp) {
    pr_warn("crashkernel: Memory value expected\n");
    return -EINVAL;
   }
   cur = tmp;
   if (end <= start) {
    pr_warn("crashkernel: end <= start\n");
    return -EINVAL;
   }
  }

  if (*cur != ':') {
   pr_warn("crashkernel: ':' expected\n");
   return -EINVAL;
  }
  cur++;

  size = memparse(cur, &tmp);
  if (cur == tmp) {
   pr_warn("Memory value expected\n");
   return -EINVAL;
  }
  cur = tmp;
  if (size >= system_ram) {
   pr_warn("crashkernel: invalid size\n");
   return -EINVAL;
  }


  if (system_ram >= start && system_ram < end) {
   *crash_size = size;
   break;
  }
 } while (*cur++ == ',');

 if (*crash_size > 0) {
  while (*cur && *cur != ' ' && *cur != '@')
   cur++;
  if (*cur == '@') {
   cur++;
   *crash_base = memparse(cur, &tmp);
   if (cur == tmp) {
    pr_warn("Memory value expected after '@'\n");
    return -EINVAL;
   }
  }
 }

 return 0;
}
static int __init parse_crashkernel_simple(char *cmdline,
        unsigned long long *crash_size,
        unsigned long long *crash_base)
{
 char *cur = cmdline;

 *crash_size = memparse(cmdline, &cur);
 if (cmdline == cur) {
  pr_warn("crashkernel: memory value expected\n");
  return -EINVAL;
 }

 if (*cur == '@')
  *crash_base = memparse(cur+1, &cur);
 else if (*cur != ' ' && *cur != '\0') {
  pr_warn("crashkernel: unrecognized char: %c\n", *cur);
  return -EINVAL;
 }

 return 0;
}

static __initdata char *suffix_tbl[] = {
 [SUFFIX_HIGH] = ",high",
 [SUFFIX_LOW] = ",low",
 [SUFFIX_NULL] = NULL,
};
static int __init parse_crashkernel_suffix(char *cmdline,
        unsigned long long *crash_size,
        const char *suffix)
{
 char *cur = cmdline;

 *crash_size = memparse(cmdline, &cur);
 if (cmdline == cur) {
  pr_warn("crashkernel: memory value expected\n");
  return -EINVAL;
 }


 if (strncmp(cur, suffix, strlen(suffix))) {
  pr_warn("crashkernel: unrecognized char: %c\n", *cur);
  return -EINVAL;
 }
 cur += strlen(suffix);
 if (*cur != ' ' && *cur != '\0') {
  pr_warn("crashkernel: unrecognized char: %c\n", *cur);
  return -EINVAL;
 }

 return 0;
}

static __init char *get_last_crashkernel(char *cmdline,
        const char *name,
        const char *suffix)
{
 char *p = cmdline, *ck_cmdline = NULL;


 p = strstr(p, name);
 while (p) {
  char *end_p = strchr(p, ' ');
  char *q;

  if (!end_p)
   end_p = p + strlen(p);

  if (!suffix) {
   int i;


   for (i = 0; suffix_tbl[i]; i++) {
    q = end_p - strlen(suffix_tbl[i]);
    if (!strncmp(q, suffix_tbl[i],
          strlen(suffix_tbl[i])))
     goto next;
   }
   ck_cmdline = p;
  } else {
   q = end_p - strlen(suffix);
   if (!strncmp(q, suffix, strlen(suffix)))
    ck_cmdline = p;
  }
next:
  p = strstr(p+1, name);
 }

 if (!ck_cmdline)
  return NULL;

 return ck_cmdline;
}

static int __init __parse_crashkernel(char *cmdline,
        unsigned long long system_ram,
        unsigned long long *crash_size,
        unsigned long long *crash_base,
        const char *name,
        const char *suffix)
{
 char *first_colon, *first_space;
 char *ck_cmdline;

 BUG_ON(!crash_size || !crash_base);
 *crash_size = 0;
 *crash_base = 0;

 ck_cmdline = get_last_crashkernel(cmdline, name, suffix);

 if (!ck_cmdline)
  return -EINVAL;

 ck_cmdline += strlen(name);

 if (suffix)
  return parse_crashkernel_suffix(ck_cmdline, crash_size,
    suffix);




 first_colon = strchr(ck_cmdline, ':');
 first_space = strchr(ck_cmdline, ' ');
 if (first_colon && (!first_space || first_colon < first_space))
  return parse_crashkernel_mem(ck_cmdline, system_ram,
    crash_size, crash_base);

 return parse_crashkernel_simple(ck_cmdline, crash_size, crash_base);
}





int __init parse_crashkernel(char *cmdline,
        unsigned long long system_ram,
        unsigned long long *crash_size,
        unsigned long long *crash_base)
{
 return __parse_crashkernel(cmdline, system_ram, crash_size, crash_base,
     "crashkernel=", NULL);
}

int __init parse_crashkernel_high(char *cmdline,
        unsigned long long system_ram,
        unsigned long long *crash_size,
        unsigned long long *crash_base)
{
 return __parse_crashkernel(cmdline, system_ram, crash_size, crash_base,
    "crashkernel=", suffix_tbl[SUFFIX_HIGH]);
}

int __init parse_crashkernel_low(char *cmdline,
        unsigned long long system_ram,
        unsigned long long *crash_size,
        unsigned long long *crash_base)
{
 return __parse_crashkernel(cmdline, system_ram, crash_size, crash_base,
    "crashkernel=", suffix_tbl[SUFFIX_LOW]);
}

static void update_vmcoreinfo_note(void)
{
 u32 *buf = vmcoreinfo_note;

 if (!vmcoreinfo_size)
  return;
 buf = append_elf_note(buf, VMCOREINFO_NOTE_NAME, 0, vmcoreinfo_data,
         vmcoreinfo_size);
 final_note(buf);
}

void crash_save_vmcoreinfo(void)
{
 vmcoreinfo_append_str("CRASHTIME=%ld\n", get_seconds());
 update_vmcoreinfo_note();
}

void vmcoreinfo_append_str(const char *fmt, ...)
{
 va_list args;
 char buf[0x50];
 size_t r;

 va_start(args, fmt);
 r = vscnprintf(buf, sizeof(buf), fmt, args);
 va_end(args);

 r = min(r, vmcoreinfo_max_size - vmcoreinfo_size);

 memcpy(&vmcoreinfo_data[vmcoreinfo_size], buf, r);

 vmcoreinfo_size += r;
}





void __weak arch_crash_save_vmcoreinfo(void)
{}

unsigned long __weak paddr_vmcoreinfo_note(void)
{
 return __pa((unsigned long)(char *)&vmcoreinfo_note);
}

static int __init crash_save_vmcoreinfo_init(void)
{
 VMCOREINFO_OSRELEASE(init_uts_ns.name.release);
 VMCOREINFO_PAGESIZE(PAGE_SIZE);

 VMCOREINFO_SYMBOL(init_uts_ns);
 VMCOREINFO_SYMBOL(node_online_map);
 VMCOREINFO_SYMBOL(swapper_pg_dir);
 VMCOREINFO_SYMBOL(_stext);
 VMCOREINFO_SYMBOL(vmap_area_list);

 VMCOREINFO_SYMBOL(mem_map);
 VMCOREINFO_SYMBOL(contig_page_data);
 VMCOREINFO_SYMBOL(mem_section);
 VMCOREINFO_LENGTH(mem_section, NR_SECTION_ROOTS);
 VMCOREINFO_STRUCT_SIZE(mem_section);
 VMCOREINFO_OFFSET(mem_section, section_mem_map);
 VMCOREINFO_STRUCT_SIZE(page);
 VMCOREINFO_STRUCT_SIZE(pglist_data);
 VMCOREINFO_STRUCT_SIZE(zone);
 VMCOREINFO_STRUCT_SIZE(free_area);
 VMCOREINFO_STRUCT_SIZE(list_head);
 VMCOREINFO_SIZE(nodemask_t);
 VMCOREINFO_OFFSET(page, flags);
 VMCOREINFO_OFFSET(page, _refcount);
 VMCOREINFO_OFFSET(page, mapping);
 VMCOREINFO_OFFSET(page, lru);
 VMCOREINFO_OFFSET(page, _mapcount);
 VMCOREINFO_OFFSET(page, private);
 VMCOREINFO_OFFSET(page, compound_dtor);
 VMCOREINFO_OFFSET(page, compound_order);
 VMCOREINFO_OFFSET(page, compound_head);
 VMCOREINFO_OFFSET(pglist_data, node_zones);
 VMCOREINFO_OFFSET(pglist_data, nr_zones);
 VMCOREINFO_OFFSET(pglist_data, node_mem_map);
 VMCOREINFO_OFFSET(pglist_data, node_start_pfn);
 VMCOREINFO_OFFSET(pglist_data, node_spanned_pages);
 VMCOREINFO_OFFSET(pglist_data, node_id);
 VMCOREINFO_OFFSET(zone, free_area);
 VMCOREINFO_OFFSET(zone, vm_stat);
 VMCOREINFO_OFFSET(zone, spanned_pages);
 VMCOREINFO_OFFSET(free_area, free_list);
 VMCOREINFO_OFFSET(list_head, next);
 VMCOREINFO_OFFSET(list_head, prev);
 VMCOREINFO_OFFSET(vmap_area, va_start);
 VMCOREINFO_OFFSET(vmap_area, list);
 VMCOREINFO_LENGTH(zone.free_area, MAX_ORDER);
 log_buf_kexec_setup();
 VMCOREINFO_LENGTH(free_area.free_list, MIGRATE_TYPES);
 VMCOREINFO_NUMBER(NR_FREE_PAGES);
 VMCOREINFO_NUMBER(PG_lru);
 VMCOREINFO_NUMBER(PG_private);
 VMCOREINFO_NUMBER(PG_swapcache);
 VMCOREINFO_NUMBER(PG_slab);
 VMCOREINFO_NUMBER(PG_hwpoison);
 VMCOREINFO_NUMBER(PG_head_mask);
 VMCOREINFO_NUMBER(PAGE_BUDDY_MAPCOUNT_VALUE);
 VMCOREINFO_NUMBER(KERNEL_IMAGE_SIZE);
 VMCOREINFO_NUMBER(HUGETLB_PAGE_DTOR);

 arch_crash_save_vmcoreinfo();
 update_vmcoreinfo_note();

 return 0;
}

subsys_initcall(crash_save_vmcoreinfo_init);





int kernel_kexec(void)
{
 int error = 0;

 if (!mutex_trylock(&kexec_mutex))
  return -EBUSY;
 if (!kexec_image) {
  error = -EINVAL;
  goto Unlock;
 }

 if (kexec_image->preserve_context) {
  lock_system_sleep();
  pm_prepare_console();
  error = freeze_processes();
  if (error) {
   error = -EBUSY;
   goto Restore_console;
  }
  suspend_console();
  error = dpm_suspend_start(PMSG_FREEZE);
  if (error)
   goto Resume_console;







  error = dpm_suspend_end(PMSG_FREEZE);
  if (error)
   goto Resume_devices;
  error = disable_nonboot_cpus();
  if (error)
   goto Enable_cpus;
  local_irq_disable();
  error = syscore_suspend();
  if (error)
   goto Enable_irqs;
 } else
 {
  kexec_in_progress = true;
  kernel_restart_prepare(NULL);
  migrate_to_reboot_cpu();







  cpu_hotplug_enable();
  pr_emerg("Starting new kernel\n");
  machine_shutdown();
 }

 machine_kexec(kexec_image);

 if (kexec_image->preserve_context) {
  syscore_resume();
 Enable_irqs:
  local_irq_enable();
 Enable_cpus:
  enable_nonboot_cpus();
  dpm_resume_start(PMSG_RESTORE);
 Resume_devices:
  dpm_resume_end(PMSG_RESTORE);
 Resume_console:
  resume_console();
  thaw_processes();
 Restore_console:
  pm_restore_console();
  unlock_system_sleep();
 }

 Unlock:
 mutex_unlock(&kexec_mutex);
 return error;
}
void __weak arch_kexec_protect_crashkres(void)
{}

void __weak arch_kexec_unprotect_crashkres(void)
{}






char __weak kexec_purgatory[0];
size_t __weak kexec_purgatory_size = 0;

static int kexec_calculate_store_digests(struct kimage *image);


int __weak arch_kexec_kernel_image_probe(struct kimage *image, void *buf,
      unsigned long buf_len)
{
 return -ENOEXEC;
}

void * __weak arch_kexec_kernel_image_load(struct kimage *image)
{
 return ERR_PTR(-ENOEXEC);
}

int __weak arch_kimage_file_post_load_cleanup(struct kimage *image)
{
 return -EINVAL;
}

int __weak arch_kexec_kernel_verify_sig(struct kimage *image, void *buf,
     unsigned long buf_len)
{
 return -EKEYREJECTED;
}


int __weak
arch_kexec_apply_relocations_add(const Elf_Ehdr *ehdr, Elf_Shdr *sechdrs,
     unsigned int relsec)
{
 pr_err("RELA relocation unsupported.\n");
 return -ENOEXEC;
}


int __weak
arch_kexec_apply_relocations(const Elf_Ehdr *ehdr, Elf_Shdr *sechdrs,
        unsigned int relsec)
{
 pr_err("REL relocation unsupported.\n");
 return -ENOEXEC;
}






void kimage_file_post_load_cleanup(struct kimage *image)
{
 struct purgatory_info *pi = &image->purgatory_info;

 vfree(image->kernel_buf);
 image->kernel_buf = NULL;

 vfree(image->initrd_buf);
 image->initrd_buf = NULL;

 kfree(image->cmdline_buf);
 image->cmdline_buf = NULL;

 vfree(pi->purgatory_buf);
 pi->purgatory_buf = NULL;

 vfree(pi->sechdrs);
 pi->sechdrs = NULL;


 arch_kimage_file_post_load_cleanup(image);






 kfree(image->image_loader_data);
 image->image_loader_data = NULL;
}





static int
kimage_file_prepare_segments(struct kimage *image, int kernel_fd, int initrd_fd,
        const char __user *cmdline_ptr,
        unsigned long cmdline_len, unsigned flags)
{
 int ret = 0;
 void *ldata;
 loff_t size;

 ret = kernel_read_file_from_fd(kernel_fd, &image->kernel_buf,
           &size, INT_MAX, READING_KEXEC_IMAGE);
 if (ret)
  return ret;
 image->kernel_buf_len = size;


 ret = arch_kexec_kernel_image_probe(image, image->kernel_buf,
         image->kernel_buf_len);
 if (ret)
  goto out;

 ret = arch_kexec_kernel_verify_sig(image, image->kernel_buf,
        image->kernel_buf_len);
 if (ret) {
  pr_debug("kernel signature verification failed.\n");
  goto out;
 }
 pr_debug("kernel signature verification successful.\n");

 if (!(flags & KEXEC_FILE_NO_INITRAMFS)) {
  ret = kernel_read_file_from_fd(initrd_fd, &image->initrd_buf,
            &size, INT_MAX,
            READING_KEXEC_INITRAMFS);
  if (ret)
   goto out;
  image->initrd_buf_len = size;
 }

 if (cmdline_len) {
  image->cmdline_buf = kzalloc(cmdline_len, GFP_KERNEL);
  if (!image->cmdline_buf) {
   ret = -ENOMEM;
   goto out;
  }

  ret = copy_from_user(image->cmdline_buf, cmdline_ptr,
         cmdline_len);
  if (ret) {
   ret = -EFAULT;
   goto out;
  }

  image->cmdline_buf_len = cmdline_len;


  if (image->cmdline_buf[cmdline_len - 1] != '\0') {
   ret = -EINVAL;
   goto out;
  }
 }


 ldata = arch_kexec_kernel_image_load(image);

 if (IS_ERR(ldata)) {
  ret = PTR_ERR(ldata);
  goto out;
 }

 image->image_loader_data = ldata;
out:

 if (ret)
  kimage_file_post_load_cleanup(image);
 return ret;
}

static int
kimage_file_alloc_init(struct kimage **rimage, int kernel_fd,
         int initrd_fd, const char __user *cmdline_ptr,
         unsigned long cmdline_len, unsigned long flags)
{
 int ret;
 struct kimage *image;
 bool kexec_on_panic = flags & KEXEC_FILE_ON_CRASH;

 image = do_kimage_alloc_init();
 if (!image)
  return -ENOMEM;

 image->file_mode = 1;

 if (kexec_on_panic) {

  image->control_page = crashk_res.start;
  image->type = KEXEC_TYPE_CRASH;
 }

 ret = kimage_file_prepare_segments(image, kernel_fd, initrd_fd,
        cmdline_ptr, cmdline_len, flags);
 if (ret)
  goto out_free_image;

 ret = sanity_check_segment_list(image);
 if (ret)
  goto out_free_post_load_bufs;

 ret = -ENOMEM;
 image->control_code_page = kimage_alloc_control_pages(image,
        get_order(KEXEC_CONTROL_PAGE_SIZE));
 if (!image->control_code_page) {
  pr_err("Could not allocate control_code_buffer\n");
  goto out_free_post_load_bufs;
 }

 if (!kexec_on_panic) {
  image->swap_page = kimage_alloc_control_pages(image, 0);
  if (!image->swap_page) {
   pr_err("Could not allocate swap buffer\n");
   goto out_free_control_pages;
  }
 }

 *rimage = image;
 return 0;
out_free_control_pages:
 kimage_free_page_list(&image->control_pages);
out_free_post_load_bufs:
 kimage_file_post_load_cleanup(image);
out_free_image:
 kfree(image);
 return ret;
}

SYSCALL_DEFINE5(kexec_file_load, int, kernel_fd, int, initrd_fd,
  unsigned long, cmdline_len, const char __user *, cmdline_ptr,
  unsigned long, flags)
{
 int ret = 0, i;
 struct kimage **dest_image, *image;


 if (!capable(CAP_SYS_BOOT) || kexec_load_disabled)
  return -EPERM;


 if (flags != (flags & KEXEC_FILE_FLAGS))
  return -EINVAL;

 image = NULL;

 if (!mutex_trylock(&kexec_mutex))
  return -EBUSY;

 dest_image = &kexec_image;
 if (flags & KEXEC_FILE_ON_CRASH) {
  dest_image = &kexec_crash_image;
  if (kexec_crash_image)
   arch_kexec_unprotect_crashkres();
 }

 if (flags & KEXEC_FILE_UNLOAD)
  goto exchange;






 if (flags & KEXEC_FILE_ON_CRASH)
  kimage_free(xchg(&kexec_crash_image, NULL));

 ret = kimage_file_alloc_init(&image, kernel_fd, initrd_fd, cmdline_ptr,
         cmdline_len, flags);
 if (ret)
  goto out;

 ret = machine_kexec_prepare(image);
 if (ret)
  goto out;

 ret = kexec_calculate_store_digests(image);
 if (ret)
  goto out;

 for (i = 0; i < image->nr_segments; i++) {
  struct kexec_segment *ksegment;

  ksegment = &image->segment[i];
  pr_debug("Loading segment %d: buf=0x%p bufsz=0x%zx mem=0x%lx memsz=0x%zx\n",
    i, ksegment->buf, ksegment->bufsz, ksegment->mem,
    ksegment->memsz);

  ret = kimage_load_segment(image, &image->segment[i]);
  if (ret)
   goto out;
 }

 kimage_terminate(image);





 kimage_file_post_load_cleanup(image);
exchange:
 image = xchg(dest_image, image);
out:
 if ((flags & KEXEC_FILE_ON_CRASH) && kexec_crash_image)
  arch_kexec_protect_crashkres();

 mutex_unlock(&kexec_mutex);
 kimage_free(image);
 return ret;
}

static int locate_mem_hole_top_down(unsigned long start, unsigned long end,
        struct kexec_buf *kbuf)
{
 struct kimage *image = kbuf->image;
 unsigned long temp_start, temp_end;

 temp_end = min(end, kbuf->buf_max);
 temp_start = temp_end - kbuf->memsz;

 do {

  temp_start = temp_start & (~(kbuf->buf_align - 1));

  if (temp_start < start || temp_start < kbuf->buf_min)
   return 0;

  temp_end = temp_start + kbuf->memsz - 1;





  if (kimage_is_destination_range(image, temp_start, temp_end)) {
   temp_start = temp_start - PAGE_SIZE;
   continue;
  }


  break;
 } while (1);


 kbuf->mem = temp_start;


 return 1;
}

static int locate_mem_hole_bottom_up(unsigned long start, unsigned long end,
         struct kexec_buf *kbuf)
{
 struct kimage *image = kbuf->image;
 unsigned long temp_start, temp_end;

 temp_start = max(start, kbuf->buf_min);

 do {
  temp_start = ALIGN(temp_start, kbuf->buf_align);
  temp_end = temp_start + kbuf->memsz - 1;

  if (temp_end > end || temp_end > kbuf->buf_max)
   return 0;




  if (kimage_is_destination_range(image, temp_start, temp_end)) {
   temp_start = temp_start + PAGE_SIZE;
   continue;
  }


  break;
 } while (1);


 kbuf->mem = temp_start;


 return 1;
}

static int locate_mem_hole_callback(u64 start, u64 end, void *arg)
{
 struct kexec_buf *kbuf = (struct kexec_buf *)arg;
 unsigned long sz = end - start + 1;


 if (sz < kbuf->memsz)
  return 0;

 if (end < kbuf->buf_min || start > kbuf->buf_max)
  return 0;





 if (kbuf->top_down)
  return locate_mem_hole_top_down(start, end, kbuf);
 return locate_mem_hole_bottom_up(start, end, kbuf);
}





int kexec_add_buffer(struct kimage *image, char *buffer, unsigned long bufsz,
       unsigned long memsz, unsigned long buf_align,
       unsigned long buf_min, unsigned long buf_max,
       bool top_down, unsigned long *load_addr)
{

 struct kexec_segment *ksegment;
 struct kexec_buf buf, *kbuf;
 int ret;


 if (!image->file_mode)
  return -EINVAL;

 if (image->nr_segments >= KEXEC_SEGMENT_MAX)
  return -EINVAL;
 if (!list_empty(&image->control_pages)) {
  WARN_ON(1);
  return -EINVAL;
 }

 memset(&buf, 0, sizeof(struct kexec_buf));
 kbuf = &buf;
 kbuf->image = image;
 kbuf->buffer = buffer;
 kbuf->bufsz = bufsz;

 kbuf->memsz = ALIGN(memsz, PAGE_SIZE);
 kbuf->buf_align = max(buf_align, PAGE_SIZE);
 kbuf->buf_min = buf_min;
 kbuf->buf_max = buf_max;
 kbuf->top_down = top_down;


 if (image->type == KEXEC_TYPE_CRASH)
  ret = walk_iomem_res_desc(crashk_res.desc,
    IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY,
    crashk_res.start, crashk_res.end, kbuf,
    locate_mem_hole_callback);
 else
  ret = walk_system_ram_res(0, -1, kbuf,
       locate_mem_hole_callback);
 if (ret != 1) {

  return -EADDRNOTAVAIL;
 }


 ksegment = &image->segment[image->nr_segments];
 ksegment->kbuf = kbuf->buffer;
 ksegment->bufsz = kbuf->bufsz;
 ksegment->mem = kbuf->mem;
 ksegment->memsz = kbuf->memsz;
 image->nr_segments++;
 *load_addr = ksegment->mem;
 return 0;
}


static int kexec_calculate_store_digests(struct kimage *image)
{
 struct crypto_shash *tfm;
 struct shash_desc *desc;
 int ret = 0, i, j, zero_buf_sz, sha_region_sz;
 size_t desc_size, nullsz;
 char *digest;
 void *zero_buf;
 struct kexec_sha_region *sha_regions;
 struct purgatory_info *pi = &image->purgatory_info;

 zero_buf = __va(page_to_pfn(ZERO_PAGE(0)) << PAGE_SHIFT);
 zero_buf_sz = PAGE_SIZE;

 tfm = crypto_alloc_shash("sha256", 0, 0);
 if (IS_ERR(tfm)) {
  ret = PTR_ERR(tfm);
  goto out;
 }

 desc_size = crypto_shash_descsize(tfm) + sizeof(*desc);
 desc = kzalloc(desc_size, GFP_KERNEL);
 if (!desc) {
  ret = -ENOMEM;
  goto out_free_tfm;
 }

 sha_region_sz = KEXEC_SEGMENT_MAX * sizeof(struct kexec_sha_region);
 sha_regions = vzalloc(sha_region_sz);
 if (!sha_regions)
  goto out_free_desc;

 desc->tfm = tfm;
 desc->flags = 0;

 ret = crypto_shash_init(desc);
 if (ret < 0)
  goto out_free_sha_regions;

 digest = kzalloc(SHA256_DIGEST_SIZE, GFP_KERNEL);
 if (!digest) {
  ret = -ENOMEM;
  goto out_free_sha_regions;
 }

 for (j = i = 0; i < image->nr_segments; i++) {
  struct kexec_segment *ksegment;

  ksegment = &image->segment[i];




  if (ksegment->kbuf == pi->purgatory_buf)
   continue;

  ret = crypto_shash_update(desc, ksegment->kbuf,
       ksegment->bufsz);
  if (ret)
   break;





  nullsz = ksegment->memsz - ksegment->bufsz;
  while (nullsz) {
   unsigned long bytes = nullsz;

   if (bytes > zero_buf_sz)
    bytes = zero_buf_sz;
   ret = crypto_shash_update(desc, zero_buf, bytes);
   if (ret)
    break;
   nullsz -= bytes;
  }

  if (ret)
   break;

  sha_regions[j].start = ksegment->mem;
  sha_regions[j].len = ksegment->memsz;
  j++;
 }

 if (!ret) {
  ret = crypto_shash_final(desc, digest);
  if (ret)
   goto out_free_digest;
  ret = kexec_purgatory_get_set_symbol(image, "sha_regions",
      sha_regions, sha_region_sz, 0);
  if (ret)
   goto out_free_digest;

  ret = kexec_purgatory_get_set_symbol(image, "sha256_digest",
      digest, SHA256_DIGEST_SIZE, 0);
  if (ret)
   goto out_free_digest;
 }

out_free_digest:
 kfree(digest);
out_free_sha_regions:
 vfree(sha_regions);
out_free_desc:
 kfree(desc);
out_free_tfm:
 kfree(tfm);
out:
 return ret;
}


static int __kexec_load_purgatory(struct kimage *image, unsigned long min,
      unsigned long max, int top_down)
{
 struct purgatory_info *pi = &image->purgatory_info;
 unsigned long align, buf_align, bss_align, buf_sz, bss_sz, bss_pad;
 unsigned long memsz, entry, load_addr, curr_load_addr, bss_addr, offset;
 unsigned char *buf_addr, *src;
 int i, ret = 0, entry_sidx = -1;
 const Elf_Shdr *sechdrs_c;
 Elf_Shdr *sechdrs = NULL;
 void *purgatory_buf = NULL;





 sechdrs_c = (void *)pi->ehdr + pi->ehdr->e_shoff;
 sechdrs = vzalloc(pi->ehdr->e_shnum * sizeof(Elf_Shdr));
 if (!sechdrs)
  return -ENOMEM;

 memcpy(sechdrs, sechdrs_c, pi->ehdr->e_shnum * sizeof(Elf_Shdr));
 for (i = 0; i < pi->ehdr->e_shnum; i++) {
  if (sechdrs[i].sh_type == SHT_NOBITS)
   continue;

  sechdrs[i].sh_offset = (unsigned long)pi->ehdr +
      sechdrs[i].sh_offset;
 }





 entry = pi->ehdr->e_entry;
 for (i = 0; i < pi->ehdr->e_shnum; i++) {
  if (!(sechdrs[i].sh_flags & SHF_ALLOC))
   continue;

  if (!(sechdrs[i].sh_flags & SHF_EXECINSTR))
   continue;


  if (sechdrs[i].sh_addr <= pi->ehdr->e_entry &&
      ((sechdrs[i].sh_addr + sechdrs[i].sh_size) >
       pi->ehdr->e_entry)) {
   entry_sidx = i;
   entry -= sechdrs[i].sh_addr;
   break;
  }
 }


 buf_align = 1;
 bss_align = 1;
 buf_sz = 0;
 bss_sz = 0;

 for (i = 0; i < pi->ehdr->e_shnum; i++) {
  if (!(sechdrs[i].sh_flags & SHF_ALLOC))
   continue;

  align = sechdrs[i].sh_addralign;
  if (sechdrs[i].sh_type != SHT_NOBITS) {
   if (buf_align < align)
    buf_align = align;
   buf_sz = ALIGN(buf_sz, align);
   buf_sz += sechdrs[i].sh_size;
  } else {

   if (bss_align < align)
    bss_align = align;
   bss_sz = ALIGN(bss_sz, align);
   bss_sz += sechdrs[i].sh_size;
  }
 }


 bss_pad = 0;
 if (buf_sz & (bss_align - 1))
  bss_pad = bss_align - (buf_sz & (bss_align - 1));

 memsz = buf_sz + bss_pad + bss_sz;


 purgatory_buf = vzalloc(buf_sz);
 if (!purgatory_buf) {
  ret = -ENOMEM;
  goto out;
 }

 if (buf_align < bss_align)
  buf_align = bss_align;


 ret = kexec_add_buffer(image, purgatory_buf, buf_sz, memsz,
    buf_align, min, max, top_down,
    &pi->purgatory_load_addr);
 if (ret)
  goto out;


 buf_addr = purgatory_buf;
 load_addr = curr_load_addr = pi->purgatory_load_addr;
 bss_addr = load_addr + buf_sz + bss_pad;

 for (i = 0; i < pi->ehdr->e_shnum; i++) {
  if (!(sechdrs[i].sh_flags & SHF_ALLOC))
   continue;

  align = sechdrs[i].sh_addralign;
  if (sechdrs[i].sh_type != SHT_NOBITS) {
   curr_load_addr = ALIGN(curr_load_addr, align);
   offset = curr_load_addr - load_addr;

   src = (char *) sechdrs[i].sh_offset;
   memcpy(buf_addr + offset, src, sechdrs[i].sh_size);


   sechdrs[i].sh_addr = curr_load_addr;





   sechdrs[i].sh_offset = (unsigned long)(buf_addr + offset);


   curr_load_addr += sechdrs[i].sh_size;
  } else {
   bss_addr = ALIGN(bss_addr, align);
   sechdrs[i].sh_addr = bss_addr;
   bss_addr += sechdrs[i].sh_size;
  }
 }


 if (entry_sidx >= 0)
  entry += sechdrs[entry_sidx].sh_addr;


 image->start = entry;


 pi->sechdrs = sechdrs;





 pi->purgatory_buf = purgatory_buf;
 return ret;
out:
 vfree(sechdrs);
 vfree(purgatory_buf);
 return ret;
}

static int kexec_apply_relocations(struct kimage *image)
{
 int i, ret;
 struct purgatory_info *pi = &image->purgatory_info;
 Elf_Shdr *sechdrs = pi->sechdrs;


 for (i = 0; i < pi->ehdr->e_shnum; i++) {
  Elf_Shdr *section, *symtab;

  if (sechdrs[i].sh_type != SHT_RELA &&
      sechdrs[i].sh_type != SHT_REL)
   continue;







  if (sechdrs[i].sh_info >= pi->ehdr->e_shnum ||
      sechdrs[i].sh_link >= pi->ehdr->e_shnum)
   return -ENOEXEC;

  section = &sechdrs[sechdrs[i].sh_info];
  symtab = &sechdrs[sechdrs[i].sh_link];

  if (!(section->sh_flags & SHF_ALLOC))
   continue;





  if (symtab->sh_link >= pi->ehdr->e_shnum)

   continue;





  if (sechdrs[i].sh_type == SHT_RELA)
   ret = arch_kexec_apply_relocations_add(pi->ehdr,
              sechdrs, i);
  else if (sechdrs[i].sh_type == SHT_REL)
   ret = arch_kexec_apply_relocations(pi->ehdr,
          sechdrs, i);
  if (ret)
   return ret;
 }

 return 0;
}


int kexec_load_purgatory(struct kimage *image, unsigned long min,
    unsigned long max, int top_down,
    unsigned long *load_addr)
{
 struct purgatory_info *pi = &image->purgatory_info;
 int ret;

 if (kexec_purgatory_size <= 0)
  return -EINVAL;

 if (kexec_purgatory_size < sizeof(Elf_Ehdr))
  return -ENOEXEC;

 pi->ehdr = (Elf_Ehdr *)kexec_purgatory;

 if (memcmp(pi->ehdr->e_ident, ELFMAG, SELFMAG) != 0
     || pi->ehdr->e_type != ET_REL
     || !elf_check_arch(pi->ehdr)
     || pi->ehdr->e_shentsize != sizeof(Elf_Shdr))
  return -ENOEXEC;

 if (pi->ehdr->e_shoff >= kexec_purgatory_size
     || (pi->ehdr->e_shnum * sizeof(Elf_Shdr) >
     kexec_purgatory_size - pi->ehdr->e_shoff))
  return -ENOEXEC;

 ret = __kexec_load_purgatory(image, min, max, top_down);
 if (ret)
  return ret;

 ret = kexec_apply_relocations(image);
 if (ret)
  goto out;

 *load_addr = pi->purgatory_load_addr;
 return 0;
out:
 vfree(pi->sechdrs);
 vfree(pi->purgatory_buf);
 return ret;
}

static Elf_Sym *kexec_purgatory_find_symbol(struct purgatory_info *pi,
         const char *name)
{
 Elf_Sym *syms;
 Elf_Shdr *sechdrs;
 Elf_Ehdr *ehdr;
 int i, k;
 const char *strtab;

 if (!pi->sechdrs || !pi->ehdr)
  return NULL;

 sechdrs = pi->sechdrs;
 ehdr = pi->ehdr;

 for (i = 0; i < ehdr->e_shnum; i++) {
  if (sechdrs[i].sh_type != SHT_SYMTAB)
   continue;

  if (sechdrs[i].sh_link >= ehdr->e_shnum)

   continue;
  strtab = (char *)sechdrs[sechdrs[i].sh_link].sh_offset;
  syms = (Elf_Sym *)sechdrs[i].sh_offset;


  for (k = 0; k < sechdrs[i].sh_size/sizeof(Elf_Sym); k++) {
   if (ELF_ST_BIND(syms[k].st_info) != STB_GLOBAL)
    continue;

   if (strcmp(strtab + syms[k].st_name, name) != 0)
    continue;

   if (syms[k].st_shndx == SHN_UNDEF ||
       syms[k].st_shndx >= ehdr->e_shnum) {
    pr_debug("Symbol: %s has bad section index %d.\n",
      name, syms[k].st_shndx);
    return NULL;
   }


   return &syms[k];
  }
 }

 return NULL;
}

void *kexec_purgatory_get_symbol_addr(struct kimage *image, const char *name)
{
 struct purgatory_info *pi = &image->purgatory_info;
 Elf_Sym *sym;
 Elf_Shdr *sechdr;

 sym = kexec_purgatory_find_symbol(pi, name);
 if (!sym)
  return ERR_PTR(-EINVAL);

 sechdr = &pi->sechdrs[sym->st_shndx];





 return (void *)(sechdr->sh_addr + sym->st_value);
}





int kexec_purgatory_get_set_symbol(struct kimage *image, const char *name,
       void *buf, unsigned int size, bool get_value)
{
 Elf_Sym *sym;
 Elf_Shdr *sechdrs;
 struct purgatory_info *pi = &image->purgatory_info;
 char *sym_buf;

 sym = kexec_purgatory_find_symbol(pi, name);
 if (!sym)
  return -EINVAL;

 if (sym->st_size != size) {
  pr_err("symbol %s size mismatch: expected %lu actual %u\n",
         name, (unsigned long)sym->st_size, size);
  return -EINVAL;
 }

 sechdrs = pi->sechdrs;

 if (sechdrs[sym->st_shndx].sh_type == SHT_NOBITS) {
  pr_err("symbol %s is in a bss section. Cannot %s\n", name,
         get_value ? "get" : "set");
  return -EINVAL;
 }

 sym_buf = (unsigned char *)sechdrs[sym->st_shndx].sh_offset +
     sym->st_value;

 if (get_value)
  memcpy((void *)buf, sym_buf, size);
 else
  memcpy((void *)sym_buf, buf, size);

 return 0;
}


extern int max_threads;


static kernel_cap_t usermodehelper_bset = CAP_FULL_SET;
static kernel_cap_t usermodehelper_inheritable = CAP_FULL_SET;
static DEFINE_SPINLOCK(umh_sysctl_lock);
static DECLARE_RWSEM(umhelper_sem);





char modprobe_path[KMOD_PATH_LEN] = "/sbin/modprobe";

static void free_modprobe_argv(struct subprocess_info *info)
{
 kfree(info->argv[3]);
 kfree(info->argv);
}

static int call_modprobe(char *module_name, int wait)
{
 struct subprocess_info *info;
 static char *envp[] = {
  "HOME=/",
  "TERM=linux",
  "PATH=/sbin:/usr/sbin:/bin:/usr/bin",
  NULL
 };

 char **argv = kmalloc(sizeof(char *[5]), GFP_KERNEL);
 if (!argv)
  goto out;

 module_name = kstrdup(module_name, GFP_KERNEL);
 if (!module_name)
  goto free_argv;

 argv[0] = modprobe_path;
 argv[1] = "-q";
 argv[2] = "--";
 argv[3] = module_name;
 argv[4] = NULL;

 info = call_usermodehelper_setup(modprobe_path, argv, envp, GFP_KERNEL,
      NULL, free_modprobe_argv, NULL);
 if (!info)
  goto free_module_name;

 return call_usermodehelper_exec(info, wait | UMH_KILLABLE);

free_module_name:
 kfree(module_name);
free_argv:
 kfree(argv);
out:
 return -ENOMEM;
}
int __request_module(bool wait, const char *fmt, ...)
{
 va_list args;
 char module_name[MODULE_NAME_LEN];
 unsigned int max_modprobes;
 int ret;
 static atomic_t kmod_concurrent = ATOMIC_INIT(0);
 static int kmod_loop_msg;







 WARN_ON_ONCE(wait && current_is_async());

 if (!modprobe_path[0])
  return 0;

 va_start(args, fmt);
 ret = vsnprintf(module_name, MODULE_NAME_LEN, fmt, args);
 va_end(args);
 if (ret >= MODULE_NAME_LEN)
  return -ENAMETOOLONG;

 ret = security_kernel_module_request(module_name);
 if (ret)
  return ret;
 max_modprobes = min(max_threads/2, MAX_KMOD_CONCURRENT);
 atomic_inc(&kmod_concurrent);
 if (atomic_read(&kmod_concurrent) > max_modprobes) {

  if (kmod_loop_msg < 5) {
   printk(KERN_ERR
          "request_module: runaway loop modprobe %s\n",
          module_name);
   kmod_loop_msg++;
  }
  atomic_dec(&kmod_concurrent);
  return -ENOMEM;
 }

 trace_module_request(module_name, wait, _RET_IP_);

 ret = call_modprobe(module_name, wait ? UMH_WAIT_PROC : UMH_WAIT_EXEC);

 atomic_dec(&kmod_concurrent);
 return ret;
}
EXPORT_SYMBOL(__request_module);

static void call_usermodehelper_freeinfo(struct subprocess_info *info)
{
 if (info->cleanup)
  (*info->cleanup)(info);
 kfree(info);
}

static void umh_complete(struct subprocess_info *sub_info)
{
 struct completion *comp = xchg(&sub_info->complete, NULL);





 if (comp)
  complete(comp);
 else
  call_usermodehelper_freeinfo(sub_info);
}




static int call_usermodehelper_exec_async(void *data)
{
 struct subprocess_info *sub_info = data;
 struct cred *new;
 int retval;

 spin_lock_irq(&current->sighand->siglock);
 flush_signal_handlers(current, 1);
 spin_unlock_irq(&current->sighand->siglock);





 set_user_nice(current, 0);

 retval = -ENOMEM;
 new = prepare_kernel_cred(current);
 if (!new)
  goto out;

 spin_lock(&umh_sysctl_lock);
 new->cap_bset = cap_intersect(usermodehelper_bset, new->cap_bset);
 new->cap_inheritable = cap_intersect(usermodehelper_inheritable,
          new->cap_inheritable);
 spin_unlock(&umh_sysctl_lock);

 if (sub_info->init) {
  retval = sub_info->init(sub_info, new);
  if (retval) {
   abort_creds(new);
   goto out;
  }
 }

 commit_creds(new);

 retval = do_execve(getname_kernel(sub_info->path),
      (const char __user *const __user *)sub_info->argv,
      (const char __user *const __user *)sub_info->envp);
out:
 sub_info->retval = retval;




 if (!(sub_info->wait & UMH_WAIT_PROC))
  umh_complete(sub_info);
 if (!retval)
  return 0;
 do_exit(0);
}


static void call_usermodehelper_exec_sync(struct subprocess_info *sub_info)
{
 pid_t pid;


 kernel_sigaction(SIGCHLD, SIG_DFL);
 pid = kernel_thread(call_usermodehelper_exec_async, sub_info, SIGCHLD);
 if (pid < 0) {
  sub_info->retval = pid;
 } else {
  int ret = -ECHILD;
  sys_wait4(pid, (int __user *)&ret, 0, NULL);






  if (ret)
   sub_info->retval = ret;
 }


 kernel_sigaction(SIGCHLD, SIG_IGN);

 umh_complete(sub_info);
}
static void call_usermodehelper_exec_work(struct work_struct *work)
{
 struct subprocess_info *sub_info =
  container_of(work, struct subprocess_info, work);

 if (sub_info->wait & UMH_WAIT_PROC) {
  call_usermodehelper_exec_sync(sub_info);
 } else {
  pid_t pid;





  pid = kernel_thread(call_usermodehelper_exec_async, sub_info,
        CLONE_PARENT | SIGCHLD);
  if (pid < 0) {
   sub_info->retval = pid;
   umh_complete(sub_info);
  }
 }
}







static enum umh_disable_depth usermodehelper_disabled = UMH_DISABLED;


static atomic_t running_helpers = ATOMIC_INIT(0);





static DECLARE_WAIT_QUEUE_HEAD(running_helpers_waitq);





static DECLARE_WAIT_QUEUE_HEAD(usermodehelper_disabled_waitq);






int usermodehelper_read_trylock(void)
{
 DEFINE_WAIT(wait);
 int ret = 0;

 down_read(&umhelper_sem);
 for (;;) {
  prepare_to_wait(&usermodehelper_disabled_waitq, &wait,
    TASK_INTERRUPTIBLE);
  if (!usermodehelper_disabled)
   break;

  if (usermodehelper_disabled == UMH_DISABLED)
   ret = -EAGAIN;

  up_read(&umhelper_sem);

  if (ret)
   break;

  schedule();
  try_to_freeze();

  down_read(&umhelper_sem);
 }
 finish_wait(&usermodehelper_disabled_waitq, &wait);
 return ret;
}
EXPORT_SYMBOL_GPL(usermodehelper_read_trylock);

long usermodehelper_read_lock_wait(long timeout)
{
 DEFINE_WAIT(wait);

 if (timeout < 0)
  return -EINVAL;

 down_read(&umhelper_sem);
 for (;;) {
  prepare_to_wait(&usermodehelper_disabled_waitq, &wait,
    TASK_UNINTERRUPTIBLE);
  if (!usermodehelper_disabled)
   break;

  up_read(&umhelper_sem);

  timeout = schedule_timeout(timeout);
  if (!timeout)
   break;

  down_read(&umhelper_sem);
 }
 finish_wait(&usermodehelper_disabled_waitq, &wait);
 return timeout;
}
EXPORT_SYMBOL_GPL(usermodehelper_read_lock_wait);

void usermodehelper_read_unlock(void)
{
 up_read(&umhelper_sem);
}
EXPORT_SYMBOL_GPL(usermodehelper_read_unlock);
void __usermodehelper_set_disable_depth(enum umh_disable_depth depth)
{
 down_write(&umhelper_sem);
 usermodehelper_disabled = depth;
 wake_up(&usermodehelper_disabled_waitq);
 up_write(&umhelper_sem);
}







int __usermodehelper_disable(enum umh_disable_depth depth)
{
 long retval;

 if (!depth)
  return -EINVAL;

 down_write(&umhelper_sem);
 usermodehelper_disabled = depth;
 up_write(&umhelper_sem);







 retval = wait_event_timeout(running_helpers_waitq,
     atomic_read(&running_helpers) == 0,
     RUNNING_HELPERS_TIMEOUT);
 if (retval)
  return 0;

 __usermodehelper_set_disable_depth(UMH_ENABLED);
 return -EAGAIN;
}

static void helper_lock(void)
{
 atomic_inc(&running_helpers);
 smp_mb__after_atomic();
}

static void helper_unlock(void)
{
 if (atomic_dec_and_test(&running_helpers))
  wake_up(&running_helpers_waitq);
}
struct subprocess_info *call_usermodehelper_setup(char *path, char **argv,
  char **envp, gfp_t gfp_mask,
  int (*init)(struct subprocess_info *info, struct cred *new),
  void (*cleanup)(struct subprocess_info *info),
  void *data)
{
 struct subprocess_info *sub_info;
 sub_info = kzalloc(sizeof(struct subprocess_info), gfp_mask);
 if (!sub_info)
  goto out;

 INIT_WORK(&sub_info->work, call_usermodehelper_exec_work);
 sub_info->path = path;
 sub_info->argv = argv;
 sub_info->envp = envp;

 sub_info->cleanup = cleanup;
 sub_info->init = init;
 sub_info->data = data;
  out:
 return sub_info;
}
EXPORT_SYMBOL(call_usermodehelper_setup);
int call_usermodehelper_exec(struct subprocess_info *sub_info, int wait)
{
 DECLARE_COMPLETION_ONSTACK(done);
 int retval = 0;

 if (!sub_info->path) {
  call_usermodehelper_freeinfo(sub_info);
  return -EINVAL;
 }
 helper_lock();
 if (usermodehelper_disabled) {
  retval = -EBUSY;
  goto out;
 }





 sub_info->complete = (wait == UMH_NO_WAIT) ? NULL : &done;
 sub_info->wait = wait;

 queue_work(system_unbound_wq, &sub_info->work);
 if (wait == UMH_NO_WAIT)
  goto unlock;

 if (wait & UMH_KILLABLE) {
  retval = wait_for_completion_killable(&done);
  if (!retval)
   goto wait_done;


  if (xchg(&sub_info->complete, NULL))
   goto unlock;

 }

 wait_for_completion(&done);
wait_done:
 retval = sub_info->retval;
out:
 call_usermodehelper_freeinfo(sub_info);
unlock:
 helper_unlock();
 return retval;
}
EXPORT_SYMBOL(call_usermodehelper_exec);
int call_usermodehelper(char *path, char **argv, char **envp, int wait)
{
 struct subprocess_info *info;
 gfp_t gfp_mask = (wait == UMH_NO_WAIT) ? GFP_ATOMIC : GFP_KERNEL;

 info = call_usermodehelper_setup(path, argv, envp, gfp_mask,
      NULL, NULL, NULL);
 if (info == NULL)
  return -ENOMEM;

 return call_usermodehelper_exec(info, wait);
}
EXPORT_SYMBOL(call_usermodehelper);

static int proc_cap_handler(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos)
{
 struct ctl_table t;
 unsigned long cap_array[_KERNEL_CAPABILITY_U32S];
 kernel_cap_t new_cap;
 int err, i;

 if (write && (!capable(CAP_SETPCAP) ||
        !capable(CAP_SYS_MODULE)))
  return -EPERM;





 spin_lock(&umh_sysctl_lock);
 for (i = 0; i < _KERNEL_CAPABILITY_U32S; i++) {
  if (table->data == CAP_BSET)
   cap_array[i] = usermodehelper_bset.cap[i];
  else if (table->data == CAP_PI)
   cap_array[i] = usermodehelper_inheritable.cap[i];
  else
   BUG();
 }
 spin_unlock(&umh_sysctl_lock);

 t = *table;
 t.data = &cap_array;





 err = proc_doulongvec_minmax(&t, write, buffer, lenp, ppos);
 if (err < 0)
  return err;





 for (i = 0; i < _KERNEL_CAPABILITY_U32S; i++)
  new_cap.cap[i] = cap_array[i];




 spin_lock(&umh_sysctl_lock);
 if (write) {
  if (table->data == CAP_BSET)
   usermodehelper_bset = cap_intersect(usermodehelper_bset, new_cap);
  if (table->data == CAP_PI)
   usermodehelper_inheritable = cap_intersect(usermodehelper_inheritable, new_cap);
 }
 spin_unlock(&umh_sysctl_lock);

 return 0;
}

struct ctl_table usermodehelper_table[] = {
 {
  .procname = "bset",
  .data = CAP_BSET,
  .maxlen = _KERNEL_CAPABILITY_U32S * sizeof(unsigned long),
  .mode = 0600,
  .proc_handler = proc_cap_handler,
 },
 {
  .procname = "inheritable",
  .data = CAP_PI,
  .maxlen = _KERNEL_CAPABILITY_U32S * sizeof(unsigned long),
  .mode = 0600,
  .proc_handler = proc_cap_handler,
 },
 { }
};








 addr = ((kprobe_opcode_t *)(kallsyms_lookup_name(name)))

static int kprobes_initialized;
static struct hlist_head kprobe_table[KPROBE_TABLE_SIZE];
static struct hlist_head kretprobe_inst_table[KPROBE_TABLE_SIZE];


static bool kprobes_all_disarmed;


static DEFINE_MUTEX(kprobe_mutex);
static DEFINE_PER_CPU(struct kprobe *, kprobe_instance) = NULL;
static struct {
 raw_spinlock_t lock ____cacheline_aligned_in_smp;
} kretprobe_table_locks[KPROBE_TABLE_SIZE];

static raw_spinlock_t *kretprobe_table_lock_ptr(unsigned long hash)
{
 return &(kretprobe_table_locks[hash].lock);
}


static LIST_HEAD(kprobe_blacklist);







struct kprobe_insn_page {
 struct list_head list;
 kprobe_opcode_t *insns;
 struct kprobe_insn_cache *cache;
 int nused;
 int ngarbage;
 char slot_used[];
};

 (offsetof(struct kprobe_insn_page, slot_used) + \
  (sizeof(char) * (slots)))

static int slots_per_page(struct kprobe_insn_cache *c)
{
 return PAGE_SIZE/(c->insn_size * sizeof(kprobe_opcode_t));
}

enum kprobe_slot_state {
 SLOT_CLEAN = 0,
 SLOT_DIRTY = 1,
 SLOT_USED = 2,
};

static void *alloc_insn_page(void)
{
 return module_alloc(PAGE_SIZE);
}

static void free_insn_page(void *page)
{
 module_memfree(page);
}

struct kprobe_insn_cache kprobe_insn_slots = {
 .mutex = __MUTEX_INITIALIZER(kprobe_insn_slots.mutex),
 .alloc = alloc_insn_page,
 .free = free_insn_page,
 .pages = LIST_HEAD_INIT(kprobe_insn_slots.pages),
 .insn_size = MAX_INSN_SIZE,
 .nr_garbage = 0,
};
static int collect_garbage_slots(struct kprobe_insn_cache *c);





kprobe_opcode_t *__get_insn_slot(struct kprobe_insn_cache *c)
{
 struct kprobe_insn_page *kip;
 kprobe_opcode_t *slot = NULL;

 mutex_lock(&c->mutex);
 retry:
 list_for_each_entry(kip, &c->pages, list) {
  if (kip->nused < slots_per_page(c)) {
   int i;
   for (i = 0; i < slots_per_page(c); i++) {
    if (kip->slot_used[i] == SLOT_CLEAN) {
     kip->slot_used[i] = SLOT_USED;
     kip->nused++;
     slot = kip->insns + (i * c->insn_size);
     goto out;
    }
   }

   kip->nused = slots_per_page(c);
   WARN_ON(1);
  }
 }


 if (c->nr_garbage && collect_garbage_slots(c) == 0)
  goto retry;


 kip = kmalloc(KPROBE_INSN_PAGE_SIZE(slots_per_page(c)), GFP_KERNEL);
 if (!kip)
  goto out;






 kip->insns = c->alloc();
 if (!kip->insns) {
  kfree(kip);
  goto out;
 }
 INIT_LIST_HEAD(&kip->list);
 memset(kip->slot_used, SLOT_CLEAN, slots_per_page(c));
 kip->slot_used[0] = SLOT_USED;
 kip->nused = 1;
 kip->ngarbage = 0;
 kip->cache = c;
 list_add(&kip->list, &c->pages);
 slot = kip->insns;
out:
 mutex_unlock(&c->mutex);
 return slot;
}


static int collect_one_slot(struct kprobe_insn_page *kip, int idx)
{
 kip->slot_used[idx] = SLOT_CLEAN;
 kip->nused--;
 if (kip->nused == 0) {






  if (!list_is_singular(&kip->list)) {
   list_del(&kip->list);
   kip->cache->free(kip->insns);
   kfree(kip);
  }
  return 1;
 }
 return 0;
}

static int collect_garbage_slots(struct kprobe_insn_cache *c)
{
 struct kprobe_insn_page *kip, *next;


 synchronize_sched();

 list_for_each_entry_safe(kip, next, &c->pages, list) {
  int i;
  if (kip->ngarbage == 0)
   continue;
  kip->ngarbage = 0;
  for (i = 0; i < slots_per_page(c); i++) {
   if (kip->slot_used[i] == SLOT_DIRTY &&
       collect_one_slot(kip, i))
    break;
  }
 }
 c->nr_garbage = 0;
 return 0;
}

void __free_insn_slot(struct kprobe_insn_cache *c,
        kprobe_opcode_t *slot, int dirty)
{
 struct kprobe_insn_page *kip;

 mutex_lock(&c->mutex);
 list_for_each_entry(kip, &c->pages, list) {
  long idx = ((long)slot - (long)kip->insns) /
    (c->insn_size * sizeof(kprobe_opcode_t));
  if (idx >= 0 && idx < slots_per_page(c)) {
   WARN_ON(kip->slot_used[idx] != SLOT_USED);
   if (dirty) {
    kip->slot_used[idx] = SLOT_DIRTY;
    kip->ngarbage++;
    if (++c->nr_garbage > slots_per_page(c))
     collect_garbage_slots(c);
   } else
    collect_one_slot(kip, idx);
   goto out;
  }
 }

 WARN_ON(1);
out:
 mutex_unlock(&c->mutex);
}


struct kprobe_insn_cache kprobe_optinsn_slots = {
 .mutex = __MUTEX_INITIALIZER(kprobe_optinsn_slots.mutex),
 .alloc = alloc_insn_page,
 .free = free_insn_page,
 .pages = LIST_HEAD_INIT(kprobe_optinsn_slots.pages),

 .nr_garbage = 0,
};


static inline void set_kprobe_instance(struct kprobe *kp)
{
 __this_cpu_write(kprobe_instance, kp);
}

static inline void reset_kprobe_instance(void)
{
 __this_cpu_write(kprobe_instance, NULL);
}







struct kprobe *get_kprobe(void *addr)
{
 struct hlist_head *head;
 struct kprobe *p;

 head = &kprobe_table[hash_ptr(addr, KPROBE_HASH_BITS)];
 hlist_for_each_entry_rcu(p, head, hlist) {
  if (p->addr == addr)
   return p;
 }

 return NULL;
}
NOKPROBE_SYMBOL(get_kprobe);

static int aggr_pre_handler(struct kprobe *p, struct pt_regs *regs);


static inline int kprobe_aggrprobe(struct kprobe *p)
{
 return p->pre_handler == aggr_pre_handler;
}


static inline int kprobe_unused(struct kprobe *p)
{
 return kprobe_aggrprobe(p) && kprobe_disabled(p) &&
        list_empty(&p->list);
}




static inline void copy_kprobe(struct kprobe *ap, struct kprobe *p)
{
 memcpy(&p->opcode, &ap->opcode, sizeof(kprobe_opcode_t));
 memcpy(&p->ainsn, &ap->ainsn, sizeof(struct arch_specific_insn));
}


static bool kprobes_allow_optimization;





void opt_pre_handler(struct kprobe *p, struct pt_regs *regs)
{
 struct kprobe *kp;

 list_for_each_entry_rcu(kp, &p->list, list) {
  if (kp->pre_handler && likely(!kprobe_disabled(kp))) {
   set_kprobe_instance(kp);
   kp->pre_handler(kp, regs);
  }
  reset_kprobe_instance();
 }
}
NOKPROBE_SYMBOL(opt_pre_handler);


static void free_aggr_kprobe(struct kprobe *p)
{
 struct optimized_kprobe *op;

 op = container_of(p, struct optimized_kprobe, kp);
 arch_remove_optimized_kprobe(op);
 arch_remove_kprobe(p);
 kfree(op);
}


static inline int kprobe_optready(struct kprobe *p)
{
 struct optimized_kprobe *op;

 if (kprobe_aggrprobe(p)) {
  op = container_of(p, struct optimized_kprobe, kp);
  return arch_prepared_optinsn(&op->optinsn);
 }

 return 0;
}


static inline int kprobe_disarmed(struct kprobe *p)
{
 struct optimized_kprobe *op;


 if (!kprobe_aggrprobe(p))
  return kprobe_disabled(p);

 op = container_of(p, struct optimized_kprobe, kp);

 return kprobe_disabled(p) && list_empty(&op->list);
}


static int kprobe_queued(struct kprobe *p)
{
 struct optimized_kprobe *op;

 if (kprobe_aggrprobe(p)) {
  op = container_of(p, struct optimized_kprobe, kp);
  if (!list_empty(&op->list))
   return 1;
 }
 return 0;
}





static struct kprobe *get_optimized_kprobe(unsigned long addr)
{
 int i;
 struct kprobe *p = NULL;
 struct optimized_kprobe *op;


 for (i = 1; !p && i < MAX_OPTIMIZED_LENGTH; i++)
  p = get_kprobe((void *)(addr - i));

 if (p && kprobe_optready(p)) {
  op = container_of(p, struct optimized_kprobe, kp);
  if (arch_within_optimized_kprobe(op, addr))
   return p;
 }

 return NULL;
}


static LIST_HEAD(optimizing_list);
static LIST_HEAD(unoptimizing_list);
static LIST_HEAD(freeing_list);

static void kprobe_optimizer(struct work_struct *work);
static DECLARE_DELAYED_WORK(optimizing_work, kprobe_optimizer);





static void do_optimize_kprobes(void)
{

 if (kprobes_all_disarmed || !kprobes_allow_optimization ||
     list_empty(&optimizing_list))
  return;
 get_online_cpus();
 mutex_lock(&text_mutex);
 arch_optimize_kprobes(&optimizing_list);
 mutex_unlock(&text_mutex);
 put_online_cpus();
}





static void do_unoptimize_kprobes(void)
{
 struct optimized_kprobe *op, *tmp;


 if (list_empty(&unoptimizing_list))
  return;


 get_online_cpus();
 mutex_lock(&text_mutex);
 arch_unoptimize_kprobes(&unoptimizing_list, &freeing_list);

 list_for_each_entry_safe(op, tmp, &freeing_list, list) {

  if (kprobe_disabled(&op->kp))
   arch_disarm_kprobe(&op->kp);
  if (kprobe_unused(&op->kp)) {





   hlist_del_rcu(&op->kp.hlist);
  } else
   list_del_init(&op->list);
 }
 mutex_unlock(&text_mutex);
 put_online_cpus();
}


static void do_free_cleaned_kprobes(void)
{
 struct optimized_kprobe *op, *tmp;

 list_for_each_entry_safe(op, tmp, &freeing_list, list) {
  BUG_ON(!kprobe_unused(&op->kp));
  list_del_init(&op->list);
  free_aggr_kprobe(&op->kp);
 }
}


static void kick_kprobe_optimizer(void)
{
 schedule_delayed_work(&optimizing_work, OPTIMIZE_DELAY);
}


static void kprobe_optimizer(struct work_struct *work)
{
 mutex_lock(&kprobe_mutex);

 mutex_lock(&module_mutex);





 do_unoptimize_kprobes();
 synchronize_sched();


 do_optimize_kprobes();


 do_free_cleaned_kprobes();

 mutex_unlock(&module_mutex);
 mutex_unlock(&kprobe_mutex);


 if (!list_empty(&optimizing_list) || !list_empty(&unoptimizing_list))
  kick_kprobe_optimizer();
}


static void wait_for_kprobe_optimizer(void)
{
 mutex_lock(&kprobe_mutex);

 while (!list_empty(&optimizing_list) || !list_empty(&unoptimizing_list)) {
  mutex_unlock(&kprobe_mutex);


  flush_delayed_work(&optimizing_work);

  cpu_relax();

  mutex_lock(&kprobe_mutex);
 }

 mutex_unlock(&kprobe_mutex);
}


static void optimize_kprobe(struct kprobe *p)
{
 struct optimized_kprobe *op;


 if (!kprobe_optready(p) || !kprobes_allow_optimization ||
     (kprobe_disabled(p) || kprobes_all_disarmed))
  return;


 if (p->break_handler || p->post_handler)
  return;

 op = container_of(p, struct optimized_kprobe, kp);


 if (arch_check_optimized_kprobe(op) < 0)
  return;


 if (op->kp.flags & KPROBE_FLAG_OPTIMIZED)
  return;
 op->kp.flags |= KPROBE_FLAG_OPTIMIZED;

 if (!list_empty(&op->list))

  list_del_init(&op->list);
 else {
  list_add(&op->list, &optimizing_list);
  kick_kprobe_optimizer();
 }
}


static void force_unoptimize_kprobe(struct optimized_kprobe *op)
{
 get_online_cpus();
 arch_unoptimize_kprobe(op);
 put_online_cpus();
 if (kprobe_disabled(&op->kp))
  arch_disarm_kprobe(&op->kp);
}


static void unoptimize_kprobe(struct kprobe *p, bool force)
{
 struct optimized_kprobe *op;

 if (!kprobe_aggrprobe(p) || kprobe_disarmed(p))
  return;

 op = container_of(p, struct optimized_kprobe, kp);
 if (!kprobe_optimized(p)) {

  if (force && !list_empty(&op->list)) {





   list_del_init(&op->list);
   force_unoptimize_kprobe(op);
  }
  return;
 }

 op->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;
 if (!list_empty(&op->list)) {

  list_del_init(&op->list);
  return;
 }

 if (force)

  force_unoptimize_kprobe(op);
 else {
  list_add(&op->list, &unoptimizing_list);
  kick_kprobe_optimizer();
 }
}


static void reuse_unused_kprobe(struct kprobe *ap)
{
 struct optimized_kprobe *op;

 BUG_ON(!kprobe_unused(ap));




 op = container_of(ap, struct optimized_kprobe, kp);
 if (unlikely(list_empty(&op->list)))
  printk(KERN_WARNING "Warning: found a stray unused "
   "aggrprobe@%p\n", ap->addr);

 ap->flags &= ~KPROBE_FLAG_DISABLED;

 BUG_ON(!kprobe_optready(ap));
 optimize_kprobe(ap);
}


static void kill_optimized_kprobe(struct kprobe *p)
{
 struct optimized_kprobe *op;

 op = container_of(p, struct optimized_kprobe, kp);
 if (!list_empty(&op->list))

  list_del_init(&op->list);
 op->kp.flags &= ~KPROBE_FLAG_OPTIMIZED;

 if (kprobe_unused(p)) {

  list_add(&op->list, &freeing_list);





  hlist_del_rcu(&op->kp.hlist);
 }


 arch_remove_optimized_kprobe(op);
}


static void prepare_optimized_kprobe(struct kprobe *p)
{
 struct optimized_kprobe *op;

 op = container_of(p, struct optimized_kprobe, kp);
 arch_prepare_optimized_kprobe(op, p);
}


static struct kprobe *alloc_aggr_kprobe(struct kprobe *p)
{
 struct optimized_kprobe *op;

 op = kzalloc(sizeof(struct optimized_kprobe), GFP_KERNEL);
 if (!op)
  return NULL;

 INIT_LIST_HEAD(&op->list);
 op->kp.addr = p->addr;
 arch_prepare_optimized_kprobe(op, p);

 return &op->kp;
}

static void init_aggr_kprobe(struct kprobe *ap, struct kprobe *p);





static void try_to_optimize_kprobe(struct kprobe *p)
{
 struct kprobe *ap;
 struct optimized_kprobe *op;


 if (kprobe_ftrace(p))
  return;


 jump_label_lock();
 mutex_lock(&text_mutex);

 ap = alloc_aggr_kprobe(p);
 if (!ap)
  goto out;

 op = container_of(ap, struct optimized_kprobe, kp);
 if (!arch_prepared_optinsn(&op->optinsn)) {

  arch_remove_optimized_kprobe(op);
  kfree(op);
  goto out;
 }

 init_aggr_kprobe(ap, p);
 optimize_kprobe(ap);

out:
 mutex_unlock(&text_mutex);
 jump_label_unlock();
}

static void optimize_all_kprobes(void)
{
 struct hlist_head *head;
 struct kprobe *p;
 unsigned int i;

 mutex_lock(&kprobe_mutex);

 if (kprobes_allow_optimization)
  goto out;

 kprobes_allow_optimization = true;
 for (i = 0; i < KPROBE_TABLE_SIZE; i++) {
  head = &kprobe_table[i];
  hlist_for_each_entry_rcu(p, head, hlist)
   if (!kprobe_disabled(p))
    optimize_kprobe(p);
 }
 printk(KERN_INFO "Kprobes globally optimized\n");
out:
 mutex_unlock(&kprobe_mutex);
}

static void unoptimize_all_kprobes(void)
{
 struct hlist_head *head;
 struct kprobe *p;
 unsigned int i;

 mutex_lock(&kprobe_mutex);

 if (!kprobes_allow_optimization) {
  mutex_unlock(&kprobe_mutex);
  return;
 }

 kprobes_allow_optimization = false;
 for (i = 0; i < KPROBE_TABLE_SIZE; i++) {
  head = &kprobe_table[i];
  hlist_for_each_entry_rcu(p, head, hlist) {
   if (!kprobe_disabled(p))
    unoptimize_kprobe(p, false);
  }
 }
 mutex_unlock(&kprobe_mutex);


 wait_for_kprobe_optimizer();
 printk(KERN_INFO "Kprobes globally unoptimized\n");
}

static DEFINE_MUTEX(kprobe_sysctl_mutex);
int sysctl_kprobes_optimization;
int proc_kprobes_optimization_handler(struct ctl_table *table, int write,
          void __user *buffer, size_t *length,
          loff_t *ppos)
{
 int ret;

 mutex_lock(&kprobe_sysctl_mutex);
 sysctl_kprobes_optimization = kprobes_allow_optimization ? 1 : 0;
 ret = proc_dointvec_minmax(table, write, buffer, length, ppos);

 if (sysctl_kprobes_optimization)
  optimize_all_kprobes();
 else
  unoptimize_all_kprobes();
 mutex_unlock(&kprobe_sysctl_mutex);

 return ret;
}


static void __arm_kprobe(struct kprobe *p)
{
 struct kprobe *_p;


 _p = get_optimized_kprobe((unsigned long)p->addr);
 if (unlikely(_p))

  unoptimize_kprobe(_p, true);

 arch_arm_kprobe(p);
 optimize_kprobe(p);
}


static void __disarm_kprobe(struct kprobe *p, bool reopt)
{
 struct kprobe *_p;


 unoptimize_kprobe(p, kprobes_all_disarmed);

 if (!kprobe_queued(p)) {
  arch_disarm_kprobe(p);

  _p = get_optimized_kprobe((unsigned long)p->addr);
  if (unlikely(_p) && reopt)
   optimize_kprobe(_p);
 }

}




static void reuse_unused_kprobe(struct kprobe *ap)
{
 printk(KERN_ERR "Error: There should be no unused kprobe here.\n");
 BUG_ON(kprobe_unused(ap));
}

static void free_aggr_kprobe(struct kprobe *p)
{
 arch_remove_kprobe(p);
 kfree(p);
}

static struct kprobe *alloc_aggr_kprobe(struct kprobe *p)
{
 return kzalloc(sizeof(struct kprobe), GFP_KERNEL);
}

static struct ftrace_ops kprobe_ftrace_ops __read_mostly = {
 .func = kprobe_ftrace_handler,
 .flags = FTRACE_OPS_FL_SAVE_REGS | FTRACE_OPS_FL_IPMODIFY,
};
static int kprobe_ftrace_enabled;


static int prepare_kprobe(struct kprobe *p)
{
 if (!kprobe_ftrace(p))
  return arch_prepare_kprobe(p);

 return arch_prepare_kprobe_ftrace(p);
}


static void arm_kprobe_ftrace(struct kprobe *p)
{
 int ret;

 ret = ftrace_set_filter_ip(&kprobe_ftrace_ops,
       (unsigned long)p->addr, 0, 0);
 WARN(ret < 0, "Failed to arm kprobe-ftrace at %p (%d)\n", p->addr, ret);
 kprobe_ftrace_enabled++;
 if (kprobe_ftrace_enabled == 1) {
  ret = register_ftrace_function(&kprobe_ftrace_ops);
  WARN(ret < 0, "Failed to init kprobe-ftrace (%d)\n", ret);
 }
}


static void disarm_kprobe_ftrace(struct kprobe *p)
{
 int ret;

 kprobe_ftrace_enabled--;
 if (kprobe_ftrace_enabled == 0) {
  ret = unregister_ftrace_function(&kprobe_ftrace_ops);
  WARN(ret < 0, "Failed to init kprobe-ftrace (%d)\n", ret);
 }
 ret = ftrace_set_filter_ip(&kprobe_ftrace_ops,
      (unsigned long)p->addr, 1, 0);
 WARN(ret < 0, "Failed to disarm kprobe-ftrace at %p (%d)\n", p->addr, ret);
}


static void arm_kprobe(struct kprobe *kp)
{
 if (unlikely(kprobe_ftrace(kp))) {
  arm_kprobe_ftrace(kp);
  return;
 }





 mutex_lock(&text_mutex);
 __arm_kprobe(kp);
 mutex_unlock(&text_mutex);
}


static void disarm_kprobe(struct kprobe *kp, bool reopt)
{
 if (unlikely(kprobe_ftrace(kp))) {
  disarm_kprobe_ftrace(kp);
  return;
 }

 mutex_lock(&text_mutex);
 __disarm_kprobe(kp, reopt);
 mutex_unlock(&text_mutex);
}





static int aggr_pre_handler(struct kprobe *p, struct pt_regs *regs)
{
 struct kprobe *kp;

 list_for_each_entry_rcu(kp, &p->list, list) {
  if (kp->pre_handler && likely(!kprobe_disabled(kp))) {
   set_kprobe_instance(kp);
   if (kp->pre_handler(kp, regs))
    return 1;
  }
  reset_kprobe_instance();
 }
 return 0;
}
NOKPROBE_SYMBOL(aggr_pre_handler);

static void aggr_post_handler(struct kprobe *p, struct pt_regs *regs,
         unsigned long flags)
{
 struct kprobe *kp;

 list_for_each_entry_rcu(kp, &p->list, list) {
  if (kp->post_handler && likely(!kprobe_disabled(kp))) {
   set_kprobe_instance(kp);
   kp->post_handler(kp, regs, flags);
   reset_kprobe_instance();
  }
 }
}
NOKPROBE_SYMBOL(aggr_post_handler);

static int aggr_fault_handler(struct kprobe *p, struct pt_regs *regs,
         int trapnr)
{
 struct kprobe *cur = __this_cpu_read(kprobe_instance);





 if (cur && cur->fault_handler) {
  if (cur->fault_handler(cur, regs, trapnr))
   return 1;
 }
 return 0;
}
NOKPROBE_SYMBOL(aggr_fault_handler);

static int aggr_break_handler(struct kprobe *p, struct pt_regs *regs)
{
 struct kprobe *cur = __this_cpu_read(kprobe_instance);
 int ret = 0;

 if (cur && cur->break_handler) {
  if (cur->break_handler(cur, regs))
   ret = 1;
 }
 reset_kprobe_instance();
 return ret;
}
NOKPROBE_SYMBOL(aggr_break_handler);


void kprobes_inc_nmissed_count(struct kprobe *p)
{
 struct kprobe *kp;
 if (!kprobe_aggrprobe(p)) {
  p->nmissed++;
 } else {
  list_for_each_entry_rcu(kp, &p->list, list)
   kp->nmissed++;
 }
 return;
}
NOKPROBE_SYMBOL(kprobes_inc_nmissed_count);

void recycle_rp_inst(struct kretprobe_instance *ri,
       struct hlist_head *head)
{
 struct kretprobe *rp = ri->rp;


 hlist_del(&ri->hlist);
 INIT_HLIST_NODE(&ri->hlist);
 if (likely(rp)) {
  raw_spin_lock(&rp->lock);
  hlist_add_head(&ri->hlist, &rp->free_instances);
  raw_spin_unlock(&rp->lock);
 } else

  hlist_add_head(&ri->hlist, head);
}
NOKPROBE_SYMBOL(recycle_rp_inst);

void kretprobe_hash_lock(struct task_struct *tsk,
    struct hlist_head **head, unsigned long *flags)
__acquires(hlist_lock)
{
 unsigned long hash = hash_ptr(tsk, KPROBE_HASH_BITS);
 raw_spinlock_t *hlist_lock;

 *head = &kretprobe_inst_table[hash];
 hlist_lock = kretprobe_table_lock_ptr(hash);
 raw_spin_lock_irqsave(hlist_lock, *flags);
}
NOKPROBE_SYMBOL(kretprobe_hash_lock);

static void kretprobe_table_lock(unsigned long hash,
     unsigned long *flags)
__acquires(hlist_lock)
{
 raw_spinlock_t *hlist_lock = kretprobe_table_lock_ptr(hash);
 raw_spin_lock_irqsave(hlist_lock, *flags);
}
NOKPROBE_SYMBOL(kretprobe_table_lock);

void kretprobe_hash_unlock(struct task_struct *tsk,
      unsigned long *flags)
__releases(hlist_lock)
{
 unsigned long hash = hash_ptr(tsk, KPROBE_HASH_BITS);
 raw_spinlock_t *hlist_lock;

 hlist_lock = kretprobe_table_lock_ptr(hash);
 raw_spin_unlock_irqrestore(hlist_lock, *flags);
}
NOKPROBE_SYMBOL(kretprobe_hash_unlock);

static void kretprobe_table_unlock(unsigned long hash,
       unsigned long *flags)
__releases(hlist_lock)
{
 raw_spinlock_t *hlist_lock = kretprobe_table_lock_ptr(hash);
 raw_spin_unlock_irqrestore(hlist_lock, *flags);
}
NOKPROBE_SYMBOL(kretprobe_table_unlock);







void kprobe_flush_task(struct task_struct *tk)
{
 struct kretprobe_instance *ri;
 struct hlist_head *head, empty_rp;
 struct hlist_node *tmp;
 unsigned long hash, flags = 0;

 if (unlikely(!kprobes_initialized))

  return;

 INIT_HLIST_HEAD(&empty_rp);
 hash = hash_ptr(tk, KPROBE_HASH_BITS);
 head = &kretprobe_inst_table[hash];
 kretprobe_table_lock(hash, &flags);
 hlist_for_each_entry_safe(ri, tmp, head, hlist) {
  if (ri->task == tk)
   recycle_rp_inst(ri, &empty_rp);
 }
 kretprobe_table_unlock(hash, &flags);
 hlist_for_each_entry_safe(ri, tmp, &empty_rp, hlist) {
  hlist_del(&ri->hlist);
  kfree(ri);
 }
}
NOKPROBE_SYMBOL(kprobe_flush_task);

static inline void free_rp_inst(struct kretprobe *rp)
{
 struct kretprobe_instance *ri;
 struct hlist_node *next;

 hlist_for_each_entry_safe(ri, next, &rp->free_instances, hlist) {
  hlist_del(&ri->hlist);
  kfree(ri);
 }
}

static void cleanup_rp_inst(struct kretprobe *rp)
{
 unsigned long flags, hash;
 struct kretprobe_instance *ri;
 struct hlist_node *next;
 struct hlist_head *head;


 for (hash = 0; hash < KPROBE_TABLE_SIZE; hash++) {
  kretprobe_table_lock(hash, &flags);
  head = &kretprobe_inst_table[hash];
  hlist_for_each_entry_safe(ri, next, head, hlist) {
   if (ri->rp == rp)
    ri->rp = NULL;
  }
  kretprobe_table_unlock(hash, &flags);
 }
 free_rp_inst(rp);
}
NOKPROBE_SYMBOL(cleanup_rp_inst);





static int add_new_kprobe(struct kprobe *ap, struct kprobe *p)
{
 BUG_ON(kprobe_gone(ap) || kprobe_gone(p));

 if (p->break_handler || p->post_handler)
  unoptimize_kprobe(ap, true);

 if (p->break_handler) {
  if (ap->break_handler)
   return -EEXIST;
  list_add_tail_rcu(&p->list, &ap->list);
  ap->break_handler = aggr_break_handler;
 } else
  list_add_rcu(&p->list, &ap->list);
 if (p->post_handler && !ap->post_handler)
  ap->post_handler = aggr_post_handler;

 return 0;
}





static void init_aggr_kprobe(struct kprobe *ap, struct kprobe *p)
{

 copy_kprobe(p, ap);
 flush_insn_slot(ap);
 ap->addr = p->addr;
 ap->flags = p->flags & ~KPROBE_FLAG_OPTIMIZED;
 ap->pre_handler = aggr_pre_handler;
 ap->fault_handler = aggr_fault_handler;

 if (p->post_handler && !kprobe_gone(p))
  ap->post_handler = aggr_post_handler;
 if (p->break_handler && !kprobe_gone(p))
  ap->break_handler = aggr_break_handler;

 INIT_LIST_HEAD(&ap->list);
 INIT_HLIST_NODE(&ap->hlist);

 list_add_rcu(&p->list, &ap->list);
 hlist_replace_rcu(&p->hlist, &ap->hlist);
}





static int register_aggr_kprobe(struct kprobe *orig_p, struct kprobe *p)
{
 int ret = 0;
 struct kprobe *ap = orig_p;


 jump_label_lock();




 get_online_cpus();
 mutex_lock(&text_mutex);

 if (!kprobe_aggrprobe(orig_p)) {

  ap = alloc_aggr_kprobe(orig_p);
  if (!ap) {
   ret = -ENOMEM;
   goto out;
  }
  init_aggr_kprobe(ap, orig_p);
 } else if (kprobe_unused(ap))

  reuse_unused_kprobe(ap);

 if (kprobe_gone(ap)) {






  ret = arch_prepare_kprobe(ap);
  if (ret)





   goto out;


  prepare_optimized_kprobe(ap);





  ap->flags = (ap->flags & ~KPROBE_FLAG_GONE)
       | KPROBE_FLAG_DISABLED;
 }


 copy_kprobe(ap, p);
 ret = add_new_kprobe(ap, p);

out:
 mutex_unlock(&text_mutex);
 put_online_cpus();
 jump_label_unlock();

 if (ret == 0 && kprobe_disabled(ap) && !kprobe_disabled(p)) {
  ap->flags &= ~KPROBE_FLAG_DISABLED;
  if (!kprobes_all_disarmed)

   arm_kprobe(ap);
 }
 return ret;
}

bool __weak arch_within_kprobe_blacklist(unsigned long addr)
{

 return addr >= (unsigned long)__kprobes_text_start &&
        addr < (unsigned long)__kprobes_text_end;
}

bool within_kprobe_blacklist(unsigned long addr)
{
 struct kprobe_blacklist_entry *ent;

 if (arch_within_kprobe_blacklist(addr))
  return true;




 list_for_each_entry(ent, &kprobe_blacklist, list) {
  if (addr >= ent->start_addr && addr < ent->end_addr)
   return true;
 }

 return false;
}







static kprobe_opcode_t *kprobe_addr(struct kprobe *p)
{
 kprobe_opcode_t *addr = p->addr;

 if ((p->symbol_name && p->addr) ||
     (!p->symbol_name && !p->addr))
  goto invalid;

 if (p->symbol_name) {
  kprobe_lookup_name(p->symbol_name, addr);
  if (!addr)
   return ERR_PTR(-ENOENT);
 }

 addr = (kprobe_opcode_t *)(((char *)addr) + p->offset);
 if (addr)
  return addr;

invalid:
 return ERR_PTR(-EINVAL);
}


static struct kprobe *__get_valid_kprobe(struct kprobe *p)
{
 struct kprobe *ap, *list_p;

 ap = get_kprobe(p->addr);
 if (unlikely(!ap))
  return NULL;

 if (p != ap) {
  list_for_each_entry_rcu(list_p, &ap->list, list)
   if (list_p == p)

    goto valid;
  return NULL;
 }
valid:
 return ap;
}


static inline int check_kprobe_rereg(struct kprobe *p)
{
 int ret = 0;

 mutex_lock(&kprobe_mutex);
 if (__get_valid_kprobe(p))
  ret = -EINVAL;
 mutex_unlock(&kprobe_mutex);

 return ret;
}

int __weak arch_check_ftrace_location(struct kprobe *p)
{
 unsigned long ftrace_addr;

 ftrace_addr = ftrace_location((unsigned long)p->addr);
 if (ftrace_addr) {

  if ((unsigned long)p->addr != ftrace_addr)
   return -EILSEQ;
  p->flags |= KPROBE_FLAG_FTRACE;
  return -EINVAL;
 }
 return 0;
}

static int check_kprobe_address_safe(struct kprobe *p,
         struct module **probed_mod)
{
 int ret;

 ret = arch_check_ftrace_location(p);
 if (ret)
  return ret;
 jump_label_lock();
 preempt_disable();


 if (!kernel_text_address((unsigned long) p->addr) ||
     within_kprobe_blacklist((unsigned long) p->addr) ||
     jump_label_text_reserved(p->addr, p->addr)) {
  ret = -EINVAL;
  goto out;
 }


 *probed_mod = __module_text_address((unsigned long) p->addr);
 if (*probed_mod) {




  if (unlikely(!try_module_get(*probed_mod))) {
   ret = -ENOENT;
   goto out;
  }





  if (within_module_init((unsigned long)p->addr, *probed_mod) &&
      (*probed_mod)->state != MODULE_STATE_COMING) {
   module_put(*probed_mod);
   *probed_mod = NULL;
   ret = -ENOENT;
  }
 }
out:
 preempt_enable();
 jump_label_unlock();

 return ret;
}

int register_kprobe(struct kprobe *p)
{
 int ret;
 struct kprobe *old_p;
 struct module *probed_mod;
 kprobe_opcode_t *addr;


 addr = kprobe_addr(p);
 if (IS_ERR(addr))
  return PTR_ERR(addr);
 p->addr = addr;

 ret = check_kprobe_rereg(p);
 if (ret)
  return ret;


 p->flags &= KPROBE_FLAG_DISABLED;
 p->nmissed = 0;
 INIT_LIST_HEAD(&p->list);

 ret = check_kprobe_address_safe(p, &probed_mod);
 if (ret)
  return ret;

 mutex_lock(&kprobe_mutex);

 old_p = get_kprobe(p->addr);
 if (old_p) {

  ret = register_aggr_kprobe(old_p, p);
  goto out;
 }

 mutex_lock(&text_mutex);
 ret = prepare_kprobe(p);
 mutex_unlock(&text_mutex);
 if (ret)
  goto out;

 INIT_HLIST_NODE(&p->hlist);
 hlist_add_head_rcu(&p->hlist,
         &kprobe_table[hash_ptr(p->addr, KPROBE_HASH_BITS)]);

 if (!kprobes_all_disarmed && !kprobe_disabled(p))
  arm_kprobe(p);


 try_to_optimize_kprobe(p);

out:
 mutex_unlock(&kprobe_mutex);

 if (probed_mod)
  module_put(probed_mod);

 return ret;
}
EXPORT_SYMBOL_GPL(register_kprobe);


static int aggr_kprobe_disabled(struct kprobe *ap)
{
 struct kprobe *kp;

 list_for_each_entry_rcu(kp, &ap->list, list)
  if (!kprobe_disabled(kp))




   return 0;

 return 1;
}


static struct kprobe *__disable_kprobe(struct kprobe *p)
{
 struct kprobe *orig_p;


 orig_p = __get_valid_kprobe(p);
 if (unlikely(orig_p == NULL))
  return NULL;

 if (!kprobe_disabled(p)) {

  if (p != orig_p)
   p->flags |= KPROBE_FLAG_DISABLED;


  if (p == orig_p || aggr_kprobe_disabled(orig_p)) {





   if (!kprobes_all_disarmed)
    disarm_kprobe(orig_p, true);
   orig_p->flags |= KPROBE_FLAG_DISABLED;
  }
 }

 return orig_p;
}




static int __unregister_kprobe_top(struct kprobe *p)
{
 struct kprobe *ap, *list_p;


 ap = __disable_kprobe(p);
 if (ap == NULL)
  return -EINVAL;

 if (ap == p)




  goto disarmed;


 WARN_ON(!kprobe_aggrprobe(ap));

 if (list_is_singular(&ap->list) && kprobe_disarmed(ap))




  goto disarmed;
 else {

  if (p->break_handler && !kprobe_gone(p))
   ap->break_handler = NULL;
  if (p->post_handler && !kprobe_gone(p)) {
   list_for_each_entry_rcu(list_p, &ap->list, list) {
    if ((list_p != p) && (list_p->post_handler))
     goto noclean;
   }
   ap->post_handler = NULL;
  }
noclean:




  list_del_rcu(&p->list);
  if (!kprobe_disabled(ap) && !kprobes_all_disarmed)




   optimize_kprobe(ap);
 }
 return 0;

disarmed:
 BUG_ON(!kprobe_disarmed(ap));
 hlist_del_rcu(&ap->hlist);
 return 0;
}

static void __unregister_kprobe_bottom(struct kprobe *p)
{
 struct kprobe *ap;

 if (list_empty(&p->list))

  arch_remove_kprobe(p);
 else if (list_is_singular(&p->list)) {

  ap = list_entry(p->list.next, struct kprobe, list);
  list_del(&p->list);
  free_aggr_kprobe(ap);
 }

}

int register_kprobes(struct kprobe **kps, int num)
{
 int i, ret = 0;

 if (num <= 0)
  return -EINVAL;
 for (i = 0; i < num; i++) {
  ret = register_kprobe(kps[i]);
  if (ret < 0) {
   if (i > 0)
    unregister_kprobes(kps, i);
   break;
  }
 }
 return ret;
}
EXPORT_SYMBOL_GPL(register_kprobes);

void unregister_kprobe(struct kprobe *p)
{
 unregister_kprobes(&p, 1);
}
EXPORT_SYMBOL_GPL(unregister_kprobe);

void unregister_kprobes(struct kprobe **kps, int num)
{
 int i;

 if (num <= 0)
  return;
 mutex_lock(&kprobe_mutex);
 for (i = 0; i < num; i++)
  if (__unregister_kprobe_top(kps[i]) < 0)
   kps[i]->addr = NULL;
 mutex_unlock(&kprobe_mutex);

 synchronize_sched();
 for (i = 0; i < num; i++)
  if (kps[i]->addr)
   __unregister_kprobe_bottom(kps[i]);
}
EXPORT_SYMBOL_GPL(unregister_kprobes);

static struct notifier_block kprobe_exceptions_nb = {
 .notifier_call = kprobe_exceptions_notify,
 .priority = 0x7fffffff
};

unsigned long __weak arch_deref_entry_point(void *entry)
{
 return (unsigned long)entry;
}

int register_jprobes(struct jprobe **jps, int num)
{
 struct jprobe *jp;
 int ret = 0, i;

 if (num <= 0)
  return -EINVAL;
 for (i = 0; i < num; i++) {
  unsigned long addr, offset;
  jp = jps[i];
  addr = arch_deref_entry_point(jp->entry);


  if (kallsyms_lookup_size_offset(addr, NULL, &offset) &&
      offset == 0) {
   jp->kp.pre_handler = setjmp_pre_handler;
   jp->kp.break_handler = longjmp_break_handler;
   ret = register_kprobe(&jp->kp);
  } else
   ret = -EINVAL;

  if (ret < 0) {
   if (i > 0)
    unregister_jprobes(jps, i);
   break;
  }
 }
 return ret;
}
EXPORT_SYMBOL_GPL(register_jprobes);

int register_jprobe(struct jprobe *jp)
{
 return register_jprobes(&jp, 1);
}
EXPORT_SYMBOL_GPL(register_jprobe);

void unregister_jprobe(struct jprobe *jp)
{
 unregister_jprobes(&jp, 1);
}
EXPORT_SYMBOL_GPL(unregister_jprobe);

void unregister_jprobes(struct jprobe **jps, int num)
{
 int i;

 if (num <= 0)
  return;
 mutex_lock(&kprobe_mutex);
 for (i = 0; i < num; i++)
  if (__unregister_kprobe_top(&jps[i]->kp) < 0)
   jps[i]->kp.addr = NULL;
 mutex_unlock(&kprobe_mutex);

 synchronize_sched();
 for (i = 0; i < num; i++) {
  if (jps[i]->kp.addr)
   __unregister_kprobe_bottom(&jps[i]->kp);
 }
}
EXPORT_SYMBOL_GPL(unregister_jprobes);





static int pre_handler_kretprobe(struct kprobe *p, struct pt_regs *regs)
{
 struct kretprobe *rp = container_of(p, struct kretprobe, kp);
 unsigned long hash, flags = 0;
 struct kretprobe_instance *ri;







 if (unlikely(in_nmi())) {
  rp->nmissed++;
  return 0;
 }


 hash = hash_ptr(current, KPROBE_HASH_BITS);
 raw_spin_lock_irqsave(&rp->lock, flags);
 if (!hlist_empty(&rp->free_instances)) {
  ri = hlist_entry(rp->free_instances.first,
    struct kretprobe_instance, hlist);
  hlist_del(&ri->hlist);
  raw_spin_unlock_irqrestore(&rp->lock, flags);

  ri->rp = rp;
  ri->task = current;

  if (rp->entry_handler && rp->entry_handler(ri, regs)) {
   raw_spin_lock_irqsave(&rp->lock, flags);
   hlist_add_head(&ri->hlist, &rp->free_instances);
   raw_spin_unlock_irqrestore(&rp->lock, flags);
   return 0;
  }

  arch_prepare_kretprobe(ri, regs);


  INIT_HLIST_NODE(&ri->hlist);
  kretprobe_table_lock(hash, &flags);
  hlist_add_head(&ri->hlist, &kretprobe_inst_table[hash]);
  kretprobe_table_unlock(hash, &flags);
 } else {
  rp->nmissed++;
  raw_spin_unlock_irqrestore(&rp->lock, flags);
 }
 return 0;
}
NOKPROBE_SYMBOL(pre_handler_kretprobe);

int register_kretprobe(struct kretprobe *rp)
{
 int ret = 0;
 struct kretprobe_instance *inst;
 int i;
 void *addr;

 if (kretprobe_blacklist_size) {
  addr = kprobe_addr(&rp->kp);
  if (IS_ERR(addr))
   return PTR_ERR(addr);

  for (i = 0; kretprobe_blacklist[i].name != NULL; i++) {
   if (kretprobe_blacklist[i].addr == addr)
    return -EINVAL;
  }
 }

 rp->kp.pre_handler = pre_handler_kretprobe;
 rp->kp.post_handler = NULL;
 rp->kp.fault_handler = NULL;
 rp->kp.break_handler = NULL;


 if (rp->maxactive <= 0) {
  rp->maxactive = max_t(unsigned int, 10, 2*num_possible_cpus());
  rp->maxactive = num_possible_cpus();
 }
 raw_spin_lock_init(&rp->lock);
 INIT_HLIST_HEAD(&rp->free_instances);
 for (i = 0; i < rp->maxactive; i++) {
  inst = kmalloc(sizeof(struct kretprobe_instance) +
          rp->data_size, GFP_KERNEL);
  if (inst == NULL) {
   free_rp_inst(rp);
   return -ENOMEM;
  }
  INIT_HLIST_NODE(&inst->hlist);
  hlist_add_head(&inst->hlist, &rp->free_instances);
 }

 rp->nmissed = 0;

 ret = register_kprobe(&rp->kp);
 if (ret != 0)
  free_rp_inst(rp);
 return ret;
}
EXPORT_SYMBOL_GPL(register_kretprobe);

int register_kretprobes(struct kretprobe **rps, int num)
{
 int ret = 0, i;

 if (num <= 0)
  return -EINVAL;
 for (i = 0; i < num; i++) {
  ret = register_kretprobe(rps[i]);
  if (ret < 0) {
   if (i > 0)
    unregister_kretprobes(rps, i);
   break;
  }
 }
 return ret;
}
EXPORT_SYMBOL_GPL(register_kretprobes);

void unregister_kretprobe(struct kretprobe *rp)
{
 unregister_kretprobes(&rp, 1);
}
EXPORT_SYMBOL_GPL(unregister_kretprobe);

void unregister_kretprobes(struct kretprobe **rps, int num)
{
 int i;

 if (num <= 0)
  return;
 mutex_lock(&kprobe_mutex);
 for (i = 0; i < num; i++)
  if (__unregister_kprobe_top(&rps[i]->kp) < 0)
   rps[i]->kp.addr = NULL;
 mutex_unlock(&kprobe_mutex);

 synchronize_sched();
 for (i = 0; i < num; i++) {
  if (rps[i]->kp.addr) {
   __unregister_kprobe_bottom(&rps[i]->kp);
   cleanup_rp_inst(rps[i]);
  }
 }
}
EXPORT_SYMBOL_GPL(unregister_kretprobes);

int register_kretprobe(struct kretprobe *rp)
{
 return -ENOSYS;
}
EXPORT_SYMBOL_GPL(register_kretprobe);

int register_kretprobes(struct kretprobe **rps, int num)
{
 return -ENOSYS;
}
EXPORT_SYMBOL_GPL(register_kretprobes);

void unregister_kretprobe(struct kretprobe *rp)
{
}
EXPORT_SYMBOL_GPL(unregister_kretprobe);

void unregister_kretprobes(struct kretprobe **rps, int num)
{
}
EXPORT_SYMBOL_GPL(unregister_kretprobes);

static int pre_handler_kretprobe(struct kprobe *p, struct pt_regs *regs)
{
 return 0;
}
NOKPROBE_SYMBOL(pre_handler_kretprobe);



static void kill_kprobe(struct kprobe *p)
{
 struct kprobe *kp;

 p->flags |= KPROBE_FLAG_GONE;
 if (kprobe_aggrprobe(p)) {




  list_for_each_entry_rcu(kp, &p->list, list)
   kp->flags |= KPROBE_FLAG_GONE;
  p->post_handler = NULL;
  p->break_handler = NULL;
  kill_optimized_kprobe(p);
 }




 arch_remove_kprobe(p);
}


int disable_kprobe(struct kprobe *kp)
{
 int ret = 0;

 mutex_lock(&kprobe_mutex);


 if (__disable_kprobe(kp) == NULL)
  ret = -EINVAL;

 mutex_unlock(&kprobe_mutex);
 return ret;
}
EXPORT_SYMBOL_GPL(disable_kprobe);


int enable_kprobe(struct kprobe *kp)
{
 int ret = 0;
 struct kprobe *p;

 mutex_lock(&kprobe_mutex);


 p = __get_valid_kprobe(kp);
 if (unlikely(p == NULL)) {
  ret = -EINVAL;
  goto out;
 }

 if (kprobe_gone(kp)) {

  ret = -EINVAL;
  goto out;
 }

 if (p != kp)
  kp->flags &= ~KPROBE_FLAG_DISABLED;

 if (!kprobes_all_disarmed && kprobe_disabled(p)) {
  p->flags &= ~KPROBE_FLAG_DISABLED;
  arm_kprobe(p);
 }
out:
 mutex_unlock(&kprobe_mutex);
 return ret;
}
EXPORT_SYMBOL_GPL(enable_kprobe);

void dump_kprobe(struct kprobe *kp)
{
 printk(KERN_WARNING "Dumping kprobe:\n");
 printk(KERN_WARNING "Name: %s\nAddress: %p\nOffset: %x\n",
        kp->symbol_name, kp->addr, kp->offset);
}
NOKPROBE_SYMBOL(dump_kprobe);
static int __init populate_kprobe_blacklist(unsigned long *start,
          unsigned long *end)
{
 unsigned long *iter;
 struct kprobe_blacklist_entry *ent;
 unsigned long entry, offset = 0, size = 0;

 for (iter = start; iter < end; iter++) {
  entry = arch_deref_entry_point((void *)*iter);

  if (!kernel_text_address(entry) ||
      !kallsyms_lookup_size_offset(entry, &size, &offset)) {
   pr_err("Failed to find blacklist at %p\n",
    (void *)entry);
   continue;
  }

  ent = kmalloc(sizeof(*ent), GFP_KERNEL);
  if (!ent)
   return -ENOMEM;
  ent->start_addr = entry;
  ent->end_addr = entry + size;
  INIT_LIST_HEAD(&ent->list);
  list_add_tail(&ent->list, &kprobe_blacklist);
 }
 return 0;
}


static int kprobes_module_callback(struct notifier_block *nb,
       unsigned long val, void *data)
{
 struct module *mod = data;
 struct hlist_head *head;
 struct kprobe *p;
 unsigned int i;
 int checkcore = (val == MODULE_STATE_GOING);

 if (val != MODULE_STATE_GOING && val != MODULE_STATE_LIVE)
  return NOTIFY_DONE;







 mutex_lock(&kprobe_mutex);
 for (i = 0; i < KPROBE_TABLE_SIZE; i++) {
  head = &kprobe_table[i];
  hlist_for_each_entry_rcu(p, head, hlist)
   if (within_module_init((unsigned long)p->addr, mod) ||
       (checkcore &&
        within_module_core((unsigned long)p->addr, mod))) {





    kill_kprobe(p);
   }
 }
 mutex_unlock(&kprobe_mutex);
 return NOTIFY_DONE;
}

static struct notifier_block kprobe_module_nb = {
 .notifier_call = kprobes_module_callback,
 .priority = 0
};


extern unsigned long __start_kprobe_blacklist[];
extern unsigned long __stop_kprobe_blacklist[];

static int __init init_kprobes(void)
{
 int i, err = 0;



 for (i = 0; i < KPROBE_TABLE_SIZE; i++) {
  INIT_HLIST_HEAD(&kprobe_table[i]);
  INIT_HLIST_HEAD(&kretprobe_inst_table[i]);
  raw_spin_lock_init(&(kretprobe_table_locks[i].lock));
 }

 err = populate_kprobe_blacklist(__start_kprobe_blacklist,
     __stop_kprobe_blacklist);
 if (err) {
  pr_err("kprobes: failed to populate blacklist: %d\n", err);
  pr_err("Please take care of using kprobes.\n");
 }

 if (kretprobe_blacklist_size) {

  for (i = 0; kretprobe_blacklist[i].name != NULL; i++) {
   kprobe_lookup_name(kretprobe_blacklist[i].name,
        kretprobe_blacklist[i].addr);
   if (!kretprobe_blacklist[i].addr)
    printk("kretprobe: lookup failed: %s\n",
           kretprobe_blacklist[i].name);
  }
 }


 kprobe_optinsn_slots.insn_size = MAX_OPTINSN_SIZE;

 kprobes_allow_optimization = true;


 kprobes_all_disarmed = false;

 err = arch_init_kprobes();
 if (!err)
  err = register_die_notifier(&kprobe_exceptions_nb);
 if (!err)
  err = register_module_notifier(&kprobe_module_nb);

 kprobes_initialized = (err == 0);

 if (!err)
  init_test_probes();
 return err;
}

static void report_probe(struct seq_file *pi, struct kprobe *p,
  const char *sym, int offset, char *modname, struct kprobe *pp)
{
 char *kprobe_type;

 if (p->pre_handler == pre_handler_kretprobe)
  kprobe_type = "r";
 else if (p->pre_handler == setjmp_pre_handler)
  kprobe_type = "j";
 else
  kprobe_type = "k";

 if (sym)
  seq_printf(pi, "%p  %s  %s+0x%x  %s ",
   p->addr, kprobe_type, sym, offset,
   (modname ? modname : " "));
 else
  seq_printf(pi, "%p  %s  %p ",
   p->addr, kprobe_type, p->addr);

 if (!pp)
  pp = p;
 seq_printf(pi, "%s%s%s%s\n",
  (kprobe_gone(p) ? "[GONE]" : ""),
  ((kprobe_disabled(p) && !kprobe_gone(p)) ? "[DISABLED]" : ""),
  (kprobe_optimized(pp) ? "[OPTIMIZED]" : ""),
  (kprobe_ftrace(pp) ? "[FTRACE]" : ""));
}

static void *kprobe_seq_start(struct seq_file *f, loff_t *pos)
{
 return (*pos < KPROBE_TABLE_SIZE) ? pos : NULL;
}

static void *kprobe_seq_next(struct seq_file *f, void *v, loff_t *pos)
{
 (*pos)++;
 if (*pos >= KPROBE_TABLE_SIZE)
  return NULL;
 return pos;
}

static void kprobe_seq_stop(struct seq_file *f, void *v)
{

}

static int show_kprobe_addr(struct seq_file *pi, void *v)
{
 struct hlist_head *head;
 struct kprobe *p, *kp;
 const char *sym = NULL;
 unsigned int i = *(loff_t *) v;
 unsigned long offset = 0;
 char *modname, namebuf[KSYM_NAME_LEN];

 head = &kprobe_table[i];
 preempt_disable();
 hlist_for_each_entry_rcu(p, head, hlist) {
  sym = kallsyms_lookup((unsigned long)p->addr, NULL,
     &offset, &modname, namebuf);
  if (kprobe_aggrprobe(p)) {
   list_for_each_entry_rcu(kp, &p->list, list)
    report_probe(pi, kp, sym, offset, modname, p);
  } else
   report_probe(pi, p, sym, offset, modname, NULL);
 }
 preempt_enable();
 return 0;
}

static const struct seq_operations kprobes_seq_ops = {
 .start = kprobe_seq_start,
 .next = kprobe_seq_next,
 .stop = kprobe_seq_stop,
 .show = show_kprobe_addr
};

static int kprobes_open(struct inode *inode, struct file *filp)
{
 return seq_open(filp, &kprobes_seq_ops);
}

static const struct file_operations debugfs_kprobes_operations = {
 .open = kprobes_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = seq_release,
};


static void *kprobe_blacklist_seq_start(struct seq_file *m, loff_t *pos)
{
 return seq_list_start(&kprobe_blacklist, *pos);
}

static void *kprobe_blacklist_seq_next(struct seq_file *m, void *v, loff_t *pos)
{
 return seq_list_next(v, &kprobe_blacklist, pos);
}

static int kprobe_blacklist_seq_show(struct seq_file *m, void *v)
{
 struct kprobe_blacklist_entry *ent =
  list_entry(v, struct kprobe_blacklist_entry, list);

 seq_printf(m, "0x%p-0x%p\t%ps\n", (void *)ent->start_addr,
     (void *)ent->end_addr, (void *)ent->start_addr);
 return 0;
}

static const struct seq_operations kprobe_blacklist_seq_ops = {
 .start = kprobe_blacklist_seq_start,
 .next = kprobe_blacklist_seq_next,
 .stop = kprobe_seq_stop,
 .show = kprobe_blacklist_seq_show,
};

static int kprobe_blacklist_open(struct inode *inode, struct file *filp)
{
 return seq_open(filp, &kprobe_blacklist_seq_ops);
}

static const struct file_operations debugfs_kprobe_blacklist_ops = {
 .open = kprobe_blacklist_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = seq_release,
};

static void arm_all_kprobes(void)
{
 struct hlist_head *head;
 struct kprobe *p;
 unsigned int i;

 mutex_lock(&kprobe_mutex);


 if (!kprobes_all_disarmed)
  goto already_enabled;






 kprobes_all_disarmed = false;

 for (i = 0; i < KPROBE_TABLE_SIZE; i++) {
  head = &kprobe_table[i];
  hlist_for_each_entry_rcu(p, head, hlist)
   if (!kprobe_disabled(p))
    arm_kprobe(p);
 }

 printk(KERN_INFO "Kprobes globally enabled\n");

already_enabled:
 mutex_unlock(&kprobe_mutex);
 return;
}

static void disarm_all_kprobes(void)
{
 struct hlist_head *head;
 struct kprobe *p;
 unsigned int i;

 mutex_lock(&kprobe_mutex);


 if (kprobes_all_disarmed) {
  mutex_unlock(&kprobe_mutex);
  return;
 }

 kprobes_all_disarmed = true;
 printk(KERN_INFO "Kprobes globally disabled\n");

 for (i = 0; i < KPROBE_TABLE_SIZE; i++) {
  head = &kprobe_table[i];
  hlist_for_each_entry_rcu(p, head, hlist) {
   if (!arch_trampoline_kprobe(p) && !kprobe_disabled(p))
    disarm_kprobe(p, false);
  }
 }
 mutex_unlock(&kprobe_mutex);


 wait_for_kprobe_optimizer();
}






static ssize_t read_enabled_file_bool(struct file *file,
        char __user *user_buf, size_t count, loff_t *ppos)
{
 char buf[3];

 if (!kprobes_all_disarmed)
  buf[0] = '1';
 else
  buf[0] = '0';
 buf[1] = '\n';
 buf[2] = 0x00;
 return simple_read_from_buffer(user_buf, count, ppos, buf, 2);
}

static ssize_t write_enabled_file_bool(struct file *file,
        const char __user *user_buf, size_t count, loff_t *ppos)
{
 char buf[32];
 size_t buf_size;

 buf_size = min(count, (sizeof(buf)-1));
 if (copy_from_user(buf, user_buf, buf_size))
  return -EFAULT;

 buf[buf_size] = '\0';
 switch (buf[0]) {
 case 'y':
 case 'Y':
 case '1':
  arm_all_kprobes();
  break;
 case 'n':
 case 'N':
 case '0':
  disarm_all_kprobes();
  break;
 default:
  return -EINVAL;
 }

 return count;
}

static const struct file_operations fops_kp = {
 .read = read_enabled_file_bool,
 .write = write_enabled_file_bool,
 .llseek = default_llseek,
};

static int __init debugfs_kprobe_init(void)
{
 struct dentry *dir, *file;
 unsigned int value = 1;

 dir = debugfs_create_dir("kprobes", NULL);
 if (!dir)
  return -ENOMEM;

 file = debugfs_create_file("list", 0444, dir, NULL,
    &debugfs_kprobes_operations);
 if (!file)
  goto error;

 file = debugfs_create_file("enabled", 0600, dir,
     &value, &fops_kp);
 if (!file)
  goto error;

 file = debugfs_create_file("blacklist", 0444, dir, NULL,
    &debugfs_kprobe_blacklist_ops);
 if (!file)
  goto error;

 return 0;

error:
 debugfs_remove(dir);
 return -ENOMEM;
}

late_initcall(debugfs_kprobe_init);

module_init(init_kprobes);


EXPORT_SYMBOL_GPL(jprobe_return);


static struct kobj_attribute _name##_attr = __ATTR_RO(_name)

static struct kobj_attribute _name##_attr = \
 __ATTR(_name, 0644, _name##_show, _name##_store)


static ssize_t uevent_seqnum_show(struct kobject *kobj,
      struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%llu\n", (unsigned long long)uevent_seqnum);
}
KERNEL_ATTR_RO(uevent_seqnum);


static ssize_t uevent_helper_show(struct kobject *kobj,
      struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%s\n", uevent_helper);
}
static ssize_t uevent_helper_store(struct kobject *kobj,
       struct kobj_attribute *attr,
       const char *buf, size_t count)
{
 if (count+1 > UEVENT_HELPER_PATH_LEN)
  return -ENOENT;
 memcpy(uevent_helper, buf, count);
 uevent_helper[count] = '\0';
 if (count && uevent_helper[count-1] == '\n')
  uevent_helper[count-1] = '\0';
 return count;
}
KERNEL_ATTR_RW(uevent_helper);

static ssize_t profiling_show(struct kobject *kobj,
      struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%d\n", prof_on);
}
static ssize_t profiling_store(struct kobject *kobj,
       struct kobj_attribute *attr,
       const char *buf, size_t count)
{
 int ret;

 if (prof_on)
  return -EEXIST;





 profile_setup((char *)buf);
 ret = profile_init();
 if (ret)
  return ret;
 ret = create_proc_profile();
 if (ret)
  return ret;
 return count;
}
KERNEL_ATTR_RW(profiling);

static ssize_t kexec_loaded_show(struct kobject *kobj,
     struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%d\n", !!kexec_image);
}
KERNEL_ATTR_RO(kexec_loaded);

static ssize_t kexec_crash_loaded_show(struct kobject *kobj,
           struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%d\n", !!kexec_crash_image);
}
KERNEL_ATTR_RO(kexec_crash_loaded);

static ssize_t kexec_crash_size_show(struct kobject *kobj,
           struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%zu\n", crash_get_memory_size());
}
static ssize_t kexec_crash_size_store(struct kobject *kobj,
       struct kobj_attribute *attr,
       const char *buf, size_t count)
{
 unsigned long cnt;
 int ret;

 if (kstrtoul(buf, 0, &cnt))
  return -EINVAL;

 ret = crash_shrink_memory(cnt);
 return ret < 0 ? ret : count;
}
KERNEL_ATTR_RW(kexec_crash_size);

static ssize_t vmcoreinfo_show(struct kobject *kobj,
          struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%lx %x\n",
         paddr_vmcoreinfo_note(),
         (unsigned int)sizeof(vmcoreinfo_note));
}
KERNEL_ATTR_RO(vmcoreinfo);



static ssize_t fscaps_show(struct kobject *kobj,
      struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%d\n", file_caps_enabled);
}
KERNEL_ATTR_RO(fscaps);

int rcu_expedited;
static ssize_t rcu_expedited_show(struct kobject *kobj,
      struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%d\n", READ_ONCE(rcu_expedited));
}
static ssize_t rcu_expedited_store(struct kobject *kobj,
       struct kobj_attribute *attr,
       const char *buf, size_t count)
{
 if (kstrtoint(buf, 0, &rcu_expedited))
  return -EINVAL;

 return count;
}
KERNEL_ATTR_RW(rcu_expedited);

int rcu_normal;
static ssize_t rcu_normal_show(struct kobject *kobj,
          struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%d\n", READ_ONCE(rcu_normal));
}
static ssize_t rcu_normal_store(struct kobject *kobj,
    struct kobj_attribute *attr,
    const char *buf, size_t count)
{
 if (kstrtoint(buf, 0, &rcu_normal))
  return -EINVAL;

 return count;
}
KERNEL_ATTR_RW(rcu_normal);




extern const void __start_notes __weak;
extern const void __stop_notes __weak;

static ssize_t notes_read(struct file *filp, struct kobject *kobj,
     struct bin_attribute *bin_attr,
     char *buf, loff_t off, size_t count)
{
 memcpy(buf, &__start_notes + off, count);
 return count;
}

static struct bin_attribute notes_attr = {
 .attr = {
  .name = "notes",
  .mode = S_IRUGO,
 },
 .read = &notes_read,
};

struct kobject *kernel_kobj;
EXPORT_SYMBOL_GPL(kernel_kobj);

static struct attribute * kernel_attrs[] = {
 &fscaps_attr.attr,
 &uevent_seqnum_attr.attr,
 &uevent_helper_attr.attr,
 &profiling_attr.attr,
 &kexec_loaded_attr.attr,
 &kexec_crash_loaded_attr.attr,
 &kexec_crash_size_attr.attr,
 &vmcoreinfo_attr.attr,
 &rcu_expedited_attr.attr,
 &rcu_normal_attr.attr,
 NULL
};

static struct attribute_group kernel_attr_group = {
 .attrs = kernel_attrs,
};

static int __init ksysfs_init(void)
{
 int error;

 kernel_kobj = kobject_create_and_add("kernel", NULL);
 if (!kernel_kobj) {
  error = -ENOMEM;
  goto exit;
 }
 error = sysfs_create_group(kernel_kobj, &kernel_attr_group);
 if (error)
  goto kset_exit;

 if (notes_size > 0) {
  notes_attr.size = notes_size;
  error = sysfs_create_bin_file(kernel_kobj, &notes_attr);
  if (error)
   goto group_exit;
 }

 return 0;

group_exit:
 sysfs_remove_group(kernel_kobj, &kernel_attr_group);
kset_exit:
 kobject_put(kernel_kobj);
exit:
 return error;
}

core_initcall(ksysfs_init);








static DEFINE_SPINLOCK(kthread_create_lock);
static LIST_HEAD(kthread_create_list);
struct task_struct *kthreadd_task;

struct kthread_create_info
{

 int (*threadfn)(void *data);
 void *data;
 int node;


 struct task_struct *result;
 struct completion *done;

 struct list_head list;
};

struct kthread {
 unsigned long flags;
 unsigned int cpu;
 void *data;
 struct completion parked;
 struct completion exited;
};

enum KTHREAD_BITS {
 KTHREAD_IS_PER_CPU = 0,
 KTHREAD_SHOULD_STOP,
 KTHREAD_SHOULD_PARK,
 KTHREAD_IS_PARKED,
};

 container_of(vfork, struct kthread, exited)

static inline struct kthread *to_kthread(struct task_struct *k)
{
 return __to_kthread(k->vfork_done);
}

static struct kthread *to_live_kthread(struct task_struct *k)
{
 struct completion *vfork = ACCESS_ONCE(k->vfork_done);
 if (likely(vfork))
  return __to_kthread(vfork);
 return NULL;
}
bool kthread_should_stop(void)
{
 return test_bit(KTHREAD_SHOULD_STOP, &to_kthread(current)->flags);
}
EXPORT_SYMBOL(kthread_should_stop);
bool kthread_should_park(void)
{
 return test_bit(KTHREAD_SHOULD_PARK, &to_kthread(current)->flags);
}
EXPORT_SYMBOL_GPL(kthread_should_park);
bool kthread_freezable_should_stop(bool *was_frozen)
{
 bool frozen = false;

 might_sleep();

 if (unlikely(freezing(current)))
  frozen = __refrigerator(true);

 if (was_frozen)
  *was_frozen = frozen;

 return kthread_should_stop();
}
EXPORT_SYMBOL_GPL(kthread_freezable_should_stop);
void *kthread_data(struct task_struct *task)
{
 return to_kthread(task)->data;
}
void *probe_kthread_data(struct task_struct *task)
{
 struct kthread *kthread = to_kthread(task);
 void *data = NULL;

 probe_kernel_read(&data, &kthread->data, sizeof(data));
 return data;
}

static void __kthread_parkme(struct kthread *self)
{
 __set_current_state(TASK_PARKED);
 while (test_bit(KTHREAD_SHOULD_PARK, &self->flags)) {
  if (!test_and_set_bit(KTHREAD_IS_PARKED, &self->flags))
   complete(&self->parked);
  schedule();
  __set_current_state(TASK_PARKED);
 }
 clear_bit(KTHREAD_IS_PARKED, &self->flags);
 __set_current_state(TASK_RUNNING);
}

void kthread_parkme(void)
{
 __kthread_parkme(to_kthread(current));
}
EXPORT_SYMBOL_GPL(kthread_parkme);

static int kthread(void *_create)
{

 struct kthread_create_info *create = _create;
 int (*threadfn)(void *data) = create->threadfn;
 void *data = create->data;
 struct completion *done;
 struct kthread self;
 int ret;

 self.flags = 0;
 self.data = data;
 init_completion(&self.exited);
 init_completion(&self.parked);
 current->vfork_done = &self.exited;


 done = xchg(&create->done, NULL);
 if (!done) {
  kfree(create);
  do_exit(-EINTR);
 }

 __set_current_state(TASK_UNINTERRUPTIBLE);
 create->result = current;
 complete(done);
 schedule();

 ret = -EINTR;

 if (!test_bit(KTHREAD_SHOULD_STOP, &self.flags)) {
  __kthread_parkme(&self);
  ret = threadfn(data);
 }

 do_exit(ret);
}


int tsk_fork_get_node(struct task_struct *tsk)
{
 if (tsk == kthreadd_task)
  return tsk->pref_node_fork;
 return NUMA_NO_NODE;
}

static void create_kthread(struct kthread_create_info *create)
{
 int pid;

 current->pref_node_fork = create->node;

 pid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);
 if (pid < 0) {

  struct completion *done = xchg(&create->done, NULL);

  if (!done) {
   kfree(create);
   return;
  }
  create->result = ERR_PTR(pid);
  complete(done);
 }
}
struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
        void *data, int node,
        const char namefmt[],
        ...)
{
 DECLARE_COMPLETION_ONSTACK(done);
 struct task_struct *task;
 struct kthread_create_info *create = kmalloc(sizeof(*create),
           GFP_KERNEL);

 if (!create)
  return ERR_PTR(-ENOMEM);
 create->threadfn = threadfn;
 create->data = data;
 create->node = node;
 create->done = &done;

 spin_lock(&kthread_create_lock);
 list_add_tail(&create->list, &kthread_create_list);
 spin_unlock(&kthread_create_lock);

 wake_up_process(kthreadd_task);





 if (unlikely(wait_for_completion_killable(&done))) {





  if (xchg(&create->done, NULL))
   return ERR_PTR(-EINTR);




  wait_for_completion(&done);
 }
 task = create->result;
 if (!IS_ERR(task)) {
  static const struct sched_param param = { .sched_priority = 0 };
  va_list args;

  va_start(args, namefmt);
  vsnprintf(task->comm, sizeof(task->comm), namefmt, args);
  va_end(args);




  sched_setscheduler_nocheck(task, SCHED_NORMAL, &param);
  set_cpus_allowed_ptr(task, cpu_all_mask);
 }
 kfree(create);
 return task;
}
EXPORT_SYMBOL(kthread_create_on_node);

static void __kthread_bind_mask(struct task_struct *p, const struct cpumask *mask, long state)
{
 unsigned long flags;

 if (!wait_task_inactive(p, state)) {
  WARN_ON(1);
  return;
 }


 raw_spin_lock_irqsave(&p->pi_lock, flags);
 do_set_cpus_allowed(p, mask);
 p->flags |= PF_NO_SETAFFINITY;
 raw_spin_unlock_irqrestore(&p->pi_lock, flags);
}

static void __kthread_bind(struct task_struct *p, unsigned int cpu, long state)
{
 __kthread_bind_mask(p, cpumask_of(cpu), state);
}

void kthread_bind_mask(struct task_struct *p, const struct cpumask *mask)
{
 __kthread_bind_mask(p, mask, TASK_UNINTERRUPTIBLE);
}
void kthread_bind(struct task_struct *p, unsigned int cpu)
{
 __kthread_bind(p, cpu, TASK_UNINTERRUPTIBLE);
}
EXPORT_SYMBOL(kthread_bind);
struct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),
       void *data, unsigned int cpu,
       const char *namefmt)
{
 struct task_struct *p;

 p = kthread_create_on_node(threadfn, data, cpu_to_node(cpu), namefmt,
       cpu);
 if (IS_ERR(p))
  return p;
 set_bit(KTHREAD_IS_PER_CPU, &to_kthread(p)->flags);
 to_kthread(p)->cpu = cpu;

 kthread_park(p);
 return p;
}

static void __kthread_unpark(struct task_struct *k, struct kthread *kthread)
{
 clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);






 if (test_and_clear_bit(KTHREAD_IS_PARKED, &kthread->flags)) {
  if (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))
   __kthread_bind(k, kthread->cpu, TASK_PARKED);
  wake_up_state(k, TASK_PARKED);
 }
}
void kthread_unpark(struct task_struct *k)
{
 struct kthread *kthread = to_live_kthread(k);

 if (kthread)
  __kthread_unpark(k, kthread);
}
EXPORT_SYMBOL_GPL(kthread_unpark);
int kthread_park(struct task_struct *k)
{
 struct kthread *kthread = to_live_kthread(k);
 int ret = -ENOSYS;

 if (kthread) {
  if (!test_bit(KTHREAD_IS_PARKED, &kthread->flags)) {
   set_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
   if (k != current) {
    wake_up_process(k);
    wait_for_completion(&kthread->parked);
   }
  }
  ret = 0;
 }
 return ret;
}
EXPORT_SYMBOL_GPL(kthread_park);
int kthread_stop(struct task_struct *k)
{
 struct kthread *kthread;
 int ret;

 trace_sched_kthread_stop(k);

 get_task_struct(k);
 kthread = to_live_kthread(k);
 if (kthread) {
  set_bit(KTHREAD_SHOULD_STOP, &kthread->flags);
  __kthread_unpark(k, kthread);
  wake_up_process(k);
  wait_for_completion(&kthread->exited);
 }
 ret = k->exit_code;
 put_task_struct(k);

 trace_sched_kthread_stop_ret(ret);
 return ret;
}
EXPORT_SYMBOL(kthread_stop);

int kthreadd(void *unused)
{
 struct task_struct *tsk = current;


 set_task_comm(tsk, "kthreadd");
 ignore_signals(tsk);
 set_cpus_allowed_ptr(tsk, cpu_all_mask);
 set_mems_allowed(node_states[N_MEMORY]);

 current->flags |= PF_NOFREEZE;

 for (;;) {
  set_current_state(TASK_INTERRUPTIBLE);
  if (list_empty(&kthread_create_list))
   schedule();
  __set_current_state(TASK_RUNNING);

  spin_lock(&kthread_create_lock);
  while (!list_empty(&kthread_create_list)) {
   struct kthread_create_info *create;

   create = list_entry(kthread_create_list.next,
         struct kthread_create_info, list);
   list_del_init(&create->list);
   spin_unlock(&kthread_create_lock);

   create_kthread(create);

   spin_lock(&kthread_create_lock);
  }
  spin_unlock(&kthread_create_lock);
 }

 return 0;
}

void __init_kthread_worker(struct kthread_worker *worker,
    const char *name,
    struct lock_class_key *key)
{
 spin_lock_init(&worker->lock);
 lockdep_set_class_and_name(&worker->lock, key, name);
 INIT_LIST_HEAD(&worker->work_list);
 worker->task = NULL;
}
EXPORT_SYMBOL_GPL(__init_kthread_worker);
int kthread_worker_fn(void *worker_ptr)
{
 struct kthread_worker *worker = worker_ptr;
 struct kthread_work *work;

 WARN_ON(worker->task);
 worker->task = current;
repeat:
 set_current_state(TASK_INTERRUPTIBLE);

 if (kthread_should_stop()) {
  __set_current_state(TASK_RUNNING);
  spin_lock_irq(&worker->lock);
  worker->task = NULL;
  spin_unlock_irq(&worker->lock);
  return 0;
 }

 work = NULL;
 spin_lock_irq(&worker->lock);
 if (!list_empty(&worker->work_list)) {
  work = list_first_entry(&worker->work_list,
     struct kthread_work, node);
  list_del_init(&work->node);
 }
 worker->current_work = work;
 spin_unlock_irq(&worker->lock);

 if (work) {
  __set_current_state(TASK_RUNNING);
  work->func(work);
 } else if (!freezing(current))
  schedule();

 try_to_freeze();
 goto repeat;
}
EXPORT_SYMBOL_GPL(kthread_worker_fn);


static void insert_kthread_work(struct kthread_worker *worker,
          struct kthread_work *work,
          struct list_head *pos)
{
 lockdep_assert_held(&worker->lock);

 list_add_tail(&work->node, pos);
 work->worker = worker;
 if (!worker->current_work && likely(worker->task))
  wake_up_process(worker->task);
}
bool queue_kthread_work(struct kthread_worker *worker,
   struct kthread_work *work)
{
 bool ret = false;
 unsigned long flags;

 spin_lock_irqsave(&worker->lock, flags);
 if (list_empty(&work->node)) {
  insert_kthread_work(worker, work, &worker->work_list);
  ret = true;
 }
 spin_unlock_irqrestore(&worker->lock, flags);
 return ret;
}
EXPORT_SYMBOL_GPL(queue_kthread_work);

struct kthread_flush_work {
 struct kthread_work work;
 struct completion done;
};

static void kthread_flush_work_fn(struct kthread_work *work)
{
 struct kthread_flush_work *fwork =
  container_of(work, struct kthread_flush_work, work);
 complete(&fwork->done);
}







void flush_kthread_work(struct kthread_work *work)
{
 struct kthread_flush_work fwork = {
  KTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),
  COMPLETION_INITIALIZER_ONSTACK(fwork.done),
 };
 struct kthread_worker *worker;
 bool noop = false;

retry:
 worker = work->worker;
 if (!worker)
  return;

 spin_lock_irq(&worker->lock);
 if (work->worker != worker) {
  spin_unlock_irq(&worker->lock);
  goto retry;
 }

 if (!list_empty(&work->node))
  insert_kthread_work(worker, &fwork.work, work->node.next);
 else if (worker->current_work == work)
  insert_kthread_work(worker, &fwork.work, worker->work_list.next);
 else
  noop = true;

 spin_unlock_irq(&worker->lock);

 if (!noop)
  wait_for_completion(&fwork.done);
}
EXPORT_SYMBOL_GPL(flush_kthread_work);
void flush_kthread_worker(struct kthread_worker *worker)
{
 struct kthread_flush_work fwork = {
  KTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),
  COMPLETION_INITIALIZER_ONSTACK(fwork.done),
 };

 queue_kthread_work(worker, &fwork.work);
 wait_for_completion(&fwork.done);
}
EXPORT_SYMBOL_GPL(flush_kthread_worker);

static DEFINE_RAW_SPINLOCK(latency_lock);

static struct latency_record latency_record[MAXLR];

int latencytop_enabled;

void clear_all_latency_tracing(struct task_struct *p)
{
 unsigned long flags;

 if (!latencytop_enabled)
  return;

 raw_spin_lock_irqsave(&latency_lock, flags);
 memset(&p->latency_record, 0, sizeof(p->latency_record));
 p->latency_record_count = 0;
 raw_spin_unlock_irqrestore(&latency_lock, flags);
}

static void clear_global_latency_tracing(void)
{
 unsigned long flags;

 raw_spin_lock_irqsave(&latency_lock, flags);
 memset(&latency_record, 0, sizeof(latency_record));
 raw_spin_unlock_irqrestore(&latency_lock, flags);
}

static void __sched
account_global_scheduler_latency(struct task_struct *tsk,
     struct latency_record *lat)
{
 int firstnonnull = MAXLR + 1;
 int i;

 if (!latencytop_enabled)
  return;


 if (!tsk->mm)
  return;

 for (i = 0; i < MAXLR; i++) {
  int q, same = 1;


  if (!latency_record[i].backtrace[0]) {
   if (firstnonnull > i)
    firstnonnull = i;
   continue;
  }
  for (q = 0; q < LT_BACKTRACEDEPTH; q++) {
   unsigned long record = lat->backtrace[q];

   if (latency_record[i].backtrace[q] != record) {
    same = 0;
    break;
   }


   if (record == 0 || record == ULONG_MAX)
    break;
  }
  if (same) {
   latency_record[i].count++;
   latency_record[i].time += lat->time;
   if (lat->time > latency_record[i].max)
    latency_record[i].max = lat->time;
   return;
  }
 }

 i = firstnonnull;
 if (i >= MAXLR - 1)
  return;


 memcpy(&latency_record[i], lat, sizeof(struct latency_record));
}




static inline void store_stacktrace(struct task_struct *tsk,
     struct latency_record *lat)
{
 struct stack_trace trace;

 memset(&trace, 0, sizeof(trace));
 trace.max_entries = LT_BACKTRACEDEPTH;
 trace.entries = &lat->backtrace[0];
 save_stack_trace_tsk(tsk, &trace);
}
void __sched
__account_scheduler_latency(struct task_struct *tsk, int usecs, int inter)
{
 unsigned long flags;
 int i, q;
 struct latency_record lat;


 if (inter && usecs > 5000)
  return;



 if (usecs <= 0)
  return;

 memset(&lat, 0, sizeof(lat));
 lat.count = 1;
 lat.time = usecs;
 lat.max = usecs;
 store_stacktrace(tsk, &lat);

 raw_spin_lock_irqsave(&latency_lock, flags);

 account_global_scheduler_latency(tsk, &lat);

 for (i = 0; i < tsk->latency_record_count; i++) {
  struct latency_record *mylat;
  int same = 1;

  mylat = &tsk->latency_record[i];
  for (q = 0; q < LT_BACKTRACEDEPTH; q++) {
   unsigned long record = lat.backtrace[q];

   if (mylat->backtrace[q] != record) {
    same = 0;
    break;
   }


   if (record == 0 || record == ULONG_MAX)
    break;
  }
  if (same) {
   mylat->count++;
   mylat->time += lat.time;
   if (lat.time > mylat->max)
    mylat->max = lat.time;
   goto out_unlock;
  }
 }




 if (tsk->latency_record_count >= LT_SAVECOUNT)
  goto out_unlock;


 i = tsk->latency_record_count++;
 memcpy(&tsk->latency_record[i], &lat, sizeof(struct latency_record));

out_unlock:
 raw_spin_unlock_irqrestore(&latency_lock, flags);
}

static int lstats_show(struct seq_file *m, void *v)
{
 int i;

 seq_puts(m, "Latency Top version : v0.1\n");

 for (i = 0; i < MAXLR; i++) {
  struct latency_record *lr = &latency_record[i];

  if (lr->backtrace[0]) {
   int q;
   seq_printf(m, "%i %lu %lu",
       lr->count, lr->time, lr->max);
   for (q = 0; q < LT_BACKTRACEDEPTH; q++) {
    unsigned long bt = lr->backtrace[q];
    if (!bt)
     break;
    if (bt == ULONG_MAX)
     break;
    seq_printf(m, " %ps", (void *)bt);
   }
   seq_puts(m, "\n");
  }
 }
 return 0;
}

static ssize_t
lstats_write(struct file *file, const char __user *buf, size_t count,
      loff_t *offs)
{
 clear_global_latency_tracing();

 return count;
}

static int lstats_open(struct inode *inode, struct file *filp)
{
 return single_open(filp, lstats_show, NULL);
}

static const struct file_operations lstats_fops = {
 .open = lstats_open,
 .read = seq_read,
 .write = lstats_write,
 .llseek = seq_lseek,
 .release = single_release,
};

static int __init init_lstats_procfs(void)
{
 proc_create("latency_stats", 0644, NULL, &lstats_fops);
 return 0;
}

int sysctl_latencytop(struct ctl_table *table, int write,
   void __user *buffer, size_t *lenp, loff_t *ppos)
{
 int err;

 err = proc_dointvec(table, write, buffer, lenp, ppos);
 if (latencytop_enabled)
  force_schedstat_enabled();

 return err;
}
device_initcall(init_lstats_procfs);


DEFINE_MUTEX(pm_mutex);




static BLOCKING_NOTIFIER_HEAD(pm_chain_head);

int register_pm_notifier(struct notifier_block *nb)
{
 return blocking_notifier_chain_register(&pm_chain_head, nb);
}
EXPORT_SYMBOL_GPL(register_pm_notifier);

int unregister_pm_notifier(struct notifier_block *nb)
{
 return blocking_notifier_chain_unregister(&pm_chain_head, nb);
}
EXPORT_SYMBOL_GPL(unregister_pm_notifier);

int pm_notifier_call_chain(unsigned long val)
{
 int ret = blocking_notifier_call_chain(&pm_chain_head, val, NULL);

 return notifier_to_errno(ret);
}


int pm_async_enabled = 1;

static ssize_t pm_async_show(struct kobject *kobj, struct kobj_attribute *attr,
        char *buf)
{
 return sprintf(buf, "%d\n", pm_async_enabled);
}

static ssize_t pm_async_store(struct kobject *kobj, struct kobj_attribute *attr,
         const char *buf, size_t n)
{
 unsigned long val;

 if (kstrtoul(buf, 10, &val))
  return -EINVAL;

 if (val > 1)
  return -EINVAL;

 pm_async_enabled = val;
 return n;
}

power_attr(pm_async);

int pm_test_level = TEST_NONE;

static const char * const pm_tests[__TEST_AFTER_LAST] = {
 [TEST_NONE] = "none",
 [TEST_CORE] = "core",
 [TEST_CPUS] = "processors",
 [TEST_PLATFORM] = "platform",
 [TEST_DEVICES] = "devices",
 [TEST_FREEZER] = "freezer",
};

static ssize_t pm_test_show(struct kobject *kobj, struct kobj_attribute *attr,
    char *buf)
{
 char *s = buf;
 int level;

 for (level = TEST_FIRST; level <= TEST_MAX; level++)
  if (pm_tests[level]) {
   if (level == pm_test_level)
    s += sprintf(s, "[%s] ", pm_tests[level]);
   else
    s += sprintf(s, "%s ", pm_tests[level]);
  }

 if (s != buf)

  *(s-1) = '\n';

 return (s - buf);
}

static ssize_t pm_test_store(struct kobject *kobj, struct kobj_attribute *attr,
    const char *buf, size_t n)
{
 const char * const *s;
 int level;
 char *p;
 int len;
 int error = -EINVAL;

 p = memchr(buf, '\n', n);
 len = p ? p - buf : n;

 lock_system_sleep();

 level = TEST_FIRST;
 for (s = &pm_tests[level]; level <= TEST_MAX; s++, level++)
  if (*s && len == strlen(*s) && !strncmp(buf, *s, len)) {
   pm_test_level = level;
   error = 0;
   break;
  }

 unlock_system_sleep();

 return error ? error : n;
}

power_attr(pm_test);

static char *suspend_step_name(enum suspend_stat_step step)
{
 switch (step) {
 case SUSPEND_FREEZE:
  return "freeze";
 case SUSPEND_PREPARE:
  return "prepare";
 case SUSPEND_SUSPEND:
  return "suspend";
 case SUSPEND_SUSPEND_NOIRQ:
  return "suspend_noirq";
 case SUSPEND_RESUME_NOIRQ:
  return "resume_noirq";
 case SUSPEND_RESUME:
  return "resume";
 default:
  return "";
 }
}

static int suspend_stats_show(struct seq_file *s, void *unused)
{
 int i, index, last_dev, last_errno, last_step;

 last_dev = suspend_stats.last_failed_dev + REC_FAILED_NUM - 1;
 last_dev %= REC_FAILED_NUM;
 last_errno = suspend_stats.last_failed_errno + REC_FAILED_NUM - 1;
 last_errno %= REC_FAILED_NUM;
 last_step = suspend_stats.last_failed_step + REC_FAILED_NUM - 1;
 last_step %= REC_FAILED_NUM;
 seq_printf(s, "%s: %d\n%s: %d\n%s: %d\n%s: %d\n%s: %d\n"
   "%s: %d\n%s: %d\n%s: %d\n%s: %d\n%s: %d\n",
   "success", suspend_stats.success,
   "fail", suspend_stats.fail,
   "failed_freeze", suspend_stats.failed_freeze,
   "failed_prepare", suspend_stats.failed_prepare,
   "failed_suspend", suspend_stats.failed_suspend,
   "failed_suspend_late",
    suspend_stats.failed_suspend_late,
   "failed_suspend_noirq",
    suspend_stats.failed_suspend_noirq,
   "failed_resume", suspend_stats.failed_resume,
   "failed_resume_early",
    suspend_stats.failed_resume_early,
   "failed_resume_noirq",
    suspend_stats.failed_resume_noirq);
 seq_printf(s, "failures:\n  last_failed_dev:\t%-s\n",
   suspend_stats.failed_devs[last_dev]);
 for (i = 1; i < REC_FAILED_NUM; i++) {
  index = last_dev + REC_FAILED_NUM - i;
  index %= REC_FAILED_NUM;
  seq_printf(s, "\t\t\t%-s\n",
   suspend_stats.failed_devs[index]);
 }
 seq_printf(s, "  last_failed_errno:\t%-d\n",
   suspend_stats.errno[last_errno]);
 for (i = 1; i < REC_FAILED_NUM; i++) {
  index = last_errno + REC_FAILED_NUM - i;
  index %= REC_FAILED_NUM;
  seq_printf(s, "\t\t\t%-d\n",
   suspend_stats.errno[index]);
 }
 seq_printf(s, "  last_failed_step:\t%-s\n",
   suspend_step_name(
    suspend_stats.failed_steps[last_step]));
 for (i = 1; i < REC_FAILED_NUM; i++) {
  index = last_step + REC_FAILED_NUM - i;
  index %= REC_FAILED_NUM;
  seq_printf(s, "\t\t\t%-s\n",
   suspend_step_name(
    suspend_stats.failed_steps[index]));
 }

 return 0;
}

static int suspend_stats_open(struct inode *inode, struct file *file)
{
 return single_open(file, suspend_stats_show, NULL);
}

static const struct file_operations suspend_stats_operations = {
 .open = suspend_stats_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
};

static int __init pm_debugfs_init(void)
{
 debugfs_create_file("suspend_stats", S_IFREG | S_IRUGO,
   NULL, NULL, &suspend_stats_operations);
 return 0;
}

late_initcall(pm_debugfs_init);








bool pm_print_times_enabled;

static ssize_t pm_print_times_show(struct kobject *kobj,
       struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%d\n", pm_print_times_enabled);
}

static ssize_t pm_print_times_store(struct kobject *kobj,
        struct kobj_attribute *attr,
        const char *buf, size_t n)
{
 unsigned long val;

 if (kstrtoul(buf, 10, &val))
  return -EINVAL;

 if (val > 1)
  return -EINVAL;

 pm_print_times_enabled = !!val;
 return n;
}

power_attr(pm_print_times);

static inline void pm_print_times_init(void)
{
 pm_print_times_enabled = !!initcall_debug;
}

static ssize_t pm_wakeup_irq_show(struct kobject *kobj,
     struct kobj_attribute *attr,
     char *buf)
{
 return pm_wakeup_irq ? sprintf(buf, "%u\n", pm_wakeup_irq) : -ENODATA;
}

power_attr_ro(pm_wakeup_irq);

static inline void pm_print_times_init(void) {}

struct kobject *power_kobj;
static ssize_t state_show(struct kobject *kobj, struct kobj_attribute *attr,
     char *buf)
{
 char *s = buf;
 suspend_state_t i;

 for (i = PM_SUSPEND_MIN; i < PM_SUSPEND_MAX; i++)
  if (pm_states[i])
   s += sprintf(s,"%s ", pm_states[i]);

 if (hibernation_available())
  s += sprintf(s, "disk ");
 if (s != buf)

  *(s-1) = '\n';
 return (s - buf);
}

static suspend_state_t decode_state(const char *buf, size_t n)
{
 suspend_state_t state;
 char *p;
 int len;

 p = memchr(buf, '\n', n);
 len = p ? p - buf : n;


 if (len == 4 && !strncmp(buf, "disk", len))
  return PM_SUSPEND_MAX;

 for (state = PM_SUSPEND_MIN; state < PM_SUSPEND_MAX; state++) {
  const char *label = pm_states[state];

  if (label && len == strlen(label) && !strncmp(buf, label, len))
   return state;
 }

 return PM_SUSPEND_ON;
}

static ssize_t state_store(struct kobject *kobj, struct kobj_attribute *attr,
      const char *buf, size_t n)
{
 suspend_state_t state;
 int error;

 error = pm_autosleep_lock();
 if (error)
  return error;

 if (pm_autosleep_state() > PM_SUSPEND_ON) {
  error = -EBUSY;
  goto out;
 }

 state = decode_state(buf, n);
 if (state < PM_SUSPEND_MAX)
  error = pm_suspend(state);
 else if (state == PM_SUSPEND_MAX)
  error = hibernate();
 else
  error = -EINVAL;

 out:
 pm_autosleep_unlock();
 return error ? error : n;
}

power_attr(state);

static ssize_t wakeup_count_show(struct kobject *kobj,
    struct kobj_attribute *attr,
    char *buf)
{
 unsigned int val;

 return pm_get_wakeup_count(&val, true) ?
  sprintf(buf, "%u\n", val) : -EINTR;
}

static ssize_t wakeup_count_store(struct kobject *kobj,
    struct kobj_attribute *attr,
    const char *buf, size_t n)
{
 unsigned int val;
 int error;

 error = pm_autosleep_lock();
 if (error)
  return error;

 if (pm_autosleep_state() > PM_SUSPEND_ON) {
  error = -EBUSY;
  goto out;
 }

 error = -EINVAL;
 if (sscanf(buf, "%u", &val) == 1) {
  if (pm_save_wakeup_count(val))
   error = n;
  else
   pm_print_active_wakeup_sources();
 }

 out:
 pm_autosleep_unlock();
 return error;
}

power_attr(wakeup_count);

static ssize_t autosleep_show(struct kobject *kobj,
         struct kobj_attribute *attr,
         char *buf)
{
 suspend_state_t state = pm_autosleep_state();

 if (state == PM_SUSPEND_ON)
  return sprintf(buf, "off\n");

 if (state < PM_SUSPEND_MAX)
  return sprintf(buf, "%s\n", pm_states[state] ?
     pm_states[state] : "error");
 return sprintf(buf, "disk\n");
 return sprintf(buf, "error");
}

static ssize_t autosleep_store(struct kobject *kobj,
          struct kobj_attribute *attr,
          const char *buf, size_t n)
{
 suspend_state_t state = decode_state(buf, n);
 int error;

 if (state == PM_SUSPEND_ON
     && strcmp(buf, "off") && strcmp(buf, "off\n"))
  return -EINVAL;

 error = pm_autosleep_set_state(state);
 return error ? error : n;
}

power_attr(autosleep);

static ssize_t wake_lock_show(struct kobject *kobj,
         struct kobj_attribute *attr,
         char *buf)
{
 return pm_show_wakelocks(buf, true);
}

static ssize_t wake_lock_store(struct kobject *kobj,
          struct kobj_attribute *attr,
          const char *buf, size_t n)
{
 int error = pm_wake_lock(buf);
 return error ? error : n;
}

power_attr(wake_lock);

static ssize_t wake_unlock_show(struct kobject *kobj,
    struct kobj_attribute *attr,
    char *buf)
{
 return pm_show_wakelocks(buf, false);
}

static ssize_t wake_unlock_store(struct kobject *kobj,
     struct kobj_attribute *attr,
     const char *buf, size_t n)
{
 int error = pm_wake_unlock(buf);
 return error ? error : n;
}

power_attr(wake_unlock);


int pm_trace_enabled;

static ssize_t pm_trace_show(struct kobject *kobj, struct kobj_attribute *attr,
        char *buf)
{
 return sprintf(buf, "%d\n", pm_trace_enabled);
}

static ssize_t
pm_trace_store(struct kobject *kobj, struct kobj_attribute *attr,
        const char *buf, size_t n)
{
 int val;

 if (sscanf(buf, "%d", &val) == 1) {
  pm_trace_enabled = !!val;
  if (pm_trace_enabled) {
   pr_warn("PM: Enabling pm_trace changes system date and time during resume.\n"
    "PM: Correct system time has to be restored manually after resume.\n");
  }
  return n;
 }
 return -EINVAL;
}

power_attr(pm_trace);

static ssize_t pm_trace_dev_match_show(struct kobject *kobj,
           struct kobj_attribute *attr,
           char *buf)
{
 return show_trace_dev_match(buf, PAGE_SIZE);
}

power_attr_ro(pm_trace_dev_match);


static ssize_t pm_freeze_timeout_show(struct kobject *kobj,
          struct kobj_attribute *attr, char *buf)
{
 return sprintf(buf, "%u\n", freeze_timeout_msecs);
}

static ssize_t pm_freeze_timeout_store(struct kobject *kobj,
           struct kobj_attribute *attr,
           const char *buf, size_t n)
{
 unsigned long val;

 if (kstrtoul(buf, 10, &val))
  return -EINVAL;

 freeze_timeout_msecs = val;
 return n;
}

power_attr(pm_freeze_timeout);


static struct attribute * g[] = {
 &state_attr.attr,
 &pm_trace_attr.attr,
 &pm_trace_dev_match_attr.attr,
 &pm_async_attr.attr,
 &wakeup_count_attr.attr,
 &autosleep_attr.attr,
 &wake_lock_attr.attr,
 &wake_unlock_attr.attr,
 &pm_test_attr.attr,
 &pm_print_times_attr.attr,
 &pm_wakeup_irq_attr.attr,
 &pm_freeze_timeout_attr.attr,
 NULL,
};

static struct attribute_group attr_group = {
 .attrs = g,
};

struct workqueue_struct *pm_wq;
EXPORT_SYMBOL_GPL(pm_wq);

static int __init pm_start_workqueue(void)
{
 pm_wq = alloc_workqueue("pm", WQ_FREEZABLE, 0);

 return pm_wq ? 0 : -ENOMEM;
}

static int __init pm_init(void)
{
 int error = pm_start_workqueue();
 if (error)
  return error;
 hibernate_image_size_init();
 hibernate_reserved_size_init();
 power_kobj = kobject_create_and_add("power", NULL);
 if (!power_kobj)
  return -ENOMEM;
 error = sysfs_create_group(power_kobj, &attr_group);
 if (error)
  return error;
 pm_print_times_init();
 return pm_autosleep_init();
}

core_initcall(pm_init);



__read_mostly bool force_irqthreads;

static int __init setup_forced_irqthreads(char *arg)
{
 force_irqthreads = true;
 return 0;
}
early_param("threadirqs", setup_forced_irqthreads);

static void __synchronize_hardirq(struct irq_desc *desc)
{
 bool inprogress;

 do {
  unsigned long flags;





  while (irqd_irq_inprogress(&desc->irq_data))
   cpu_relax();


  raw_spin_lock_irqsave(&desc->lock, flags);
  inprogress = irqd_irq_inprogress(&desc->irq_data);
  raw_spin_unlock_irqrestore(&desc->lock, flags);


 } while (inprogress);
}
bool synchronize_hardirq(unsigned int irq)
{
 struct irq_desc *desc = irq_to_desc(irq);

 if (desc) {
  __synchronize_hardirq(desc);
  return !atomic_read(&desc->threads_active);
 }

 return true;
}
EXPORT_SYMBOL(synchronize_hardirq);
void synchronize_irq(unsigned int irq)
{
 struct irq_desc *desc = irq_to_desc(irq);

 if (desc) {
  __synchronize_hardirq(desc);





  wait_event(desc->wait_for_threads,
      !atomic_read(&desc->threads_active));
 }
}
EXPORT_SYMBOL(synchronize_irq);

cpumask_var_t irq_default_affinity;

static int __irq_can_set_affinity(struct irq_desc *desc)
{
 if (!desc || !irqd_can_balance(&desc->irq_data) ||
     !desc->irq_data.chip || !desc->irq_data.chip->irq_set_affinity)
  return 0;
 return 1;
}






int irq_can_set_affinity(unsigned int irq)
{
 return __irq_can_set_affinity(irq_to_desc(irq));
}
void irq_set_thread_affinity(struct irq_desc *desc)
{
 struct irqaction *action;

 for_each_action_of_desc(desc, action)
  if (action->thread)
   set_bit(IRQTF_AFFINITY, &action->thread_flags);
}

static inline bool irq_can_move_pcntxt(struct irq_data *data)
{
 return irqd_can_move_in_process_context(data);
}
static inline bool irq_move_pending(struct irq_data *data)
{
 return irqd_is_setaffinity_pending(data);
}
static inline void
irq_copy_pending(struct irq_desc *desc, const struct cpumask *mask)
{
 cpumask_copy(desc->pending_mask, mask);
}
static inline void
irq_get_pending(struct cpumask *mask, struct irq_desc *desc)
{
 cpumask_copy(mask, desc->pending_mask);
}
static inline bool irq_can_move_pcntxt(struct irq_data *data) { return true; }
static inline bool irq_move_pending(struct irq_data *data) { return false; }
static inline void
irq_copy_pending(struct irq_desc *desc, const struct cpumask *mask) { }
static inline void
irq_get_pending(struct cpumask *mask, struct irq_desc *desc) { }

int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,
   bool force)
{
 struct irq_desc *desc = irq_data_to_desc(data);
 struct irq_chip *chip = irq_data_get_irq_chip(data);
 int ret;

 ret = chip->irq_set_affinity(data, mask, force);
 switch (ret) {
 case IRQ_SET_MASK_OK:
 case IRQ_SET_MASK_OK_DONE:
  cpumask_copy(desc->irq_common_data.affinity, mask);
 case IRQ_SET_MASK_OK_NOCOPY:
  irq_set_thread_affinity(desc);
  ret = 0;
 }

 return ret;
}

int irq_set_affinity_locked(struct irq_data *data, const struct cpumask *mask,
       bool force)
{
 struct irq_chip *chip = irq_data_get_irq_chip(data);
 struct irq_desc *desc = irq_data_to_desc(data);
 int ret = 0;

 if (!chip || !chip->irq_set_affinity)
  return -EINVAL;

 if (irq_can_move_pcntxt(data)) {
  ret = irq_do_set_affinity(data, mask, force);
 } else {
  irqd_set_move_pending(data);
  irq_copy_pending(desc, mask);
 }

 if (desc->affinity_notify) {
  kref_get(&desc->affinity_notify->kref);
  schedule_work(&desc->affinity_notify->work);
 }
 irqd_set(data, IRQD_AFFINITY_SET);

 return ret;
}

int __irq_set_affinity(unsigned int irq, const struct cpumask *mask, bool force)
{
 struct irq_desc *desc = irq_to_desc(irq);
 unsigned long flags;
 int ret;

 if (!desc)
  return -EINVAL;

 raw_spin_lock_irqsave(&desc->lock, flags);
 ret = irq_set_affinity_locked(irq_desc_get_irq_data(desc), mask, force);
 raw_spin_unlock_irqrestore(&desc->lock, flags);
 return ret;
}

int irq_set_affinity_hint(unsigned int irq, const struct cpumask *m)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_lock(irq, &flags, IRQ_GET_DESC_CHECK_GLOBAL);

 if (!desc)
  return -EINVAL;
 desc->affinity_hint = m;
 irq_put_desc_unlock(desc, flags);

 if (m)
  __irq_set_affinity(irq, m, false);
 return 0;
}
EXPORT_SYMBOL_GPL(irq_set_affinity_hint);

static void irq_affinity_notify(struct work_struct *work)
{
 struct irq_affinity_notify *notify =
  container_of(work, struct irq_affinity_notify, work);
 struct irq_desc *desc = irq_to_desc(notify->irq);
 cpumask_var_t cpumask;
 unsigned long flags;

 if (!desc || !alloc_cpumask_var(&cpumask, GFP_KERNEL))
  goto out;

 raw_spin_lock_irqsave(&desc->lock, flags);
 if (irq_move_pending(&desc->irq_data))
  irq_get_pending(cpumask, desc);
 else
  cpumask_copy(cpumask, desc->irq_common_data.affinity);
 raw_spin_unlock_irqrestore(&desc->lock, flags);

 notify->notify(notify, cpumask);

 free_cpumask_var(cpumask);
out:
 kref_put(&notify->kref, notify->release);
}
int
irq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify)
{
 struct irq_desc *desc = irq_to_desc(irq);
 struct irq_affinity_notify *old_notify;
 unsigned long flags;


 might_sleep();

 if (!desc)
  return -EINVAL;


 if (notify) {
  notify->irq = irq;
  kref_init(&notify->kref);
  INIT_WORK(&notify->work, irq_affinity_notify);
 }

 raw_spin_lock_irqsave(&desc->lock, flags);
 old_notify = desc->affinity_notify;
 desc->affinity_notify = notify;
 raw_spin_unlock_irqrestore(&desc->lock, flags);

 if (old_notify)
  kref_put(&old_notify->kref, old_notify->release);

 return 0;
}
EXPORT_SYMBOL_GPL(irq_set_affinity_notifier);




static int setup_affinity(struct irq_desc *desc, struct cpumask *mask)
{
 struct cpumask *set = irq_default_affinity;
 int node = irq_desc_get_node(desc);


 if (!__irq_can_set_affinity(desc))
  return 0;





 if (irqd_has_set(&desc->irq_data, IRQD_AFFINITY_SET)) {
  if (cpumask_intersects(desc->irq_common_data.affinity,
           cpu_online_mask))
   set = desc->irq_common_data.affinity;
  else
   irqd_clear(&desc->irq_data, IRQD_AFFINITY_SET);
 }

 cpumask_and(mask, cpu_online_mask, set);
 if (node != NUMA_NO_NODE) {
  const struct cpumask *nodemask = cpumask_of_node(node);


  if (cpumask_intersects(mask, nodemask))
   cpumask_and(mask, mask, nodemask);
 }
 irq_do_set_affinity(&desc->irq_data, mask, false);
 return 0;
}

static inline int setup_affinity(struct irq_desc *d, struct cpumask *mask)
{
 return irq_select_affinity(irq_desc_get_irq(d));
}




int irq_select_affinity_usr(unsigned int irq, struct cpumask *mask)
{
 struct irq_desc *desc = irq_to_desc(irq);
 unsigned long flags;
 int ret;

 raw_spin_lock_irqsave(&desc->lock, flags);
 ret = setup_affinity(desc, mask);
 raw_spin_unlock_irqrestore(&desc->lock, flags);
 return ret;
}

static inline int
setup_affinity(struct irq_desc *desc, struct cpumask *mask)
{
 return 0;
}
int irq_set_vcpu_affinity(unsigned int irq, void *vcpu_info)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_lock(irq, &flags, 0);
 struct irq_data *data;
 struct irq_chip *chip;
 int ret = -ENOSYS;

 if (!desc)
  return -EINVAL;

 data = irq_desc_get_irq_data(desc);
 chip = irq_data_get_irq_chip(data);
 if (chip && chip->irq_set_vcpu_affinity)
  ret = chip->irq_set_vcpu_affinity(data, vcpu_info);
 irq_put_desc_unlock(desc, flags);

 return ret;
}
EXPORT_SYMBOL_GPL(irq_set_vcpu_affinity);

void __disable_irq(struct irq_desc *desc)
{
 if (!desc->depth++)
  irq_disable(desc);
}

static int __disable_irq_nosync(unsigned int irq)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_buslock(irq, &flags, IRQ_GET_DESC_CHECK_GLOBAL);

 if (!desc)
  return -EINVAL;
 __disable_irq(desc);
 irq_put_desc_busunlock(desc, flags);
 return 0;
}
void disable_irq_nosync(unsigned int irq)
{
 __disable_irq_nosync(irq);
}
EXPORT_SYMBOL(disable_irq_nosync);
void disable_irq(unsigned int irq)
{
 if (!__disable_irq_nosync(irq))
  synchronize_irq(irq);
}
EXPORT_SYMBOL(disable_irq);
bool disable_hardirq(unsigned int irq)
{
 if (!__disable_irq_nosync(irq))
  return synchronize_hardirq(irq);

 return false;
}
EXPORT_SYMBOL_GPL(disable_hardirq);

void __enable_irq(struct irq_desc *desc)
{
 switch (desc->depth) {
 case 0:
 err_out:
  WARN(1, KERN_WARNING "Unbalanced enable for IRQ %d\n",
       irq_desc_get_irq(desc));
  break;
 case 1: {
  if (desc->istate & IRQS_SUSPENDED)
   goto err_out;

  irq_settings_set_noprobe(desc);
  irq_enable(desc);
  check_irq_resend(desc);

 }
 default:
  desc->depth--;
 }
}
void enable_irq(unsigned int irq)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_buslock(irq, &flags, IRQ_GET_DESC_CHECK_GLOBAL);

 if (!desc)
  return;
 if (WARN(!desc->irq_data.chip,
   KERN_ERR "enable_irq before setup/request_irq: irq %u\n", irq))
  goto out;

 __enable_irq(desc);
out:
 irq_put_desc_busunlock(desc, flags);
}
EXPORT_SYMBOL(enable_irq);

static int set_irq_wake_real(unsigned int irq, unsigned int on)
{
 struct irq_desc *desc = irq_to_desc(irq);
 int ret = -ENXIO;

 if (irq_desc_get_chip(desc)->flags & IRQCHIP_SKIP_SET_WAKE)
  return 0;

 if (desc->irq_data.chip->irq_set_wake)
  ret = desc->irq_data.chip->irq_set_wake(&desc->irq_data, on);

 return ret;
}
int irq_set_irq_wake(unsigned int irq, unsigned int on)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_buslock(irq, &flags, IRQ_GET_DESC_CHECK_GLOBAL);
 int ret = 0;

 if (!desc)
  return -EINVAL;




 if (on) {
  if (desc->wake_depth++ == 0) {
   ret = set_irq_wake_real(irq, on);
   if (ret)
    desc->wake_depth = 0;
   else
    irqd_set(&desc->irq_data, IRQD_WAKEUP_STATE);
  }
 } else {
  if (desc->wake_depth == 0) {
   WARN(1, "Unbalanced IRQ %d wake disable\n", irq);
  } else if (--desc->wake_depth == 0) {
   ret = set_irq_wake_real(irq, on);
   if (ret)
    desc->wake_depth = 1;
   else
    irqd_clear(&desc->irq_data, IRQD_WAKEUP_STATE);
  }
 }
 irq_put_desc_busunlock(desc, flags);
 return ret;
}
EXPORT_SYMBOL(irq_set_irq_wake);






int can_request_irq(unsigned int irq, unsigned long irqflags)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_lock(irq, &flags, 0);
 int canrequest = 0;

 if (!desc)
  return 0;

 if (irq_settings_can_request(desc)) {
  if (!desc->action ||
      irqflags & desc->action->flags & IRQF_SHARED)
   canrequest = 1;
 }
 irq_put_desc_unlock(desc, flags);
 return canrequest;
}

int __irq_set_trigger(struct irq_desc *desc, unsigned long flags)
{
 struct irq_chip *chip = desc->irq_data.chip;
 int ret, unmask = 0;

 if (!chip || !chip->irq_set_type) {




  pr_debug("No set_type function for IRQ %d (%s)\n",
    irq_desc_get_irq(desc),
    chip ? (chip->name ? : "unknown") : "unknown");
  return 0;
 }

 flags &= IRQ_TYPE_SENSE_MASK;

 if (chip->flags & IRQCHIP_SET_TYPE_MASKED) {
  if (!irqd_irq_masked(&desc->irq_data))
   mask_irq(desc);
  if (!irqd_irq_disabled(&desc->irq_data))
   unmask = 1;
 }


 ret = chip->irq_set_type(&desc->irq_data, flags);

 switch (ret) {
 case IRQ_SET_MASK_OK:
 case IRQ_SET_MASK_OK_DONE:
  irqd_clear(&desc->irq_data, IRQD_TRIGGER_MASK);
  irqd_set(&desc->irq_data, flags);

 case IRQ_SET_MASK_OK_NOCOPY:
  flags = irqd_get_trigger_type(&desc->irq_data);
  irq_settings_set_trigger_mask(desc, flags);
  irqd_clear(&desc->irq_data, IRQD_LEVEL);
  irq_settings_clr_level(desc);
  if (flags & IRQ_TYPE_LEVEL_MASK) {
   irq_settings_set_level(desc);
   irqd_set(&desc->irq_data, IRQD_LEVEL);
  }

  ret = 0;
  break;
 default:
  pr_err("Setting trigger mode %lu for irq %u failed (%pF)\n",
         flags, irq_desc_get_irq(desc), chip->irq_set_type);
 }
 if (unmask)
  unmask_irq(desc);
 return ret;
}

int irq_set_parent(int irq, int parent_irq)
{
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_lock(irq, &flags, 0);

 if (!desc)
  return -EINVAL;

 desc->parent_irq = parent_irq;

 irq_put_desc_unlock(desc, flags);
 return 0;
}






static irqreturn_t irq_default_primary_handler(int irq, void *dev_id)
{
 return IRQ_WAKE_THREAD;
}





static irqreturn_t irq_nested_primary_handler(int irq, void *dev_id)
{
 WARN(1, "Primary handler called for nested irq %d\n", irq);
 return IRQ_NONE;
}

static irqreturn_t irq_forced_secondary_handler(int irq, void *dev_id)
{
 WARN(1, "Secondary action handler called for irq %d\n", irq);
 return IRQ_NONE;
}

static int irq_wait_for_interrupt(struct irqaction *action)
{
 set_current_state(TASK_INTERRUPTIBLE);

 while (!kthread_should_stop()) {

  if (test_and_clear_bit(IRQTF_RUNTHREAD,
           &action->thread_flags)) {
   __set_current_state(TASK_RUNNING);
   return 0;
  }
  schedule();
  set_current_state(TASK_INTERRUPTIBLE);
 }
 __set_current_state(TASK_RUNNING);
 return -1;
}






static void irq_finalize_oneshot(struct irq_desc *desc,
     struct irqaction *action)
{
 if (!(desc->istate & IRQS_ONESHOT) ||
     action->handler == irq_forced_secondary_handler)
  return;
again:
 chip_bus_lock(desc);
 raw_spin_lock_irq(&desc->lock);
 if (unlikely(irqd_irq_inprogress(&desc->irq_data))) {
  raw_spin_unlock_irq(&desc->lock);
  chip_bus_sync_unlock(desc);
  cpu_relax();
  goto again;
 }






 if (test_bit(IRQTF_RUNTHREAD, &action->thread_flags))
  goto out_unlock;

 desc->threads_oneshot &= ~action->thread_mask;

 if (!desc->threads_oneshot && !irqd_irq_disabled(&desc->irq_data) &&
     irqd_irq_masked(&desc->irq_data))
  unmask_threaded_irq(desc);

out_unlock:
 raw_spin_unlock_irq(&desc->lock);
 chip_bus_sync_unlock(desc);
}




static void
irq_thread_check_affinity(struct irq_desc *desc, struct irqaction *action)
{
 cpumask_var_t mask;
 bool valid = true;

 if (!test_and_clear_bit(IRQTF_AFFINITY, &action->thread_flags))
  return;





 if (!alloc_cpumask_var(&mask, GFP_KERNEL)) {
  set_bit(IRQTF_AFFINITY, &action->thread_flags);
  return;
 }

 raw_spin_lock_irq(&desc->lock);




 if (desc->irq_common_data.affinity)
  cpumask_copy(mask, desc->irq_common_data.affinity);
 else
  valid = false;
 raw_spin_unlock_irq(&desc->lock);

 if (valid)
  set_cpus_allowed_ptr(current, mask);
 free_cpumask_var(mask);
}
static inline void
irq_thread_check_affinity(struct irq_desc *desc, struct irqaction *action) { }







static irqreturn_t
irq_forced_thread_fn(struct irq_desc *desc, struct irqaction *action)
{
 irqreturn_t ret;

 local_bh_disable();
 ret = action->thread_fn(action->irq, action->dev_id);
 irq_finalize_oneshot(desc, action);
 local_bh_enable();
 return ret;
}






static irqreturn_t irq_thread_fn(struct irq_desc *desc,
  struct irqaction *action)
{
 irqreturn_t ret;

 ret = action->thread_fn(action->irq, action->dev_id);
 irq_finalize_oneshot(desc, action);
 return ret;
}

static void wake_threads_waitq(struct irq_desc *desc)
{
 if (atomic_dec_and_test(&desc->threads_active))
  wake_up(&desc->wait_for_threads);
}

static void irq_thread_dtor(struct callback_head *unused)
{
 struct task_struct *tsk = current;
 struct irq_desc *desc;
 struct irqaction *action;

 if (WARN_ON_ONCE(!(current->flags & PF_EXITING)))
  return;

 action = kthread_data(tsk);

 pr_err("exiting task \"%s\" (%d) is an active IRQ thread (irq %d)\n",
        tsk->comm, tsk->pid, action->irq);


 desc = irq_to_desc(action->irq);




 if (test_and_clear_bit(IRQTF_RUNTHREAD, &action->thread_flags))
  wake_threads_waitq(desc);


 irq_finalize_oneshot(desc, action);
}

static void irq_wake_secondary(struct irq_desc *desc, struct irqaction *action)
{
 struct irqaction *secondary = action->secondary;

 if (WARN_ON_ONCE(!secondary))
  return;

 raw_spin_lock_irq(&desc->lock);
 __irq_wake_thread(desc, secondary);
 raw_spin_unlock_irq(&desc->lock);
}




static int irq_thread(void *data)
{
 struct callback_head on_exit_work;
 struct irqaction *action = data;
 struct irq_desc *desc = irq_to_desc(action->irq);
 irqreturn_t (*handler_fn)(struct irq_desc *desc,
   struct irqaction *action);

 if (force_irqthreads && test_bit(IRQTF_FORCED_THREAD,
     &action->thread_flags))
  handler_fn = irq_forced_thread_fn;
 else
  handler_fn = irq_thread_fn;

 init_task_work(&on_exit_work, irq_thread_dtor);
 task_work_add(current, &on_exit_work, false);

 irq_thread_check_affinity(desc, action);

 while (!irq_wait_for_interrupt(action)) {
  irqreturn_t action_ret;

  irq_thread_check_affinity(desc, action);

  action_ret = handler_fn(desc, action);
  if (action_ret == IRQ_HANDLED)
   atomic_inc(&desc->threads_handled);
  if (action_ret == IRQ_WAKE_THREAD)
   irq_wake_secondary(desc, action);

  wake_threads_waitq(desc);
 }
 task_work_cancel(current, irq_thread_dtor);
 return 0;
}







void irq_wake_thread(unsigned int irq, void *dev_id)
{
 struct irq_desc *desc = irq_to_desc(irq);
 struct irqaction *action;
 unsigned long flags;

 if (!desc || WARN_ON(irq_settings_is_per_cpu_devid(desc)))
  return;

 raw_spin_lock_irqsave(&desc->lock, flags);
 for_each_action_of_desc(desc, action) {
  if (action->dev_id == dev_id) {
   if (action->thread)
    __irq_wake_thread(desc, action);
   break;
  }
 }
 raw_spin_unlock_irqrestore(&desc->lock, flags);
}
EXPORT_SYMBOL_GPL(irq_wake_thread);

static int irq_setup_forced_threading(struct irqaction *new)
{
 if (!force_irqthreads)
  return 0;
 if (new->flags & (IRQF_NO_THREAD | IRQF_PERCPU | IRQF_ONESHOT))
  return 0;

 new->flags |= IRQF_ONESHOT;






 if (new->handler != irq_default_primary_handler && new->thread_fn) {

  new->secondary = kzalloc(sizeof(struct irqaction), GFP_KERNEL);
  if (!new->secondary)
   return -ENOMEM;
  new->secondary->handler = irq_forced_secondary_handler;
  new->secondary->thread_fn = new->thread_fn;
  new->secondary->dev_id = new->dev_id;
  new->secondary->irq = new->irq;
  new->secondary->name = new->name;
 }

 set_bit(IRQTF_FORCED_THREAD, &new->thread_flags);
 new->thread_fn = new->handler;
 new->handler = irq_default_primary_handler;
 return 0;
}

static int irq_request_resources(struct irq_desc *desc)
{
 struct irq_data *d = &desc->irq_data;
 struct irq_chip *c = d->chip;

 return c->irq_request_resources ? c->irq_request_resources(d) : 0;
}

static void irq_release_resources(struct irq_desc *desc)
{
 struct irq_data *d = &desc->irq_data;
 struct irq_chip *c = d->chip;

 if (c->irq_release_resources)
  c->irq_release_resources(d);
}

static int
setup_irq_thread(struct irqaction *new, unsigned int irq, bool secondary)
{
 struct task_struct *t;
 struct sched_param param = {
  .sched_priority = MAX_USER_RT_PRIO/2,
 };

 if (!secondary) {
  t = kthread_create(irq_thread, new, "irq/%d-%s", irq,
       new->name);
 } else {
  t = kthread_create(irq_thread, new, "irq/%d-s-%s", irq,
       new->name);
  param.sched_priority -= 1;
 }

 if (IS_ERR(t))
  return PTR_ERR(t);

 sched_setscheduler_nocheck(t, SCHED_FIFO, &param);






 get_task_struct(t);
 new->thread = t;
 set_bit(IRQTF_AFFINITY, &new->thread_flags);
 return 0;
}





static int
__setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
{
 struct irqaction *old, **old_ptr;
 unsigned long flags, thread_mask = 0;
 int ret, nested, shared = 0;
 cpumask_var_t mask;

 if (!desc)
  return -EINVAL;

 if (desc->irq_data.chip == &no_irq_chip)
  return -ENOSYS;
 if (!try_module_get(desc->owner))
  return -ENODEV;

 new->irq = irq;





 nested = irq_settings_is_nested_thread(desc);
 if (nested) {
  if (!new->thread_fn) {
   ret = -EINVAL;
   goto out_mput;
  }





  new->handler = irq_nested_primary_handler;
 } else {
  if (irq_settings_can_thread(desc)) {
   ret = irq_setup_forced_threading(new);
   if (ret)
    goto out_mput;
  }
 }






 if (new->thread_fn && !nested) {
  ret = setup_irq_thread(new, irq, false);
  if (ret)
   goto out_mput;
  if (new->secondary) {
   ret = setup_irq_thread(new->secondary, irq, true);
   if (ret)
    goto out_thread;
  }
 }

 if (!alloc_cpumask_var(&mask, GFP_KERNEL)) {
  ret = -ENOMEM;
  goto out_thread;
 }
 if (desc->irq_data.chip->flags & IRQCHIP_ONESHOT_SAFE)
  new->flags &= ~IRQF_ONESHOT;




 raw_spin_lock_irqsave(&desc->lock, flags);
 old_ptr = &desc->action;
 old = *old_ptr;
 if (old) {







  if (!((old->flags & new->flags) & IRQF_SHARED) ||
      ((old->flags ^ new->flags) & IRQF_TRIGGER_MASK) ||
      ((old->flags ^ new->flags) & IRQF_ONESHOT))
   goto mismatch;


  if ((old->flags & IRQF_PERCPU) !=
      (new->flags & IRQF_PERCPU))
   goto mismatch;


  do {





   thread_mask |= old->thread_mask;
   old_ptr = &old->next;
   old = *old_ptr;
  } while (old);
  shared = 1;
 }






 if (new->flags & IRQF_ONESHOT) {




  if (thread_mask == ~0UL) {
   ret = -EBUSY;
   goto out_mask;
  }
  new->thread_mask = 1 << ffz(thread_mask);

 } else if (new->handler == irq_default_primary_handler &&
     !(desc->irq_data.chip->flags & IRQCHIP_ONESHOT_SAFE)) {
  pr_err("Threaded irq requested with handler=NULL and !ONESHOT for irq %d\n",
         irq);
  ret = -EINVAL;
  goto out_mask;
 }

 if (!shared) {
  ret = irq_request_resources(desc);
  if (ret) {
   pr_err("Failed to request resources for %s (irq %d) on irqchip %s\n",
          new->name, irq, desc->irq_data.chip->name);
   goto out_mask;
  }

  init_waitqueue_head(&desc->wait_for_threads);


  if (new->flags & IRQF_TRIGGER_MASK) {
   ret = __irq_set_trigger(desc,
      new->flags & IRQF_TRIGGER_MASK);

   if (ret)
    goto out_mask;
  }

  desc->istate &= ~(IRQS_AUTODETECT | IRQS_SPURIOUS_DISABLED | \
      IRQS_ONESHOT | IRQS_WAITING);
  irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);

  if (new->flags & IRQF_PERCPU) {
   irqd_set(&desc->irq_data, IRQD_PER_CPU);
   irq_settings_set_per_cpu(desc);
  }

  if (new->flags & IRQF_ONESHOT)
   desc->istate |= IRQS_ONESHOT;

  if (irq_settings_can_autoenable(desc))
   irq_startup(desc, true);
  else

   desc->depth = 1;


  if (new->flags & IRQF_NOBALANCING) {
   irq_settings_set_no_balancing(desc);
   irqd_set(&desc->irq_data, IRQD_NO_BALANCING);
  }


  setup_affinity(desc, mask);

 } else if (new->flags & IRQF_TRIGGER_MASK) {
  unsigned int nmsk = new->flags & IRQF_TRIGGER_MASK;
  unsigned int omsk = irq_settings_get_trigger_mask(desc);

  if (nmsk != omsk)

   pr_warn("irq %d uses trigger mode %u; requested %u\n",
    irq, nmsk, omsk);
 }

 *old_ptr = new;

 irq_pm_install_action(desc, new);


 desc->irq_count = 0;
 desc->irqs_unhandled = 0;





 if (shared && (desc->istate & IRQS_SPURIOUS_DISABLED)) {
  desc->istate &= ~IRQS_SPURIOUS_DISABLED;
  __enable_irq(desc);
 }

 raw_spin_unlock_irqrestore(&desc->lock, flags);





 if (new->thread)
  wake_up_process(new->thread);
 if (new->secondary)
  wake_up_process(new->secondary->thread);

 register_irq_proc(irq, desc);
 new->dir = NULL;
 register_handler_proc(irq, new);
 free_cpumask_var(mask);

 return 0;

mismatch:
 if (!(new->flags & IRQF_PROBE_SHARED)) {
  pr_err("Flags mismatch irq %d. %08x (%s) vs. %08x (%s)\n",
         irq, new->flags, new->name, old->flags, old->name);
  dump_stack();
 }
 ret = -EBUSY;

out_mask:
 raw_spin_unlock_irqrestore(&desc->lock, flags);
 free_cpumask_var(mask);

out_thread:
 if (new->thread) {
  struct task_struct *t = new->thread;

  new->thread = NULL;
  kthread_stop(t);
  put_task_struct(t);
 }
 if (new->secondary && new->secondary->thread) {
  struct task_struct *t = new->secondary->thread;

  new->secondary->thread = NULL;
  kthread_stop(t);
  put_task_struct(t);
 }
out_mput:
 module_put(desc->owner);
 return ret;
}
int setup_irq(unsigned int irq, struct irqaction *act)
{
 int retval;
 struct irq_desc *desc = irq_to_desc(irq);

 if (!desc || WARN_ON(irq_settings_is_per_cpu_devid(desc)))
  return -EINVAL;
 chip_bus_lock(desc);
 retval = __setup_irq(irq, desc, act);
 chip_bus_sync_unlock(desc);

 return retval;
}
EXPORT_SYMBOL_GPL(setup_irq);





static struct irqaction *__free_irq(unsigned int irq, void *dev_id)
{
 struct irq_desc *desc = irq_to_desc(irq);
 struct irqaction *action, **action_ptr;
 unsigned long flags;

 WARN(in_interrupt(), "Trying to free IRQ %d from IRQ context!\n", irq);

 if (!desc)
  return NULL;

 chip_bus_lock(desc);
 raw_spin_lock_irqsave(&desc->lock, flags);





 action_ptr = &desc->action;
 for (;;) {
  action = *action_ptr;

  if (!action) {
   WARN(1, "Trying to free already-free IRQ %d\n", irq);
   raw_spin_unlock_irqrestore(&desc->lock, flags);
   chip_bus_sync_unlock(desc);
   return NULL;
  }

  if (action->dev_id == dev_id)
   break;
  action_ptr = &action->next;
 }


 *action_ptr = action->next;

 irq_pm_remove_action(desc, action);


 if (!desc->action) {
  irq_settings_clr_disable_unlazy(desc);
  irq_shutdown(desc);
  irq_release_resources(desc);
 }


 if (WARN_ON_ONCE(desc->affinity_hint))
  desc->affinity_hint = NULL;

 raw_spin_unlock_irqrestore(&desc->lock, flags);
 chip_bus_sync_unlock(desc);

 unregister_handler_proc(irq, action);


 synchronize_irq(irq);

 if (action->flags & IRQF_SHARED) {
  local_irq_save(flags);
  action->handler(irq, dev_id);
  local_irq_restore(flags);
 }

 if (action->thread) {
  kthread_stop(action->thread);
  put_task_struct(action->thread);
  if (action->secondary && action->secondary->thread) {
   kthread_stop(action->secondary->thread);
   put_task_struct(action->secondary->thread);
  }
 }

 module_put(desc->owner);
 kfree(action->secondary);
 return action;
}
void remove_irq(unsigned int irq, struct irqaction *act)
{
 struct irq_desc *desc = irq_to_desc(irq);

 if (desc && !WARN_ON(irq_settings_is_per_cpu_devid(desc)))
     __free_irq(irq, act->dev_id);
}
EXPORT_SYMBOL_GPL(remove_irq);
void free_irq(unsigned int irq, void *dev_id)
{
 struct irq_desc *desc = irq_to_desc(irq);

 if (!desc || WARN_ON(irq_settings_is_per_cpu_devid(desc)))
  return;

 if (WARN_ON(desc->affinity_notify))
  desc->affinity_notify = NULL;

 kfree(__free_irq(irq, dev_id));
}
EXPORT_SYMBOL(free_irq);
int request_threaded_irq(unsigned int irq, irq_handler_t handler,
    irq_handler_t thread_fn, unsigned long irqflags,
    const char *devname, void *dev_id)
{
 struct irqaction *action;
 struct irq_desc *desc;
 int retval;

 if (irq == IRQ_NOTCONNECTED)
  return -ENOTCONN;
 if (((irqflags & IRQF_SHARED) && !dev_id) ||
     (!(irqflags & IRQF_SHARED) && (irqflags & IRQF_COND_SUSPEND)) ||
     ((irqflags & IRQF_NO_SUSPEND) && (irqflags & IRQF_COND_SUSPEND)))
  return -EINVAL;

 desc = irq_to_desc(irq);
 if (!desc)
  return -EINVAL;

 if (!irq_settings_can_request(desc) ||
     WARN_ON(irq_settings_is_per_cpu_devid(desc)))
  return -EINVAL;

 if (!handler) {
  if (!thread_fn)
   return -EINVAL;
  handler = irq_default_primary_handler;
 }

 action = kzalloc(sizeof(struct irqaction), GFP_KERNEL);
 if (!action)
  return -ENOMEM;

 action->handler = handler;
 action->thread_fn = thread_fn;
 action->flags = irqflags;
 action->name = devname;
 action->dev_id = dev_id;

 chip_bus_lock(desc);
 retval = __setup_irq(irq, desc, action);
 chip_bus_sync_unlock(desc);

 if (retval) {
  kfree(action->secondary);
  kfree(action);
 }

 if (!retval && (irqflags & IRQF_SHARED)) {






  unsigned long flags;

  disable_irq(irq);
  local_irq_save(flags);

  handler(irq, dev_id);

  local_irq_restore(flags);
  enable_irq(irq);
 }
 return retval;
}
EXPORT_SYMBOL(request_threaded_irq);
int request_any_context_irq(unsigned int irq, irq_handler_t handler,
       unsigned long flags, const char *name, void *dev_id)
{
 struct irq_desc *desc;
 int ret;

 if (irq == IRQ_NOTCONNECTED)
  return -ENOTCONN;

 desc = irq_to_desc(irq);
 if (!desc)
  return -EINVAL;

 if (irq_settings_is_nested_thread(desc)) {
  ret = request_threaded_irq(irq, NULL, handler,
        flags, name, dev_id);
  return !ret ? IRQC_IS_NESTED : ret;
 }

 ret = request_irq(irq, handler, flags, name, dev_id);
 return !ret ? IRQC_IS_HARDIRQ : ret;
}
EXPORT_SYMBOL_GPL(request_any_context_irq);

void enable_percpu_irq(unsigned int irq, unsigned int type)
{
 unsigned int cpu = smp_processor_id();
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_lock(irq, &flags, IRQ_GET_DESC_CHECK_PERCPU);

 if (!desc)
  return;

 type &= IRQ_TYPE_SENSE_MASK;
 if (type != IRQ_TYPE_NONE) {
  int ret;

  ret = __irq_set_trigger(desc, type);

  if (ret) {
   WARN(1, "failed to set type for IRQ%d\n", irq);
   goto out;
  }
 }

 irq_percpu_enable(desc, cpu);
out:
 irq_put_desc_unlock(desc, flags);
}
EXPORT_SYMBOL_GPL(enable_percpu_irq);
bool irq_percpu_is_enabled(unsigned int irq)
{
 unsigned int cpu = smp_processor_id();
 struct irq_desc *desc;
 unsigned long flags;
 bool is_enabled;

 desc = irq_get_desc_lock(irq, &flags, IRQ_GET_DESC_CHECK_PERCPU);
 if (!desc)
  return false;

 is_enabled = cpumask_test_cpu(cpu, desc->percpu_enabled);
 irq_put_desc_unlock(desc, flags);

 return is_enabled;
}
EXPORT_SYMBOL_GPL(irq_percpu_is_enabled);

void disable_percpu_irq(unsigned int irq)
{
 unsigned int cpu = smp_processor_id();
 unsigned long flags;
 struct irq_desc *desc = irq_get_desc_lock(irq, &flags, IRQ_GET_DESC_CHECK_PERCPU);

 if (!desc)
  return;

 irq_percpu_disable(desc, cpu);
 irq_put_desc_unlock(desc, flags);
}
EXPORT_SYMBOL_GPL(disable_percpu_irq);




static struct irqaction *__free_percpu_irq(unsigned int irq, void __percpu *dev_id)
{
 struct irq_desc *desc = irq_to_desc(irq);
 struct irqaction *action;
 unsigned long flags;

 WARN(in_interrupt(), "Trying to free IRQ %d from IRQ context!\n", irq);

 if (!desc)
  return NULL;

 raw_spin_lock_irqsave(&desc->lock, flags);

 action = desc->action;
 if (!action || action->percpu_dev_id != dev_id) {
  WARN(1, "Trying to free already-free IRQ %d\n", irq);
  goto bad;
 }

 if (!cpumask_empty(desc->percpu_enabled)) {
  WARN(1, "percpu IRQ %d still enabled on CPU%d!\n",
       irq, cpumask_first(desc->percpu_enabled));
  goto bad;
 }


 desc->action = NULL;

 raw_spin_unlock_irqrestore(&desc->lock, flags);

 unregister_handler_proc(irq, action);

 module_put(desc->owner);
 return action;

bad:
 raw_spin_unlock_irqrestore(&desc->lock, flags);
 return NULL;
}
void remove_percpu_irq(unsigned int irq, struct irqaction *act)
{
 struct irq_desc *desc = irq_to_desc(irq);

 if (desc && irq_settings_is_per_cpu_devid(desc))
     __free_percpu_irq(irq, act->percpu_dev_id);
}
void free_percpu_irq(unsigned int irq, void __percpu *dev_id)
{
 struct irq_desc *desc = irq_to_desc(irq);

 if (!desc || !irq_settings_is_per_cpu_devid(desc))
  return;

 chip_bus_lock(desc);
 kfree(__free_percpu_irq(irq, dev_id));
 chip_bus_sync_unlock(desc);
}
EXPORT_SYMBOL_GPL(free_percpu_irq);
int setup_percpu_irq(unsigned int irq, struct irqaction *act)
{
 struct irq_desc *desc = irq_to_desc(irq);
 int retval;

 if (!desc || !irq_settings_is_per_cpu_devid(desc))
  return -EINVAL;
 chip_bus_lock(desc);
 retval = __setup_irq(irq, desc, act);
 chip_bus_sync_unlock(desc);

 return retval;
}
int request_percpu_irq(unsigned int irq, irq_handler_t handler,
         const char *devname, void __percpu *dev_id)
{
 struct irqaction *action;
 struct irq_desc *desc;
 int retval;

 if (!dev_id)
  return -EINVAL;

 desc = irq_to_desc(irq);
 if (!desc || !irq_settings_can_request(desc) ||
     !irq_settings_is_per_cpu_devid(desc))
  return -EINVAL;

 action = kzalloc(sizeof(struct irqaction), GFP_KERNEL);
 if (!action)
  return -ENOMEM;

 action->handler = handler;
 action->flags = IRQF_PERCPU | IRQF_NO_SUSPEND;
 action->name = devname;
 action->percpu_dev_id = dev_id;

 chip_bus_lock(desc);
 retval = __setup_irq(irq, desc, action);
 chip_bus_sync_unlock(desc);

 if (retval)
  kfree(action);

 return retval;
}
EXPORT_SYMBOL_GPL(request_percpu_irq);
int irq_get_irqchip_state(unsigned int irq, enum irqchip_irq_state which,
     bool *state)
{
 struct irq_desc *desc;
 struct irq_data *data;
 struct irq_chip *chip;
 unsigned long flags;
 int err = -EINVAL;

 desc = irq_get_desc_buslock(irq, &flags, 0);
 if (!desc)
  return err;

 data = irq_desc_get_irq_data(desc);

 do {
  chip = irq_data_get_irq_chip(data);
  if (chip->irq_get_irqchip_state)
   break;
  data = data->parent_data;
  data = NULL;
 } while (data);

 if (data)
  err = chip->irq_get_irqchip_state(data, which, state);

 irq_put_desc_busunlock(desc, flags);
 return err;
}
EXPORT_SYMBOL_GPL(irq_get_irqchip_state);
int irq_set_irqchip_state(unsigned int irq, enum irqchip_irq_state which,
     bool val)
{
 struct irq_desc *desc;
 struct irq_data *data;
 struct irq_chip *chip;
 unsigned long flags;
 int err = -EINVAL;

 desc = irq_get_desc_buslock(irq, &flags, 0);
 if (!desc)
  return err;

 data = irq_desc_get_irq_data(desc);

 do {
  chip = irq_data_get_irq_chip(data);
  if (chip->irq_set_irqchip_state)
   break;
  data = data->parent_data;
  data = NULL;
 } while (data);

 if (data)
  err = chip->irq_set_irqchip_state(data, which, val);

 irq_put_desc_busunlock(desc, flags);
 return err;
}
EXPORT_SYMBOL_GPL(irq_set_irqchip_state);





SYSCALL_DEFINE2(membarrier, int, cmd, int, flags)
{
 if (unlikely(flags))
  return -EINVAL;
 switch (cmd) {
 case MEMBARRIER_CMD_QUERY:
  return MEMBARRIER_CMD_BITMASK;
 case MEMBARRIER_CMD_SHARED:
  if (num_online_cpus() > 1)
   synchronize_sched();
  return 0;
 default:
  return -EINVAL;
 }
}


__weak void __iomem *ioremap_cache(resource_size_t offset, unsigned long size)
{
 return ioremap(offset, size);
}

static void *arch_memremap_wb(resource_size_t offset, unsigned long size)
{
 return (__force void *)ioremap_cache(offset, size);
}

static void *try_ram_remap(resource_size_t offset, size_t size)
{
 unsigned long pfn = PHYS_PFN(offset);


 if (pfn_valid(pfn) && !PageHighMem(pfn_to_page(pfn)))
  return __va(offset);
 return NULL;
}
void *memremap(resource_size_t offset, size_t size, unsigned long flags)
{
 int is_ram = region_intersects(offset, size,
           IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE);
 void *addr = NULL;

 if (!flags)
  return NULL;

 if (is_ram == REGION_MIXED) {
  WARN_ONCE(1, "memremap attempted on mixed range %pa size: %#lx\n",
    &offset, (unsigned long) size);
  return NULL;
 }


 if (flags & MEMREMAP_WB) {






  if (is_ram == REGION_INTERSECTS)
   addr = try_ram_remap(offset, size);
  if (!addr)
   addr = arch_memremap_wb(offset, size);
 }







 if (!addr && is_ram == REGION_INTERSECTS && flags != MEMREMAP_WB) {
  WARN_ONCE(1, "memremap attempted on ram %pa size: %#lx\n",
    &offset, (unsigned long) size);
  return NULL;
 }

 if (!addr && (flags & MEMREMAP_WT))
  addr = ioremap_wt(offset, size);

 if (!addr && (flags & MEMREMAP_WC))
  addr = ioremap_wc(offset, size);

 return addr;
}
EXPORT_SYMBOL(memremap);

void memunmap(void *addr)
{
 if (is_vmalloc_addr(addr))
  iounmap((void __iomem *) addr);
}
EXPORT_SYMBOL(memunmap);

static void devm_memremap_release(struct device *dev, void *res)
{
 memunmap(*(void **)res);
}

static int devm_memremap_match(struct device *dev, void *res, void *match_data)
{
 return *(void **)res == match_data;
}

void *devm_memremap(struct device *dev, resource_size_t offset,
  size_t size, unsigned long flags)
{
 void **ptr, *addr;

 ptr = devres_alloc_node(devm_memremap_release, sizeof(*ptr), GFP_KERNEL,
   dev_to_node(dev));
 if (!ptr)
  return ERR_PTR(-ENOMEM);

 addr = memremap(offset, size, flags);
 if (addr) {
  *ptr = addr;
  devres_add(dev, ptr);
 } else {
  devres_free(ptr);
  return ERR_PTR(-ENXIO);
 }

 return addr;
}
EXPORT_SYMBOL(devm_memremap);

void devm_memunmap(struct device *dev, void *addr)
{
 WARN_ON(devres_release(dev, devm_memremap_release,
    devm_memremap_match, addr));
}
EXPORT_SYMBOL(devm_memunmap);

pfn_t phys_to_pfn_t(phys_addr_t addr, u64 flags)
{
 return __pfn_to_pfn_t(addr >> PAGE_SHIFT, flags);
}
EXPORT_SYMBOL(phys_to_pfn_t);

static DEFINE_MUTEX(pgmap_lock);
static RADIX_TREE(pgmap_radix, GFP_KERNEL);

struct page_map {
 struct resource res;
 struct percpu_ref *ref;
 struct dev_pagemap pgmap;
 struct vmem_altmap altmap;
};

void get_zone_device_page(struct page *page)
{
 percpu_ref_get(page->pgmap->ref);
}
EXPORT_SYMBOL(get_zone_device_page);

void put_zone_device_page(struct page *page)
{
 put_dev_pagemap(page->pgmap);
}
EXPORT_SYMBOL(put_zone_device_page);

static void pgmap_radix_release(struct resource *res)
{
 resource_size_t key, align_start, align_size, align_end;

 align_start = res->start & ~(SECTION_SIZE - 1);
 align_size = ALIGN(resource_size(res), SECTION_SIZE);
 align_end = align_start + align_size - 1;

 mutex_lock(&pgmap_lock);
 for (key = res->start; key <= res->end; key += SECTION_SIZE)
  radix_tree_delete(&pgmap_radix, key >> PA_SECTION_SHIFT);
 mutex_unlock(&pgmap_lock);
}

static unsigned long pfn_first(struct page_map *page_map)
{
 struct dev_pagemap *pgmap = &page_map->pgmap;
 const struct resource *res = &page_map->res;
 struct vmem_altmap *altmap = pgmap->altmap;
 unsigned long pfn;

 pfn = res->start >> PAGE_SHIFT;
 if (altmap)
  pfn += vmem_altmap_offset(altmap);
 return pfn;
}

static unsigned long pfn_end(struct page_map *page_map)
{
 const struct resource *res = &page_map->res;

 return (res->start + resource_size(res)) >> PAGE_SHIFT;
}

 for (pfn = pfn_first(map); pfn < pfn_end(map); pfn++)

static void devm_memremap_pages_release(struct device *dev, void *data)
{
 struct page_map *page_map = data;
 struct resource *res = &page_map->res;
 resource_size_t align_start, align_size;
 struct dev_pagemap *pgmap = &page_map->pgmap;

 if (percpu_ref_tryget_live(pgmap->ref)) {
  dev_WARN(dev, "%s: page mapping is still live!\n", __func__);
  percpu_ref_put(pgmap->ref);
 }


 align_start = res->start & ~(SECTION_SIZE - 1);
 align_size = ALIGN(resource_size(res), SECTION_SIZE);
 arch_remove_memory(align_start, align_size);
 pgmap_radix_release(res);
 dev_WARN_ONCE(dev, pgmap->altmap && pgmap->altmap->alloc,
   "%s: failed to free all reserved pages\n", __func__);
}


struct dev_pagemap *find_dev_pagemap(resource_size_t phys)
{
 struct page_map *page_map;

 WARN_ON_ONCE(!rcu_read_lock_held());

 page_map = radix_tree_lookup(&pgmap_radix, phys >> PA_SECTION_SHIFT);
 return page_map ? &page_map->pgmap : NULL;
}
void *devm_memremap_pages(struct device *dev, struct resource *res,
  struct percpu_ref *ref, struct vmem_altmap *altmap)
{
 resource_size_t key, align_start, align_size, align_end;
 struct dev_pagemap *pgmap;
 struct page_map *page_map;
 int error, nid, is_ram;
 unsigned long pfn;

 align_start = res->start & ~(SECTION_SIZE - 1);
 align_size = ALIGN(res->start + resource_size(res), SECTION_SIZE)
  - align_start;
 is_ram = region_intersects(align_start, align_size,
  IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE);

 if (is_ram == REGION_MIXED) {
  WARN_ONCE(1, "%s attempted on mixed region %pr\n",
    __func__, res);
  return ERR_PTR(-ENXIO);
 }

 if (is_ram == REGION_INTERSECTS)
  return __va(res->start);

 if (altmap && !IS_ENABLED(CONFIG_SPARSEMEM_VMEMMAP)) {
  dev_err(dev, "%s: altmap requires CONFIG_SPARSEMEM_VMEMMAP=y\n",
    __func__);
  return ERR_PTR(-ENXIO);
 }

 if (!ref)
  return ERR_PTR(-EINVAL);

 page_map = devres_alloc_node(devm_memremap_pages_release,
   sizeof(*page_map), GFP_KERNEL, dev_to_node(dev));
 if (!page_map)
  return ERR_PTR(-ENOMEM);
 pgmap = &page_map->pgmap;

 memcpy(&page_map->res, res, sizeof(*res));

 pgmap->dev = dev;
 if (altmap) {
  memcpy(&page_map->altmap, altmap, sizeof(*altmap));
  pgmap->altmap = &page_map->altmap;
 }
 pgmap->ref = ref;
 pgmap->res = &page_map->res;

 mutex_lock(&pgmap_lock);
 error = 0;
 align_end = align_start + align_size - 1;
 for (key = align_start; key <= align_end; key += SECTION_SIZE) {
  struct dev_pagemap *dup;

  rcu_read_lock();
  dup = find_dev_pagemap(key);
  rcu_read_unlock();
  if (dup) {
   dev_err(dev, "%s: %pr collides with mapping for %s\n",
     __func__, res, dev_name(dup->dev));
   error = -EBUSY;
   break;
  }
  error = radix_tree_insert(&pgmap_radix, key >> PA_SECTION_SHIFT,
    page_map);
  if (error) {
   dev_err(dev, "%s: failed: %d\n", __func__, error);
   break;
  }
 }
 mutex_unlock(&pgmap_lock);
 if (error)
  goto err_radix;

 nid = dev_to_node(dev);
 if (nid < 0)
  nid = numa_mem_id();

 error = arch_add_memory(nid, align_start, align_size, true);
 if (error)
  goto err_add_memory;

 for_each_device_pfn(pfn, page_map) {
  struct page *page = pfn_to_page(pfn);







  list_del(&page->lru);
  page->pgmap = pgmap;
 }
 devres_add(dev, page_map);
 return __va(res->start);

 err_add_memory:
 err_radix:
 pgmap_radix_release(res);
 devres_free(page_map);
 return ERR_PTR(error);
}
EXPORT_SYMBOL(devm_memremap_pages);

unsigned long vmem_altmap_offset(struct vmem_altmap *altmap)
{

 return altmap->reserve + altmap->free;
}

void vmem_altmap_free(struct vmem_altmap *altmap, unsigned long nr_pfns)
{
 altmap->alloc -= nr_pfns;
}

struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)
{
 struct page *page = (struct page *) memmap_start;
 struct dev_pagemap *pgmap;







 rcu_read_lock();
 pgmap = find_dev_pagemap(__pfn_to_phys(page_to_pfn(page)));
 rcu_read_unlock();

 return pgmap ? pgmap->altmap : NULL;
}



void irq_move_masked_irq(struct irq_data *idata)
{
 struct irq_desc *desc = irq_data_to_desc(idata);
 struct irq_chip *chip = desc->irq_data.chip;

 if (likely(!irqd_is_setaffinity_pending(&desc->irq_data)))
  return;

 irqd_clr_move_pending(&desc->irq_data);




 if (irqd_is_per_cpu(&desc->irq_data)) {
  WARN_ON(1);
  return;
 }

 if (unlikely(cpumask_empty(desc->pending_mask)))
  return;

 if (!chip->irq_set_affinity)
  return;

 assert_raw_spin_locked(&desc->lock);
 if (cpumask_any_and(desc->pending_mask, cpu_online_mask) < nr_cpu_ids)
  irq_do_set_affinity(&desc->irq_data, desc->pending_mask, false);

 cpumask_clear(desc->pending_mask);
}

void irq_move_irq(struct irq_data *idata)
{
 bool masked;






 idata = irq_desc_get_irq_data(irq_data_to_desc(idata));

 if (likely(!irqd_is_setaffinity_pending(idata)))
  return;

 if (unlikely(irqd_irq_disabled(idata)))
  return;






 masked = irqd_irq_masked(idata);
 if (!masked)
  idata->chip->irq_mask(idata);
 irq_move_masked_irq(idata);
 if (!masked)
  idata->chip->irq_unmask(idata);
}

















DEFINE_MUTEX(module_mutex);
EXPORT_SYMBOL_GPL(module_mutex);
static LIST_HEAD(modules);

static __always_inline unsigned long __mod_tree_val(struct latch_tree_node *n)
{
 struct module_layout *layout = container_of(n, struct module_layout, mtn.node);

 return (unsigned long)layout->base;
}

static __always_inline unsigned long __mod_tree_size(struct latch_tree_node *n)
{
 struct module_layout *layout = container_of(n, struct module_layout, mtn.node);

 return (unsigned long)layout->size;
}

static __always_inline bool
mod_tree_less(struct latch_tree_node *a, struct latch_tree_node *b)
{
 return __mod_tree_val(a) < __mod_tree_val(b);
}

static __always_inline int
mod_tree_comp(void *key, struct latch_tree_node *n)
{
 unsigned long val = (unsigned long)key;
 unsigned long start, end;

 start = __mod_tree_val(n);
 if (val < start)
  return -1;

 end = start + __mod_tree_size(n);
 if (val >= end)
  return 1;

 return 0;
}

static const struct latch_tree_ops mod_tree_ops = {
 .less = mod_tree_less,
 .comp = mod_tree_comp,
};

static struct mod_tree_root {
 struct latch_tree_root root;
 unsigned long addr_min;
 unsigned long addr_max;
} mod_tree __cacheline_aligned = {
 .addr_min = -1UL,
};


static noinline void __mod_tree_insert(struct mod_tree_node *node)
{
 latch_tree_insert(&node->node, &mod_tree.root, &mod_tree_ops);
}

static void __mod_tree_remove(struct mod_tree_node *node)
{
 latch_tree_erase(&node->node, &mod_tree.root, &mod_tree_ops);
}





static void mod_tree_insert(struct module *mod)
{
 mod->core_layout.mtn.mod = mod;
 mod->init_layout.mtn.mod = mod;

 __mod_tree_insert(&mod->core_layout.mtn);
 if (mod->init_layout.size)
  __mod_tree_insert(&mod->init_layout.mtn);
}

static void mod_tree_remove_init(struct module *mod)
{
 if (mod->init_layout.size)
  __mod_tree_remove(&mod->init_layout.mtn);
}

static void mod_tree_remove(struct module *mod)
{
 __mod_tree_remove(&mod->core_layout.mtn);
 mod_tree_remove_init(mod);
}

static struct module *mod_find(unsigned long addr)
{
 struct latch_tree_node *ltn;

 ltn = latch_tree_find((void *)addr, &mod_tree.root, &mod_tree_ops);
 if (!ltn)
  return NULL;

 return container_of(ltn, struct mod_tree_node, node)->mod;
}


static unsigned long module_addr_min = -1UL, module_addr_max = 0;

static void mod_tree_insert(struct module *mod) { }
static void mod_tree_remove_init(struct module *mod) { }
static void mod_tree_remove(struct module *mod) { }

static struct module *mod_find(unsigned long addr)
{
 struct module *mod;

 list_for_each_entry_rcu(mod, &modules, list) {
  if (within_module(addr, mod))
   return mod;
 }

 return NULL;
}






static void __mod_update_bounds(void *base, unsigned int size)
{
 unsigned long min = (unsigned long)base;
 unsigned long max = min + size;

 if (min < module_addr_min)
  module_addr_min = min;
 if (max > module_addr_max)
  module_addr_max = max;
}

static void mod_update_bounds(struct module *mod)
{
 __mod_update_bounds(mod->core_layout.base, mod->core_layout.size);
 if (mod->init_layout.size)
  __mod_update_bounds(mod->init_layout.base, mod->init_layout.size);
}

struct list_head *kdb_modules = &modules;

static void module_assert_mutex(void)
{
 lockdep_assert_held(&module_mutex);
}

static void module_assert_mutex_or_preempt(void)
{
 if (unlikely(!debug_locks))
  return;

 WARN_ON(!rcu_read_lock_sched_held() &&
  !lockdep_is_held(&module_mutex));
}

static bool sig_enforce = IS_ENABLED(CONFIG_MODULE_SIG_FORCE);
module_param(sig_enforce, bool_enable_only, 0644);


int modules_disabled = 0;
core_param(nomodule, modules_disabled, bint, 0);


static DECLARE_WAIT_QUEUE_HEAD(module_wq);

static BLOCKING_NOTIFIER_HEAD(module_notify_list);

int register_module_notifier(struct notifier_block *nb)
{
 return blocking_notifier_chain_register(&module_notify_list, nb);
}
EXPORT_SYMBOL(register_module_notifier);

int unregister_module_notifier(struct notifier_block *nb)
{
 return blocking_notifier_chain_unregister(&module_notify_list, nb);
}
EXPORT_SYMBOL(unregister_module_notifier);

struct load_info {
 Elf_Ehdr *hdr;
 unsigned long len;
 Elf_Shdr *sechdrs;
 char *secstrings, *strtab;
 unsigned long symoffs, stroffs;
 struct _ddebug *debug;
 unsigned int num_debug;
 bool sig_ok;
 unsigned long mod_kallsyms_init_off;
 struct {
  unsigned int sym, str, mod, vers, info, pcpu;
 } index;
};



static inline int strong_try_module_get(struct module *mod)
{
 BUG_ON(mod && mod->state == MODULE_STATE_UNFORMED);
 if (mod && mod->state == MODULE_STATE_COMING)
  return -EBUSY;
 if (try_module_get(mod))
  return 0;
 else
  return -ENOENT;
}

static inline void add_taint_module(struct module *mod, unsigned flag,
        enum lockdep_ok lockdep_ok)
{
 add_taint(flag, lockdep_ok);
 mod->taints |= (1U << flag);
}





void __module_put_and_exit(struct module *mod, long code)
{
 module_put(mod);
 do_exit(code);
}
EXPORT_SYMBOL(__module_put_and_exit);


static unsigned int find_sec(const struct load_info *info, const char *name)
{
 unsigned int i;

 for (i = 1; i < info->hdr->e_shnum; i++) {
  Elf_Shdr *shdr = &info->sechdrs[i];

  if ((shdr->sh_flags & SHF_ALLOC)
      && strcmp(info->secstrings + shdr->sh_name, name) == 0)
   return i;
 }
 return 0;
}


static void *section_addr(const struct load_info *info, const char *name)
{

 return (void *)info->sechdrs[find_sec(info, name)].sh_addr;
}


static void *section_objs(const struct load_info *info,
     const char *name,
     size_t object_size,
     unsigned int *num)
{
 unsigned int sec = find_sec(info, name);


 *num = info->sechdrs[sec].sh_size / object_size;
 return (void *)info->sechdrs[sec].sh_addr;
}


extern const struct kernel_symbol __start___ksymtab[];
extern const struct kernel_symbol __stop___ksymtab[];
extern const struct kernel_symbol __start___ksymtab_gpl[];
extern const struct kernel_symbol __stop___ksymtab_gpl[];
extern const struct kernel_symbol __start___ksymtab_gpl_future[];
extern const struct kernel_symbol __stop___ksymtab_gpl_future[];
extern const unsigned long __start___kcrctab[];
extern const unsigned long __start___kcrctab_gpl[];
extern const unsigned long __start___kcrctab_gpl_future[];
extern const struct kernel_symbol __start___ksymtab_unused[];
extern const struct kernel_symbol __stop___ksymtab_unused[];
extern const struct kernel_symbol __start___ksymtab_unused_gpl[];
extern const struct kernel_symbol __stop___ksymtab_unused_gpl[];
extern const unsigned long __start___kcrctab_unused[];
extern const unsigned long __start___kcrctab_unused_gpl[];


static bool each_symbol_in_section(const struct symsearch *arr,
       unsigned int arrsize,
       struct module *owner,
       bool (*fn)(const struct symsearch *syms,
           struct module *owner,
           void *data),
       void *data)
{
 unsigned int j;

 for (j = 0; j < arrsize; j++) {
  if (fn(&arr[j], owner, data))
   return true;
 }

 return false;
}


bool each_symbol_section(bool (*fn)(const struct symsearch *arr,
        struct module *owner,
        void *data),
    void *data)
{
 struct module *mod;
 static const struct symsearch arr[] = {
  { __start___ksymtab, __stop___ksymtab, __start___kcrctab,
    NOT_GPL_ONLY, false },
  { __start___ksymtab_gpl, __stop___ksymtab_gpl,
    __start___kcrctab_gpl,
    GPL_ONLY, false },
  { __start___ksymtab_gpl_future, __stop___ksymtab_gpl_future,
    __start___kcrctab_gpl_future,
    WILL_BE_GPL_ONLY, false },
  { __start___ksymtab_unused, __stop___ksymtab_unused,
    __start___kcrctab_unused,
    NOT_GPL_ONLY, true },
  { __start___ksymtab_unused_gpl, __stop___ksymtab_unused_gpl,
    __start___kcrctab_unused_gpl,
    GPL_ONLY, true },
 };

 module_assert_mutex_or_preempt();

 if (each_symbol_in_section(arr, ARRAY_SIZE(arr), NULL, fn, data))
  return true;

 list_for_each_entry_rcu(mod, &modules, list) {
  struct symsearch arr[] = {
   { mod->syms, mod->syms + mod->num_syms, mod->crcs,
     NOT_GPL_ONLY, false },
   { mod->gpl_syms, mod->gpl_syms + mod->num_gpl_syms,
     mod->gpl_crcs,
     GPL_ONLY, false },
   { mod->gpl_future_syms,
     mod->gpl_future_syms + mod->num_gpl_future_syms,
     mod->gpl_future_crcs,
     WILL_BE_GPL_ONLY, false },
   { mod->unused_syms,
     mod->unused_syms + mod->num_unused_syms,
     mod->unused_crcs,
     NOT_GPL_ONLY, true },
   { mod->unused_gpl_syms,
     mod->unused_gpl_syms + mod->num_unused_gpl_syms,
     mod->unused_gpl_crcs,
     GPL_ONLY, true },
  };

  if (mod->state == MODULE_STATE_UNFORMED)
   continue;

  if (each_symbol_in_section(arr, ARRAY_SIZE(arr), mod, fn, data))
   return true;
 }
 return false;
}
EXPORT_SYMBOL_GPL(each_symbol_section);

struct find_symbol_arg {

 const char *name;
 bool gplok;
 bool warn;


 struct module *owner;
 const unsigned long *crc;
 const struct kernel_symbol *sym;
};

static bool check_symbol(const struct symsearch *syms,
     struct module *owner,
     unsigned int symnum, void *data)
{
 struct find_symbol_arg *fsa = data;

 if (!fsa->gplok) {
  if (syms->licence == GPL_ONLY)
   return false;
  if (syms->licence == WILL_BE_GPL_ONLY && fsa->warn) {
   pr_warn("Symbol %s is being used by a non-GPL module, "
    "which will not be allowed in the future\n",
    fsa->name);
  }
 }

 if (syms->unused && fsa->warn) {
  pr_warn("Symbol %s is marked as UNUSED, however this module is "
   "using it.\n", fsa->name);
  pr_warn("This symbol will go away in the future.\n");
  pr_warn("Please evaluate if this is the right api to use and "
   "if it really is, submit a report to the linux kernel "
   "mailing list together with submitting your code for "
   "inclusion.\n");
 }

 fsa->owner = owner;
 fsa->crc = symversion(syms->crcs, symnum);
 fsa->sym = &syms->start[symnum];
 return true;
}

static int cmp_name(const void *va, const void *vb)
{
 const char *a;
 const struct kernel_symbol *b;
 a = va; b = vb;
 return strcmp(a, b->name);
}

static bool find_symbol_in_section(const struct symsearch *syms,
       struct module *owner,
       void *data)
{
 struct find_symbol_arg *fsa = data;
 struct kernel_symbol *sym;

 sym = bsearch(fsa->name, syms->start, syms->stop - syms->start,
   sizeof(struct kernel_symbol), cmp_name);

 if (sym != NULL && check_symbol(syms, owner, sym - syms->start, data))
  return true;

 return false;
}



const struct kernel_symbol *find_symbol(const char *name,
     struct module **owner,
     const unsigned long **crc,
     bool gplok,
     bool warn)
{
 struct find_symbol_arg fsa;

 fsa.name = name;
 fsa.gplok = gplok;
 fsa.warn = warn;

 if (each_symbol_section(find_symbol_in_section, &fsa)) {
  if (owner)
   *owner = fsa.owner;
  if (crc)
   *crc = fsa.crc;
  return fsa.sym;
 }

 pr_debug("Failed to find symbol %s\n", name);
 return NULL;
}
EXPORT_SYMBOL_GPL(find_symbol);





static struct module *find_module_all(const char *name, size_t len,
          bool even_unformed)
{
 struct module *mod;

 module_assert_mutex_or_preempt();

 list_for_each_entry(mod, &modules, list) {
  if (!even_unformed && mod->state == MODULE_STATE_UNFORMED)
   continue;
  if (strlen(mod->name) == len && !memcmp(mod->name, name, len))
   return mod;
 }
 return NULL;
}

struct module *find_module(const char *name)
{
 module_assert_mutex();
 return find_module_all(name, strlen(name), false);
}
EXPORT_SYMBOL_GPL(find_module);


static inline void __percpu *mod_percpu(struct module *mod)
{
 return mod->percpu;
}

static int percpu_modalloc(struct module *mod, struct load_info *info)
{
 Elf_Shdr *pcpusec = &info->sechdrs[info->index.pcpu];
 unsigned long align = pcpusec->sh_addralign;

 if (!pcpusec->sh_size)
  return 0;

 if (align > PAGE_SIZE) {
  pr_warn("%s: per-cpu alignment %li > %li\n",
   mod->name, align, PAGE_SIZE);
  align = PAGE_SIZE;
 }

 mod->percpu = __alloc_reserved_percpu(pcpusec->sh_size, align);
 if (!mod->percpu) {
  pr_warn("%s: Could not allocate %lu bytes percpu data\n",
   mod->name, (unsigned long)pcpusec->sh_size);
  return -ENOMEM;
 }
 mod->percpu_size = pcpusec->sh_size;
 return 0;
}

static void percpu_modfree(struct module *mod)
{
 free_percpu(mod->percpu);
}

static unsigned int find_pcpusec(struct load_info *info)
{
 return find_sec(info, ".data..percpu");
}

static void percpu_modcopy(struct module *mod,
      const void *from, unsigned long size)
{
 int cpu;

 for_each_possible_cpu(cpu)
  memcpy(per_cpu_ptr(mod->percpu, cpu), from, size);
}
bool is_module_percpu_address(unsigned long addr)
{
 struct module *mod;
 unsigned int cpu;

 preempt_disable();

 list_for_each_entry_rcu(mod, &modules, list) {
  if (mod->state == MODULE_STATE_UNFORMED)
   continue;
  if (!mod->percpu_size)
   continue;
  for_each_possible_cpu(cpu) {
   void *start = per_cpu_ptr(mod->percpu, cpu);

   if ((void *)addr >= start &&
       (void *)addr < start + mod->percpu_size) {
    preempt_enable();
    return true;
   }
  }
 }

 preempt_enable();
 return false;
}


static inline void __percpu *mod_percpu(struct module *mod)
{
 return NULL;
}
static int percpu_modalloc(struct module *mod, struct load_info *info)
{

 if (info->sechdrs[info->index.pcpu].sh_size != 0)
  return -ENOMEM;
 return 0;
}
static inline void percpu_modfree(struct module *mod)
{
}
static unsigned int find_pcpusec(struct load_info *info)
{
 return 0;
}
static inline void percpu_modcopy(struct module *mod,
      const void *from, unsigned long size)
{

 BUG_ON(size != 0);
}
bool is_module_percpu_address(unsigned long addr)
{
 return false;
}


static void setup_modinfo_##field(struct module *mod, const char *s) \
{ \
 mod->field = kstrdup(s, GFP_KERNEL); \
} \
static ssize_t show_modinfo_##field(struct module_attribute *mattr, \
   struct module_kobject *mk, char *buffer) \
{ \
 return scnprintf(buffer, PAGE_SIZE, "%s\n", mk->mod->field); \
} \
static int modinfo_##field##_exists(struct module *mod) \
{ \
 return mod->field != NULL; \
} \
static void free_modinfo_##field(struct module *mod) \
{ \
 kfree(mod->field); \
 mod->field = NULL; \
} \
static struct module_attribute modinfo_##field = { \
 .attr = { .name = __stringify(field), .mode = 0444 }, \
 .show = show_modinfo_##field, \
 .setup = setup_modinfo_##field, \
 .test = modinfo_##field##_exists, \
 .free = free_modinfo_##field, \
};

MODINFO_ATTR(version);
MODINFO_ATTR(srcversion);

static char last_unloaded_module[MODULE_NAME_LEN+1];


EXPORT_TRACEPOINT_SYMBOL(module_get);




static int module_unload_init(struct module *mod)
{




 atomic_set(&mod->refcnt, MODULE_REF_BASE);

 INIT_LIST_HEAD(&mod->source_list);
 INIT_LIST_HEAD(&mod->target_list);


 atomic_inc(&mod->refcnt);

 return 0;
}


static int already_uses(struct module *a, struct module *b)
{
 struct module_use *use;

 list_for_each_entry(use, &b->source_list, source_list) {
  if (use->source == a) {
   pr_debug("%s uses %s!\n", a->name, b->name);
   return 1;
  }
 }
 pr_debug("%s does not use %s!\n", a->name, b->name);
 return 0;
}
static int add_module_usage(struct module *a, struct module *b)
{
 struct module_use *use;

 pr_debug("Allocating new usage for %s.\n", a->name);
 use = kmalloc(sizeof(*use), GFP_ATOMIC);
 if (!use) {
  pr_warn("%s: out of memory loading\n", a->name);
  return -ENOMEM;
 }

 use->source = a;
 use->target = b;
 list_add(&use->source_list, &b->source_list);
 list_add(&use->target_list, &a->target_list);
 return 0;
}


int ref_module(struct module *a, struct module *b)
{
 int err;

 if (b == NULL || already_uses(a, b))
  return 0;


 err = strong_try_module_get(b);
 if (err)
  return err;

 err = add_module_usage(a, b);
 if (err) {
  module_put(b);
  return err;
 }
 return 0;
}
EXPORT_SYMBOL_GPL(ref_module);


static void module_unload_free(struct module *mod)
{
 struct module_use *use, *tmp;

 mutex_lock(&module_mutex);
 list_for_each_entry_safe(use, tmp, &mod->target_list, target_list) {
  struct module *i = use->target;
  pr_debug("%s unusing %s\n", mod->name, i->name);
  module_put(i);
  list_del(&use->source_list);
  list_del(&use->target_list);
  kfree(use);
 }
 mutex_unlock(&module_mutex);
}

static inline int try_force_unload(unsigned int flags)
{
 int ret = (flags & O_TRUNC);
 if (ret)
  add_taint(TAINT_FORCED_RMMOD, LOCKDEP_NOW_UNRELIABLE);
 return ret;
}
static inline int try_force_unload(unsigned int flags)
{
 return 0;
}


static int try_release_module_ref(struct module *mod)
{
 int ret;


 ret = atomic_sub_return(MODULE_REF_BASE, &mod->refcnt);
 BUG_ON(ret < 0);
 if (ret)

  ret = atomic_add_unless(&mod->refcnt, MODULE_REF_BASE, 0);

 return ret;
}

static int try_stop_module(struct module *mod, int flags, int *forced)
{

 if (try_release_module_ref(mod) != 0) {
  *forced = try_force_unload(flags);
  if (!(*forced))
   return -EWOULDBLOCK;
 }


 mod->state = MODULE_STATE_GOING;

 return 0;
}
int module_refcount(struct module *mod)
{
 return atomic_read(&mod->refcnt) - MODULE_REF_BASE;
}
EXPORT_SYMBOL(module_refcount);


static void free_module(struct module *mod);

SYSCALL_DEFINE2(delete_module, const char __user *, name_user,
  unsigned int, flags)
{
 struct module *mod;
 char name[MODULE_NAME_LEN];
 int ret, forced = 0;

 if (!capable(CAP_SYS_MODULE) || modules_disabled)
  return -EPERM;

 if (strncpy_from_user(name, name_user, MODULE_NAME_LEN-1) < 0)
  return -EFAULT;
 name[MODULE_NAME_LEN-1] = '\0';

 if (mutex_lock_interruptible(&module_mutex) != 0)
  return -EINTR;

 mod = find_module(name);
 if (!mod) {
  ret = -ENOENT;
  goto out;
 }

 if (!list_empty(&mod->source_list)) {

  ret = -EWOULDBLOCK;
  goto out;
 }


 if (mod->state != MODULE_STATE_LIVE) {

  pr_debug("%s already dying\n", mod->name);
  ret = -EBUSY;
  goto out;
 }


 if (mod->init && !mod->exit) {
  forced = try_force_unload(flags);
  if (!forced) {

   ret = -EBUSY;
   goto out;
  }
 }


 ret = try_stop_module(mod, flags, &forced);
 if (ret != 0)
  goto out;

 mutex_unlock(&module_mutex);

 if (mod->exit != NULL)
  mod->exit();
 blocking_notifier_call_chain(&module_notify_list,
         MODULE_STATE_GOING, mod);
 klp_module_going(mod);
 ftrace_release_mod(mod);

 async_synchronize_full();


 strlcpy(last_unloaded_module, mod->name, sizeof(last_unloaded_module));

 free_module(mod);
 return 0;
out:
 mutex_unlock(&module_mutex);
 return ret;
}

static inline void print_unload_info(struct seq_file *m, struct module *mod)
{
 struct module_use *use;
 int printed_something = 0;

 seq_printf(m, " %i ", module_refcount(mod));





 list_for_each_entry(use, &mod->source_list, source_list) {
  printed_something = 1;
  seq_printf(m, "%s,", use->source->name);
 }

 if (mod->init != NULL && mod->exit == NULL) {
  printed_something = 1;
  seq_puts(m, "[permanent],");
 }

 if (!printed_something)
  seq_puts(m, "-");
}

void __symbol_put(const char *symbol)
{
 struct module *owner;

 preempt_disable();
 if (!find_symbol(symbol, &owner, NULL, true, false))
  BUG();
 module_put(owner);
 preempt_enable();
}
EXPORT_SYMBOL(__symbol_put);


void symbol_put_addr(void *addr)
{
 struct module *modaddr;
 unsigned long a = (unsigned long)dereference_function_descriptor(addr);

 if (core_kernel_text(a))
  return;





 preempt_disable();
 modaddr = __module_text_address(a);
 BUG_ON(!modaddr);
 module_put(modaddr);
 preempt_enable();
}
EXPORT_SYMBOL_GPL(symbol_put_addr);

static ssize_t show_refcnt(struct module_attribute *mattr,
      struct module_kobject *mk, char *buffer)
{
 return sprintf(buffer, "%i\n", module_refcount(mk->mod));
}

static struct module_attribute modinfo_refcnt =
 __ATTR(refcnt, 0444, show_refcnt, NULL);

void __module_get(struct module *module)
{
 if (module) {
  preempt_disable();
  atomic_inc(&module->refcnt);
  trace_module_get(module, _RET_IP_);
  preempt_enable();
 }
}
EXPORT_SYMBOL(__module_get);

bool try_module_get(struct module *module)
{
 bool ret = true;

 if (module) {
  preempt_disable();

  if (likely(module_is_live(module) &&
      atomic_inc_not_zero(&module->refcnt) != 0))
   trace_module_get(module, _RET_IP_);
  else
   ret = false;

  preempt_enable();
 }
 return ret;
}
EXPORT_SYMBOL(try_module_get);

void module_put(struct module *module)
{
 int ret;

 if (module) {
  preempt_disable();
  ret = atomic_dec_if_positive(&module->refcnt);
  WARN_ON(ret < 0);
  trace_module_put(module, _RET_IP_);
  preempt_enable();
 }
}
EXPORT_SYMBOL(module_put);

static inline void print_unload_info(struct seq_file *m, struct module *mod)
{

 seq_puts(m, " - -");
}

static inline void module_unload_free(struct module *mod)
{
}

int ref_module(struct module *a, struct module *b)
{
 return strong_try_module_get(b);
}
EXPORT_SYMBOL_GPL(ref_module);

static inline int module_unload_init(struct module *mod)
{
 return 0;
}

static size_t module_flags_taint(struct module *mod, char *buf)
{
 size_t l = 0;

 if (mod->taints & (1 << TAINT_PROPRIETARY_MODULE))
  buf[l++] = 'P';
 if (mod->taints & (1 << TAINT_OOT_MODULE))
  buf[l++] = 'O';
 if (mod->taints & (1 << TAINT_FORCED_MODULE))
  buf[l++] = 'F';
 if (mod->taints & (1 << TAINT_CRAP))
  buf[l++] = 'C';
 if (mod->taints & (1 << TAINT_UNSIGNED_MODULE))
  buf[l++] = 'E';





 return l;
}

static ssize_t show_initstate(struct module_attribute *mattr,
         struct module_kobject *mk, char *buffer)
{
 const char *state = "unknown";

 switch (mk->mod->state) {
 case MODULE_STATE_LIVE:
  state = "live";
  break;
 case MODULE_STATE_COMING:
  state = "coming";
  break;
 case MODULE_STATE_GOING:
  state = "going";
  break;
 default:
  BUG();
 }
 return sprintf(buffer, "%s\n", state);
}

static struct module_attribute modinfo_initstate =
 __ATTR(initstate, 0444, show_initstate, NULL);

static ssize_t store_uevent(struct module_attribute *mattr,
       struct module_kobject *mk,
       const char *buffer, size_t count)
{
 enum kobject_action action;

 if (kobject_action_type(buffer, count, &action) == 0)
  kobject_uevent(&mk->kobj, action);
 return count;
}

struct module_attribute module_uevent =
 __ATTR(uevent, 0200, NULL, store_uevent);

static ssize_t show_coresize(struct module_attribute *mattr,
        struct module_kobject *mk, char *buffer)
{
 return sprintf(buffer, "%u\n", mk->mod->core_layout.size);
}

static struct module_attribute modinfo_coresize =
 __ATTR(coresize, 0444, show_coresize, NULL);

static ssize_t show_initsize(struct module_attribute *mattr,
        struct module_kobject *mk, char *buffer)
{
 return sprintf(buffer, "%u\n", mk->mod->init_layout.size);
}

static struct module_attribute modinfo_initsize =
 __ATTR(initsize, 0444, show_initsize, NULL);

static ssize_t show_taint(struct module_attribute *mattr,
     struct module_kobject *mk, char *buffer)
{
 size_t l;

 l = module_flags_taint(mk->mod, buffer);
 buffer[l++] = '\n';
 return l;
}

static struct module_attribute modinfo_taint =
 __ATTR(taint, 0444, show_taint, NULL);

static struct module_attribute *modinfo_attrs[] = {
 &module_uevent,
 &modinfo_version,
 &modinfo_srcversion,
 &modinfo_initstate,
 &modinfo_coresize,
 &modinfo_initsize,
 &modinfo_taint,
 &modinfo_refcnt,
 NULL,
};

static const char vermagic[] = VERMAGIC_STRING;

static int try_to_force_load(struct module *mod, const char *reason)
{
 if (!test_taint(TAINT_FORCED_MODULE))
  pr_warn("%s: %s: kernel tainted.\n", mod->name, reason);
 add_taint_module(mod, TAINT_FORCED_MODULE, LOCKDEP_NOW_UNRELIABLE);
 return 0;
 return -ENOEXEC;
}


static unsigned long maybe_relocated(unsigned long crc,
         const struct module *crc_owner)
{
 if (crc_owner == NULL)
  return crc - (unsigned long)reloc_start;
 return crc;
}

static int check_version(Elf_Shdr *sechdrs,
    unsigned int versindex,
    const char *symname,
    struct module *mod,
    const unsigned long *crc,
    const struct module *crc_owner)
{
 unsigned int i, num_versions;
 struct modversion_info *versions;


 if (!crc)
  return 1;


 if (versindex == 0)
  return try_to_force_load(mod, symname) == 0;

 versions = (void *) sechdrs[versindex].sh_addr;
 num_versions = sechdrs[versindex].sh_size
  / sizeof(struct modversion_info);

 for (i = 0; i < num_versions; i++) {
  if (strcmp(versions[i].name, symname) != 0)
   continue;

  if (versions[i].crc == maybe_relocated(*crc, crc_owner))
   return 1;
  pr_debug("Found checksum %lX vs module %lX\n",
         maybe_relocated(*crc, crc_owner), versions[i].crc);
  goto bad_version;
 }

 pr_warn("%s: no symbol version for %s\n", mod->name, symname);
 return 0;

bad_version:
 pr_warn("%s: disagrees about version of symbol %s\n",
        mod->name, symname);
 return 0;
}

static inline int check_modstruct_version(Elf_Shdr *sechdrs,
       unsigned int versindex,
       struct module *mod)
{
 const unsigned long *crc;





 preempt_disable();
 if (!find_symbol(VMLINUX_SYMBOL_STR(module_layout), NULL,
    &crc, true, false)) {
  preempt_enable();
  BUG();
 }
 preempt_enable();
 return check_version(sechdrs, versindex,
        VMLINUX_SYMBOL_STR(module_layout), mod, crc,
        NULL);
}


static inline int same_magic(const char *amagic, const char *bmagic,
        bool has_crcs)
{
 if (has_crcs) {
  amagic += strcspn(amagic, " ");
  bmagic += strcspn(bmagic, " ");
 }
 return strcmp(amagic, bmagic) == 0;
}
static inline int check_version(Elf_Shdr *sechdrs,
    unsigned int versindex,
    const char *symname,
    struct module *mod,
    const unsigned long *crc,
    const struct module *crc_owner)
{
 return 1;
}

static inline int check_modstruct_version(Elf_Shdr *sechdrs,
       unsigned int versindex,
       struct module *mod)
{
 return 1;
}

static inline int same_magic(const char *amagic, const char *bmagic,
        bool has_crcs)
{
 return strcmp(amagic, bmagic) == 0;
}


static const struct kernel_symbol *resolve_symbol(struct module *mod,
        const struct load_info *info,
        const char *name,
        char ownername[])
{
 struct module *owner;
 const struct kernel_symbol *sym;
 const unsigned long *crc;
 int err;






 sched_annotate_sleep();
 mutex_lock(&module_mutex);
 sym = find_symbol(name, &owner, &crc,
     !(mod->taints & (1 << TAINT_PROPRIETARY_MODULE)), true);
 if (!sym)
  goto unlock;

 if (!check_version(info->sechdrs, info->index.vers, name, mod, crc,
      owner)) {
  sym = ERR_PTR(-EINVAL);
  goto getname;
 }

 err = ref_module(mod, owner);
 if (err) {
  sym = ERR_PTR(err);
  goto getname;
 }

getname:

 strncpy(ownername, module_name(owner), MODULE_NAME_LEN);
unlock:
 mutex_unlock(&module_mutex);
 return sym;
}

static const struct kernel_symbol *
resolve_symbol_wait(struct module *mod,
      const struct load_info *info,
      const char *name)
{
 const struct kernel_symbol *ksym;
 char owner[MODULE_NAME_LEN];

 if (wait_event_interruptible_timeout(module_wq,
   !IS_ERR(ksym = resolve_symbol(mod, info, name, owner))
   || PTR_ERR(ksym) != -EBUSY,
          30 * HZ) <= 0) {
  pr_warn("%s: gave up waiting for init of module %s.\n",
   mod->name, owner);
 }
 return ksym;
}






static inline bool sect_empty(const Elf_Shdr *sect)
{
 return !(sect->sh_flags & SHF_ALLOC) || sect->sh_size == 0;
}

struct module_sect_attr {
 struct module_attribute mattr;
 char *name;
 unsigned long address;
};

struct module_sect_attrs {
 struct attribute_group grp;
 unsigned int nsections;
 struct module_sect_attr attrs[0];
};

static ssize_t module_sect_show(struct module_attribute *mattr,
    struct module_kobject *mk, char *buf)
{
 struct module_sect_attr *sattr =
  container_of(mattr, struct module_sect_attr, mattr);
 return sprintf(buf, "0x%pK\n", (void *)sattr->address);
}

static void free_sect_attrs(struct module_sect_attrs *sect_attrs)
{
 unsigned int section;

 for (section = 0; section < sect_attrs->nsections; section++)
  kfree(sect_attrs->attrs[section].name);
 kfree(sect_attrs);
}

static void add_sect_attrs(struct module *mod, const struct load_info *info)
{
 unsigned int nloaded = 0, i, size[2];
 struct module_sect_attrs *sect_attrs;
 struct module_sect_attr *sattr;
 struct attribute **gattr;


 for (i = 0; i < info->hdr->e_shnum; i++)
  if (!sect_empty(&info->sechdrs[i]))
   nloaded++;
 size[0] = ALIGN(sizeof(*sect_attrs)
   + nloaded * sizeof(sect_attrs->attrs[0]),
   sizeof(sect_attrs->grp.attrs[0]));
 size[1] = (nloaded + 1) * sizeof(sect_attrs->grp.attrs[0]);
 sect_attrs = kzalloc(size[0] + size[1], GFP_KERNEL);
 if (sect_attrs == NULL)
  return;


 sect_attrs->grp.name = "sections";
 sect_attrs->grp.attrs = (void *)sect_attrs + size[0];

 sect_attrs->nsections = 0;
 sattr = &sect_attrs->attrs[0];
 gattr = &sect_attrs->grp.attrs[0];
 for (i = 0; i < info->hdr->e_shnum; i++) {
  Elf_Shdr *sec = &info->sechdrs[i];
  if (sect_empty(sec))
   continue;
  sattr->address = sec->sh_addr;
  sattr->name = kstrdup(info->secstrings + sec->sh_name,
     GFP_KERNEL);
  if (sattr->name == NULL)
   goto out;
  sect_attrs->nsections++;
  sysfs_attr_init(&sattr->mattr.attr);
  sattr->mattr.show = module_sect_show;
  sattr->mattr.store = NULL;
  sattr->mattr.attr.name = sattr->name;
  sattr->mattr.attr.mode = S_IRUGO;
  *(gattr++) = &(sattr++)->mattr.attr;
 }
 *gattr = NULL;

 if (sysfs_create_group(&mod->mkobj.kobj, &sect_attrs->grp))
  goto out;

 mod->sect_attrs = sect_attrs;
 return;
  out:
 free_sect_attrs(sect_attrs);
}

static void remove_sect_attrs(struct module *mod)
{
 if (mod->sect_attrs) {
  sysfs_remove_group(&mod->mkobj.kobj,
       &mod->sect_attrs->grp);


  free_sect_attrs(mod->sect_attrs);
  mod->sect_attrs = NULL;
 }
}





struct module_notes_attrs {
 struct kobject *dir;
 unsigned int notes;
 struct bin_attribute attrs[0];
};

static ssize_t module_notes_read(struct file *filp, struct kobject *kobj,
     struct bin_attribute *bin_attr,
     char *buf, loff_t pos, size_t count)
{



 memcpy(buf, bin_attr->private + pos, count);
 return count;
}

static void free_notes_attrs(struct module_notes_attrs *notes_attrs,
        unsigned int i)
{
 if (notes_attrs->dir) {
  while (i-- > 0)
   sysfs_remove_bin_file(notes_attrs->dir,
           &notes_attrs->attrs[i]);
  kobject_put(notes_attrs->dir);
 }
 kfree(notes_attrs);
}

static void add_notes_attrs(struct module *mod, const struct load_info *info)
{
 unsigned int notes, loaded, i;
 struct module_notes_attrs *notes_attrs;
 struct bin_attribute *nattr;


 if (!mod->sect_attrs)
  return;


 notes = 0;
 for (i = 0; i < info->hdr->e_shnum; i++)
  if (!sect_empty(&info->sechdrs[i]) &&
      (info->sechdrs[i].sh_type == SHT_NOTE))
   ++notes;

 if (notes == 0)
  return;

 notes_attrs = kzalloc(sizeof(*notes_attrs)
         + notes * sizeof(notes_attrs->attrs[0]),
         GFP_KERNEL);
 if (notes_attrs == NULL)
  return;

 notes_attrs->notes = notes;
 nattr = &notes_attrs->attrs[0];
 for (loaded = i = 0; i < info->hdr->e_shnum; ++i) {
  if (sect_empty(&info->sechdrs[i]))
   continue;
  if (info->sechdrs[i].sh_type == SHT_NOTE) {
   sysfs_bin_attr_init(nattr);
   nattr->attr.name = mod->sect_attrs->attrs[loaded].name;
   nattr->attr.mode = S_IRUGO;
   nattr->size = info->sechdrs[i].sh_size;
   nattr->private = (void *) info->sechdrs[i].sh_addr;
   nattr->read = module_notes_read;
   ++nattr;
  }
  ++loaded;
 }

 notes_attrs->dir = kobject_create_and_add("notes", &mod->mkobj.kobj);
 if (!notes_attrs->dir)
  goto out;

 for (i = 0; i < notes; ++i)
  if (sysfs_create_bin_file(notes_attrs->dir,
       &notes_attrs->attrs[i]))
   goto out;

 mod->notes_attrs = notes_attrs;
 return;

  out:
 free_notes_attrs(notes_attrs, i);
}

static void remove_notes_attrs(struct module *mod)
{
 if (mod->notes_attrs)
  free_notes_attrs(mod->notes_attrs, mod->notes_attrs->notes);
}


static inline void add_sect_attrs(struct module *mod,
      const struct load_info *info)
{
}

static inline void remove_sect_attrs(struct module *mod)
{
}

static inline void add_notes_attrs(struct module *mod,
       const struct load_info *info)
{
}

static inline void remove_notes_attrs(struct module *mod)
{
}

static void add_usage_links(struct module *mod)
{
 struct module_use *use;
 int nowarn;

 mutex_lock(&module_mutex);
 list_for_each_entry(use, &mod->target_list, target_list) {
  nowarn = sysfs_create_link(use->target->holders_dir,
        &mod->mkobj.kobj, mod->name);
 }
 mutex_unlock(&module_mutex);
}

static void del_usage_links(struct module *mod)
{
 struct module_use *use;

 mutex_lock(&module_mutex);
 list_for_each_entry(use, &mod->target_list, target_list)
  sysfs_remove_link(use->target->holders_dir, mod->name);
 mutex_unlock(&module_mutex);
}

static int module_add_modinfo_attrs(struct module *mod)
{
 struct module_attribute *attr;
 struct module_attribute *temp_attr;
 int error = 0;
 int i;

 mod->modinfo_attrs = kzalloc((sizeof(struct module_attribute) *
     (ARRAY_SIZE(modinfo_attrs) + 1)),
     GFP_KERNEL);
 if (!mod->modinfo_attrs)
  return -ENOMEM;

 temp_attr = mod->modinfo_attrs;
 for (i = 0; (attr = modinfo_attrs[i]) && !error; i++) {
  if (!attr->test ||
      (attr->test && attr->test(mod))) {
   memcpy(temp_attr, attr, sizeof(*temp_attr));
   sysfs_attr_init(&temp_attr->attr);
   error = sysfs_create_file(&mod->mkobj.kobj,
     &temp_attr->attr);
   ++temp_attr;
  }
 }
 return error;
}

static void module_remove_modinfo_attrs(struct module *mod)
{
 struct module_attribute *attr;
 int i;

 for (i = 0; (attr = &mod->modinfo_attrs[i]); i++) {

  if (!attr->attr.name)
   break;
  sysfs_remove_file(&mod->mkobj.kobj, &attr->attr);
  if (attr->free)
   attr->free(mod);
 }
 kfree(mod->modinfo_attrs);
}

static void mod_kobject_put(struct module *mod)
{
 DECLARE_COMPLETION_ONSTACK(c);
 mod->mkobj.kobj_completion = &c;
 kobject_put(&mod->mkobj.kobj);
 wait_for_completion(&c);
}

static int mod_sysfs_init(struct module *mod)
{
 int err;
 struct kobject *kobj;

 if (!module_sysfs_initialized) {
  pr_err("%s: module sysfs not initialized\n", mod->name);
  err = -EINVAL;
  goto out;
 }

 kobj = kset_find_obj(module_kset, mod->name);
 if (kobj) {
  pr_err("%s: module is already loaded\n", mod->name);
  kobject_put(kobj);
  err = -EINVAL;
  goto out;
 }

 mod->mkobj.mod = mod;

 memset(&mod->mkobj.kobj, 0, sizeof(mod->mkobj.kobj));
 mod->mkobj.kobj.kset = module_kset;
 err = kobject_init_and_add(&mod->mkobj.kobj, &module_ktype, NULL,
       "%s", mod->name);
 if (err)
  mod_kobject_put(mod);


out:
 return err;
}

static int mod_sysfs_setup(struct module *mod,
      const struct load_info *info,
      struct kernel_param *kparam,
      unsigned int num_params)
{
 int err;

 err = mod_sysfs_init(mod);
 if (err)
  goto out;

 mod->holders_dir = kobject_create_and_add("holders", &mod->mkobj.kobj);
 if (!mod->holders_dir) {
  err = -ENOMEM;
  goto out_unreg;
 }

 err = module_param_sysfs_setup(mod, kparam, num_params);
 if (err)
  goto out_unreg_holders;

 err = module_add_modinfo_attrs(mod);
 if (err)
  goto out_unreg_param;

 add_usage_links(mod);
 add_sect_attrs(mod, info);
 add_notes_attrs(mod, info);

 kobject_uevent(&mod->mkobj.kobj, KOBJ_ADD);
 return 0;

out_unreg_param:
 module_param_sysfs_remove(mod);
out_unreg_holders:
 kobject_put(mod->holders_dir);
out_unreg:
 mod_kobject_put(mod);
out:
 return err;
}

static void mod_sysfs_fini(struct module *mod)
{
 remove_notes_attrs(mod);
 remove_sect_attrs(mod);
 mod_kobject_put(mod);
}

static void init_param_lock(struct module *mod)
{
 mutex_init(&mod->param_lock);
}

static int mod_sysfs_setup(struct module *mod,
      const struct load_info *info,
      struct kernel_param *kparam,
      unsigned int num_params)
{
 return 0;
}

static void mod_sysfs_fini(struct module *mod)
{
}

static void module_remove_modinfo_attrs(struct module *mod)
{
}

static void del_usage_links(struct module *mod)
{
}

static void init_param_lock(struct module *mod)
{
}

static void mod_sysfs_teardown(struct module *mod)
{
 del_usage_links(mod);
 module_remove_modinfo_attrs(mod);
 module_param_sysfs_remove(mod);
 kobject_put(mod->mkobj.drivers_dir);
 kobject_put(mod->holders_dir);
 mod_sysfs_fini(mod);
}

static void frob_text(const struct module_layout *layout,
        int (*set_memory)(unsigned long start, int num_pages))
{
 BUG_ON((unsigned long)layout->base & (PAGE_SIZE-1));
 BUG_ON((unsigned long)layout->text_size & (PAGE_SIZE-1));
 set_memory((unsigned long)layout->base,
     layout->text_size >> PAGE_SHIFT);
}

static void frob_rodata(const struct module_layout *layout,
   int (*set_memory)(unsigned long start, int num_pages))
{
 BUG_ON((unsigned long)layout->base & (PAGE_SIZE-1));
 BUG_ON((unsigned long)layout->text_size & (PAGE_SIZE-1));
 BUG_ON((unsigned long)layout->ro_size & (PAGE_SIZE-1));
 set_memory((unsigned long)layout->base + layout->text_size,
     (layout->ro_size - layout->text_size) >> PAGE_SHIFT);
}

static void frob_writable_data(const struct module_layout *layout,
          int (*set_memory)(unsigned long start, int num_pages))
{
 BUG_ON((unsigned long)layout->base & (PAGE_SIZE-1));
 BUG_ON((unsigned long)layout->ro_size & (PAGE_SIZE-1));
 BUG_ON((unsigned long)layout->size & (PAGE_SIZE-1));
 set_memory((unsigned long)layout->base + layout->ro_size,
     (layout->size - layout->ro_size) >> PAGE_SHIFT);
}


void module_disable_ro(const struct module *mod)
{
 frob_text(&mod->core_layout, set_memory_rw);
 frob_rodata(&mod->core_layout, set_memory_rw);
 frob_text(&mod->init_layout, set_memory_rw);
 frob_rodata(&mod->init_layout, set_memory_rw);
}

void module_enable_ro(const struct module *mod)
{
 frob_text(&mod->core_layout, set_memory_ro);
 frob_rodata(&mod->core_layout, set_memory_ro);
 frob_text(&mod->init_layout, set_memory_ro);
 frob_rodata(&mod->init_layout, set_memory_ro);
}

static void module_enable_nx(const struct module *mod)
{
 frob_rodata(&mod->core_layout, set_memory_nx);
 frob_writable_data(&mod->core_layout, set_memory_nx);
 frob_rodata(&mod->init_layout, set_memory_nx);
 frob_writable_data(&mod->init_layout, set_memory_nx);
}

static void module_disable_nx(const struct module *mod)
{
 frob_rodata(&mod->core_layout, set_memory_x);
 frob_writable_data(&mod->core_layout, set_memory_x);
 frob_rodata(&mod->init_layout, set_memory_x);
 frob_writable_data(&mod->init_layout, set_memory_x);
}


void set_all_modules_text_rw(void)
{
 struct module *mod;

 mutex_lock(&module_mutex);
 list_for_each_entry_rcu(mod, &modules, list) {
  if (mod->state == MODULE_STATE_UNFORMED)
   continue;

  frob_text(&mod->core_layout, set_memory_rw);
  frob_text(&mod->init_layout, set_memory_rw);
 }
 mutex_unlock(&module_mutex);
}


void set_all_modules_text_ro(void)
{
 struct module *mod;

 mutex_lock(&module_mutex);
 list_for_each_entry_rcu(mod, &modules, list) {
  if (mod->state == MODULE_STATE_UNFORMED)
   continue;

  frob_text(&mod->core_layout, set_memory_ro);
  frob_text(&mod->init_layout, set_memory_ro);
 }
 mutex_unlock(&module_mutex);
}

static void disable_ro_nx(const struct module_layout *layout)
{
 frob_text(layout, set_memory_rw);
 frob_rodata(layout, set_memory_rw);
 frob_rodata(layout, set_memory_x);
 frob_writable_data(layout, set_memory_x);
}

static void disable_ro_nx(const struct module_layout *layout) { }
static void module_enable_nx(const struct module *mod) { }
static void module_disable_nx(const struct module *mod) { }






static int copy_module_elf(struct module *mod, struct load_info *info)
{
 unsigned int size, symndx;
 int ret;

 size = sizeof(*mod->klp_info);
 mod->klp_info = kmalloc(size, GFP_KERNEL);
 if (mod->klp_info == NULL)
  return -ENOMEM;


 size = sizeof(mod->klp_info->hdr);
 memcpy(&mod->klp_info->hdr, info->hdr, size);


 size = sizeof(*info->sechdrs) * info->hdr->e_shnum;
 mod->klp_info->sechdrs = kmalloc(size, GFP_KERNEL);
 if (mod->klp_info->sechdrs == NULL) {
  ret = -ENOMEM;
  goto free_info;
 }
 memcpy(mod->klp_info->sechdrs, info->sechdrs, size);


 size = info->sechdrs[info->hdr->e_shstrndx].sh_size;
 mod->klp_info->secstrings = kmalloc(size, GFP_KERNEL);
 if (mod->klp_info->secstrings == NULL) {
  ret = -ENOMEM;
  goto free_sechdrs;
 }
 memcpy(mod->klp_info->secstrings, info->secstrings, size);


 symndx = info->index.sym;
 mod->klp_info->symndx = symndx;







 mod->klp_info->sechdrs[symndx].sh_addr = \
  (unsigned long) mod->core_kallsyms.symtab;

 return 0;

free_sechdrs:
 kfree(mod->klp_info->sechdrs);
free_info:
 kfree(mod->klp_info);
 return ret;
}

static void free_module_elf(struct module *mod)
{
 kfree(mod->klp_info->sechdrs);
 kfree(mod->klp_info->secstrings);
 kfree(mod->klp_info);
}
static int copy_module_elf(struct module *mod, struct load_info *info)
{
 return 0;
}

static void free_module_elf(struct module *mod)
{
}

void __weak module_memfree(void *module_region)
{
 vfree(module_region);
}

void __weak module_arch_cleanup(struct module *mod)
{
}

void __weak module_arch_freeing_init(struct module *mod)
{
}


static void free_module(struct module *mod)
{
 trace_module_free(mod);

 mod_sysfs_teardown(mod);



 mutex_lock(&module_mutex);
 mod->state = MODULE_STATE_UNFORMED;
 mutex_unlock(&module_mutex);


 ddebug_remove_module(mod->name);


 module_arch_cleanup(mod);


 module_unload_free(mod);


 destroy_params(mod->kp, mod->num_kp);

 if (is_livepatch_module(mod))
  free_module_elf(mod);


 mutex_lock(&module_mutex);

 list_del_rcu(&mod->list);
 mod_tree_remove(mod);

 module_bug_cleanup(mod);

 synchronize_sched();
 mutex_unlock(&module_mutex);


 disable_ro_nx(&mod->init_layout);
 module_arch_freeing_init(mod);
 module_memfree(mod->init_layout.base);
 kfree(mod->args);
 percpu_modfree(mod);


 lockdep_free_key_range(mod->core_layout.base, mod->core_layout.size);


 disable_ro_nx(&mod->core_layout);
 module_memfree(mod->core_layout.base);

 update_protections(current->mm);
}

void *__symbol_get(const char *symbol)
{
 struct module *owner;
 const struct kernel_symbol *sym;

 preempt_disable();
 sym = find_symbol(symbol, &owner, NULL, true, true);
 if (sym && strong_try_module_get(owner))
  sym = NULL;
 preempt_enable();

 return sym ? (void *)sym->value : NULL;
}
EXPORT_SYMBOL_GPL(__symbol_get);







static int verify_export_symbols(struct module *mod)
{
 unsigned int i;
 struct module *owner;
 const struct kernel_symbol *s;
 struct {
  const struct kernel_symbol *sym;
  unsigned int num;
 } arr[] = {
  { mod->syms, mod->num_syms },
  { mod->gpl_syms, mod->num_gpl_syms },
  { mod->gpl_future_syms, mod->num_gpl_future_syms },
  { mod->unused_syms, mod->num_unused_syms },
  { mod->unused_gpl_syms, mod->num_unused_gpl_syms },
 };

 for (i = 0; i < ARRAY_SIZE(arr); i++) {
  for (s = arr[i].sym; s < arr[i].sym + arr[i].num; s++) {
   if (find_symbol(s->name, &owner, NULL, true, false)) {
    pr_err("%s: exports duplicate symbol %s"
           " (owned by %s)\n",
           mod->name, s->name, module_name(owner));
    return -ENOEXEC;
   }
  }
 }
 return 0;
}


static int simplify_symbols(struct module *mod, const struct load_info *info)
{
 Elf_Shdr *symsec = &info->sechdrs[info->index.sym];
 Elf_Sym *sym = (void *)symsec->sh_addr;
 unsigned long secbase;
 unsigned int i;
 int ret = 0;
 const struct kernel_symbol *ksym;

 for (i = 1; i < symsec->sh_size / sizeof(Elf_Sym); i++) {
  const char *name = info->strtab + sym[i].st_name;

  switch (sym[i].st_shndx) {
  case SHN_COMMON:

   if (!strncmp(name, "__gnu_lto", 9))
    break;



   pr_debug("Common symbol: %s\n", name);
   pr_warn("%s: please compile with -fno-common\n",
          mod->name);
   ret = -ENOEXEC;
   break;

  case SHN_ABS:

   pr_debug("Absolute symbol: 0x%08lx\n",
          (long)sym[i].st_value);
   break;

  case SHN_LIVEPATCH:

   break;

  case SHN_UNDEF:
   ksym = resolve_symbol_wait(mod, info, name);

   if (ksym && !IS_ERR(ksym)) {
    sym[i].st_value = ksym->value;
    break;
   }


   if (!ksym && ELF_ST_BIND(sym[i].st_info) == STB_WEAK)
    break;

   pr_warn("%s: Unknown symbol %s (err %li)\n",
    mod->name, name, PTR_ERR(ksym));
   ret = PTR_ERR(ksym) ?: -ENOENT;
   break;

  default:

   if (sym[i].st_shndx == info->index.pcpu)
    secbase = (unsigned long)mod_percpu(mod);
   else
    secbase = info->sechdrs[sym[i].st_shndx].sh_addr;
   sym[i].st_value += secbase;
   break;
  }
 }

 return ret;
}

static int apply_relocations(struct module *mod, const struct load_info *info)
{
 unsigned int i;
 int err = 0;


 for (i = 1; i < info->hdr->e_shnum; i++) {
  unsigned int infosec = info->sechdrs[i].sh_info;


  if (infosec >= info->hdr->e_shnum)
   continue;


  if (!(info->sechdrs[infosec].sh_flags & SHF_ALLOC))
   continue;


  if (info->sechdrs[i].sh_flags & SHF_RELA_LIVEPATCH)
   continue;

  if (info->sechdrs[i].sh_type == SHT_REL)
   err = apply_relocate(info->sechdrs, info->strtab,
          info->index.sym, i, mod);
  else if (info->sechdrs[i].sh_type == SHT_RELA)
   err = apply_relocate_add(info->sechdrs, info->strtab,
       info->index.sym, i, mod);
  if (err < 0)
   break;
 }
 return err;
}


unsigned int __weak arch_mod_section_prepend(struct module *mod,
          unsigned int section)
{

 return 0;
}


static long get_offset(struct module *mod, unsigned int *size,
         Elf_Shdr *sechdr, unsigned int section)
{
 long ret;

 *size += arch_mod_section_prepend(mod, section);
 ret = ALIGN(*size, sechdr->sh_addralign ?: 1);
 *size = ret + sechdr->sh_size;
 return ret;
}





static void layout_sections(struct module *mod, struct load_info *info)
{
 static unsigned long const masks[][2] = {



  { SHF_EXECINSTR | SHF_ALLOC, ARCH_SHF_SMALL },
  { SHF_ALLOC, SHF_WRITE | ARCH_SHF_SMALL },
  { SHF_WRITE | SHF_ALLOC, ARCH_SHF_SMALL },
  { ARCH_SHF_SMALL | SHF_ALLOC, 0 }
 };
 unsigned int m, i;

 for (i = 0; i < info->hdr->e_shnum; i++)
  info->sechdrs[i].sh_entsize = ~0UL;

 pr_debug("Core section allocation order:\n");
 for (m = 0; m < ARRAY_SIZE(masks); ++m) {
  for (i = 0; i < info->hdr->e_shnum; ++i) {
   Elf_Shdr *s = &info->sechdrs[i];
   const char *sname = info->secstrings + s->sh_name;

   if ((s->sh_flags & masks[m][0]) != masks[m][0]
       || (s->sh_flags & masks[m][1])
       || s->sh_entsize != ~0UL
       || strstarts(sname, ".init"))
    continue;
   s->sh_entsize = get_offset(mod, &mod->core_layout.size, s, i);
   pr_debug("\t%s\n", sname);
  }
  switch (m) {
  case 0:
   mod->core_layout.size = debug_align(mod->core_layout.size);
   mod->core_layout.text_size = mod->core_layout.size;
   break;
  case 1:
   mod->core_layout.size = debug_align(mod->core_layout.size);
   mod->core_layout.ro_size = mod->core_layout.size;
   break;
  case 3:
   mod->core_layout.size = debug_align(mod->core_layout.size);
   break;
  }
 }

 pr_debug("Init section allocation order:\n");
 for (m = 0; m < ARRAY_SIZE(masks); ++m) {
  for (i = 0; i < info->hdr->e_shnum; ++i) {
   Elf_Shdr *s = &info->sechdrs[i];
   const char *sname = info->secstrings + s->sh_name;

   if ((s->sh_flags & masks[m][0]) != masks[m][0]
       || (s->sh_flags & masks[m][1])
       || s->sh_entsize != ~0UL
       || !strstarts(sname, ".init"))
    continue;
   s->sh_entsize = (get_offset(mod, &mod->init_layout.size, s, i)
      | INIT_OFFSET_MASK);
   pr_debug("\t%s\n", sname);
  }
  switch (m) {
  case 0:
   mod->init_layout.size = debug_align(mod->init_layout.size);
   mod->init_layout.text_size = mod->init_layout.size;
   break;
  case 1:
   mod->init_layout.size = debug_align(mod->init_layout.size);
   mod->init_layout.ro_size = mod->init_layout.size;
   break;
  case 3:
   mod->init_layout.size = debug_align(mod->init_layout.size);
   break;
  }
 }
}

static void set_license(struct module *mod, const char *license)
{
 if (!license)
  license = "unspecified";

 if (!license_is_gpl_compatible(license)) {
  if (!test_taint(TAINT_PROPRIETARY_MODULE))
   pr_warn("%s: module license '%s' taints kernel.\n",
    mod->name, license);
  add_taint_module(mod, TAINT_PROPRIETARY_MODULE,
     LOCKDEP_NOW_UNRELIABLE);
 }
}


static char *next_string(char *string, unsigned long *secsize)
{

 while (string[0]) {
  string++;
  if ((*secsize)-- <= 1)
   return NULL;
 }


 while (!string[0]) {
  string++;
  if ((*secsize)-- <= 1)
   return NULL;
 }
 return string;
}

static char *get_modinfo(struct load_info *info, const char *tag)
{
 char *p;
 unsigned int taglen = strlen(tag);
 Elf_Shdr *infosec = &info->sechdrs[info->index.info];
 unsigned long size = infosec->sh_size;

 for (p = (char *)infosec->sh_addr; p; p = next_string(p, &size)) {
  if (strncmp(p, tag, taglen) == 0 && p[taglen] == '=')
   return p + taglen + 1;
 }
 return NULL;
}

static void setup_modinfo(struct module *mod, struct load_info *info)
{
 struct module_attribute *attr;
 int i;

 for (i = 0; (attr = modinfo_attrs[i]); i++) {
  if (attr->setup)
   attr->setup(mod, get_modinfo(info, attr->attr.name));
 }
}

static void free_modinfo(struct module *mod)
{
 struct module_attribute *attr;
 int i;

 for (i = 0; (attr = modinfo_attrs[i]); i++) {
  if (attr->free)
   attr->free(mod);
 }
}



static const struct kernel_symbol *lookup_symbol(const char *name,
 const struct kernel_symbol *start,
 const struct kernel_symbol *stop)
{
 return bsearch(name, start, stop - start,
   sizeof(struct kernel_symbol), cmp_name);
}

static int is_exported(const char *name, unsigned long value,
         const struct module *mod)
{
 const struct kernel_symbol *ks;
 if (!mod)
  ks = lookup_symbol(name, __start___ksymtab, __stop___ksymtab);
 else
  ks = lookup_symbol(name, mod->syms, mod->syms + mod->num_syms);
 return ks != NULL && ks->value == value;
}


static char elf_type(const Elf_Sym *sym, const struct load_info *info)
{
 const Elf_Shdr *sechdrs = info->sechdrs;

 if (ELF_ST_BIND(sym->st_info) == STB_WEAK) {
  if (ELF_ST_TYPE(sym->st_info) == STT_OBJECT)
   return 'v';
  else
   return 'w';
 }
 if (sym->st_shndx == SHN_UNDEF)
  return 'U';
 if (sym->st_shndx == SHN_ABS || sym->st_shndx == info->index.pcpu)
  return 'a';
 if (sym->st_shndx >= SHN_LORESERVE)
  return '?';
 if (sechdrs[sym->st_shndx].sh_flags & SHF_EXECINSTR)
  return 't';
 if (sechdrs[sym->st_shndx].sh_flags & SHF_ALLOC
     && sechdrs[sym->st_shndx].sh_type != SHT_NOBITS) {
  if (!(sechdrs[sym->st_shndx].sh_flags & SHF_WRITE))
   return 'r';
  else if (sechdrs[sym->st_shndx].sh_flags & ARCH_SHF_SMALL)
   return 'g';
  else
   return 'd';
 }
 if (sechdrs[sym->st_shndx].sh_type == SHT_NOBITS) {
  if (sechdrs[sym->st_shndx].sh_flags & ARCH_SHF_SMALL)
   return 's';
  else
   return 'b';
 }
 if (strstarts(info->secstrings + sechdrs[sym->st_shndx].sh_name,
        ".debug")) {
  return 'n';
 }
 return '?';
}

static bool is_core_symbol(const Elf_Sym *src, const Elf_Shdr *sechdrs,
   unsigned int shnum, unsigned int pcpundx)
{
 const Elf_Shdr *sec;

 if (src->st_shndx == SHN_UNDEF
     || src->st_shndx >= shnum
     || !src->st_name)
  return false;

 if (src->st_shndx == pcpundx)
  return true;

 sec = sechdrs + src->st_shndx;
 if (!(sec->sh_flags & SHF_ALLOC)
     || !(sec->sh_flags & SHF_EXECINSTR)
     || (sec->sh_entsize & INIT_OFFSET_MASK))
  return false;

 return true;
}
static void layout_symtab(struct module *mod, struct load_info *info)
{
 Elf_Shdr *symsect = info->sechdrs + info->index.sym;
 Elf_Shdr *strsect = info->sechdrs + info->index.str;
 const Elf_Sym *src;
 unsigned int i, nsrc, ndst, strtab_size = 0;


 symsect->sh_flags |= SHF_ALLOC;
 symsect->sh_entsize = get_offset(mod, &mod->init_layout.size, symsect,
      info->index.sym) | INIT_OFFSET_MASK;
 pr_debug("\t%s\n", info->secstrings + symsect->sh_name);

 src = (void *)info->hdr + symsect->sh_offset;
 nsrc = symsect->sh_size / sizeof(*src);


 for (ndst = i = 0; i < nsrc; i++) {
  if (i == 0 || is_livepatch_module(mod) ||
      is_core_symbol(src+i, info->sechdrs, info->hdr->e_shnum,
       info->index.pcpu)) {
   strtab_size += strlen(&info->strtab[src[i].st_name])+1;
   ndst++;
  }
 }


 info->symoffs = ALIGN(mod->core_layout.size, symsect->sh_addralign ?: 1);
 info->stroffs = mod->core_layout.size = info->symoffs + ndst * sizeof(Elf_Sym);
 mod->core_layout.size += strtab_size;
 mod->core_layout.size = debug_align(mod->core_layout.size);


 strsect->sh_flags |= SHF_ALLOC;
 strsect->sh_entsize = get_offset(mod, &mod->init_layout.size, strsect,
      info->index.str) | INIT_OFFSET_MASK;
 pr_debug("\t%s\n", info->secstrings + strsect->sh_name);


 mod->init_layout.size = ALIGN(mod->init_layout.size,
          __alignof__(struct mod_kallsyms));
 info->mod_kallsyms_init_off = mod->init_layout.size;
 mod->init_layout.size += sizeof(struct mod_kallsyms);
 mod->init_layout.size = debug_align(mod->init_layout.size);
}






static void add_kallsyms(struct module *mod, const struct load_info *info)
{
 unsigned int i, ndst;
 const Elf_Sym *src;
 Elf_Sym *dst;
 char *s;
 Elf_Shdr *symsec = &info->sechdrs[info->index.sym];


 mod->kallsyms = mod->init_layout.base + info->mod_kallsyms_init_off;

 mod->kallsyms->symtab = (void *)symsec->sh_addr;
 mod->kallsyms->num_symtab = symsec->sh_size / sizeof(Elf_Sym);

 mod->kallsyms->strtab = (void *)info->sechdrs[info->index.str].sh_addr;


 for (i = 0; i < mod->kallsyms->num_symtab; i++)
  mod->kallsyms->symtab[i].st_info
   = elf_type(&mod->kallsyms->symtab[i], info);


 mod->core_kallsyms.symtab = dst = mod->core_layout.base + info->symoffs;
 mod->core_kallsyms.strtab = s = mod->core_layout.base + info->stroffs;
 src = mod->kallsyms->symtab;
 for (ndst = i = 0; i < mod->kallsyms->num_symtab; i++) {
  if (i == 0 || is_livepatch_module(mod) ||
      is_core_symbol(src+i, info->sechdrs, info->hdr->e_shnum,
       info->index.pcpu)) {
   dst[ndst] = src[i];
   dst[ndst++].st_name = s - mod->core_kallsyms.strtab;
   s += strlcpy(s, &mod->kallsyms->strtab[src[i].st_name],
         KSYM_NAME_LEN) + 1;
  }
 }
 mod->core_kallsyms.num_symtab = ndst;
}
static inline void layout_symtab(struct module *mod, struct load_info *info)
{
}

static void add_kallsyms(struct module *mod, const struct load_info *info)
{
}

static void dynamic_debug_setup(struct _ddebug *debug, unsigned int num)
{
 if (!debug)
  return;
 if (ddebug_add_module(debug, num, debug->modname))
  pr_err("dynamic debug error adding module: %s\n",
   debug->modname);
}

static void dynamic_debug_remove(struct _ddebug *debug)
{
 if (debug)
  ddebug_remove_module(debug->modname);
}

void * __weak module_alloc(unsigned long size)
{
 return vmalloc_exec(size);
}

static void kmemleak_load_module(const struct module *mod,
     const struct load_info *info)
{
 unsigned int i;


 kmemleak_scan_area(mod, sizeof(struct module), GFP_KERNEL);

 for (i = 1; i < info->hdr->e_shnum; i++) {

  if (!(info->sechdrs[i].sh_flags & SHF_ALLOC) ||
      !(info->sechdrs[i].sh_flags & SHF_WRITE) ||
      (info->sechdrs[i].sh_flags & SHF_EXECINSTR))
   continue;

  kmemleak_scan_area((void *)info->sechdrs[i].sh_addr,
       info->sechdrs[i].sh_size, GFP_KERNEL);
 }
}
static inline void kmemleak_load_module(const struct module *mod,
     const struct load_info *info)
{
}

static int module_sig_check(struct load_info *info)
{
 int err = -ENOKEY;
 const unsigned long markerlen = sizeof(MODULE_SIG_STRING) - 1;
 const void *mod = info->hdr;

 if (info->len > markerlen &&
     memcmp(mod + info->len - markerlen, MODULE_SIG_STRING, markerlen) == 0) {

  info->len -= markerlen;
  err = mod_verify_sig(mod, &info->len);
 }

 if (!err) {
  info->sig_ok = true;
  return 0;
 }


 if (err == -ENOKEY && !sig_enforce)
  err = 0;

 return err;
}
static int module_sig_check(struct load_info *info)
{
 return 0;
}


static int elf_header_check(struct load_info *info)
{
 if (info->len < sizeof(*(info->hdr)))
  return -ENOEXEC;

 if (memcmp(info->hdr->e_ident, ELFMAG, SELFMAG) != 0
     || info->hdr->e_type != ET_REL
     || !elf_check_arch(info->hdr)
     || info->hdr->e_shentsize != sizeof(Elf_Shdr))
  return -ENOEXEC;

 if (info->hdr->e_shoff >= info->len
     || (info->hdr->e_shnum * sizeof(Elf_Shdr) >
  info->len - info->hdr->e_shoff))
  return -ENOEXEC;

 return 0;
}


static int copy_chunked_from_user(void *dst, const void __user *usrc, unsigned long len)
{
 do {
  unsigned long n = min(len, COPY_CHUNK_SIZE);

  if (copy_from_user(dst, usrc, n) != 0)
   return -EFAULT;
  cond_resched();
  dst += n;
  usrc += n;
  len -= n;
 } while (len);
 return 0;
}

static int find_livepatch_modinfo(struct module *mod, struct load_info *info)
{
 mod->klp = get_modinfo(info, "livepatch") ? true : false;

 return 0;
}
static int find_livepatch_modinfo(struct module *mod, struct load_info *info)
{
 if (get_modinfo(info, "livepatch")) {
  pr_err("%s: module is marked as livepatch module, but livepatch support is disabled",
         mod->name);
  return -ENOEXEC;
 }

 return 0;
}


static int copy_module_from_user(const void __user *umod, unsigned long len,
      struct load_info *info)
{
 int err;

 info->len = len;
 if (info->len < sizeof(*(info->hdr)))
  return -ENOEXEC;

 err = security_kernel_read_file(NULL, READING_MODULE);
 if (err)
  return err;


 info->hdr = __vmalloc(info->len,
   GFP_KERNEL | __GFP_HIGHMEM | __GFP_NOWARN, PAGE_KERNEL);
 if (!info->hdr)
  return -ENOMEM;

 if (copy_chunked_from_user(info->hdr, umod, info->len) != 0) {
  vfree(info->hdr);
  return -EFAULT;
 }

 return 0;
}

static void free_copy(struct load_info *info)
{
 vfree(info->hdr);
}

static int rewrite_section_headers(struct load_info *info, int flags)
{
 unsigned int i;


 info->sechdrs[0].sh_addr = 0;

 for (i = 1; i < info->hdr->e_shnum; i++) {
  Elf_Shdr *shdr = &info->sechdrs[i];
  if (shdr->sh_type != SHT_NOBITS
      && info->len < shdr->sh_offset + shdr->sh_size) {
   pr_err("Module len %lu truncated\n", info->len);
   return -ENOEXEC;
  }



  shdr->sh_addr = (size_t)info->hdr + shdr->sh_offset;


  if (strstarts(info->secstrings+shdr->sh_name, ".exit"))
   shdr->sh_flags &= ~(unsigned long)SHF_ALLOC;
 }


 if (flags & MODULE_INIT_IGNORE_MODVERSIONS)
  info->index.vers = 0;
 else
  info->index.vers = find_sec(info, "__versions");
 info->index.info = find_sec(info, ".modinfo");
 info->sechdrs[info->index.info].sh_flags &= ~(unsigned long)SHF_ALLOC;
 info->sechdrs[info->index.vers].sh_flags &= ~(unsigned long)SHF_ALLOC;
 return 0;
}
static struct module *setup_load_info(struct load_info *info, int flags)
{
 unsigned int i;
 int err;
 struct module *mod;


 info->sechdrs = (void *)info->hdr + info->hdr->e_shoff;
 info->secstrings = (void *)info->hdr
  + info->sechdrs[info->hdr->e_shstrndx].sh_offset;

 err = rewrite_section_headers(info, flags);
 if (err)
  return ERR_PTR(err);


 for (i = 1; i < info->hdr->e_shnum; i++) {
  if (info->sechdrs[i].sh_type == SHT_SYMTAB) {
   info->index.sym = i;
   info->index.str = info->sechdrs[i].sh_link;
   info->strtab = (char *)info->hdr
    + info->sechdrs[info->index.str].sh_offset;
   break;
  }
 }

 info->index.mod = find_sec(info, ".gnu.linkonce.this_module");
 if (!info->index.mod) {
  pr_warn("No module found in object\n");
  return ERR_PTR(-ENOEXEC);
 }

 mod = (void *)info->sechdrs[info->index.mod].sh_addr;

 if (info->index.sym == 0) {
  pr_warn("%s: module has no symbols (stripped?)\n", mod->name);
  return ERR_PTR(-ENOEXEC);
 }

 info->index.pcpu = find_pcpusec(info);


 if (!check_modstruct_version(info->sechdrs, info->index.vers, mod))
  return ERR_PTR(-ENOEXEC);

 return mod;
}

static int check_modinfo(struct module *mod, struct load_info *info, int flags)
{
 const char *modmagic = get_modinfo(info, "vermagic");
 int err;

 if (flags & MODULE_INIT_IGNORE_VERMAGIC)
  modmagic = NULL;


 if (!modmagic) {
  err = try_to_force_load(mod, "bad vermagic");
  if (err)
   return err;
 } else if (!same_magic(modmagic, vermagic, info->index.vers)) {
  pr_err("%s: version magic '%s' should be '%s'\n",
         mod->name, modmagic, vermagic);
  return -ENOEXEC;
 }

 if (!get_modinfo(info, "intree"))
  add_taint_module(mod, TAINT_OOT_MODULE, LOCKDEP_STILL_OK);

 if (get_modinfo(info, "staging")) {
  add_taint_module(mod, TAINT_CRAP, LOCKDEP_STILL_OK);
  pr_warn("%s: module is from the staging directory, the quality "
   "is unknown, you have been warned.\n", mod->name);
 }

 err = find_livepatch_modinfo(mod, info);
 if (err)
  return err;


 set_license(mod, get_modinfo(info, "license"));

 return 0;
}

static int find_module_sections(struct module *mod, struct load_info *info)
{
 mod->kp = section_objs(info, "__param",
          sizeof(*mod->kp), &mod->num_kp);
 mod->syms = section_objs(info, "__ksymtab",
     sizeof(*mod->syms), &mod->num_syms);
 mod->crcs = section_addr(info, "__kcrctab");
 mod->gpl_syms = section_objs(info, "__ksymtab_gpl",
         sizeof(*mod->gpl_syms),
         &mod->num_gpl_syms);
 mod->gpl_crcs = section_addr(info, "__kcrctab_gpl");
 mod->gpl_future_syms = section_objs(info,
         "__ksymtab_gpl_future",
         sizeof(*mod->gpl_future_syms),
         &mod->num_gpl_future_syms);
 mod->gpl_future_crcs = section_addr(info, "__kcrctab_gpl_future");

 mod->unused_syms = section_objs(info, "__ksymtab_unused",
     sizeof(*mod->unused_syms),
     &mod->num_unused_syms);
 mod->unused_crcs = section_addr(info, "__kcrctab_unused");
 mod->unused_gpl_syms = section_objs(info, "__ksymtab_unused_gpl",
         sizeof(*mod->unused_gpl_syms),
         &mod->num_unused_gpl_syms);
 mod->unused_gpl_crcs = section_addr(info, "__kcrctab_unused_gpl");
 mod->ctors = section_objs(info, ".ctors",
      sizeof(*mod->ctors), &mod->num_ctors);
 if (!mod->ctors)
  mod->ctors = section_objs(info, ".init_array",
    sizeof(*mod->ctors), &mod->num_ctors);
 else if (find_sec(info, ".init_array")) {




  pr_warn("%s: has both .ctors and .init_array.\n",
         mod->name);
  return -EINVAL;
 }

 mod->tracepoints_ptrs = section_objs(info, "__tracepoints_ptrs",
          sizeof(*mod->tracepoints_ptrs),
          &mod->num_tracepoints);
 mod->jump_entries = section_objs(info, "__jump_table",
     sizeof(*mod->jump_entries),
     &mod->num_jump_entries);
 mod->trace_events = section_objs(info, "_ftrace_events",
      sizeof(*mod->trace_events),
      &mod->num_trace_events);
 mod->trace_enums = section_objs(info, "_ftrace_enum_map",
     sizeof(*mod->trace_enums),
     &mod->num_trace_enums);
 mod->trace_bprintk_fmt_start = section_objs(info, "__trace_printk_fmt",
      sizeof(*mod->trace_bprintk_fmt_start),
      &mod->num_trace_bprintk_fmt);

 mod->ftrace_callsites = section_objs(info, "__mcount_loc",
          sizeof(*mod->ftrace_callsites),
          &mod->num_ftrace_callsites);

 mod->extable = section_objs(info, "__ex_table",
        sizeof(*mod->extable), &mod->num_exentries);

 if (section_addr(info, "__obsparm"))
  pr_warn("%s: Ignoring obsolete parameters\n", mod->name);

 info->debug = section_objs(info, "__verbose",
       sizeof(*info->debug), &info->num_debug);

 return 0;
}

static int move_module(struct module *mod, struct load_info *info)
{
 int i;
 void *ptr;


 ptr = module_alloc(mod->core_layout.size);





 kmemleak_not_leak(ptr);
 if (!ptr)
  return -ENOMEM;

 memset(ptr, 0, mod->core_layout.size);
 mod->core_layout.base = ptr;

 if (mod->init_layout.size) {
  ptr = module_alloc(mod->init_layout.size);






  kmemleak_ignore(ptr);
  if (!ptr) {
   module_memfree(mod->core_layout.base);
   return -ENOMEM;
  }
  memset(ptr, 0, mod->init_layout.size);
  mod->init_layout.base = ptr;
 } else
  mod->init_layout.base = NULL;


 pr_debug("final section addresses:\n");
 for (i = 0; i < info->hdr->e_shnum; i++) {
  void *dest;
  Elf_Shdr *shdr = &info->sechdrs[i];

  if (!(shdr->sh_flags & SHF_ALLOC))
   continue;

  if (shdr->sh_entsize & INIT_OFFSET_MASK)
   dest = mod->init_layout.base
    + (shdr->sh_entsize & ~INIT_OFFSET_MASK);
  else
   dest = mod->core_layout.base + shdr->sh_entsize;

  if (shdr->sh_type != SHT_NOBITS)
   memcpy(dest, (void *)shdr->sh_addr, shdr->sh_size);

  shdr->sh_addr = (unsigned long)dest;
  pr_debug("\t0x%lx %s\n",
    (long)shdr->sh_addr, info->secstrings + shdr->sh_name);
 }

 return 0;
}

static int check_module_license_and_versions(struct module *mod)
{





 if (strcmp(mod->name, "ndiswrapper") == 0)
  add_taint(TAINT_PROPRIETARY_MODULE, LOCKDEP_NOW_UNRELIABLE);


 if (strcmp(mod->name, "driverloader") == 0)
  add_taint_module(mod, TAINT_PROPRIETARY_MODULE,
     LOCKDEP_NOW_UNRELIABLE);


 if (strcmp(mod->name, "lve") == 0)
  add_taint_module(mod, TAINT_PROPRIETARY_MODULE,
     LOCKDEP_NOW_UNRELIABLE);

 if ((mod->num_syms && !mod->crcs)
     || (mod->num_gpl_syms && !mod->gpl_crcs)
     || (mod->num_gpl_future_syms && !mod->gpl_future_crcs)
     || (mod->num_unused_syms && !mod->unused_crcs)
     || (mod->num_unused_gpl_syms && !mod->unused_gpl_crcs)
  ) {
  return try_to_force_load(mod,
      "no versions for exported symbols");
 }
 return 0;
}

static void flush_module_icache(const struct module *mod)
{
 mm_segment_t old_fs;


 old_fs = get_fs();
 set_fs(KERNEL_DS);






 if (mod->init_layout.base)
  flush_icache_range((unsigned long)mod->init_layout.base,
       (unsigned long)mod->init_layout.base
       + mod->init_layout.size);
 flush_icache_range((unsigned long)mod->core_layout.base,
      (unsigned long)mod->core_layout.base + mod->core_layout.size);

 set_fs(old_fs);
}

int __weak module_frob_arch_sections(Elf_Ehdr *hdr,
         Elf_Shdr *sechdrs,
         char *secstrings,
         struct module *mod)
{
 return 0;
}

static struct module *layout_and_allocate(struct load_info *info, int flags)
{

 struct module *mod;
 int err;

 mod = setup_load_info(info, flags);
 if (IS_ERR(mod))
  return mod;

 err = check_modinfo(mod, info, flags);
 if (err)
  return ERR_PTR(err);


 err = module_frob_arch_sections(info->hdr, info->sechdrs,
     info->secstrings, mod);
 if (err < 0)
  return ERR_PTR(err);


 info->sechdrs[info->index.pcpu].sh_flags &= ~(unsigned long)SHF_ALLOC;




 layout_sections(mod, info);
 layout_symtab(mod, info);


 err = move_module(mod, info);
 if (err)
  return ERR_PTR(err);


 mod = (void *)info->sechdrs[info->index.mod].sh_addr;
 kmemleak_load_module(mod, info);
 return mod;
}


static void module_deallocate(struct module *mod, struct load_info *info)
{
 percpu_modfree(mod);
 module_arch_freeing_init(mod);
 module_memfree(mod->init_layout.base);
 module_memfree(mod->core_layout.base);
}

int __weak module_finalize(const Elf_Ehdr *hdr,
      const Elf_Shdr *sechdrs,
      struct module *me)
{
 return 0;
}

static int post_relocation(struct module *mod, const struct load_info *info)
{

 sort_extable(mod->extable, mod->extable + mod->num_exentries);


 percpu_modcopy(mod, (void *)info->sechdrs[info->index.pcpu].sh_addr,
         info->sechdrs[info->index.pcpu].sh_size);


 add_kallsyms(mod, info);


 return module_finalize(info->hdr, info->sechdrs, mod);
}


static bool finished_loading(const char *name)
{
 struct module *mod;
 bool ret;






 sched_annotate_sleep();
 mutex_lock(&module_mutex);
 mod = find_module_all(name, strlen(name), true);
 ret = !mod || mod->state == MODULE_STATE_LIVE
  || mod->state == MODULE_STATE_GOING;
 mutex_unlock(&module_mutex);

 return ret;
}


static void do_mod_ctors(struct module *mod)
{
 unsigned long i;

 for (i = 0; i < mod->num_ctors; i++)
  mod->ctors[i]();
}


struct mod_initfree {
 struct rcu_head rcu;
 void *module_init;
};

static void do_free_init(struct rcu_head *head)
{
 struct mod_initfree *m = container_of(head, struct mod_initfree, rcu);
 module_memfree(m->module_init);
 kfree(m);
}







static noinline int do_init_module(struct module *mod)
{
 int ret = 0;
 struct mod_initfree *freeinit;

 freeinit = kmalloc(sizeof(*freeinit), GFP_KERNEL);
 if (!freeinit) {
  ret = -ENOMEM;
  goto fail;
 }
 freeinit->module_init = mod->init_layout.base;





 current->flags &= ~PF_USED_ASYNC;

 do_mod_ctors(mod);

 if (mod->init != NULL)
  ret = do_one_initcall(mod->init);
 if (ret < 0) {
  goto fail_free_freeinit;
 }
 if (ret > 0) {
  pr_warn("%s: '%s'->init suspiciously returned %d, it should "
   "follow 0/-E convention\n"
   "%s: loading module anyway...\n",
   __func__, mod->name, ret, __func__);
  dump_stack();
 }


 mod->state = MODULE_STATE_LIVE;
 blocking_notifier_call_chain(&module_notify_list,
         MODULE_STATE_LIVE, mod);
 if (!mod->async_probe_requested && (current->flags & PF_USED_ASYNC))
  async_synchronize_full();

 mutex_lock(&module_mutex);

 module_put(mod);
 trim_init_extable(mod);

 rcu_assign_pointer(mod->kallsyms, &mod->core_kallsyms);
 mod_tree_remove_init(mod);
 disable_ro_nx(&mod->init_layout);
 module_arch_freeing_init(mod);
 mod->init_layout.base = NULL;
 mod->init_layout.size = 0;
 mod->init_layout.ro_size = 0;
 mod->init_layout.text_size = 0;






 call_rcu_sched(&freeinit->rcu, do_free_init);
 mutex_unlock(&module_mutex);
 wake_up_all(&module_wq);

 return 0;

fail_free_freeinit:
 kfree(freeinit);
fail:

 mod->state = MODULE_STATE_GOING;
 synchronize_sched();
 module_put(mod);
 blocking_notifier_call_chain(&module_notify_list,
         MODULE_STATE_GOING, mod);
 klp_module_going(mod);
 ftrace_release_mod(mod);
 free_module(mod);
 wake_up_all(&module_wq);
 return ret;
}

static int may_init_module(void)
{
 if (!capable(CAP_SYS_MODULE) || modules_disabled)
  return -EPERM;

 return 0;
}






static int add_unformed_module(struct module *mod)
{
 int err;
 struct module *old;

 mod->state = MODULE_STATE_UNFORMED;

again:
 mutex_lock(&module_mutex);
 old = find_module_all(mod->name, strlen(mod->name), true);
 if (old != NULL) {
  if (old->state == MODULE_STATE_COMING
      || old->state == MODULE_STATE_UNFORMED) {

   mutex_unlock(&module_mutex);
   err = wait_event_interruptible(module_wq,
            finished_loading(mod->name));
   if (err)
    goto out_unlocked;
   goto again;
  }
  err = -EEXIST;
  goto out;
 }
 mod_update_bounds(mod);
 list_add_rcu(&mod->list, &modules);
 mod_tree_insert(mod);
 err = 0;

out:
 mutex_unlock(&module_mutex);
out_unlocked:
 return err;
}

static int complete_formation(struct module *mod, struct load_info *info)
{
 int err;

 mutex_lock(&module_mutex);


 err = verify_export_symbols(mod);
 if (err < 0)
  goto out;


 module_bug_finalize(info->hdr, info->sechdrs, mod);


 module_enable_ro(mod);
 module_enable_nx(mod);



 mod->state = MODULE_STATE_COMING;
 mutex_unlock(&module_mutex);

 return 0;

out:
 mutex_unlock(&module_mutex);
 return err;
}

static int prepare_coming_module(struct module *mod)
{
 int err;

 ftrace_module_enable(mod);
 err = klp_module_coming(mod);
 if (err)
  return err;

 blocking_notifier_call_chain(&module_notify_list,
         MODULE_STATE_COMING, mod);
 return 0;
}

static int unknown_module_param_cb(char *param, char *val, const char *modname,
       void *arg)
{
 struct module *mod = arg;
 int ret;

 if (strcmp(param, "async_probe") == 0) {
  mod->async_probe_requested = true;
  return 0;
 }


 ret = ddebug_dyndbg_module_param_cb(param, val, modname);
 if (ret != 0)
  pr_warn("%s: unknown parameter '%s' ignored\n", modname, param);
 return 0;
}



static int load_module(struct load_info *info, const char __user *uargs,
         int flags)
{
 struct module *mod;
 long err;
 char *after_dashes;

 err = module_sig_check(info);
 if (err)
  goto free_copy;

 err = elf_header_check(info);
 if (err)
  goto free_copy;


 mod = layout_and_allocate(info, flags);
 if (IS_ERR(mod)) {
  err = PTR_ERR(mod);
  goto free_copy;
 }


 err = add_unformed_module(mod);
 if (err)
  goto free_module;

 mod->sig_ok = info->sig_ok;
 if (!mod->sig_ok) {
  pr_notice_once("%s: module verification failed: signature "
          "and/or required key missing - tainting "
          "kernel\n", mod->name);
  add_taint_module(mod, TAINT_UNSIGNED_MODULE, LOCKDEP_STILL_OK);
 }


 err = percpu_modalloc(mod, info);
 if (err)
  goto unlink_mod;


 err = module_unload_init(mod);
 if (err)
  goto unlink_mod;

 init_param_lock(mod);



 err = find_module_sections(mod, info);
 if (err)
  goto free_unload;

 err = check_module_license_and_versions(mod);
 if (err)
  goto free_unload;


 setup_modinfo(mod, info);


 err = simplify_symbols(mod, info);
 if (err < 0)
  goto free_modinfo;

 err = apply_relocations(mod, info);
 if (err < 0)
  goto free_modinfo;

 err = post_relocation(mod, info);
 if (err < 0)
  goto free_modinfo;

 flush_module_icache(mod);


 mod->args = strndup_user(uargs, ~0UL >> 1);
 if (IS_ERR(mod->args)) {
  err = PTR_ERR(mod->args);
  goto free_arch_cleanup;
 }

 dynamic_debug_setup(info->debug, info->num_debug);


 ftrace_module_init(mod);


 err = complete_formation(mod, info);
 if (err)
  goto ddebug_cleanup;

 err = prepare_coming_module(mod);
 if (err)
  goto bug_cleanup;


 after_dashes = parse_args(mod->name, mod->args, mod->kp, mod->num_kp,
      -32768, 32767, mod,
      unknown_module_param_cb);
 if (IS_ERR(after_dashes)) {
  err = PTR_ERR(after_dashes);
  goto coming_cleanup;
 } else if (after_dashes) {
  pr_warn("%s: parameters '%s' after `--' ignored\n",
         mod->name, after_dashes);
 }


 err = mod_sysfs_setup(mod, info, mod->kp, mod->num_kp);
 if (err < 0)
  goto coming_cleanup;

 if (is_livepatch_module(mod)) {
  err = copy_module_elf(mod, info);
  if (err < 0)
   goto sysfs_cleanup;
 }


 free_copy(info);


 trace_module_load(mod);

 return do_init_module(mod);

 sysfs_cleanup:
 mod_sysfs_teardown(mod);
 coming_cleanup:
 blocking_notifier_call_chain(&module_notify_list,
         MODULE_STATE_GOING, mod);
 klp_module_going(mod);
 bug_cleanup:

 mutex_lock(&module_mutex);
 module_bug_cleanup(mod);
 mutex_unlock(&module_mutex);


 module_disable_ro(mod);
 module_disable_nx(mod);

 ddebug_cleanup:
 dynamic_debug_remove(info->debug);
 synchronize_sched();
 kfree(mod->args);
 free_arch_cleanup:
 module_arch_cleanup(mod);
 free_modinfo:
 free_modinfo(mod);
 free_unload:
 module_unload_free(mod);
 unlink_mod:
 mutex_lock(&module_mutex);

 list_del_rcu(&mod->list);
 mod_tree_remove(mod);
 wake_up_all(&module_wq);

 synchronize_sched();
 mutex_unlock(&module_mutex);
 free_module:





 ftrace_release_mod(mod);

 lockdep_free_key_range(mod->core_layout.base, mod->core_layout.size);

 module_deallocate(mod, info);
 free_copy:
 free_copy(info);
 return err;
}

SYSCALL_DEFINE3(init_module, void __user *, umod,
  unsigned long, len, const char __user *, uargs)
{
 int err;
 struct load_info info = { };

 err = may_init_module();
 if (err)
  return err;

 pr_debug("init_module: umod=%p, len=%lu, uargs=%p\n",
        umod, len, uargs);

 err = copy_module_from_user(umod, len, &info);
 if (err)
  return err;

 return load_module(&info, uargs, 0);
}

SYSCALL_DEFINE3(finit_module, int, fd, const char __user *, uargs, int, flags)
{
 struct load_info info = { };
 loff_t size;
 void *hdr;
 int err;

 err = may_init_module();
 if (err)
  return err;

 pr_debug("finit_module: fd=%d, uargs=%p, flags=%i\n", fd, uargs, flags);

 if (flags & ~(MODULE_INIT_IGNORE_MODVERSIONS
        |MODULE_INIT_IGNORE_VERMAGIC))
  return -EINVAL;

 err = kernel_read_file_from_fd(fd, &hdr, &size, INT_MAX,
           READING_MODULE);
 if (err)
  return err;
 info.hdr = hdr;
 info.len = size;

 return load_module(&info, uargs, flags);
}

static inline int within(unsigned long addr, void *start, unsigned long size)
{
 return ((void *)addr >= start && (void *)addr < start + size);
}





static inline int is_arm_mapping_symbol(const char *str)
{
 if (str[0] == '.' && str[1] == 'L')
  return true;
 return str[0] == '$' && strchr("axtd", str[1])
        && (str[2] == '\0' || str[2] == '.');
}

static const char *symname(struct mod_kallsyms *kallsyms, unsigned int symnum)
{
 return kallsyms->strtab + kallsyms->symtab[symnum].st_name;
}

static const char *get_ksymbol(struct module *mod,
          unsigned long addr,
          unsigned long *size,
          unsigned long *offset)
{
 unsigned int i, best = 0;
 unsigned long nextval;
 struct mod_kallsyms *kallsyms = rcu_dereference_sched(mod->kallsyms);


 if (within_module_init(addr, mod))
  nextval = (unsigned long)mod->init_layout.base+mod->init_layout.text_size;
 else
  nextval = (unsigned long)mod->core_layout.base+mod->core_layout.text_size;



 for (i = 1; i < kallsyms->num_symtab; i++) {
  if (kallsyms->symtab[i].st_shndx == SHN_UNDEF)
   continue;



  if (*symname(kallsyms, i) == '\0'
      || is_arm_mapping_symbol(symname(kallsyms, i)))
   continue;

  if (kallsyms->symtab[i].st_value <= addr
      && kallsyms->symtab[i].st_value > kallsyms->symtab[best].st_value)
   best = i;
  if (kallsyms->symtab[i].st_value > addr
      && kallsyms->symtab[i].st_value < nextval)
   nextval = kallsyms->symtab[i].st_value;
 }

 if (!best)
  return NULL;

 if (size)
  *size = nextval - kallsyms->symtab[best].st_value;
 if (offset)
  *offset = addr - kallsyms->symtab[best].st_value;
 return symname(kallsyms, best);
}



const char *module_address_lookup(unsigned long addr,
       unsigned long *size,
       unsigned long *offset,
       char **modname,
       char *namebuf)
{
 const char *ret = NULL;
 struct module *mod;

 preempt_disable();
 mod = __module_address(addr);
 if (mod) {
  if (modname)
   *modname = mod->name;
  ret = get_ksymbol(mod, addr, size, offset);
 }

 if (ret) {
  strncpy(namebuf, ret, KSYM_NAME_LEN - 1);
  ret = namebuf;
 }
 preempt_enable();

 return ret;
}

int lookup_module_symbol_name(unsigned long addr, char *symname)
{
 struct module *mod;

 preempt_disable();
 list_for_each_entry_rcu(mod, &modules, list) {
  if (mod->state == MODULE_STATE_UNFORMED)
   continue;
  if (within_module(addr, mod)) {
   const char *sym;

   sym = get_ksymbol(mod, addr, NULL, NULL);
   if (!sym)
    goto out;
   strlcpy(symname, sym, KSYM_NAME_LEN);
   preempt_enable();
   return 0;
  }
 }
out:
 preempt_enable();
 return -ERANGE;
}

int lookup_module_symbol_attrs(unsigned long addr, unsigned long *size,
   unsigned long *offset, char *modname, char *name)
{
 struct module *mod;

 preempt_disable();
 list_for_each_entry_rcu(mod, &modules, list) {
  if (mod->state == MODULE_STATE_UNFORMED)
   continue;
  if (within_module(addr, mod)) {
   const char *sym;

   sym = get_ksymbol(mod, addr, size, offset);
   if (!sym)
    goto out;
   if (modname)
    strlcpy(modname, mod->name, MODULE_NAME_LEN);
   if (name)
    strlcpy(name, sym, KSYM_NAME_LEN);
   preempt_enable();
   return 0;
  }
 }
out:
 preempt_enable();
 return -ERANGE;
}

int module_get_kallsym(unsigned int symnum, unsigned long *value, char *type,
   char *name, char *module_name, int *exported)
{
 struct module *mod;

 preempt_disable();
 list_for_each_entry_rcu(mod, &modules, list) {
  struct mod_kallsyms *kallsyms;

  if (mod->state == MODULE_STATE_UNFORMED)
   continue;
  kallsyms = rcu_dereference_sched(mod->kallsyms);
  if (symnum < kallsyms->num_symtab) {
   *value = kallsyms->symtab[symnum].st_value;
   *type = kallsyms->symtab[symnum].st_info;
   strlcpy(name, symname(kallsyms, symnum), KSYM_NAME_LEN);
   strlcpy(module_name, mod->name, MODULE_NAME_LEN);
   *exported = is_exported(name, *value, mod);
   preempt_enable();
   return 0;
  }
  symnum -= kallsyms->num_symtab;
 }
 preempt_enable();
 return -ERANGE;
}

static unsigned long mod_find_symname(struct module *mod, const char *name)
{
 unsigned int i;
 struct mod_kallsyms *kallsyms = rcu_dereference_sched(mod->kallsyms);

 for (i = 0; i < kallsyms->num_symtab; i++)
  if (strcmp(name, symname(kallsyms, i)) == 0 &&
      kallsyms->symtab[i].st_info != 'U')
   return kallsyms->symtab[i].st_value;
 return 0;
}


unsigned long module_kallsyms_lookup_name(const char *name)
{
 struct module *mod;
 char *colon;
 unsigned long ret = 0;


 preempt_disable();
 if ((colon = strchr(name, ':')) != NULL) {
  if ((mod = find_module_all(name, colon - name, false)) != NULL)
   ret = mod_find_symname(mod, colon+1);
 } else {
  list_for_each_entry_rcu(mod, &modules, list) {
   if (mod->state == MODULE_STATE_UNFORMED)
    continue;
   if ((ret = mod_find_symname(mod, name)) != 0)
    break;
  }
 }
 preempt_enable();
 return ret;
}

int module_kallsyms_on_each_symbol(int (*fn)(void *, const char *,
          struct module *, unsigned long),
       void *data)
{
 struct module *mod;
 unsigned int i;
 int ret;

 module_assert_mutex();

 list_for_each_entry(mod, &modules, list) {

  struct mod_kallsyms *kallsyms = mod->kallsyms;

  if (mod->state == MODULE_STATE_UNFORMED)
   continue;
  for (i = 0; i < kallsyms->num_symtab; i++) {
   ret = fn(data, symname(kallsyms, i),
     mod, kallsyms->symtab[i].st_value);
   if (ret != 0)
    return ret;
  }
 }
 return 0;
}

static char *module_flags(struct module *mod, char *buf)
{
 int bx = 0;

 BUG_ON(mod->state == MODULE_STATE_UNFORMED);
 if (mod->taints ||
     mod->state == MODULE_STATE_GOING ||
     mod->state == MODULE_STATE_COMING) {
  buf[bx++] = '(';
  bx += module_flags_taint(mod, buf + bx);

  if (mod->state == MODULE_STATE_GOING)
   buf[bx++] = '-';

  if (mod->state == MODULE_STATE_COMING)
   buf[bx++] = '+';
  buf[bx++] = ')';
 }
 buf[bx] = '\0';

 return buf;
}


static void *m_start(struct seq_file *m, loff_t *pos)
{
 mutex_lock(&module_mutex);
 return seq_list_start(&modules, *pos);
}

static void *m_next(struct seq_file *m, void *p, loff_t *pos)
{
 return seq_list_next(p, &modules, pos);
}

static void m_stop(struct seq_file *m, void *p)
{
 mutex_unlock(&module_mutex);
}

static int m_show(struct seq_file *m, void *p)
{
 struct module *mod = list_entry(p, struct module, list);
 char buf[8];


 if (mod->state == MODULE_STATE_UNFORMED)
  return 0;

 seq_printf(m, "%s %u",
     mod->name, mod->init_layout.size + mod->core_layout.size);
 print_unload_info(m, mod);


 seq_printf(m, " %s",
     mod->state == MODULE_STATE_GOING ? "Unloading" :
     mod->state == MODULE_STATE_COMING ? "Loading" :
     "Live");

 seq_printf(m, " 0x%pK", mod->core_layout.base);


 if (mod->taints)
  seq_printf(m, " %s", module_flags(mod, buf));

 seq_puts(m, "\n");
 return 0;
}






static const struct seq_operations modules_op = {
 .start = m_start,
 .next = m_next,
 .stop = m_stop,
 .show = m_show
};

static int modules_open(struct inode *inode, struct file *file)
{
 return seq_open(file, &modules_op);
}

static const struct file_operations proc_modules_operations = {
 .open = modules_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = seq_release,
};

static int __init proc_modules_init(void)
{
 proc_create("modules", 0, NULL, &proc_modules_operations);
 return 0;
}
module_init(proc_modules_init);


const struct exception_table_entry *search_module_extables(unsigned long addr)
{
 const struct exception_table_entry *e = NULL;
 struct module *mod;

 preempt_disable();
 list_for_each_entry_rcu(mod, &modules, list) {
  if (mod->state == MODULE_STATE_UNFORMED)
   continue;
  if (mod->num_exentries == 0)
   continue;

  e = search_extable(mod->extable,
       mod->extable + mod->num_exentries - 1,
       addr);
  if (e)
   break;
 }
 preempt_enable();



 return e;
}
bool is_module_address(unsigned long addr)
{
 bool ret;

 preempt_disable();
 ret = __module_address(addr) != NULL;
 preempt_enable();

 return ret;
}
struct module *__module_address(unsigned long addr)
{
 struct module *mod;

 if (addr < module_addr_min || addr > module_addr_max)
  return NULL;

 module_assert_mutex_or_preempt();

 mod = mod_find(addr);
 if (mod) {
  BUG_ON(!within_module(addr, mod));
  if (mod->state == MODULE_STATE_UNFORMED)
   mod = NULL;
 }
 return mod;
}
EXPORT_SYMBOL_GPL(__module_address);
bool is_module_text_address(unsigned long addr)
{
 bool ret;

 preempt_disable();
 ret = __module_text_address(addr) != NULL;
 preempt_enable();

 return ret;
}
struct module *__module_text_address(unsigned long addr)
{
 struct module *mod = __module_address(addr);
 if (mod) {

  if (!within(addr, mod->init_layout.base, mod->init_layout.text_size)
      && !within(addr, mod->core_layout.base, mod->core_layout.text_size))
   mod = NULL;
 }
 return mod;
}
EXPORT_SYMBOL_GPL(__module_text_address);


void print_modules(void)
{
 struct module *mod;
 char buf[8];

 printk(KERN_DEFAULT "Modules linked in:");

 preempt_disable();
 list_for_each_entry_rcu(mod, &modules, list) {
  if (mod->state == MODULE_STATE_UNFORMED)
   continue;
  pr_cont(" %s%s", mod->name, module_flags(mod, buf));
 }
 preempt_enable();
 if (last_unloaded_module[0])
  pr_cont(" [last unloaded: %s]", last_unloaded_module);
 pr_cont("\n");
}



void module_layout(struct module *mod,
     struct modversion_info *ver,
     struct kernel_param *kp,
     struct kernel_symbol *ks,
     struct tracepoint * const *tp)
{
}
EXPORT_SYMBOL(module_layout);

enum pkey_id_type {
 PKEY_ID_PGP,
 PKEY_ID_X509,
 PKEY_ID_PKCS7,
};
struct module_signature {
 u8 algo;
 u8 hash;
 u8 id_type;
 u8 signer_len;
 u8 key_id_len;
 u8 __pad[3];
 __be32 sig_len;
};




int mod_verify_sig(const void *mod, unsigned long *_modlen)
{
 struct module_signature ms;
 size_t modlen = *_modlen, sig_len;

 pr_devel("==>%s(,%zu)\n", __func__, modlen);

 if (modlen <= sizeof(ms))
  return -EBADMSG;

 memcpy(&ms, mod + (modlen - sizeof(ms)), sizeof(ms));
 modlen -= sizeof(ms);

 sig_len = be32_to_cpu(ms.sig_len);
 if (sig_len >= modlen)
  return -EBADMSG;
 modlen -= sig_len;
 *_modlen = modlen;

 if (ms.id_type != PKEY_ID_PKCS7) {
  pr_err("Module is not signed with expected PKCS#7 message\n");
  return -ENOPKG;
 }

 if (ms.algo != 0 ||
     ms.hash != 0 ||
     ms.signer_len != 0 ||
     ms.key_id_len != 0 ||
     ms.__pad[0] != 0 ||
     ms.__pad[1] != 0 ||
     ms.__pad[2] != 0) {
  pr_err("PKCS#7 signature info has unexpected non-zero params\n");
  return -EBADMSG;
 }

 return verify_pkcs7_signature(mod, modlen, mod + modlen, sig_len,
          NULL, VERIFYING_MODULE_SIGNATURE,
          NULL, NULL);
}



struct msi_desc *alloc_msi_entry(struct device *dev)
{
 struct msi_desc *desc = kzalloc(sizeof(*desc), GFP_KERNEL);
 if (!desc)
  return NULL;

 INIT_LIST_HEAD(&desc->list);
 desc->dev = dev;

 return desc;
}

void free_msi_entry(struct msi_desc *entry)
{
 kfree(entry);
}

void __get_cached_msi_msg(struct msi_desc *entry, struct msi_msg *msg)
{
 *msg = entry->msg;
}

void get_cached_msi_msg(unsigned int irq, struct msi_msg *msg)
{
 struct msi_desc *entry = irq_get_msi_desc(irq);

 __get_cached_msi_msg(entry, msg);
}
EXPORT_SYMBOL_GPL(get_cached_msi_msg);

static inline void irq_chip_write_msi_msg(struct irq_data *data,
       struct msi_msg *msg)
{
 data->chip->irq_write_msi_msg(data, msg);
}
int msi_domain_set_affinity(struct irq_data *irq_data,
       const struct cpumask *mask, bool force)
{
 struct irq_data *parent = irq_data->parent_data;
 struct msi_msg msg;
 int ret;

 ret = parent->chip->irq_set_affinity(parent, mask, force);
 if (ret >= 0 && ret != IRQ_SET_MASK_OK_DONE) {
  BUG_ON(irq_chip_compose_msi_msg(irq_data, &msg));
  irq_chip_write_msi_msg(irq_data, &msg);
 }

 return ret;
}

static void msi_domain_activate(struct irq_domain *domain,
    struct irq_data *irq_data)
{
 struct msi_msg msg;

 BUG_ON(irq_chip_compose_msi_msg(irq_data, &msg));
 irq_chip_write_msi_msg(irq_data, &msg);
}

static void msi_domain_deactivate(struct irq_domain *domain,
      struct irq_data *irq_data)
{
 struct msi_msg msg;

 memset(&msg, 0, sizeof(msg));
 irq_chip_write_msi_msg(irq_data, &msg);
}

static int msi_domain_alloc(struct irq_domain *domain, unsigned int virq,
       unsigned int nr_irqs, void *arg)
{
 struct msi_domain_info *info = domain->host_data;
 struct msi_domain_ops *ops = info->ops;
 irq_hw_number_t hwirq = ops->get_hwirq(info, arg);
 int i, ret;

 if (irq_find_mapping(domain, hwirq) > 0)
  return -EEXIST;

 if (domain->parent) {
  ret = irq_domain_alloc_irqs_parent(domain, virq, nr_irqs, arg);
  if (ret < 0)
   return ret;
 }

 for (i = 0; i < nr_irqs; i++) {
  ret = ops->msi_init(domain, info, virq + i, hwirq + i, arg);
  if (ret < 0) {
   if (ops->msi_free) {
    for (i--; i > 0; i--)
     ops->msi_free(domain, info, virq + i);
   }
   irq_domain_free_irqs_top(domain, virq, nr_irqs);
   return ret;
  }
 }

 return 0;
}

static void msi_domain_free(struct irq_domain *domain, unsigned int virq,
       unsigned int nr_irqs)
{
 struct msi_domain_info *info = domain->host_data;
 int i;

 if (info->ops->msi_free) {
  for (i = 0; i < nr_irqs; i++)
   info->ops->msi_free(domain, info, virq + i);
 }
 irq_domain_free_irqs_top(domain, virq, nr_irqs);
}

static const struct irq_domain_ops msi_domain_ops = {
 .alloc = msi_domain_alloc,
 .free = msi_domain_free,
 .activate = msi_domain_activate,
 .deactivate = msi_domain_deactivate,
};

static irq_hw_number_t msi_domain_ops_get_hwirq(struct msi_domain_info *info,
      msi_alloc_info_t *arg)
{
 return arg->hwirq;
}

static int msi_domain_ops_prepare(struct irq_domain *domain, struct device *dev,
      int nvec, msi_alloc_info_t *arg)
{
 memset(arg, 0, sizeof(*arg));
 return 0;
}

static void msi_domain_ops_set_desc(msi_alloc_info_t *arg,
        struct msi_desc *desc)
{
 arg->desc = desc;
}

static int msi_domain_ops_init(struct irq_domain *domain,
          struct msi_domain_info *info,
          unsigned int virq, irq_hw_number_t hwirq,
          msi_alloc_info_t *arg)
{
 irq_domain_set_hwirq_and_chip(domain, virq, hwirq, info->chip,
          info->chip_data);
 if (info->handler && info->handler_name) {
  __irq_set_handler(virq, info->handler, 0, info->handler_name);
  if (info->handler_data)
   irq_set_handler_data(virq, info->handler_data);
 }
 return 0;
}

static int msi_domain_ops_check(struct irq_domain *domain,
    struct msi_domain_info *info,
    struct device *dev)
{
 return 0;
}

static struct msi_domain_ops msi_domain_ops_default = {
 .get_hwirq = msi_domain_ops_get_hwirq,
 .msi_init = msi_domain_ops_init,
 .msi_check = msi_domain_ops_check,
 .msi_prepare = msi_domain_ops_prepare,
 .set_desc = msi_domain_ops_set_desc,
};

static void msi_domain_update_dom_ops(struct msi_domain_info *info)
{
 struct msi_domain_ops *ops = info->ops;

 if (ops == NULL) {
  info->ops = &msi_domain_ops_default;
  return;
 }

 if (ops->get_hwirq == NULL)
  ops->get_hwirq = msi_domain_ops_default.get_hwirq;
 if (ops->msi_init == NULL)
  ops->msi_init = msi_domain_ops_default.msi_init;
 if (ops->msi_check == NULL)
  ops->msi_check = msi_domain_ops_default.msi_check;
 if (ops->msi_prepare == NULL)
  ops->msi_prepare = msi_domain_ops_default.msi_prepare;
 if (ops->set_desc == NULL)
  ops->set_desc = msi_domain_ops_default.set_desc;
}

static void msi_domain_update_chip_ops(struct msi_domain_info *info)
{
 struct irq_chip *chip = info->chip;

 BUG_ON(!chip || !chip->irq_mask || !chip->irq_unmask);
 if (!chip->irq_set_affinity)
  chip->irq_set_affinity = msi_domain_set_affinity;
}







struct irq_domain *msi_create_irq_domain(struct fwnode_handle *fwnode,
      struct msi_domain_info *info,
      struct irq_domain *parent)
{
 if (info->flags & MSI_FLAG_USE_DEF_DOM_OPS)
  msi_domain_update_dom_ops(info);
 if (info->flags & MSI_FLAG_USE_DEF_CHIP_OPS)
  msi_domain_update_chip_ops(info);

 return irq_domain_create_hierarchy(parent, 0, 0, fwnode,
        &msi_domain_ops, info);
}

int msi_domain_prepare_irqs(struct irq_domain *domain, struct device *dev,
       int nvec, msi_alloc_info_t *arg)
{
 struct msi_domain_info *info = domain->host_data;
 struct msi_domain_ops *ops = info->ops;
 int ret;

 ret = ops->msi_check(domain, info, dev);
 if (ret == 0)
  ret = ops->msi_prepare(domain, dev, nvec, arg);

 return ret;
}

int msi_domain_populate_irqs(struct irq_domain *domain, struct device *dev,
        int virq, int nvec, msi_alloc_info_t *arg)
{
 struct msi_domain_info *info = domain->host_data;
 struct msi_domain_ops *ops = info->ops;
 struct msi_desc *desc;
 int ret = 0;

 for_each_msi_entry(desc, dev) {

  if (WARN_ON(!desc->irq || desc->nvec_used != 1)) {
   ret = -EINVAL;
   break;
  }

  if (!(desc->irq >= virq && desc->irq < (virq + nvec)))
   continue;

  ops->set_desc(arg, desc);

  ret = irq_domain_alloc_irqs_recursive(domain, virq, 1, arg);
  if (ret)
   break;

  irq_set_msi_desc_off(virq, 0, desc);
 }

 if (ret) {

  for_each_msi_entry(desc, dev) {
   if (!(desc->irq >= virq && desc->irq < (virq + nvec)))
    continue;

   irq_domain_free_irqs_common(domain, desc->irq, 1);
  }
 }

 return ret;
}
int msi_domain_alloc_irqs(struct irq_domain *domain, struct device *dev,
     int nvec)
{
 struct msi_domain_info *info = domain->host_data;
 struct msi_domain_ops *ops = info->ops;
 msi_alloc_info_t arg;
 struct msi_desc *desc;
 int i, ret, virq = -1;

 ret = msi_domain_prepare_irqs(domain, dev, nvec, &arg);
 if (ret)
  return ret;

 for_each_msi_entry(desc, dev) {
  ops->set_desc(&arg, desc);
  if (info->flags & MSI_FLAG_IDENTITY_MAP)
   virq = (int)ops->get_hwirq(info, &arg);
  else
   virq = -1;

  virq = __irq_domain_alloc_irqs(domain, virq, desc->nvec_used,
            dev_to_node(dev), &arg, false);
  if (virq < 0) {
   ret = -ENOSPC;
   if (ops->handle_error)
    ret = ops->handle_error(domain, desc, ret);
   if (ops->msi_finish)
    ops->msi_finish(&arg, ret);
   return ret;
  }

  for (i = 0; i < desc->nvec_used; i++)
   irq_set_msi_desc_off(virq, i, desc);
 }

 if (ops->msi_finish)
  ops->msi_finish(&arg, 0);

 for_each_msi_entry(desc, dev) {
  if (desc->nvec_used == 1)
   dev_dbg(dev, "irq %d for MSI\n", virq);
  else
   dev_dbg(dev, "irq [%d-%d] for MSI\n",
    virq, virq + desc->nvec_used - 1);
 }

 return 0;
}







void msi_domain_free_irqs(struct irq_domain *domain, struct device *dev)
{
 struct msi_desc *desc;

 for_each_msi_entry(desc, dev) {





  if (desc->irq) {
   irq_domain_free_irqs(desc->irq, desc->nvec_used);
   desc->irq = 0;
  }
 }
}
struct msi_domain_info *msi_get_domain_info(struct irq_domain *domain)
{
 return (struct msi_domain_info *)domain->host_data;
}


DEFINE_PER_CPU(printk_func_t, printk_func) = vprintk_default;
static int printk_nmi_irq_ready;
atomic_t nmi_message_lost;

    sizeof(atomic_t) - sizeof(struct irq_work))

struct nmi_seq_buf {
 atomic_t len;
 struct irq_work work;
 unsigned char buffer[NMI_LOG_BUF_LEN];
};
static DEFINE_PER_CPU(struct nmi_seq_buf, nmi_print_seq);







static int vprintk_nmi(const char *fmt, va_list args)
{
 struct nmi_seq_buf *s = this_cpu_ptr(&nmi_print_seq);
 int add = 0;
 size_t len;

again:
 len = atomic_read(&s->len);

 if (len >= sizeof(s->buffer)) {
  atomic_inc(&nmi_message_lost);
  return 0;
 }





 if (!len)
  smp_rmb();

 add = vsnprintf(s->buffer + len, sizeof(s->buffer) - len, fmt, args);






 if (atomic_cmpxchg(&s->len, len, len + add) != len)
  goto again;


 if (add && printk_nmi_irq_ready) {

  smp_rmb();
  irq_work_queue(&s->work);
 }

 return add;
}





static void print_nmi_seq_line(struct nmi_seq_buf *s, int start, int end)
{
 const char *buf = s->buffer + start;






 if (in_nmi())
  printk_deferred("%.*s", (end - start) + 1, buf);
 else
  printk("%.*s", (end - start) + 1, buf);

}





static void __printk_nmi_flush(struct irq_work *work)
{
 static raw_spinlock_t read_lock =
  __RAW_SPIN_LOCK_INITIALIZER(read_lock);
 struct nmi_seq_buf *s = container_of(work, struct nmi_seq_buf, work);
 unsigned long flags;
 size_t len, size;
 int i, last_i;
 raw_spin_lock_irqsave(&read_lock, flags);

 i = 0;
more:
 len = atomic_read(&s->len);






 if (i && i >= len)
  pr_err("printk_nmi_flush: internal error: i=%d >= len=%zu\n",
         i, len);

 if (!len)
  goto out;


 smp_rmb();

 size = min(len, sizeof(s->buffer));
 last_i = i;


 for (; i < size; i++) {
  if (s->buffer[i] == '\n') {
   print_nmi_seq_line(s, last_i, i);
   last_i = i + 1;
  }
 }

 if (last_i < size) {
  print_nmi_seq_line(s, last_i, size - 1);
  pr_cont("\n");
 }







 if (atomic_cmpxchg(&s->len, len, 0) != len)
  goto more;

out:
 raw_spin_unlock_irqrestore(&read_lock, flags);
}
void printk_nmi_flush(void)
{
 int cpu;

 for_each_possible_cpu(cpu)
  __printk_nmi_flush(&per_cpu(nmi_print_seq, cpu).work);
}
void printk_nmi_flush_on_panic(void)
{




 if (in_nmi() && raw_spin_is_locked(&logbuf_lock)) {
  if (num_online_cpus() > 1)
   return;

  debug_locks_off();
  raw_spin_lock_init(&logbuf_lock);
 }

 printk_nmi_flush();
}

void __init printk_nmi_init(void)
{
 int cpu;

 for_each_possible_cpu(cpu) {
  struct nmi_seq_buf *s = &per_cpu(nmi_print_seq, cpu);

  init_irq_work(&s->work, __printk_nmi_flush);
 }


 smp_wmb();
 printk_nmi_irq_ready = 1;


 printk_nmi_flush();
}

void printk_nmi_enter(void)
{
 this_cpu_write(printk_func, vprintk_nmi);
}

void printk_nmi_exit(void)
{
 this_cpu_write(printk_func, vprintk_default);
}






BLOCKING_NOTIFIER_HEAD(reboot_notifier_list);






static int notifier_chain_register(struct notifier_block **nl,
  struct notifier_block *n)
{
 while ((*nl) != NULL) {
  if (n->priority > (*nl)->priority)
   break;
  nl = &((*nl)->next);
 }
 n->next = *nl;
 rcu_assign_pointer(*nl, n);
 return 0;
}

static int notifier_chain_cond_register(struct notifier_block **nl,
  struct notifier_block *n)
{
 while ((*nl) != NULL) {
  if ((*nl) == n)
   return 0;
  if (n->priority > (*nl)->priority)
   break;
  nl = &((*nl)->next);
 }
 n->next = *nl;
 rcu_assign_pointer(*nl, n);
 return 0;
}

static int notifier_chain_unregister(struct notifier_block **nl,
  struct notifier_block *n)
{
 while ((*nl) != NULL) {
  if ((*nl) == n) {
   rcu_assign_pointer(*nl, n->next);
   return 0;
  }
  nl = &((*nl)->next);
 }
 return -ENOENT;
}
static int notifier_call_chain(struct notifier_block **nl,
          unsigned long val, void *v,
          int nr_to_call, int *nr_calls)
{
 int ret = NOTIFY_DONE;
 struct notifier_block *nb, *next_nb;

 nb = rcu_dereference_raw(*nl);

 while (nb && nr_to_call) {
  next_nb = rcu_dereference_raw(nb->next);

  if (unlikely(!func_ptr_is_kernel_text(nb->notifier_call))) {
   WARN(1, "Invalid notifier called!");
   nb = next_nb;
   continue;
  }
  ret = nb->notifier_call(nb, val, v);

  if (nr_calls)
   (*nr_calls)++;

  if ((ret & NOTIFY_STOP_MASK) == NOTIFY_STOP_MASK)
   break;
  nb = next_nb;
  nr_to_call--;
 }
 return ret;
}
NOKPROBE_SYMBOL(notifier_call_chain);
int atomic_notifier_chain_register(struct atomic_notifier_head *nh,
  struct notifier_block *n)
{
 unsigned long flags;
 int ret;

 spin_lock_irqsave(&nh->lock, flags);
 ret = notifier_chain_register(&nh->head, n);
 spin_unlock_irqrestore(&nh->lock, flags);
 return ret;
}
EXPORT_SYMBOL_GPL(atomic_notifier_chain_register);
int atomic_notifier_chain_unregister(struct atomic_notifier_head *nh,
  struct notifier_block *n)
{
 unsigned long flags;
 int ret;

 spin_lock_irqsave(&nh->lock, flags);
 ret = notifier_chain_unregister(&nh->head, n);
 spin_unlock_irqrestore(&nh->lock, flags);
 synchronize_rcu();
 return ret;
}
EXPORT_SYMBOL_GPL(atomic_notifier_chain_unregister);
int __atomic_notifier_call_chain(struct atomic_notifier_head *nh,
     unsigned long val, void *v,
     int nr_to_call, int *nr_calls)
{
 int ret;

 rcu_read_lock();
 ret = notifier_call_chain(&nh->head, val, v, nr_to_call, nr_calls);
 rcu_read_unlock();
 return ret;
}
EXPORT_SYMBOL_GPL(__atomic_notifier_call_chain);
NOKPROBE_SYMBOL(__atomic_notifier_call_chain);

int atomic_notifier_call_chain(struct atomic_notifier_head *nh,
          unsigned long val, void *v)
{
 return __atomic_notifier_call_chain(nh, val, v, -1, NULL);
}
EXPORT_SYMBOL_GPL(atomic_notifier_call_chain);
NOKPROBE_SYMBOL(atomic_notifier_call_chain);
int blocking_notifier_chain_register(struct blocking_notifier_head *nh,
  struct notifier_block *n)
{
 int ret;






 if (unlikely(system_state == SYSTEM_BOOTING))
  return notifier_chain_register(&nh->head, n);

 down_write(&nh->rwsem);
 ret = notifier_chain_register(&nh->head, n);
 up_write(&nh->rwsem);
 return ret;
}
EXPORT_SYMBOL_GPL(blocking_notifier_chain_register);
int blocking_notifier_chain_cond_register(struct blocking_notifier_head *nh,
  struct notifier_block *n)
{
 int ret;

 down_write(&nh->rwsem);
 ret = notifier_chain_cond_register(&nh->head, n);
 up_write(&nh->rwsem);
 return ret;
}
EXPORT_SYMBOL_GPL(blocking_notifier_chain_cond_register);
int blocking_notifier_chain_unregister(struct blocking_notifier_head *nh,
  struct notifier_block *n)
{
 int ret;






 if (unlikely(system_state == SYSTEM_BOOTING))
  return notifier_chain_unregister(&nh->head, n);

 down_write(&nh->rwsem);
 ret = notifier_chain_unregister(&nh->head, n);
 up_write(&nh->rwsem);
 return ret;
}
EXPORT_SYMBOL_GPL(blocking_notifier_chain_unregister);
int __blocking_notifier_call_chain(struct blocking_notifier_head *nh,
       unsigned long val, void *v,
       int nr_to_call, int *nr_calls)
{
 int ret = NOTIFY_DONE;






 if (rcu_access_pointer(nh->head)) {
  down_read(&nh->rwsem);
  ret = notifier_call_chain(&nh->head, val, v, nr_to_call,
     nr_calls);
  up_read(&nh->rwsem);
 }
 return ret;
}
EXPORT_SYMBOL_GPL(__blocking_notifier_call_chain);

int blocking_notifier_call_chain(struct blocking_notifier_head *nh,
  unsigned long val, void *v)
{
 return __blocking_notifier_call_chain(nh, val, v, -1, NULL);
}
EXPORT_SYMBOL_GPL(blocking_notifier_call_chain);
int raw_notifier_chain_register(struct raw_notifier_head *nh,
  struct notifier_block *n)
{
 return notifier_chain_register(&nh->head, n);
}
EXPORT_SYMBOL_GPL(raw_notifier_chain_register);
int raw_notifier_chain_unregister(struct raw_notifier_head *nh,
  struct notifier_block *n)
{
 return notifier_chain_unregister(&nh->head, n);
}
EXPORT_SYMBOL_GPL(raw_notifier_chain_unregister);
int __raw_notifier_call_chain(struct raw_notifier_head *nh,
         unsigned long val, void *v,
         int nr_to_call, int *nr_calls)
{
 return notifier_call_chain(&nh->head, val, v, nr_to_call, nr_calls);
}
EXPORT_SYMBOL_GPL(__raw_notifier_call_chain);

int raw_notifier_call_chain(struct raw_notifier_head *nh,
  unsigned long val, void *v)
{
 return __raw_notifier_call_chain(nh, val, v, -1, NULL);
}
EXPORT_SYMBOL_GPL(raw_notifier_call_chain);

int srcu_notifier_chain_register(struct srcu_notifier_head *nh,
  struct notifier_block *n)
{
 int ret;






 if (unlikely(system_state == SYSTEM_BOOTING))
  return notifier_chain_register(&nh->head, n);

 mutex_lock(&nh->mutex);
 ret = notifier_chain_register(&nh->head, n);
 mutex_unlock(&nh->mutex);
 return ret;
}
EXPORT_SYMBOL_GPL(srcu_notifier_chain_register);
int srcu_notifier_chain_unregister(struct srcu_notifier_head *nh,
  struct notifier_block *n)
{
 int ret;






 if (unlikely(system_state == SYSTEM_BOOTING))
  return notifier_chain_unregister(&nh->head, n);

 mutex_lock(&nh->mutex);
 ret = notifier_chain_unregister(&nh->head, n);
 mutex_unlock(&nh->mutex);
 synchronize_srcu(&nh->srcu);
 return ret;
}
EXPORT_SYMBOL_GPL(srcu_notifier_chain_unregister);
int __srcu_notifier_call_chain(struct srcu_notifier_head *nh,
          unsigned long val, void *v,
          int nr_to_call, int *nr_calls)
{
 int ret;
 int idx;

 idx = srcu_read_lock(&nh->srcu);
 ret = notifier_call_chain(&nh->head, val, v, nr_to_call, nr_calls);
 srcu_read_unlock(&nh->srcu, idx);
 return ret;
}
EXPORT_SYMBOL_GPL(__srcu_notifier_call_chain);

int srcu_notifier_call_chain(struct srcu_notifier_head *nh,
  unsigned long val, void *v)
{
 return __srcu_notifier_call_chain(nh, val, v, -1, NULL);
}
EXPORT_SYMBOL_GPL(srcu_notifier_call_chain);
void srcu_init_notifier_head(struct srcu_notifier_head *nh)
{
 mutex_init(&nh->mutex);
 if (init_srcu_struct(&nh->srcu) < 0)
  BUG();
 nh->head = NULL;
}
EXPORT_SYMBOL_GPL(srcu_init_notifier_head);


static ATOMIC_NOTIFIER_HEAD(die_chain);

int notrace notify_die(enum die_val val, const char *str,
        struct pt_regs *regs, long err, int trap, int sig)
{
 struct die_args args = {
  .regs = regs,
  .str = str,
  .err = err,
  .trapnr = trap,
  .signr = sig,

 };
 RCU_LOCKDEP_WARN(!rcu_is_watching(),
      "notify_die called but RCU thinks we're quiescent");
 return atomic_notifier_call_chain(&die_chain, val, &args);
}
NOKPROBE_SYMBOL(notify_die);

int register_die_notifier(struct notifier_block *nb)
{
 vmalloc_sync_all();
 return atomic_notifier_chain_register(&die_chain, nb);
}
EXPORT_SYMBOL_GPL(register_die_notifier);

int unregister_die_notifier(struct notifier_block *nb)
{
 return atomic_notifier_chain_unregister(&die_chain, nb);
}
EXPORT_SYMBOL_GPL(unregister_die_notifier);

static struct kmem_cache *nsproxy_cachep;

struct nsproxy init_nsproxy = {
 .count = ATOMIC_INIT(1),
 .uts_ns = &init_uts_ns,
 .ipc_ns = &init_ipc_ns,
 .mnt_ns = NULL,
 .pid_ns_for_children = &init_pid_ns,
 .net_ns = &init_net,
 .cgroup_ns = &init_cgroup_ns,
};

static inline struct nsproxy *create_nsproxy(void)
{
 struct nsproxy *nsproxy;

 nsproxy = kmem_cache_alloc(nsproxy_cachep, GFP_KERNEL);
 if (nsproxy)
  atomic_set(&nsproxy->count, 1);
 return nsproxy;
}






static struct nsproxy *create_new_namespaces(unsigned long flags,
 struct task_struct *tsk, struct user_namespace *user_ns,
 struct fs_struct *new_fs)
{
 struct nsproxy *new_nsp;
 int err;

 new_nsp = create_nsproxy();
 if (!new_nsp)
  return ERR_PTR(-ENOMEM);

 new_nsp->mnt_ns = copy_mnt_ns(flags, tsk->nsproxy->mnt_ns, user_ns, new_fs);
 if (IS_ERR(new_nsp->mnt_ns)) {
  err = PTR_ERR(new_nsp->mnt_ns);
  goto out_ns;
 }

 new_nsp->uts_ns = copy_utsname(flags, user_ns, tsk->nsproxy->uts_ns);
 if (IS_ERR(new_nsp->uts_ns)) {
  err = PTR_ERR(new_nsp->uts_ns);
  goto out_uts;
 }

 new_nsp->ipc_ns = copy_ipcs(flags, user_ns, tsk->nsproxy->ipc_ns);
 if (IS_ERR(new_nsp->ipc_ns)) {
  err = PTR_ERR(new_nsp->ipc_ns);
  goto out_ipc;
 }

 new_nsp->pid_ns_for_children =
  copy_pid_ns(flags, user_ns, tsk->nsproxy->pid_ns_for_children);
 if (IS_ERR(new_nsp->pid_ns_for_children)) {
  err = PTR_ERR(new_nsp->pid_ns_for_children);
  goto out_pid;
 }

 new_nsp->cgroup_ns = copy_cgroup_ns(flags, user_ns,
         tsk->nsproxy->cgroup_ns);
 if (IS_ERR(new_nsp->cgroup_ns)) {
  err = PTR_ERR(new_nsp->cgroup_ns);
  goto out_cgroup;
 }

 new_nsp->net_ns = copy_net_ns(flags, user_ns, tsk->nsproxy->net_ns);
 if (IS_ERR(new_nsp->net_ns)) {
  err = PTR_ERR(new_nsp->net_ns);
  goto out_net;
 }

 return new_nsp;

out_net:
 put_cgroup_ns(new_nsp->cgroup_ns);
out_cgroup:
 if (new_nsp->pid_ns_for_children)
  put_pid_ns(new_nsp->pid_ns_for_children);
out_pid:
 if (new_nsp->ipc_ns)
  put_ipc_ns(new_nsp->ipc_ns);
out_ipc:
 if (new_nsp->uts_ns)
  put_uts_ns(new_nsp->uts_ns);
out_uts:
 if (new_nsp->mnt_ns)
  put_mnt_ns(new_nsp->mnt_ns);
out_ns:
 kmem_cache_free(nsproxy_cachep, new_nsp);
 return ERR_PTR(err);
}





int copy_namespaces(unsigned long flags, struct task_struct *tsk)
{
 struct nsproxy *old_ns = tsk->nsproxy;
 struct user_namespace *user_ns = task_cred_xxx(tsk, user_ns);
 struct nsproxy *new_ns;

 if (likely(!(flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
         CLONE_NEWPID | CLONE_NEWNET |
         CLONE_NEWCGROUP)))) {
  get_nsproxy(old_ns);
  return 0;
 }

 if (!ns_capable(user_ns, CAP_SYS_ADMIN))
  return -EPERM;
 if ((flags & (CLONE_NEWIPC | CLONE_SYSVSEM)) ==
  (CLONE_NEWIPC | CLONE_SYSVSEM))
  return -EINVAL;

 new_ns = create_new_namespaces(flags, tsk, user_ns, tsk->fs);
 if (IS_ERR(new_ns))
  return PTR_ERR(new_ns);

 tsk->nsproxy = new_ns;
 return 0;
}

void free_nsproxy(struct nsproxy *ns)
{
 if (ns->mnt_ns)
  put_mnt_ns(ns->mnt_ns);
 if (ns->uts_ns)
  put_uts_ns(ns->uts_ns);
 if (ns->ipc_ns)
  put_ipc_ns(ns->ipc_ns);
 if (ns->pid_ns_for_children)
  put_pid_ns(ns->pid_ns_for_children);
 put_cgroup_ns(ns->cgroup_ns);
 put_net(ns->net_ns);
 kmem_cache_free(nsproxy_cachep, ns);
}





int unshare_nsproxy_namespaces(unsigned long unshare_flags,
 struct nsproxy **new_nsp, struct cred *new_cred, struct fs_struct *new_fs)
{
 struct user_namespace *user_ns;
 int err = 0;

 if (!(unshare_flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
          CLONE_NEWNET | CLONE_NEWPID | CLONE_NEWCGROUP)))
  return 0;

 user_ns = new_cred ? new_cred->user_ns : current_user_ns();
 if (!ns_capable(user_ns, CAP_SYS_ADMIN))
  return -EPERM;

 *new_nsp = create_new_namespaces(unshare_flags, current, user_ns,
      new_fs ? new_fs : current->fs);
 if (IS_ERR(*new_nsp)) {
  err = PTR_ERR(*new_nsp);
  goto out;
 }

out:
 return err;
}

void switch_task_namespaces(struct task_struct *p, struct nsproxy *new)
{
 struct nsproxy *ns;

 might_sleep();

 task_lock(p);
 ns = p->nsproxy;
 p->nsproxy = new;
 task_unlock(p);

 if (ns && atomic_dec_and_test(&ns->count))
  free_nsproxy(ns);
}

void exit_task_namespaces(struct task_struct *p)
{
 switch_task_namespaces(p, NULL);
}

SYSCALL_DEFINE2(setns, int, fd, int, nstype)
{
 struct task_struct *tsk = current;
 struct nsproxy *new_nsproxy;
 struct file *file;
 struct ns_common *ns;
 int err;

 file = proc_ns_fget(fd);
 if (IS_ERR(file))
  return PTR_ERR(file);

 err = -EINVAL;
 ns = get_proc_ns(file_inode(file));
 if (nstype && (ns->ops->type != nstype))
  goto out;

 new_nsproxy = create_new_namespaces(0, tsk, current_user_ns(), tsk->fs);
 if (IS_ERR(new_nsproxy)) {
  err = PTR_ERR(new_nsproxy);
  goto out;
 }

 err = ns->ops->install(new_nsproxy, ns);
 if (err) {
  free_nsproxy(new_nsproxy);
  goto out;
 }
 switch_task_namespaces(tsk, new_nsproxy);
out:
 fput(file);
 return err;
}

int __init nsproxy_cache_init(void)
{
 nsproxy_cachep = KMEM_CACHE(nsproxy, SLAB_PANIC);
 return 0;
}








unsigned long tick_usec = TICK_USEC;


unsigned long tick_nsec;

static u64 tick_length;
static u64 tick_length_base;

 (((MAX_TICKADJ * NSEC_PER_USEC) << NTP_SCALE_SHIFT) / NTP_INTERVAL_FREQ)
static int time_state = TIME_OK;


static int time_status = STA_UNSYNC;


static s64 time_offset;


static long time_constant = 2;


static long time_maxerror = NTP_PHASE_LIMIT;


static long time_esterror = NTP_PHASE_LIMIT;


static s64 time_freq;


static time64_t time_reftime;

static long time_adjust;


static s64 ntp_tick_adj;


static time64_t ntp_next_leap_sec = TIME64_MAX;










static int pps_valid;
static long pps_tf[3];
static long pps_jitter;
static struct timespec64 pps_fbase;
static int pps_shift;
static int pps_intcnt;
static s64 pps_freq;
static long pps_stabil;




static long pps_calcnt;
static long pps_jitcnt;
static long pps_stbcnt;
static long pps_errcnt;





static inline s64 ntp_offset_chunk(s64 offset)
{
 if (time_status & STA_PPSTIME && time_status & STA_PPSSIGNAL)
  return offset;
 else
  return shift_right(offset, SHIFT_PLL + time_constant);
}

static inline void pps_reset_freq_interval(void)
{


 pps_shift = PPS_INTMIN;
 pps_intcnt = 0;
}




static inline void pps_clear(void)
{
 pps_reset_freq_interval();
 pps_tf[0] = 0;
 pps_tf[1] = 0;
 pps_tf[2] = 0;
 pps_fbase.tv_sec = pps_fbase.tv_nsec = 0;
 pps_freq = 0;
}





static inline void pps_dec_valid(void)
{
 if (pps_valid > 0)
  pps_valid--;
 else {
  time_status &= ~(STA_PPSSIGNAL | STA_PPSJITTER |
     STA_PPSWANDER | STA_PPSERROR);
  pps_clear();
 }
}

static inline void pps_set_freq(s64 freq)
{
 pps_freq = freq;
}

static inline int is_error_status(int status)
{
 return (status & (STA_UNSYNC|STA_CLOCKERR))



  || ((status & (STA_PPSFREQ|STA_PPSTIME))
   && !(status & STA_PPSSIGNAL))


  || ((status & (STA_PPSTIME|STA_PPSJITTER))
   == (STA_PPSTIME|STA_PPSJITTER))



  || ((status & STA_PPSFREQ)
   && (status & (STA_PPSWANDER|STA_PPSERROR)));
}

static inline void pps_fill_timex(struct timex *txc)
{
 txc->ppsfreq = shift_right((pps_freq >> PPM_SCALE_INV_SHIFT) *
      PPM_SCALE_INV, NTP_SCALE_SHIFT);
 txc->jitter = pps_jitter;
 if (!(time_status & STA_NANO))
  txc->jitter /= NSEC_PER_USEC;
 txc->shift = pps_shift;
 txc->stabil = pps_stabil;
 txc->jitcnt = pps_jitcnt;
 txc->calcnt = pps_calcnt;
 txc->errcnt = pps_errcnt;
 txc->stbcnt = pps_stbcnt;
}


static inline s64 ntp_offset_chunk(s64 offset)
{
 return shift_right(offset, SHIFT_PLL + time_constant);
}

static inline void pps_reset_freq_interval(void) {}
static inline void pps_clear(void) {}
static inline void pps_dec_valid(void) {}
static inline void pps_set_freq(s64 freq) {}

static inline int is_error_status(int status)
{
 return status & (STA_UNSYNC|STA_CLOCKERR);
}

static inline void pps_fill_timex(struct timex *txc)
{

 txc->ppsfreq = 0;
 txc->jitter = 0;
 txc->shift = 0;
 txc->stabil = 0;
 txc->jitcnt = 0;
 txc->calcnt = 0;
 txc->errcnt = 0;
 txc->stbcnt = 0;
}







static inline int ntp_synced(void)
{
 return !(time_status & STA_UNSYNC);
}
static void ntp_update_frequency(void)
{
 u64 second_length;
 u64 new_base;

 second_length = (u64)(tick_usec * NSEC_PER_USEC * USER_HZ)
      << NTP_SCALE_SHIFT;

 second_length += ntp_tick_adj;
 second_length += time_freq;

 tick_nsec = div_u64(second_length, HZ) >> NTP_SCALE_SHIFT;
 new_base = div_u64(second_length, NTP_INTERVAL_FREQ);





 tick_length += new_base - tick_length_base;
 tick_length_base = new_base;
}

static inline s64 ntp_update_offset_fll(s64 offset64, long secs)
{
 time_status &= ~STA_MODE;

 if (secs < MINSEC)
  return 0;

 if (!(time_status & STA_FLL) && (secs <= MAXSEC))
  return 0;

 time_status |= STA_MODE;

 return div64_long(offset64 << (NTP_SCALE_SHIFT - SHIFT_FLL), secs);
}

static void ntp_update_offset(long offset)
{
 s64 freq_adj;
 s64 offset64;
 long secs;

 if (!(time_status & STA_PLL))
  return;

 if (!(time_status & STA_NANO)) {

  offset = clamp(offset, -USEC_PER_SEC, USEC_PER_SEC);
  offset *= NSEC_PER_USEC;
 }





 offset = clamp(offset, -MAXPHASE, MAXPHASE);





 secs = (long)(__ktime_get_real_seconds() - time_reftime);
 if (unlikely(time_status & STA_FREQHOLD))
  secs = 0;

 time_reftime = __ktime_get_real_seconds();

 offset64 = offset;
 freq_adj = ntp_update_offset_fll(offset64, secs);






 if (unlikely(secs > 1 << (SHIFT_PLL + 1 + time_constant)))
  secs = 1 << (SHIFT_PLL + 1 + time_constant);

 freq_adj += (offset64 * secs) <<
   (NTP_SCALE_SHIFT - 2 * (SHIFT_PLL + 2 + time_constant));

 freq_adj = min(freq_adj + time_freq, MAXFREQ_SCALED);

 time_freq = max(freq_adj, -MAXFREQ_SCALED);

 time_offset = div_s64(offset64 << NTP_SCALE_SHIFT, NTP_INTERVAL_FREQ);
}




void ntp_clear(void)
{
 time_adjust = 0;
 time_status |= STA_UNSYNC;
 time_maxerror = NTP_PHASE_LIMIT;
 time_esterror = NTP_PHASE_LIMIT;

 ntp_update_frequency();

 tick_length = tick_length_base;
 time_offset = 0;

 ntp_next_leap_sec = TIME64_MAX;

 pps_clear();
}


u64 ntp_tick_length(void)
{
 return tick_length;
}







ktime_t ntp_get_next_leap(void)
{
 ktime_t ret;

 if ((time_state == TIME_INS) && (time_status & STA_INS))
  return ktime_set(ntp_next_leap_sec, 0);
 ret.tv64 = KTIME_MAX;
 return ret;
}
int second_overflow(time64_t secs)
{
 s64 delta;
 int leap = 0;
 s32 rem;






 switch (time_state) {
 case TIME_OK:
  if (time_status & STA_INS) {
   time_state = TIME_INS;
   div_s64_rem(secs, SECS_PER_DAY, &rem);
   ntp_next_leap_sec = secs + SECS_PER_DAY - rem;
  } else if (time_status & STA_DEL) {
   time_state = TIME_DEL;
   div_s64_rem(secs + 1, SECS_PER_DAY, &rem);
   ntp_next_leap_sec = secs + SECS_PER_DAY - rem;
  }
  break;
 case TIME_INS:
  if (!(time_status & STA_INS)) {
   ntp_next_leap_sec = TIME64_MAX;
   time_state = TIME_OK;
  } else if (secs == ntp_next_leap_sec) {
   leap = -1;
   time_state = TIME_OOP;
   printk(KERN_NOTICE
    "Clock: inserting leap second 23:59:60 UTC\n");
  }
  break;
 case TIME_DEL:
  if (!(time_status & STA_DEL)) {
   ntp_next_leap_sec = TIME64_MAX;
   time_state = TIME_OK;
  } else if (secs == ntp_next_leap_sec) {
   leap = 1;
   ntp_next_leap_sec = TIME64_MAX;
   time_state = TIME_WAIT;
   printk(KERN_NOTICE
    "Clock: deleting leap second 23:59:59 UTC\n");
  }
  break;
 case TIME_OOP:
  ntp_next_leap_sec = TIME64_MAX;
  time_state = TIME_WAIT;
  break;
 case TIME_WAIT:
  if (!(time_status & (STA_INS | STA_DEL)))
   time_state = TIME_OK;
  break;
 }



 time_maxerror += MAXFREQ / NSEC_PER_USEC;
 if (time_maxerror > NTP_PHASE_LIMIT) {
  time_maxerror = NTP_PHASE_LIMIT;
  time_status |= STA_UNSYNC;
 }


 tick_length = tick_length_base;

 delta = ntp_offset_chunk(time_offset);
 time_offset -= delta;
 tick_length += delta;


 pps_dec_valid();

 if (!time_adjust)
  goto out;

 if (time_adjust > MAX_TICKADJ) {
  time_adjust -= MAX_TICKADJ;
  tick_length += MAX_TICKADJ_SCALED;
  goto out;
 }

 if (time_adjust < -MAX_TICKADJ) {
  time_adjust += MAX_TICKADJ;
  tick_length -= MAX_TICKADJ_SCALED;
  goto out;
 }

 tick_length += (s64)(time_adjust * NSEC_PER_USEC / NTP_INTERVAL_FREQ)
        << NTP_SCALE_SHIFT;
 time_adjust = 0;

out:
 return leap;
}

int __weak update_persistent_clock(struct timespec now)
{
 return -ENODEV;
}

int __weak update_persistent_clock64(struct timespec64 now64)
{
 struct timespec now;

 now = timespec64_to_timespec(now64);
 return update_persistent_clock(now);
}

static void sync_cmos_clock(struct work_struct *work);

static DECLARE_DELAYED_WORK(sync_cmos_work, sync_cmos_clock);

static void sync_cmos_clock(struct work_struct *work)
{
 struct timespec64 now;
 struct timespec64 next;
 int fail = 1;
 if (!ntp_synced()) {




  return;
 }

 getnstimeofday64(&now);
 if (abs(now.tv_nsec - (NSEC_PER_SEC / 2)) <= tick_nsec * 5) {
  struct timespec64 adjust = now;

  fail = -ENODEV;
  if (persistent_clock_is_local)
   adjust.tv_sec -= (sys_tz.tz_minuteswest * 60);
  fail = update_persistent_clock64(adjust);

  if (fail == -ENODEV)
   fail = rtc_set_ntp_time(adjust);
 }

 next.tv_nsec = (NSEC_PER_SEC / 2) - now.tv_nsec - (TICK_NSEC / 2);
 if (next.tv_nsec <= 0)
  next.tv_nsec += NSEC_PER_SEC;

 if (!fail || fail == -ENODEV)
  next.tv_sec = 659;
 else
  next.tv_sec = 0;

 if (next.tv_nsec >= NSEC_PER_SEC) {
  next.tv_sec++;
  next.tv_nsec -= NSEC_PER_SEC;
 }
 queue_delayed_work(system_power_efficient_wq,
      &sync_cmos_work, timespec64_to_jiffies(&next));
}

void ntp_notify_cmos_timer(void)
{
 queue_delayed_work(system_power_efficient_wq, &sync_cmos_work, 0);
}

void ntp_notify_cmos_timer(void) { }





static inline void process_adj_status(struct timex *txc, struct timespec64 *ts)
{
 if ((time_status & STA_PLL) && !(txc->status & STA_PLL)) {
  time_state = TIME_OK;
  time_status = STA_UNSYNC;
  ntp_next_leap_sec = TIME64_MAX;

  pps_reset_freq_interval();
 }





 if (!(time_status & STA_PLL) && (txc->status & STA_PLL))
  time_reftime = __ktime_get_real_seconds();


 time_status &= STA_RONLY;
 time_status |= txc->status & ~STA_RONLY;
}


static inline void process_adjtimex_modes(struct timex *txc,
      struct timespec64 *ts,
      s32 *time_tai)
{
 if (txc->modes & ADJ_STATUS)
  process_adj_status(txc, ts);

 if (txc->modes & ADJ_NANO)
  time_status |= STA_NANO;

 if (txc->modes & ADJ_MICRO)
  time_status &= ~STA_NANO;

 if (txc->modes & ADJ_FREQUENCY) {
  time_freq = txc->freq * PPM_SCALE;
  time_freq = min(time_freq, MAXFREQ_SCALED);
  time_freq = max(time_freq, -MAXFREQ_SCALED);

  pps_set_freq(time_freq);
 }

 if (txc->modes & ADJ_MAXERROR)
  time_maxerror = txc->maxerror;

 if (txc->modes & ADJ_ESTERROR)
  time_esterror = txc->esterror;

 if (txc->modes & ADJ_TIMECONST) {
  time_constant = txc->constant;
  if (!(time_status & STA_NANO))
   time_constant += 4;
  time_constant = min(time_constant, (long)MAXTC);
  time_constant = max(time_constant, 0l);
 }

 if (txc->modes & ADJ_TAI && txc->constant > 0)
  *time_tai = txc->constant;

 if (txc->modes & ADJ_OFFSET)
  ntp_update_offset(txc->offset);

 if (txc->modes & ADJ_TICK)
  tick_usec = txc->tick;

 if (txc->modes & (ADJ_TICK|ADJ_FREQUENCY|ADJ_OFFSET))
  ntp_update_frequency();
}






int ntp_validate_timex(struct timex *txc)
{
 if (txc->modes & ADJ_ADJTIME) {

  if (!(txc->modes & ADJ_OFFSET_SINGLESHOT))
   return -EINVAL;
  if (!(txc->modes & ADJ_OFFSET_READONLY) &&
      !capable(CAP_SYS_TIME))
   return -EPERM;
 } else {

   if (txc->modes && !capable(CAP_SYS_TIME))
   return -EPERM;




  if (txc->modes & ADJ_TICK &&
      (txc->tick < 900000/USER_HZ ||
       txc->tick > 1100000/USER_HZ))
   return -EINVAL;
 }

 if (txc->modes & ADJ_SETOFFSET) {

  if (!capable(CAP_SYS_TIME))
   return -EPERM;

  if (txc->modes & ADJ_NANO) {
   struct timespec ts;

   ts.tv_sec = txc->time.tv_sec;
   ts.tv_nsec = txc->time.tv_usec;
   if (!timespec_inject_offset_valid(&ts))
    return -EINVAL;

  } else {
   if (!timeval_inject_offset_valid(&txc->time))
    return -EINVAL;
  }
 }





 if ((txc->modes & ADJ_FREQUENCY) && (BITS_PER_LONG == 64)) {
  if (LLONG_MIN / PPM_SCALE > txc->freq)
   return -EINVAL;
  if (LLONG_MAX / PPM_SCALE < txc->freq)
   return -EINVAL;
 }

 return 0;
}






int __do_adjtimex(struct timex *txc, struct timespec64 *ts, s32 *time_tai)
{
 int result;

 if (txc->modes & ADJ_ADJTIME) {
  long save_adjust = time_adjust;

  if (!(txc->modes & ADJ_OFFSET_READONLY)) {

   time_adjust = txc->offset;
   ntp_update_frequency();
  }
  txc->offset = save_adjust;
 } else {


  if (txc->modes)
   process_adjtimex_modes(txc, ts, time_tai);

  txc->offset = shift_right(time_offset * NTP_INTERVAL_FREQ,
      NTP_SCALE_SHIFT);
  if (!(time_status & STA_NANO))
   txc->offset /= NSEC_PER_USEC;
 }

 result = time_state;

 if (is_error_status(time_status))
  result = TIME_ERROR;

 txc->freq = shift_right((time_freq >> PPM_SCALE_INV_SHIFT) *
      PPM_SCALE_INV, NTP_SCALE_SHIFT);
 txc->maxerror = time_maxerror;
 txc->esterror = time_esterror;
 txc->status = time_status;
 txc->constant = time_constant;
 txc->precision = 1;
 txc->tolerance = MAXFREQ_SCALED / PPM_SCALE;
 txc->tick = tick_usec;
 txc->tai = *time_tai;


 pps_fill_timex(txc);

 txc->time.tv_sec = (time_t)ts->tv_sec;
 txc->time.tv_usec = ts->tv_nsec;
 if (!(time_status & STA_NANO))
  txc->time.tv_usec /= NSEC_PER_USEC;


 if (unlikely(ts->tv_sec >= ntp_next_leap_sec)) {
  if ((time_state == TIME_INS) && (time_status & STA_INS)) {
   result = TIME_OOP;
   txc->tai++;
   txc->time.tv_sec--;
  }
  if ((time_state == TIME_DEL) && (time_status & STA_DEL)) {
   result = TIME_WAIT;
   txc->tai--;
   txc->time.tv_sec++;
  }
  if ((time_state == TIME_OOP) &&
     (ts->tv_sec == ntp_next_leap_sec)) {
   result = TIME_WAIT;
  }
 }

 return result;
}






struct pps_normtime {
 s64 sec;
 long nsec;
};



static inline struct pps_normtime pps_normalize_ts(struct timespec64 ts)
{
 struct pps_normtime norm = {
  .sec = ts.tv_sec,
  .nsec = ts.tv_nsec
 };

 if (norm.nsec > (NSEC_PER_SEC >> 1)) {
  norm.nsec -= NSEC_PER_SEC;
  norm.sec++;
 }

 return norm;
}


static inline long pps_phase_filter_get(long *jitter)
{
 *jitter = pps_tf[0] - pps_tf[1];
 if (*jitter < 0)
  *jitter = -*jitter;


 return pps_tf[0];
}


static inline void pps_phase_filter_add(long err)
{
 pps_tf[2] = pps_tf[1];
 pps_tf[1] = pps_tf[0];
 pps_tf[0] = err;
}




static inline void pps_dec_freq_interval(void)
{
 if (--pps_intcnt <= -PPS_INTCOUNT) {
  pps_intcnt = -PPS_INTCOUNT;
  if (pps_shift > PPS_INTMIN) {
   pps_shift--;
   pps_intcnt = 0;
  }
 }
}




static inline void pps_inc_freq_interval(void)
{
 if (++pps_intcnt >= PPS_INTCOUNT) {
  pps_intcnt = PPS_INTCOUNT;
  if (pps_shift < PPS_INTMAX) {
   pps_shift++;
   pps_intcnt = 0;
  }
 }
}
static long hardpps_update_freq(struct pps_normtime freq_norm)
{
 long delta, delta_mod;
 s64 ftemp;


 if (freq_norm.sec > (2 << pps_shift)) {
  time_status |= STA_PPSERROR;
  pps_errcnt++;
  pps_dec_freq_interval();
  printk_deferred(KERN_ERR
   "hardpps: PPSERROR: interval too long - %lld s\n",
   freq_norm.sec);
  return 0;
 }





 ftemp = div_s64(((s64)(-freq_norm.nsec)) << NTP_SCALE_SHIFT,
   freq_norm.sec);
 delta = shift_right(ftemp - pps_freq, NTP_SCALE_SHIFT);
 pps_freq = ftemp;
 if (delta > PPS_MAXWANDER || delta < -PPS_MAXWANDER) {
  printk_deferred(KERN_WARNING
    "hardpps: PPSWANDER: change=%ld\n", delta);
  time_status |= STA_PPSWANDER;
  pps_stbcnt++;
  pps_dec_freq_interval();
 } else {
  pps_inc_freq_interval();
 }





 delta_mod = delta;
 if (delta_mod < 0)
  delta_mod = -delta_mod;
 pps_stabil += (div_s64(((s64)delta_mod) <<
    (NTP_SCALE_SHIFT - SHIFT_USEC),
    NSEC_PER_USEC) - pps_stabil) >> PPS_INTMIN;


 if ((time_status & STA_PPSFREQ) != 0 &&
     (time_status & STA_FREQHOLD) == 0) {
  time_freq = pps_freq;
  ntp_update_frequency();
 }

 return delta;
}


static void hardpps_update_phase(long error)
{
 long correction = -error;
 long jitter;


 pps_phase_filter_add(correction);
 correction = pps_phase_filter_get(&jitter);





 if (jitter > (pps_jitter << PPS_POPCORN)) {
  printk_deferred(KERN_WARNING
    "hardpps: PPSJITTER: jitter=%ld, limit=%ld\n",
    jitter, (pps_jitter << PPS_POPCORN));
  time_status |= STA_PPSJITTER;
  pps_jitcnt++;
 } else if (time_status & STA_PPSTIME) {

  time_offset = div_s64(((s64)correction) << NTP_SCALE_SHIFT,
    NTP_INTERVAL_FREQ);

  time_adjust = 0;
 }

 pps_jitter += (jitter - pps_jitter) >> PPS_INTMIN;
}
void __hardpps(const struct timespec64 *phase_ts, const struct timespec64 *raw_ts)
{
 struct pps_normtime pts_norm, freq_norm;

 pts_norm = pps_normalize_ts(*phase_ts);


 time_status &= ~(STA_PPSJITTER | STA_PPSWANDER | STA_PPSERROR);


 time_status |= STA_PPSSIGNAL;
 pps_valid = PPS_VALID;



 if (unlikely(pps_fbase.tv_sec == 0)) {
  pps_fbase = *raw_ts;
  return;
 }


 freq_norm = pps_normalize_ts(timespec64_sub(*raw_ts, pps_fbase));



 if ((freq_norm.sec == 0) ||
   (freq_norm.nsec > MAXFREQ * freq_norm.sec) ||
   (freq_norm.nsec < -MAXFREQ * freq_norm.sec)) {
  time_status |= STA_PPSJITTER;

  pps_fbase = *raw_ts;
  printk_deferred(KERN_ERR "hardpps: PPSJITTER: bad pulse\n");
  return;
 }




 if (freq_norm.sec >= (1 << pps_shift)) {
  pps_calcnt++;

  pps_fbase = *raw_ts;
  hardpps_update_freq(freq_norm);
 }

 hardpps_update_phase(pts_norm.nsec);

}

static int __init ntp_tick_adj_setup(char *str)
{
 int rc = kstrtol(str, 0, (long *)&ntp_tick_adj);

 if (rc)
  return rc;
 ntp_tick_adj <<= NTP_SCALE_SHIFT;

 return 1;
}

__setup("ntp_tick_adj=", ntp_tick_adj_setup);

void __init ntp_init(void)
{
 ntp_clear();
}


static int padata_index_to_cpu(struct parallel_data *pd, int cpu_index)
{
 int cpu, target_cpu;

 target_cpu = cpumask_first(pd->cpumask.pcpu);
 for (cpu = 0; cpu < cpu_index; cpu++)
  target_cpu = cpumask_next(target_cpu, pd->cpumask.pcpu);

 return target_cpu;
}

static int padata_cpu_hash(struct parallel_data *pd)
{
 unsigned int seq_nr;
 int cpu_index;






 seq_nr = atomic_inc_return(&pd->seq_nr);
 cpu_index = seq_nr % cpumask_weight(pd->cpumask.pcpu);

 return padata_index_to_cpu(pd, cpu_index);
}

static void padata_parallel_worker(struct work_struct *parallel_work)
{
 struct padata_parallel_queue *pqueue;
 struct parallel_data *pd;
 struct padata_instance *pinst;
 LIST_HEAD(local_list);

 local_bh_disable();
 pqueue = container_of(parallel_work,
         struct padata_parallel_queue, work);
 pd = pqueue->pd;
 pinst = pd->pinst;

 spin_lock(&pqueue->parallel.lock);
 list_replace_init(&pqueue->parallel.list, &local_list);
 spin_unlock(&pqueue->parallel.lock);

 while (!list_empty(&local_list)) {
  struct padata_priv *padata;

  padata = list_entry(local_list.next,
        struct padata_priv, list);

  list_del_init(&padata->list);

  padata->parallel(padata);
 }

 local_bh_enable();
}
int padata_do_parallel(struct padata_instance *pinst,
         struct padata_priv *padata, int cb_cpu)
{
 int target_cpu, err;
 struct padata_parallel_queue *queue;
 struct parallel_data *pd;

 rcu_read_lock_bh();

 pd = rcu_dereference_bh(pinst->pd);

 err = -EINVAL;
 if (!(pinst->flags & PADATA_INIT) || pinst->flags & PADATA_INVALID)
  goto out;

 if (!cpumask_test_cpu(cb_cpu, pd->cpumask.cbcpu))
  goto out;

 err = -EBUSY;
 if ((pinst->flags & PADATA_RESET))
  goto out;

 if (atomic_read(&pd->refcnt) >= MAX_OBJ_NUM)
  goto out;

 err = 0;
 atomic_inc(&pd->refcnt);
 padata->pd = pd;
 padata->cb_cpu = cb_cpu;

 target_cpu = padata_cpu_hash(pd);
 queue = per_cpu_ptr(pd->pqueue, target_cpu);

 spin_lock(&queue->parallel.lock);
 list_add_tail(&padata->list, &queue->parallel.list);
 spin_unlock(&queue->parallel.lock);

 queue_work_on(target_cpu, pinst->wq, &queue->work);

out:
 rcu_read_unlock_bh();

 return err;
}
EXPORT_SYMBOL(padata_do_parallel);
static struct padata_priv *padata_get_next(struct parallel_data *pd)
{
 int cpu, num_cpus;
 unsigned int next_nr, next_index;
 struct padata_parallel_queue *next_queue;
 struct padata_priv *padata;
 struct padata_list *reorder;

 num_cpus = cpumask_weight(pd->cpumask.pcpu);





 next_nr = pd->processed;
 next_index = next_nr % num_cpus;
 cpu = padata_index_to_cpu(pd, next_index);
 next_queue = per_cpu_ptr(pd->pqueue, cpu);

 padata = NULL;

 reorder = &next_queue->reorder;

 if (!list_empty(&reorder->list)) {
  padata = list_entry(reorder->list.next,
        struct padata_priv, list);

  spin_lock(&reorder->lock);
  list_del_init(&padata->list);
  atomic_dec(&pd->reorder_objects);
  spin_unlock(&reorder->lock);

  pd->processed++;

  goto out;
 }

 if (__this_cpu_read(pd->pqueue->cpu_index) == next_queue->cpu_index) {
  padata = ERR_PTR(-ENODATA);
  goto out;
 }

 padata = ERR_PTR(-EINPROGRESS);
out:
 return padata;
}

static void padata_reorder(struct parallel_data *pd)
{
 int cb_cpu;
 struct padata_priv *padata;
 struct padata_serial_queue *squeue;
 struct padata_instance *pinst = pd->pinst;
 if (!spin_trylock_bh(&pd->lock))
  return;

 while (1) {
  padata = padata_get_next(pd);







  if (!padata || PTR_ERR(padata) == -EINPROGRESS)
   break;






  if (PTR_ERR(padata) == -ENODATA) {
   del_timer(&pd->timer);
   spin_unlock_bh(&pd->lock);
   return;
  }

  cb_cpu = padata->cb_cpu;
  squeue = per_cpu_ptr(pd->squeue, cb_cpu);

  spin_lock(&squeue->serial.lock);
  list_add_tail(&padata->list, &squeue->serial.list);
  spin_unlock(&squeue->serial.lock);

  queue_work_on(cb_cpu, pinst->wq, &squeue->work);
 }

 spin_unlock_bh(&pd->lock);






 if (atomic_read(&pd->reorder_objects)
   && !(pinst->flags & PADATA_RESET))
  mod_timer(&pd->timer, jiffies + HZ);
 else
  del_timer(&pd->timer);

 return;
}

static void padata_reorder_timer(unsigned long arg)
{
 struct parallel_data *pd = (struct parallel_data *)arg;

 padata_reorder(pd);
}

static void padata_serial_worker(struct work_struct *serial_work)
{
 struct padata_serial_queue *squeue;
 struct parallel_data *pd;
 LIST_HEAD(local_list);

 local_bh_disable();
 squeue = container_of(serial_work, struct padata_serial_queue, work);
 pd = squeue->pd;

 spin_lock(&squeue->serial.lock);
 list_replace_init(&squeue->serial.list, &local_list);
 spin_unlock(&squeue->serial.lock);

 while (!list_empty(&local_list)) {
  struct padata_priv *padata;

  padata = list_entry(local_list.next,
        struct padata_priv, list);

  list_del_init(&padata->list);

  padata->serial(padata);
  atomic_dec(&pd->refcnt);
 }
 local_bh_enable();
}
void padata_do_serial(struct padata_priv *padata)
{
 int cpu;
 struct padata_parallel_queue *pqueue;
 struct parallel_data *pd;

 pd = padata->pd;

 cpu = get_cpu();
 pqueue = per_cpu_ptr(pd->pqueue, cpu);

 spin_lock(&pqueue->reorder.lock);
 atomic_inc(&pd->reorder_objects);
 list_add_tail(&padata->list, &pqueue->reorder.list);
 spin_unlock(&pqueue->reorder.lock);

 put_cpu();

 padata_reorder(pd);
}
EXPORT_SYMBOL(padata_do_serial);

static int padata_setup_cpumasks(struct parallel_data *pd,
     const struct cpumask *pcpumask,
     const struct cpumask *cbcpumask)
{
 if (!alloc_cpumask_var(&pd->cpumask.pcpu, GFP_KERNEL))
  return -ENOMEM;

 cpumask_and(pd->cpumask.pcpu, pcpumask, cpu_online_mask);
 if (!alloc_cpumask_var(&pd->cpumask.cbcpu, GFP_KERNEL)) {
  free_cpumask_var(pd->cpumask.cbcpu);
  return -ENOMEM;
 }

 cpumask_and(pd->cpumask.cbcpu, cbcpumask, cpu_online_mask);
 return 0;
}

static void __padata_list_init(struct padata_list *pd_list)
{
 INIT_LIST_HEAD(&pd_list->list);
 spin_lock_init(&pd_list->lock);
}


static void padata_init_squeues(struct parallel_data *pd)
{
 int cpu;
 struct padata_serial_queue *squeue;

 for_each_cpu(cpu, pd->cpumask.cbcpu) {
  squeue = per_cpu_ptr(pd->squeue, cpu);
  squeue->pd = pd;
  __padata_list_init(&squeue->serial);
  INIT_WORK(&squeue->work, padata_serial_worker);
 }
}


static void padata_init_pqueues(struct parallel_data *pd)
{
 int cpu_index, cpu;
 struct padata_parallel_queue *pqueue;

 cpu_index = 0;
 for_each_cpu(cpu, pd->cpumask.pcpu) {
  pqueue = per_cpu_ptr(pd->pqueue, cpu);
  pqueue->pd = pd;
  pqueue->cpu_index = cpu_index;
  cpu_index++;

  __padata_list_init(&pqueue->reorder);
  __padata_list_init(&pqueue->parallel);
  INIT_WORK(&pqueue->work, padata_parallel_worker);
  atomic_set(&pqueue->num_obj, 0);
 }
}


static struct parallel_data *padata_alloc_pd(struct padata_instance *pinst,
          const struct cpumask *pcpumask,
          const struct cpumask *cbcpumask)
{
 struct parallel_data *pd;

 pd = kzalloc(sizeof(struct parallel_data), GFP_KERNEL);
 if (!pd)
  goto err;

 pd->pqueue = alloc_percpu(struct padata_parallel_queue);
 if (!pd->pqueue)
  goto err_free_pd;

 pd->squeue = alloc_percpu(struct padata_serial_queue);
 if (!pd->squeue)
  goto err_free_pqueue;
 if (padata_setup_cpumasks(pd, pcpumask, cbcpumask) < 0)
  goto err_free_squeue;

 padata_init_pqueues(pd);
 padata_init_squeues(pd);
 setup_timer(&pd->timer, padata_reorder_timer, (unsigned long)pd);
 atomic_set(&pd->seq_nr, -1);
 atomic_set(&pd->reorder_objects, 0);
 atomic_set(&pd->refcnt, 0);
 pd->pinst = pinst;
 spin_lock_init(&pd->lock);

 return pd;

err_free_squeue:
 free_percpu(pd->squeue);
err_free_pqueue:
 free_percpu(pd->pqueue);
err_free_pd:
 kfree(pd);
err:
 return NULL;
}

static void padata_free_pd(struct parallel_data *pd)
{
 free_cpumask_var(pd->cpumask.pcpu);
 free_cpumask_var(pd->cpumask.cbcpu);
 free_percpu(pd->pqueue);
 free_percpu(pd->squeue);
 kfree(pd);
}


static void padata_flush_queues(struct parallel_data *pd)
{
 int cpu;
 struct padata_parallel_queue *pqueue;
 struct padata_serial_queue *squeue;

 for_each_cpu(cpu, pd->cpumask.pcpu) {
  pqueue = per_cpu_ptr(pd->pqueue, cpu);
  flush_work(&pqueue->work);
 }

 del_timer_sync(&pd->timer);

 if (atomic_read(&pd->reorder_objects))
  padata_reorder(pd);

 for_each_cpu(cpu, pd->cpumask.cbcpu) {
  squeue = per_cpu_ptr(pd->squeue, cpu);
  flush_work(&squeue->work);
 }

 BUG_ON(atomic_read(&pd->refcnt) != 0);
}

static void __padata_start(struct padata_instance *pinst)
{
 pinst->flags |= PADATA_INIT;
}

static void __padata_stop(struct padata_instance *pinst)
{
 if (!(pinst->flags & PADATA_INIT))
  return;

 pinst->flags &= ~PADATA_INIT;

 synchronize_rcu();

 get_online_cpus();
 padata_flush_queues(pinst->pd);
 put_online_cpus();
}


static void padata_replace(struct padata_instance *pinst,
      struct parallel_data *pd_new)
{
 struct parallel_data *pd_old = pinst->pd;
 int notification_mask = 0;

 pinst->flags |= PADATA_RESET;

 rcu_assign_pointer(pinst->pd, pd_new);

 synchronize_rcu();

 if (!cpumask_equal(pd_old->cpumask.pcpu, pd_new->cpumask.pcpu))
  notification_mask |= PADATA_CPU_PARALLEL;
 if (!cpumask_equal(pd_old->cpumask.cbcpu, pd_new->cpumask.cbcpu))
  notification_mask |= PADATA_CPU_SERIAL;

 padata_flush_queues(pd_old);
 padata_free_pd(pd_old);

 if (notification_mask)
  blocking_notifier_call_chain(&pinst->cpumask_change_notifier,
          notification_mask,
          &pd_new->cpumask);

 pinst->flags &= ~PADATA_RESET;
}
int padata_register_cpumask_notifier(struct padata_instance *pinst,
         struct notifier_block *nblock)
{
 return blocking_notifier_chain_register(&pinst->cpumask_change_notifier,
      nblock);
}
EXPORT_SYMBOL(padata_register_cpumask_notifier);
int padata_unregister_cpumask_notifier(struct padata_instance *pinst,
           struct notifier_block *nblock)
{
 return blocking_notifier_chain_unregister(
  &pinst->cpumask_change_notifier,
  nblock);
}
EXPORT_SYMBOL(padata_unregister_cpumask_notifier);



static bool padata_validate_cpumask(struct padata_instance *pinst,
        const struct cpumask *cpumask)
{
 if (!cpumask_intersects(cpumask, cpu_online_mask)) {
  pinst->flags |= PADATA_INVALID;
  return false;
 }

 pinst->flags &= ~PADATA_INVALID;
 return true;
}

static int __padata_set_cpumasks(struct padata_instance *pinst,
     cpumask_var_t pcpumask,
     cpumask_var_t cbcpumask)
{
 int valid;
 struct parallel_data *pd;

 valid = padata_validate_cpumask(pinst, pcpumask);
 if (!valid) {
  __padata_stop(pinst);
  goto out_replace;
 }

 valid = padata_validate_cpumask(pinst, cbcpumask);
 if (!valid)
  __padata_stop(pinst);

out_replace:
 pd = padata_alloc_pd(pinst, pcpumask, cbcpumask);
 if (!pd)
  return -ENOMEM;

 cpumask_copy(pinst->cpumask.pcpu, pcpumask);
 cpumask_copy(pinst->cpumask.cbcpu, cbcpumask);

 padata_replace(pinst, pd);

 if (valid)
  __padata_start(pinst);

 return 0;
}
int padata_set_cpumask(struct padata_instance *pinst, int cpumask_type,
         cpumask_var_t cpumask)
{
 struct cpumask *serial_mask, *parallel_mask;
 int err = -EINVAL;

 mutex_lock(&pinst->lock);
 get_online_cpus();

 switch (cpumask_type) {
 case PADATA_CPU_PARALLEL:
  serial_mask = pinst->cpumask.cbcpu;
  parallel_mask = cpumask;
  break;
 case PADATA_CPU_SERIAL:
  parallel_mask = pinst->cpumask.pcpu;
  serial_mask = cpumask;
  break;
 default:
   goto out;
 }

 err = __padata_set_cpumasks(pinst, parallel_mask, serial_mask);

out:
 put_online_cpus();
 mutex_unlock(&pinst->lock);

 return err;
}
EXPORT_SYMBOL(padata_set_cpumask);






int padata_start(struct padata_instance *pinst)
{
 int err = 0;

 mutex_lock(&pinst->lock);

 if (pinst->flags & PADATA_INVALID)
  err = -EINVAL;

  __padata_start(pinst);

 mutex_unlock(&pinst->lock);

 return err;
}
EXPORT_SYMBOL(padata_start);






void padata_stop(struct padata_instance *pinst)
{
 mutex_lock(&pinst->lock);
 __padata_stop(pinst);
 mutex_unlock(&pinst->lock);
}
EXPORT_SYMBOL(padata_stop);


static int __padata_add_cpu(struct padata_instance *pinst, int cpu)
{
 struct parallel_data *pd;

 if (cpumask_test_cpu(cpu, cpu_online_mask)) {
  pd = padata_alloc_pd(pinst, pinst->cpumask.pcpu,
         pinst->cpumask.cbcpu);
  if (!pd)
   return -ENOMEM;

  padata_replace(pinst, pd);

  if (padata_validate_cpumask(pinst, pinst->cpumask.pcpu) &&
      padata_validate_cpumask(pinst, pinst->cpumask.cbcpu))
   __padata_start(pinst);
 }

 return 0;
}

static int __padata_remove_cpu(struct padata_instance *pinst, int cpu)
{
 struct parallel_data *pd = NULL;

 if (cpumask_test_cpu(cpu, cpu_online_mask)) {

  if (!padata_validate_cpumask(pinst, pinst->cpumask.pcpu) ||
      !padata_validate_cpumask(pinst, pinst->cpumask.cbcpu))
   __padata_stop(pinst);

  pd = padata_alloc_pd(pinst, pinst->cpumask.pcpu,
         pinst->cpumask.cbcpu);
  if (!pd)
   return -ENOMEM;

  padata_replace(pinst, pd);

  cpumask_clear_cpu(cpu, pd->cpumask.cbcpu);
  cpumask_clear_cpu(cpu, pd->cpumask.pcpu);
 }

 return 0;
}
int padata_remove_cpu(struct padata_instance *pinst, int cpu, int mask)
{
 int err;

 if (!(mask & (PADATA_CPU_SERIAL | PADATA_CPU_PARALLEL)))
  return -EINVAL;

 mutex_lock(&pinst->lock);

 get_online_cpus();
 if (mask & PADATA_CPU_SERIAL)
  cpumask_clear_cpu(cpu, pinst->cpumask.cbcpu);
 if (mask & PADATA_CPU_PARALLEL)
  cpumask_clear_cpu(cpu, pinst->cpumask.pcpu);

 err = __padata_remove_cpu(pinst, cpu);
 put_online_cpus();

 mutex_unlock(&pinst->lock);

 return err;
}
EXPORT_SYMBOL(padata_remove_cpu);

static inline int pinst_has_cpu(struct padata_instance *pinst, int cpu)
{
 return cpumask_test_cpu(cpu, pinst->cpumask.pcpu) ||
  cpumask_test_cpu(cpu, pinst->cpumask.cbcpu);
}


static int padata_cpu_callback(struct notifier_block *nfb,
          unsigned long action, void *hcpu)
{
 int err;
 struct padata_instance *pinst;
 int cpu = (unsigned long)hcpu;

 pinst = container_of(nfb, struct padata_instance, cpu_notifier);

 switch (action) {
 case CPU_ONLINE:
 case CPU_ONLINE_FROZEN:
 case CPU_DOWN_FAILED:
 case CPU_DOWN_FAILED_FROZEN:
  if (!pinst_has_cpu(pinst, cpu))
   break;
  mutex_lock(&pinst->lock);
  err = __padata_add_cpu(pinst, cpu);
  mutex_unlock(&pinst->lock);
  if (err)
   return notifier_from_errno(err);
  break;

 case CPU_DOWN_PREPARE:
 case CPU_DOWN_PREPARE_FROZEN:
 case CPU_UP_CANCELED:
 case CPU_UP_CANCELED_FROZEN:
  if (!pinst_has_cpu(pinst, cpu))
   break;
  mutex_lock(&pinst->lock);
  err = __padata_remove_cpu(pinst, cpu);
  mutex_unlock(&pinst->lock);
  if (err)
   return notifier_from_errno(err);
  break;
 }

 return NOTIFY_OK;
}

static void __padata_free(struct padata_instance *pinst)
{
 unregister_hotcpu_notifier(&pinst->cpu_notifier);

 padata_stop(pinst);
 padata_free_pd(pinst->pd);
 free_cpumask_var(pinst->cpumask.pcpu);
 free_cpumask_var(pinst->cpumask.cbcpu);
 kfree(pinst);
}

 container_of(_kobj, struct padata_instance, kobj)
 container_of(_attr, struct padata_sysfs_entry, attr)

static void padata_sysfs_release(struct kobject *kobj)
{
 struct padata_instance *pinst = kobj2pinst(kobj);
 __padata_free(pinst);
}

struct padata_sysfs_entry {
 struct attribute attr;
 ssize_t (*show)(struct padata_instance *, struct attribute *, char *);
 ssize_t (*store)(struct padata_instance *, struct attribute *,
    const char *, size_t);
};

static ssize_t show_cpumask(struct padata_instance *pinst,
       struct attribute *attr, char *buf)
{
 struct cpumask *cpumask;
 ssize_t len;

 mutex_lock(&pinst->lock);
 if (!strcmp(attr->name, "serial_cpumask"))
  cpumask = pinst->cpumask.cbcpu;
 else
  cpumask = pinst->cpumask.pcpu;

 len = snprintf(buf, PAGE_SIZE, "%*pb\n",
         nr_cpu_ids, cpumask_bits(cpumask));
 mutex_unlock(&pinst->lock);
 return len < PAGE_SIZE ? len : -EINVAL;
}

static ssize_t store_cpumask(struct padata_instance *pinst,
        struct attribute *attr,
        const char *buf, size_t count)
{
 cpumask_var_t new_cpumask;
 ssize_t ret;
 int mask_type;

 if (!alloc_cpumask_var(&new_cpumask, GFP_KERNEL))
  return -ENOMEM;

 ret = bitmap_parse(buf, count, cpumask_bits(new_cpumask),
      nr_cpumask_bits);
 if (ret < 0)
  goto out;

 mask_type = !strcmp(attr->name, "serial_cpumask") ?
  PADATA_CPU_SERIAL : PADATA_CPU_PARALLEL;
 ret = padata_set_cpumask(pinst, mask_type, new_cpumask);
 if (!ret)
  ret = count;

out:
 free_cpumask_var(new_cpumask);
 return ret;
}

 static struct padata_sysfs_entry _name##_attr = \
  __ATTR(_name, 0644, _show_name, _store_name)
 static struct padata_sysfs_entry _name##_attr = \
  __ATTR(_name, 0400, _show_name, NULL)

PADATA_ATTR_RW(serial_cpumask, show_cpumask, store_cpumask);
PADATA_ATTR_RW(parallel_cpumask, show_cpumask, store_cpumask);






static struct attribute *padata_default_attrs[] = {
 &serial_cpumask_attr.attr,
 &parallel_cpumask_attr.attr,
 NULL,
};

static ssize_t padata_sysfs_show(struct kobject *kobj,
     struct attribute *attr, char *buf)
{
 struct padata_instance *pinst;
 struct padata_sysfs_entry *pentry;
 ssize_t ret = -EIO;

 pinst = kobj2pinst(kobj);
 pentry = attr2pentry(attr);
 if (pentry->show)
  ret = pentry->show(pinst, attr, buf);

 return ret;
}

static ssize_t padata_sysfs_store(struct kobject *kobj, struct attribute *attr,
      const char *buf, size_t count)
{
 struct padata_instance *pinst;
 struct padata_sysfs_entry *pentry;
 ssize_t ret = -EIO;

 pinst = kobj2pinst(kobj);
 pentry = attr2pentry(attr);
 if (pentry->show)
  ret = pentry->store(pinst, attr, buf, count);

 return ret;
}

static const struct sysfs_ops padata_sysfs_ops = {
 .show = padata_sysfs_show,
 .store = padata_sysfs_store,
};

static struct kobj_type padata_attr_type = {
 .sysfs_ops = &padata_sysfs_ops,
 .default_attrs = padata_default_attrs,
 .release = padata_sysfs_release,
};
struct padata_instance *padata_alloc_possible(struct workqueue_struct *wq)
{
 return padata_alloc(wq, cpu_possible_mask, cpu_possible_mask);
}
EXPORT_SYMBOL(padata_alloc_possible);
struct padata_instance *padata_alloc(struct workqueue_struct *wq,
         const struct cpumask *pcpumask,
         const struct cpumask *cbcpumask)
{
 struct padata_instance *pinst;
 struct parallel_data *pd = NULL;

 pinst = kzalloc(sizeof(struct padata_instance), GFP_KERNEL);
 if (!pinst)
  goto err;

 get_online_cpus();
 if (!alloc_cpumask_var(&pinst->cpumask.pcpu, GFP_KERNEL))
  goto err_free_inst;
 if (!alloc_cpumask_var(&pinst->cpumask.cbcpu, GFP_KERNEL)) {
  free_cpumask_var(pinst->cpumask.pcpu);
  goto err_free_inst;
 }
 if (!padata_validate_cpumask(pinst, pcpumask) ||
     !padata_validate_cpumask(pinst, cbcpumask))
  goto err_free_masks;

 pd = padata_alloc_pd(pinst, pcpumask, cbcpumask);
 if (!pd)
  goto err_free_masks;

 rcu_assign_pointer(pinst->pd, pd);

 pinst->wq = wq;

 cpumask_copy(pinst->cpumask.pcpu, pcpumask);
 cpumask_copy(pinst->cpumask.cbcpu, cbcpumask);

 pinst->flags = 0;

 put_online_cpus();

 BLOCKING_INIT_NOTIFIER_HEAD(&pinst->cpumask_change_notifier);
 kobject_init(&pinst->kobj, &padata_attr_type);
 mutex_init(&pinst->lock);

 pinst->cpu_notifier.notifier_call = padata_cpu_callback;
 pinst->cpu_notifier.priority = 0;
 register_hotcpu_notifier(&pinst->cpu_notifier);

 return pinst;

err_free_masks:
 free_cpumask_var(pinst->cpumask.pcpu);
 free_cpumask_var(pinst->cpumask.cbcpu);
err_free_inst:
 kfree(pinst);
 put_online_cpus();
err:
 return NULL;
}






void padata_free(struct padata_instance *pinst)
{
 kobject_put(&pinst->kobj);
}
EXPORT_SYMBOL(padata_free);


int panic_on_oops = CONFIG_PANIC_ON_OOPS_VALUE;
static unsigned long tainted_mask;
static int pause_on_oops;
static int pause_on_oops_flag;
static DEFINE_SPINLOCK(pause_on_oops_lock);
bool crash_kexec_post_notifiers;
int panic_on_warn __read_mostly;

int panic_timeout = CONFIG_PANIC_TIMEOUT;
EXPORT_SYMBOL_GPL(panic_timeout);

ATOMIC_NOTIFIER_HEAD(panic_notifier_list);

EXPORT_SYMBOL(panic_notifier_list);

static long no_blink(int state)
{
 return 0;
}


long (*panic_blink)(int state);
EXPORT_SYMBOL(panic_blink);




void __weak panic_smp_self_stop(void)
{
 while (1)
  cpu_relax();
}





void __weak nmi_panic_self_stop(struct pt_regs *regs)
{
 panic_smp_self_stop();
}

atomic_t panic_cpu = ATOMIC_INIT(PANIC_CPU_INVALID);







void nmi_panic(struct pt_regs *regs, const char *msg)
{
 int old_cpu, cpu;

 cpu = raw_smp_processor_id();
 old_cpu = atomic_cmpxchg(&panic_cpu, PANIC_CPU_INVALID, cpu);

 if (old_cpu == PANIC_CPU_INVALID)
  panic("%s", msg);
 else if (old_cpu != cpu)
  nmi_panic_self_stop(regs);
}
EXPORT_SYMBOL(nmi_panic);
void panic(const char *fmt, ...)
{
 static char buf[1024];
 va_list args;
 long i, i_next = 0;
 int state = 0;
 int old_cpu, this_cpu;







 local_irq_disable();
 this_cpu = raw_smp_processor_id();
 old_cpu = atomic_cmpxchg(&panic_cpu, PANIC_CPU_INVALID, this_cpu);

 if (old_cpu != PANIC_CPU_INVALID && old_cpu != this_cpu)
  panic_smp_self_stop();

 console_verbose();
 bust_spinlocks(1);
 va_start(args, fmt);
 vsnprintf(buf, sizeof(buf), fmt, args);
 va_end(args);
 pr_emerg("Kernel panic - not syncing: %s\n", buf);



 if (!test_taint(TAINT_DIE) && oops_in_progress <= 1)
  dump_stack();
 if (!crash_kexec_post_notifiers) {
  printk_nmi_flush_on_panic();
  __crash_kexec(NULL);
 }






 smp_send_stop();





 atomic_notifier_call_chain(&panic_notifier_list, 0, buf);


 printk_nmi_flush_on_panic();
 kmsg_dump(KMSG_DUMP_PANIC);
 if (crash_kexec_post_notifiers)
  __crash_kexec(NULL);

 bust_spinlocks(0);
 debug_locks_off();
 console_flush_on_panic();

 if (!panic_blink)
  panic_blink = no_blink;

 if (panic_timeout > 0) {




  pr_emerg("Rebooting in %d seconds..", panic_timeout);

  for (i = 0; i < panic_timeout * 1000; i += PANIC_TIMER_STEP) {
   touch_nmi_watchdog();
   if (i >= i_next) {
    i += panic_blink(state ^= 1);
    i_next = i + 3600 / PANIC_BLINK_SPD;
   }
   mdelay(PANIC_TIMER_STEP);
  }
 }
 if (panic_timeout != 0) {





  emergency_restart();
 }
 {
  extern int stop_a_enabled;

  stop_a_enabled = 1;
  pr_emerg("Press Stop-A (L1-A) to return to the boot prom\n");
 }
 {
  unsigned long caller;

  caller = (unsigned long)__builtin_return_address(0);
  disabled_wait(caller);
 }
 pr_emerg("---[ end Kernel panic - not syncing: %s\n", buf);
 local_irq_enable();
 for (i = 0; ; i += PANIC_TIMER_STEP) {
  touch_softlockup_watchdog();
  if (i >= i_next) {
   i += panic_blink(state ^= 1);
   i_next = i + 3600 / PANIC_BLINK_SPD;
  }
  mdelay(PANIC_TIMER_STEP);
 }
}

EXPORT_SYMBOL(panic);


struct tnt {
 u8 bit;
 char true;
 char false;
};

static const struct tnt tnts[] = {
 { TAINT_PROPRIETARY_MODULE, 'P', 'G' },
 { TAINT_FORCED_MODULE, 'F', ' ' },
 { TAINT_CPU_OUT_OF_SPEC, 'S', ' ' },
 { TAINT_FORCED_RMMOD, 'R', ' ' },
 { TAINT_MACHINE_CHECK, 'M', ' ' },
 { TAINT_BAD_PAGE, 'B', ' ' },
 { TAINT_USER, 'U', ' ' },
 { TAINT_DIE, 'D', ' ' },
 { TAINT_OVERRIDDEN_ACPI_TABLE, 'A', ' ' },
 { TAINT_WARN, 'W', ' ' },
 { TAINT_CRAP, 'C', ' ' },
 { TAINT_FIRMWARE_WORKAROUND, 'I', ' ' },
 { TAINT_OOT_MODULE, 'O', ' ' },
 { TAINT_UNSIGNED_MODULE, 'E', ' ' },
 { TAINT_SOFTLOCKUP, 'L', ' ' },
 { TAINT_LIVEPATCH, 'K', ' ' },
};
const char *print_tainted(void)
{
 static char buf[ARRAY_SIZE(tnts) + sizeof("Tainted: ")];

 if (tainted_mask) {
  char *s;
  int i;

  s = buf + sprintf(buf, "Tainted: ");
  for (i = 0; i < ARRAY_SIZE(tnts); i++) {
   const struct tnt *t = &tnts[i];
   *s++ = test_bit(t->bit, &tainted_mask) ?
     t->true : t->false;
  }
  *s = 0;
 } else
  snprintf(buf, sizeof(buf), "Not tainted");

 return buf;
}

int test_taint(unsigned flag)
{
 return test_bit(flag, &tainted_mask);
}
EXPORT_SYMBOL(test_taint);

unsigned long get_taint(void)
{
 return tainted_mask;
}
void add_taint(unsigned flag, enum lockdep_ok lockdep_ok)
{
 if (lockdep_ok == LOCKDEP_NOW_UNRELIABLE && __debug_locks_off())
  pr_warn("Disabling lock debugging due to kernel taint\n");

 set_bit(flag, &tainted_mask);
}
EXPORT_SYMBOL(add_taint);

static void spin_msec(int msecs)
{
 int i;

 for (i = 0; i < msecs; i++) {
  touch_nmi_watchdog();
  mdelay(1);
 }
}





static void do_oops_enter_exit(void)
{
 unsigned long flags;
 static int spin_counter;

 if (!pause_on_oops)
  return;

 spin_lock_irqsave(&pause_on_oops_lock, flags);
 if (pause_on_oops_flag == 0) {

  pause_on_oops_flag = 1;
 } else {

  if (!spin_counter) {

   spin_counter = pause_on_oops;
   do {
    spin_unlock(&pause_on_oops_lock);
    spin_msec(MSEC_PER_SEC);
    spin_lock(&pause_on_oops_lock);
   } while (--spin_counter);
   pause_on_oops_flag = 0;
  } else {

   while (spin_counter) {
    spin_unlock(&pause_on_oops_lock);
    spin_msec(1);
    spin_lock(&pause_on_oops_lock);
   }
  }
 }
 spin_unlock_irqrestore(&pause_on_oops_lock, flags);
}





int oops_may_print(void)
{
 return pause_on_oops_flag == 0;
}
void oops_enter(void)
{
 tracing_off();

 debug_locks_off();
 do_oops_enter_exit();
}




static u64 oops_id;

static int init_oops_id(void)
{
 if (!oops_id)
  get_random_bytes(&oops_id, sizeof(oops_id));
 else
  oops_id++;

 return 0;
}
late_initcall(init_oops_id);

void print_oops_end_marker(void)
{
 init_oops_id();
 pr_warn("---[ end trace %016llx ]---\n", (unsigned long long)oops_id);
}





void oops_exit(void)
{
 do_oops_enter_exit();
 print_oops_end_marker();
 kmsg_dump(KMSG_DUMP_OOPS);
}

struct warn_args {
 const char *fmt;
 va_list args;
};

void __warn(const char *file, int line, void *caller, unsigned taint,
     struct pt_regs *regs, struct warn_args *args)
{
 disable_trace_on_warning();

 pr_warn("------------[ cut here ]------------\n");

 if (file)
  pr_warn("WARNING: CPU: %d PID: %d at %s:%d %pS\n",
   raw_smp_processor_id(), current->pid, file, line,
   caller);
 else
  pr_warn("WARNING: CPU: %d PID: %d at %pS\n",
   raw_smp_processor_id(), current->pid, caller);

 if (args)
  vprintk(args->fmt, args->args);

 if (panic_on_warn) {






  panic_on_warn = 0;
  panic("panic_on_warn set ...\n");
 }

 print_modules();

 if (regs)
  show_regs(regs);
 else
  dump_stack();

 print_oops_end_marker();


 add_taint(taint, LOCKDEP_STILL_OK);
}

void warn_slowpath_fmt(const char *file, int line, const char *fmt, ...)
{
 struct warn_args args;

 args.fmt = fmt;
 va_start(args.args, fmt);
 __warn(file, line, __builtin_return_address(0), TAINT_WARN, NULL,
        &args);
 va_end(args.args);
}
EXPORT_SYMBOL(warn_slowpath_fmt);

void warn_slowpath_fmt_taint(const char *file, int line,
        unsigned taint, const char *fmt, ...)
{
 struct warn_args args;

 args.fmt = fmt;
 va_start(args.args, fmt);
 __warn(file, line, __builtin_return_address(0), taint, NULL, &args);
 va_end(args.args);
}
EXPORT_SYMBOL(warn_slowpath_fmt_taint);

void warn_slowpath_null(const char *file, int line)
{
 __warn(file, line, __builtin_return_address(0), TAINT_WARN, NULL, NULL);
}
EXPORT_SYMBOL(warn_slowpath_null);






__visible void __stack_chk_fail(void)
{
 panic("stack-protector: Kernel stack is corrupted in: %p\n",
  __builtin_return_address(0));
}
EXPORT_SYMBOL(__stack_chk_fail);


core_param(panic, panic_timeout, int, 0644);
core_param(pause_on_oops, pause_on_oops, int, 0644);
core_param(panic_on_warn, panic_on_warn, int, 0644);

static int __init setup_crash_kexec_post_notifiers(char *s)
{
 crash_kexec_post_notifiers = true;
 return 0;
}
early_param("crash_kexec_post_notifiers", setup_crash_kexec_post_notifiers);

static int __init oops_setup(char *s)
{
 if (!s)
  return -EINVAL;
 if (!strcmp(s, "panic"))
  panic_on_oops = 1;
 return 0;
}
early_param("oops", oops_setup);


static DEFINE_MUTEX(param_lock);



static inline void check_kparam_locked(struct module *mod)
{
 BUG_ON(!mutex_is_locked(KPARAM_MUTEX(mod)));
}
static inline void check_kparam_locked(struct module *mod)
{
}


struct kmalloced_param {
 struct list_head list;
 char val[];
};
static LIST_HEAD(kmalloced_params);
static DEFINE_SPINLOCK(kmalloced_params_lock);

static void *kmalloc_parameter(unsigned int size)
{
 struct kmalloced_param *p;

 p = kmalloc(sizeof(*p) + size, GFP_KERNEL);
 if (!p)
  return NULL;

 spin_lock(&kmalloced_params_lock);
 list_add(&p->list, &kmalloced_params);
 spin_unlock(&kmalloced_params_lock);

 return p->val;
}


static void maybe_kfree_parameter(void *param)
{
 struct kmalloced_param *p;

 spin_lock(&kmalloced_params_lock);
 list_for_each_entry(p, &kmalloced_params, list) {
  if (p->val == param) {
   list_del(&p->list);
   kfree(p);
   break;
  }
 }
 spin_unlock(&kmalloced_params_lock);
}

static char dash2underscore(char c)
{
 if (c == '-')
  return '_';
 return c;
}

bool parameqn(const char *a, const char *b, size_t n)
{
 size_t i;

 for (i = 0; i < n; i++) {
  if (dash2underscore(a[i]) != dash2underscore(b[i]))
   return false;
 }
 return true;
}

bool parameq(const char *a, const char *b)
{
 return parameqn(a, b, strlen(a)+1);
}

static void param_check_unsafe(const struct kernel_param *kp)
{
 if (kp->flags & KERNEL_PARAM_FL_UNSAFE) {
  pr_warn("Setting dangerous option %s - tainting kernel\n",
   kp->name);
  add_taint(TAINT_USER, LOCKDEP_STILL_OK);
 }
}

static int parse_one(char *param,
       char *val,
       const char *doing,
       const struct kernel_param *params,
       unsigned num_params,
       s16 min_level,
       s16 max_level,
       void *arg,
       int (*handle_unknown)(char *param, char *val,
         const char *doing, void *arg))
{
 unsigned int i;
 int err;


 for (i = 0; i < num_params; i++) {
  if (parameq(param, params[i].name)) {
   if (params[i].level < min_level
       || params[i].level > max_level)
    return 0;

   if (!val &&
       !(params[i].ops->flags & KERNEL_PARAM_OPS_FL_NOARG))
    return -EINVAL;
   pr_debug("handling %s with %p\n", param,
    params[i].ops->set);
   kernel_param_lock(params[i].mod);
   param_check_unsafe(&params[i]);
   err = params[i].ops->set(val, &params[i]);
   kernel_param_unlock(params[i].mod);
   return err;
  }
 }

 if (handle_unknown) {
  pr_debug("doing %s: %s='%s'\n", doing, param, val);
  return handle_unknown(param, val, doing, arg);
 }

 pr_debug("Unknown argument '%s'\n", param);
 return -ENOENT;
}



static char *next_arg(char *args, char **param, char **val)
{
 unsigned int i, equals = 0;
 int in_quote = 0, quoted = 0;
 char *next;

 if (*args == '"') {
  args++;
  in_quote = 1;
  quoted = 1;
 }

 for (i = 0; args[i]; i++) {
  if (isspace(args[i]) && !in_quote)
   break;
  if (equals == 0) {
   if (args[i] == '=')
    equals = i;
  }
  if (args[i] == '"')
   in_quote = !in_quote;
 }

 *param = args;
 if (!equals)
  *val = NULL;
 else {
  args[equals] = '\0';
  *val = args + equals + 1;


  if (**val == '"') {
   (*val)++;
   if (args[i-1] == '"')
    args[i-1] = '\0';
  }
 }
 if (quoted && args[i-1] == '"')
  args[i-1] = '\0';

 if (args[i]) {
  args[i] = '\0';
  next = args + i + 1;
 } else
  next = args + i;


 return skip_spaces(next);
}


char *parse_args(const char *doing,
   char *args,
   const struct kernel_param *params,
   unsigned num,
   s16 min_level,
   s16 max_level,
   void *arg,
   int (*unknown)(char *param, char *val,
    const char *doing, void *arg))
{
 char *param, *val, *err = NULL;


 args = skip_spaces(args);

 if (*args)
  pr_debug("doing %s, parsing ARGS: '%s'\n", doing, args);

 while (*args) {
  int ret;
  int irq_was_disabled;

  args = next_arg(args, &param, &val);

  if (!val && strcmp(param, "--") == 0)
   return err ?: args;
  irq_was_disabled = irqs_disabled();
  ret = parse_one(param, val, doing, params, num,
    min_level, max_level, arg, unknown);
  if (irq_was_disabled && !irqs_disabled())
   pr_warn("%s: option '%s' enabled irq's!\n",
    doing, param);

  switch (ret) {
  case 0:
   continue;
  case -ENOENT:
   pr_err("%s: Unknown parameter `%s'\n", doing, param);
   break;
  case -ENOSPC:
   pr_err("%s: `%s' too large for parameter `%s'\n",
          doing, val ?: "", param);
   break;
  default:
   pr_err("%s: `%s' invalid for parameter `%s'\n",
          doing, val ?: "", param);
   break;
  }

  err = ERR_PTR(ret);
 }

 return err;
}


 int param_set_##name(const char *val, const struct kernel_param *kp) \
 { \
  return strtolfn(val, 0, (type *)kp->arg); \
 } \
 int param_get_##name(char *buffer, const struct kernel_param *kp) \
 { \
  return scnprintf(buffer, PAGE_SIZE, format, \
    *((type *)kp->arg)); \
 } \
 const struct kernel_param_ops param_ops_##name = { \
  .set = param_set_##name, \
  .get = param_get_##name, \
 }; \
 EXPORT_SYMBOL(param_set_##name); \
 EXPORT_SYMBOL(param_get_##name); \
 EXPORT_SYMBOL(param_ops_##name)


STANDARD_PARAM_DEF(byte, unsigned char, "%hhu", kstrtou8);
STANDARD_PARAM_DEF(short, short, "%hi", kstrtos16);
STANDARD_PARAM_DEF(ushort, unsigned short, "%hu", kstrtou16);
STANDARD_PARAM_DEF(int, int, "%i", kstrtoint);
STANDARD_PARAM_DEF(uint, unsigned int, "%u", kstrtouint);
STANDARD_PARAM_DEF(long, long, "%li", kstrtol);
STANDARD_PARAM_DEF(ulong, unsigned long, "%lu", kstrtoul);
STANDARD_PARAM_DEF(ullong, unsigned long long, "%llu", kstrtoull);

int param_set_charp(const char *val, const struct kernel_param *kp)
{
 if (strlen(val) > 1024) {
  pr_err("%s: string parameter too long\n", kp->name);
  return -ENOSPC;
 }

 maybe_kfree_parameter(*(char **)kp->arg);



 if (slab_is_available()) {
  *(char **)kp->arg = kmalloc_parameter(strlen(val)+1);
  if (!*(char **)kp->arg)
   return -ENOMEM;
  strcpy(*(char **)kp->arg, val);
 } else
  *(const char **)kp->arg = val;

 return 0;
}
EXPORT_SYMBOL(param_set_charp);

int param_get_charp(char *buffer, const struct kernel_param *kp)
{
 return scnprintf(buffer, PAGE_SIZE, "%s", *((char **)kp->arg));
}
EXPORT_SYMBOL(param_get_charp);

void param_free_charp(void *arg)
{
 maybe_kfree_parameter(*((char **)arg));
}
EXPORT_SYMBOL(param_free_charp);

const struct kernel_param_ops param_ops_charp = {
 .set = param_set_charp,
 .get = param_get_charp,
 .free = param_free_charp,
};
EXPORT_SYMBOL(param_ops_charp);


int param_set_bool(const char *val, const struct kernel_param *kp)
{

 if (!val) val = "1";


 return strtobool(val, kp->arg);
}
EXPORT_SYMBOL(param_set_bool);

int param_get_bool(char *buffer, const struct kernel_param *kp)
{

 return sprintf(buffer, "%c", *(bool *)kp->arg ? 'Y' : 'N');
}
EXPORT_SYMBOL(param_get_bool);

const struct kernel_param_ops param_ops_bool = {
 .flags = KERNEL_PARAM_OPS_FL_NOARG,
 .set = param_set_bool,
 .get = param_get_bool,
};
EXPORT_SYMBOL(param_ops_bool);

int param_set_bool_enable_only(const char *val, const struct kernel_param *kp)
{
 int err = 0;
 bool new_value;
 bool orig_value = *(bool *)kp->arg;
 struct kernel_param dummy_kp = *kp;

 dummy_kp.arg = &new_value;

 err = param_set_bool(val, &dummy_kp);
 if (err)
  return err;


 if (!new_value && orig_value)
  return -EROFS;

 if (new_value)
  err = param_set_bool(val, kp);

 return err;
}
EXPORT_SYMBOL_GPL(param_set_bool_enable_only);

const struct kernel_param_ops param_ops_bool_enable_only = {
 .flags = KERNEL_PARAM_OPS_FL_NOARG,
 .set = param_set_bool_enable_only,
 .get = param_get_bool,
};
EXPORT_SYMBOL_GPL(param_ops_bool_enable_only);


int param_set_invbool(const char *val, const struct kernel_param *kp)
{
 int ret;
 bool boolval;
 struct kernel_param dummy;

 dummy.arg = &boolval;
 ret = param_set_bool(val, &dummy);
 if (ret == 0)
  *(bool *)kp->arg = !boolval;
 return ret;
}
EXPORT_SYMBOL(param_set_invbool);

int param_get_invbool(char *buffer, const struct kernel_param *kp)
{
 return sprintf(buffer, "%c", (*(bool *)kp->arg) ? 'N' : 'Y');
}
EXPORT_SYMBOL(param_get_invbool);

const struct kernel_param_ops param_ops_invbool = {
 .set = param_set_invbool,
 .get = param_get_invbool,
};
EXPORT_SYMBOL(param_ops_invbool);

int param_set_bint(const char *val, const struct kernel_param *kp)
{

 struct kernel_param boolkp = *kp;
 bool v;
 int ret;

 boolkp.arg = &v;

 ret = param_set_bool(val, &boolkp);
 if (ret == 0)
  *(int *)kp->arg = v;
 return ret;
}
EXPORT_SYMBOL(param_set_bint);

const struct kernel_param_ops param_ops_bint = {
 .flags = KERNEL_PARAM_OPS_FL_NOARG,
 .set = param_set_bint,
 .get = param_get_int,
};
EXPORT_SYMBOL(param_ops_bint);


static int param_array(struct module *mod,
         const char *name,
         const char *val,
         unsigned int min, unsigned int max,
         void *elem, int elemsize,
         int (*set)(const char *, const struct kernel_param *kp),
         s16 level,
         unsigned int *num)
{
 int ret;
 struct kernel_param kp;
 char save;


 kp.name = name;
 kp.arg = elem;
 kp.level = level;

 *num = 0;

 do {
  int len;

  if (*num == max) {
   pr_err("%s: can only take %i arguments\n", name, max);
   return -EINVAL;
  }
  len = strcspn(val, ",");


  save = val[len];
  ((char *)val)[len] = '\0';
  check_kparam_locked(mod);
  ret = set(val, &kp);

  if (ret != 0)
   return ret;
  kp.arg += elemsize;
  val += len+1;
  (*num)++;
 } while (save == ',');

 if (*num < min) {
  pr_err("%s: needs at least %i arguments\n", name, min);
  return -EINVAL;
 }
 return 0;
}

static int param_array_set(const char *val, const struct kernel_param *kp)
{
 const struct kparam_array *arr = kp->arr;
 unsigned int temp_num;

 return param_array(kp->mod, kp->name, val, 1, arr->max, arr->elem,
      arr->elemsize, arr->ops->set, kp->level,
      arr->num ?: &temp_num);
}

static int param_array_get(char *buffer, const struct kernel_param *kp)
{
 int i, off, ret;
 const struct kparam_array *arr = kp->arr;
 struct kernel_param p = *kp;

 for (i = off = 0; i < (arr->num ? *arr->num : arr->max); i++) {
  if (i)
   buffer[off++] = ',';
  p.arg = arr->elem + arr->elemsize * i;
  check_kparam_locked(p.mod);
  ret = arr->ops->get(buffer + off, &p);
  if (ret < 0)
   return ret;
  off += ret;
 }
 buffer[off] = '\0';
 return off;
}

static void param_array_free(void *arg)
{
 unsigned int i;
 const struct kparam_array *arr = arg;

 if (arr->ops->free)
  for (i = 0; i < (arr->num ? *arr->num : arr->max); i++)
   arr->ops->free(arr->elem + arr->elemsize * i);
}

const struct kernel_param_ops param_array_ops = {
 .set = param_array_set,
 .get = param_array_get,
 .free = param_array_free,
};
EXPORT_SYMBOL(param_array_ops);

int param_set_copystring(const char *val, const struct kernel_param *kp)
{
 const struct kparam_string *kps = kp->str;

 if (strlen(val)+1 > kps->maxlen) {
  pr_err("%s: string doesn't fit in %u chars.\n",
         kp->name, kps->maxlen-1);
  return -ENOSPC;
 }
 strcpy(kps->string, val);
 return 0;
}
EXPORT_SYMBOL(param_set_copystring);

int param_get_string(char *buffer, const struct kernel_param *kp)
{
 const struct kparam_string *kps = kp->str;
 return strlcpy(buffer, kps->string, kps->maxlen);
}
EXPORT_SYMBOL(param_get_string);

const struct kernel_param_ops param_ops_string = {
 .set = param_set_copystring,
 .get = param_get_string,
};
EXPORT_SYMBOL(param_ops_string);



struct param_attribute
{
 struct module_attribute mattr;
 const struct kernel_param *param;
};

struct module_param_attrs
{
 unsigned int num;
 struct attribute_group grp;
 struct param_attribute attrs[0];
};


static ssize_t param_attr_show(struct module_attribute *mattr,
          struct module_kobject *mk, char *buf)
{
 int count;
 struct param_attribute *attribute = to_param_attr(mattr);

 if (!attribute->param->ops->get)
  return -EPERM;

 kernel_param_lock(mk->mod);
 count = attribute->param->ops->get(buf, attribute->param);
 kernel_param_unlock(mk->mod);
 if (count > 0) {
  strcat(buf, "\n");
  ++count;
 }
 return count;
}


static ssize_t param_attr_store(struct module_attribute *mattr,
    struct module_kobject *mk,
    const char *buf, size_t len)
{
  int err;
 struct param_attribute *attribute = to_param_attr(mattr);

 if (!attribute->param->ops->set)
  return -EPERM;

 kernel_param_lock(mk->mod);
 param_check_unsafe(attribute->param);
 err = attribute->param->ops->set(buf, attribute->param);
 kernel_param_unlock(mk->mod);
 if (!err)
  return len;
 return err;
}


void kernel_param_lock(struct module *mod)
{
 mutex_lock(KPARAM_MUTEX(mod));
}

void kernel_param_unlock(struct module *mod)
{
 mutex_unlock(KPARAM_MUTEX(mod));
}

EXPORT_SYMBOL(kernel_param_lock);
EXPORT_SYMBOL(kernel_param_unlock);
static __modinit int add_sysfs_param(struct module_kobject *mk,
         const struct kernel_param *kp,
         const char *name)
{
 struct module_param_attrs *new_mp;
 struct attribute **new_attrs;
 unsigned int i;


 BUG_ON(!kp->perm);

 if (!mk->mp) {

  mk->mp = kzalloc(sizeof(*mk->mp), GFP_KERNEL);
  if (!mk->mp)
   return -ENOMEM;
  mk->mp->grp.name = "parameters";

  mk->mp->grp.attrs = kzalloc(sizeof(mk->mp->grp.attrs[0]),
         GFP_KERNEL);

  if (!mk->mp->grp.attrs)
   return -ENOMEM;
 }


 new_mp = krealloc(mk->mp,
     sizeof(*mk->mp) +
     sizeof(mk->mp->attrs[0]) * (mk->mp->num + 1),
     GFP_KERNEL);
 if (!new_mp)
  return -ENOMEM;
 mk->mp = new_mp;


 new_attrs = krealloc(mk->mp->grp.attrs,
        sizeof(mk->mp->grp.attrs[0]) * (mk->mp->num + 2),
        GFP_KERNEL);
 if (!new_attrs)
  return -ENOMEM;
 mk->mp->grp.attrs = new_attrs;


 memset(&mk->mp->attrs[mk->mp->num], 0, sizeof(mk->mp->attrs[0]));
 sysfs_attr_init(&mk->mp->attrs[mk->mp->num].mattr.attr);
 mk->mp->attrs[mk->mp->num].param = kp;
 mk->mp->attrs[mk->mp->num].mattr.show = param_attr_show;

 if ((kp->perm & (S_IWUSR | S_IWGRP | S_IWOTH)) != 0)
  mk->mp->attrs[mk->mp->num].mattr.store = param_attr_store;
 else
  mk->mp->attrs[mk->mp->num].mattr.store = NULL;
 mk->mp->attrs[mk->mp->num].mattr.attr.name = (char *)name;
 mk->mp->attrs[mk->mp->num].mattr.attr.mode = kp->perm;
 mk->mp->num++;


 for (i = 0; i < mk->mp->num; i++)
  mk->mp->grp.attrs[i] = &mk->mp->attrs[i].mattr.attr;
 mk->mp->grp.attrs[mk->mp->num] = NULL;
 return 0;
}

static void free_module_param_attrs(struct module_kobject *mk)
{
 if (mk->mp)
  kfree(mk->mp->grp.attrs);
 kfree(mk->mp);
 mk->mp = NULL;
}
int module_param_sysfs_setup(struct module *mod,
        const struct kernel_param *kparam,
        unsigned int num_params)
{
 int i, err;
 bool params = false;

 for (i = 0; i < num_params; i++) {
  if (kparam[i].perm == 0)
   continue;
  err = add_sysfs_param(&mod->mkobj, &kparam[i], kparam[i].name);
  if (err) {
   free_module_param_attrs(&mod->mkobj);
   return err;
  }
  params = true;
 }

 if (!params)
  return 0;


 err = sysfs_create_group(&mod->mkobj.kobj, &mod->mkobj.mp->grp);
 if (err)
  free_module_param_attrs(&mod->mkobj);
 return err;
}
void module_param_sysfs_remove(struct module *mod)
{
 if (mod->mkobj.mp) {
  sysfs_remove_group(&mod->mkobj.kobj, &mod->mkobj.mp->grp);


  free_module_param_attrs(&mod->mkobj);
 }
}

void destroy_params(const struct kernel_param *params, unsigned num)
{
 unsigned int i;

 for (i = 0; i < num; i++)
  if (params[i].ops->free)
   params[i].ops->free(params[i].arg);
}

static struct module_kobject * __init locate_module_kobject(const char *name)
{
 struct module_kobject *mk;
 struct kobject *kobj;
 int err;

 kobj = kset_find_obj(module_kset, name);
 if (kobj) {
  mk = to_module_kobject(kobj);
 } else {
  mk = kzalloc(sizeof(struct module_kobject), GFP_KERNEL);
  BUG_ON(!mk);

  mk->mod = THIS_MODULE;
  mk->kobj.kset = module_kset;
  err = kobject_init_and_add(&mk->kobj, &module_ktype, NULL,
        "%s", name);
  if (!err)
   err = sysfs_create_file(&mk->kobj, &module_uevent.attr);
  if (err) {
   kobject_put(&mk->kobj);
   pr_crit("Adding module '%s' to sysfs failed (%d), the system may be unstable.\n",
    name, err);
   return NULL;
  }


  kobject_get(&mk->kobj);
 }

 return mk;
}

static void __init kernel_add_sysfs_param(const char *name,
       const struct kernel_param *kparam,
       unsigned int name_skip)
{
 struct module_kobject *mk;
 int err;

 mk = locate_module_kobject(name);
 if (!mk)
  return;


 if (mk->mp)
  sysfs_remove_group(&mk->kobj, &mk->mp->grp);


 err = add_sysfs_param(mk, kparam, kparam->name + name_skip);
 BUG_ON(err);
 err = sysfs_create_group(&mk->kobj, &mk->mp->grp);
 BUG_ON(err);
 kobject_uevent(&mk->kobj, KOBJ_ADD);
 kobject_put(&mk->kobj);
}
static void __init param_sysfs_builtin(void)
{
 const struct kernel_param *kp;
 unsigned int name_len;
 char modname[MODULE_NAME_LEN];

 for (kp = __start___param; kp < __stop___param; kp++) {
  char *dot;

  if (kp->perm == 0)
   continue;

  dot = strchr(kp->name, '.');
  if (!dot) {

   strcpy(modname, "kernel");
   name_len = 0;
  } else {
   name_len = dot - kp->name + 1;
   strlcpy(modname, kp->name, name_len);
  }
  kernel_add_sysfs_param(modname, kp, name_len);
 }
}

ssize_t __modver_version_show(struct module_attribute *mattr,
         struct module_kobject *mk, char *buf)
{
 struct module_version_attribute *vattr =
  container_of(mattr, struct module_version_attribute, mattr);

 return scnprintf(buf, PAGE_SIZE, "%s\n", vattr->version);
}

extern const struct module_version_attribute *__start___modver[];
extern const struct module_version_attribute *__stop___modver[];

static void __init version_sysfs_builtin(void)
{
 const struct module_version_attribute **p;
 struct module_kobject *mk;
 int err;

 for (p = __start___modver; p < __stop___modver; p++) {
  const struct module_version_attribute *vattr = *p;

  mk = locate_module_kobject(vattr->module_name);
  if (mk) {
   err = sysfs_create_file(&mk->kobj, &vattr->mattr.attr);
   WARN_ON_ONCE(err);
   kobject_uevent(&mk->kobj, KOBJ_ADD);
   kobject_put(&mk->kobj);
  }
 }
}



static ssize_t module_attr_show(struct kobject *kobj,
    struct attribute *attr,
    char *buf)
{
 struct module_attribute *attribute;
 struct module_kobject *mk;
 int ret;

 attribute = to_module_attr(attr);
 mk = to_module_kobject(kobj);

 if (!attribute->show)
  return -EIO;

 ret = attribute->show(attribute, mk, buf);

 return ret;
}

static ssize_t module_attr_store(struct kobject *kobj,
    struct attribute *attr,
    const char *buf, size_t len)
{
 struct module_attribute *attribute;
 struct module_kobject *mk;
 int ret;

 attribute = to_module_attr(attr);
 mk = to_module_kobject(kobj);

 if (!attribute->store)
  return -EIO;

 ret = attribute->store(attribute, mk, buf, len);

 return ret;
}

static const struct sysfs_ops module_sysfs_ops = {
 .show = module_attr_show,
 .store = module_attr_store,
};

static int uevent_filter(struct kset *kset, struct kobject *kobj)
{
 struct kobj_type *ktype = get_ktype(kobj);

 if (ktype == &module_ktype)
  return 1;
 return 0;
}

static const struct kset_uevent_ops module_uevent_ops = {
 .filter = uevent_filter,
};

struct kset *module_kset;
int module_sysfs_initialized;

static void module_kobj_release(struct kobject *kobj)
{
 struct module_kobject *mk = to_module_kobject(kobj);
 complete(mk->kobj_completion);
}

struct kobj_type module_ktype = {
 .release = module_kobj_release,
 .sysfs_ops = &module_sysfs_ops,
};




static int __init param_sysfs_init(void)
{
 module_kset = kset_create_and_add("module", &module_uevent_ops, NULL);
 if (!module_kset) {
  printk(KERN_WARNING "%s (%d): error creating kset\n",
   __FILE__, __LINE__);
  return -ENOMEM;
 }
 module_sysfs_initialized = 1;

 version_sysfs_builtin();
 param_sysfs_builtin();

 return 0;
}
subsys_initcall(param_sysfs_init);








int pcpu_freelist_init(struct pcpu_freelist *s)
{
 int cpu;

 s->freelist = alloc_percpu(struct pcpu_freelist_head);
 if (!s->freelist)
  return -ENOMEM;

 for_each_possible_cpu(cpu) {
  struct pcpu_freelist_head *head = per_cpu_ptr(s->freelist, cpu);

  raw_spin_lock_init(&head->lock);
  head->first = NULL;
 }
 return 0;
}

void pcpu_freelist_destroy(struct pcpu_freelist *s)
{
 free_percpu(s->freelist);
}

static inline void __pcpu_freelist_push(struct pcpu_freelist_head *head,
     struct pcpu_freelist_node *node)
{
 raw_spin_lock(&head->lock);
 node->next = head->first;
 head->first = node;
 raw_spin_unlock(&head->lock);
}

void pcpu_freelist_push(struct pcpu_freelist *s,
   struct pcpu_freelist_node *node)
{
 struct pcpu_freelist_head *head = this_cpu_ptr(s->freelist);

 __pcpu_freelist_push(head, node);
}

void pcpu_freelist_populate(struct pcpu_freelist *s, void *buf, u32 elem_size,
       u32 nr_elems)
{
 struct pcpu_freelist_head *head;
 unsigned long flags;
 int i, cpu, pcpu_entries;

 pcpu_entries = nr_elems / num_possible_cpus() + 1;
 i = 0;





 local_irq_save(flags);
 for_each_possible_cpu(cpu) {
again:
  head = per_cpu_ptr(s->freelist, cpu);
  __pcpu_freelist_push(head, buf);
  i++;
  buf += elem_size;
  if (i == nr_elems)
   break;
  if (i % pcpu_entries)
   goto again;
 }
 local_irq_restore(flags);
}

struct pcpu_freelist_node *pcpu_freelist_pop(struct pcpu_freelist *s)
{
 struct pcpu_freelist_head *head;
 struct pcpu_freelist_node *node;
 int orig_cpu, cpu;

 orig_cpu = cpu = raw_smp_processor_id();
 while (1) {
  head = per_cpu_ptr(s->freelist, cpu);
  raw_spin_lock(&head->lock);
  node = head->first;
  if (node) {
   head->first = node->next;
   raw_spin_unlock(&head->lock);
   return node;
  }
  raw_spin_unlock(&head->lock);
  cpu = cpumask_next(cpu, cpu_possible_mask);
  if (cpu >= nr_cpu_ids)
   cpu = 0;
  if (cpu == orig_cpu)
   return NULL;
 }
}

 hash_long((unsigned long)nr + (unsigned long)ns, pidhash_shift)
static struct hlist_head *pid_hash;
static unsigned int pidhash_shift = 4;
struct pid init_struct_pid = INIT_STRUCT_PID;

int pid_max = PID_MAX_DEFAULT;


int pid_max_min = RESERVED_PIDS + 1;
int pid_max_max = PID_MAX_LIMIT;

static inline int mk_pid(struct pid_namespace *pid_ns,
  struct pidmap *map, int off)
{
 return (map - pid_ns->pidmap)*BITS_PER_PAGE + off;
}

  find_next_zero_bit((map)->page, BITS_PER_PAGE, off)







struct pid_namespace init_pid_ns = {
 .kref = {
  .refcount = ATOMIC_INIT(2),
 },
 .pidmap = {
  [ 0 ... PIDMAP_ENTRIES-1] = { ATOMIC_INIT(BITS_PER_PAGE), NULL }
 },
 .last_pid = 0,
 .nr_hashed = PIDNS_HASH_ADDING,
 .level = 0,
 .child_reaper = &init_task,
 .user_ns = &init_user_ns,
 .ns.inum = PROC_PID_INIT_INO,
 .ns.ops = &pidns_operations,
};
EXPORT_SYMBOL_GPL(init_pid_ns);
static __cacheline_aligned_in_smp DEFINE_SPINLOCK(pidmap_lock);

static void free_pidmap(struct upid *upid)
{
 int nr = upid->nr;
 struct pidmap *map = upid->ns->pidmap + nr / BITS_PER_PAGE;
 int offset = nr & BITS_PER_PAGE_MASK;

 clear_bit(offset, map->page);
 atomic_inc(&map->nr_free);
}




static int pid_before(int base, int a, int b)
{






 return (unsigned)(a - base) < (unsigned)(b - base);
}
static void set_last_pid(struct pid_namespace *pid_ns, int base, int pid)
{
 int prev;
 int last_write = base;
 do {
  prev = last_write;
  last_write = cmpxchg(&pid_ns->last_pid, prev, pid);
 } while ((prev != last_write) && (pid_before(base, last_write, pid)));
}

static int alloc_pidmap(struct pid_namespace *pid_ns)
{
 int i, offset, max_scan, pid, last = pid_ns->last_pid;
 struct pidmap *map;

 pid = last + 1;
 if (pid >= pid_max)
  pid = RESERVED_PIDS;
 offset = pid & BITS_PER_PAGE_MASK;
 map = &pid_ns->pidmap[pid/BITS_PER_PAGE];





 max_scan = DIV_ROUND_UP(pid_max, BITS_PER_PAGE) - !offset;
 for (i = 0; i <= max_scan; ++i) {
  if (unlikely(!map->page)) {
   void *page = kzalloc(PAGE_SIZE, GFP_KERNEL);




   spin_lock_irq(&pidmap_lock);
   if (!map->page) {
    map->page = page;
    page = NULL;
   }
   spin_unlock_irq(&pidmap_lock);
   kfree(page);
   if (unlikely(!map->page))
    return -ENOMEM;
  }
  if (likely(atomic_read(&map->nr_free))) {
   for ( ; ; ) {
    if (!test_and_set_bit(offset, map->page)) {
     atomic_dec(&map->nr_free);
     set_last_pid(pid_ns, last, pid);
     return pid;
    }
    offset = find_next_offset(map, offset);
    if (offset >= BITS_PER_PAGE)
     break;
    pid = mk_pid(pid_ns, map, offset);
    if (pid >= pid_max)
     break;
   }
  }
  if (map < &pid_ns->pidmap[(pid_max-1)/BITS_PER_PAGE]) {
   ++map;
   offset = 0;
  } else {
   map = &pid_ns->pidmap[0];
   offset = RESERVED_PIDS;
   if (unlikely(last == offset))
    break;
  }
  pid = mk_pid(pid_ns, map, offset);
 }
 return -EAGAIN;
}

int next_pidmap(struct pid_namespace *pid_ns, unsigned int last)
{
 int offset;
 struct pidmap *map, *end;

 if (last >= PID_MAX_LIMIT)
  return -1;

 offset = (last + 1) & BITS_PER_PAGE_MASK;
 map = &pid_ns->pidmap[(last + 1)/BITS_PER_PAGE];
 end = &pid_ns->pidmap[PIDMAP_ENTRIES];
 for (; map < end; map++, offset = 0) {
  if (unlikely(!map->page))
   continue;
  offset = find_next_bit((map)->page, BITS_PER_PAGE, offset);
  if (offset < BITS_PER_PAGE)
   return mk_pid(pid_ns, map, offset);
 }
 return -1;
}

void put_pid(struct pid *pid)
{
 struct pid_namespace *ns;

 if (!pid)
  return;

 ns = pid->numbers[pid->level].ns;
 if ((atomic_read(&pid->count) == 1) ||
      atomic_dec_and_test(&pid->count)) {
  kmem_cache_free(ns->pid_cachep, pid);
  put_pid_ns(ns);
 }
}
EXPORT_SYMBOL_GPL(put_pid);

static void delayed_put_pid(struct rcu_head *rhp)
{
 struct pid *pid = container_of(rhp, struct pid, rcu);
 put_pid(pid);
}

void free_pid(struct pid *pid)
{

 int i;
 unsigned long flags;

 spin_lock_irqsave(&pidmap_lock, flags);
 for (i = 0; i <= pid->level; i++) {
  struct upid *upid = pid->numbers + i;
  struct pid_namespace *ns = upid->ns;
  hlist_del_rcu(&upid->pid_chain);
  switch(--ns->nr_hashed) {
  case 2:
  case 1:




   wake_up_process(ns->child_reaper);
   break;
  case PIDNS_HASH_ADDING:

   WARN_ON(ns->child_reaper);
   ns->nr_hashed = 0;

  case 0:
   schedule_work(&ns->proc_work);
   break;
  }
 }
 spin_unlock_irqrestore(&pidmap_lock, flags);

 for (i = 0; i <= pid->level; i++)
  free_pidmap(pid->numbers + i);

 call_rcu(&pid->rcu, delayed_put_pid);
}

struct pid *alloc_pid(struct pid_namespace *ns)
{
 struct pid *pid;
 enum pid_type type;
 int i, nr;
 struct pid_namespace *tmp;
 struct upid *upid;
 int retval = -ENOMEM;

 pid = kmem_cache_alloc(ns->pid_cachep, GFP_KERNEL);
 if (!pid)
  return ERR_PTR(retval);

 tmp = ns;
 pid->level = ns->level;
 for (i = ns->level; i >= 0; i--) {
  nr = alloc_pidmap(tmp);
  if (nr < 0) {
   retval = nr;
   goto out_free;
  }

  pid->numbers[i].nr = nr;
  pid->numbers[i].ns = tmp;
  tmp = tmp->parent;
 }

 if (unlikely(is_child_reaper(pid))) {
  if (pid_ns_prepare_proc(ns))
   goto out_free;
 }

 get_pid_ns(ns);
 atomic_set(&pid->count, 1);
 for (type = 0; type < PIDTYPE_MAX; ++type)
  INIT_HLIST_HEAD(&pid->tasks[type]);

 upid = pid->numbers + ns->level;
 spin_lock_irq(&pidmap_lock);
 if (!(ns->nr_hashed & PIDNS_HASH_ADDING))
  goto out_unlock;
 for ( ; upid >= pid->numbers; --upid) {
  hlist_add_head_rcu(&upid->pid_chain,
    &pid_hash[pid_hashfn(upid->nr, upid->ns)]);
  upid->ns->nr_hashed++;
 }
 spin_unlock_irq(&pidmap_lock);

 return pid;

out_unlock:
 spin_unlock_irq(&pidmap_lock);
 put_pid_ns(ns);

out_free:
 while (++i <= ns->level)
  free_pidmap(pid->numbers + i);

 kmem_cache_free(ns->pid_cachep, pid);
 return ERR_PTR(retval);
}

void disable_pid_allocation(struct pid_namespace *ns)
{
 spin_lock_irq(&pidmap_lock);
 ns->nr_hashed &= ~PIDNS_HASH_ADDING;
 spin_unlock_irq(&pidmap_lock);
}

struct pid *find_pid_ns(int nr, struct pid_namespace *ns)
{
 struct upid *pnr;

 hlist_for_each_entry_rcu(pnr,
   &pid_hash[pid_hashfn(nr, ns)], pid_chain)
  if (pnr->nr == nr && pnr->ns == ns)
   return container_of(pnr, struct pid,
     numbers[ns->level]);

 return NULL;
}
EXPORT_SYMBOL_GPL(find_pid_ns);

struct pid *find_vpid(int nr)
{
 return find_pid_ns(nr, task_active_pid_ns(current));
}
EXPORT_SYMBOL_GPL(find_vpid);




void attach_pid(struct task_struct *task, enum pid_type type)
{
 struct pid_link *link = &task->pids[type];
 hlist_add_head_rcu(&link->node, &link->pid->tasks[type]);
}

static void __change_pid(struct task_struct *task, enum pid_type type,
   struct pid *new)
{
 struct pid_link *link;
 struct pid *pid;
 int tmp;

 link = &task->pids[type];
 pid = link->pid;

 hlist_del_rcu(&link->node);
 link->pid = new;

 for (tmp = PIDTYPE_MAX; --tmp >= 0; )
  if (!hlist_empty(&pid->tasks[tmp]))
   return;

 free_pid(pid);
}

void detach_pid(struct task_struct *task, enum pid_type type)
{
 __change_pid(task, type, NULL);
}

void change_pid(struct task_struct *task, enum pid_type type,
  struct pid *pid)
{
 __change_pid(task, type, pid);
 attach_pid(task, type);
}


void transfer_pid(struct task_struct *old, struct task_struct *new,
      enum pid_type type)
{
 new->pids[type].pid = old->pids[type].pid;
 hlist_replace_rcu(&old->pids[type].node, &new->pids[type].node);
}

struct task_struct *pid_task(struct pid *pid, enum pid_type type)
{
 struct task_struct *result = NULL;
 if (pid) {
  struct hlist_node *first;
  first = rcu_dereference_check(hlist_first_rcu(&pid->tasks[type]),
           lockdep_tasklist_lock_is_held());
  if (first)
   result = hlist_entry(first, struct task_struct, pids[(type)].node);
 }
 return result;
}
EXPORT_SYMBOL(pid_task);




struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns)
{
 RCU_LOCKDEP_WARN(!rcu_read_lock_held(),
    "find_task_by_pid_ns() needs rcu_read_lock() protection");
 return pid_task(find_pid_ns(nr, ns), PIDTYPE_PID);
}

struct task_struct *find_task_by_vpid(pid_t vnr)
{
 return find_task_by_pid_ns(vnr, task_active_pid_ns(current));
}

struct pid *get_task_pid(struct task_struct *task, enum pid_type type)
{
 struct pid *pid;
 rcu_read_lock();
 if (type != PIDTYPE_PID)
  task = task->group_leader;
 pid = get_pid(rcu_dereference(task->pids[type].pid));
 rcu_read_unlock();
 return pid;
}
EXPORT_SYMBOL_GPL(get_task_pid);

struct task_struct *get_pid_task(struct pid *pid, enum pid_type type)
{
 struct task_struct *result;
 rcu_read_lock();
 result = pid_task(pid, type);
 if (result)
  get_task_struct(result);
 rcu_read_unlock();
 return result;
}
EXPORT_SYMBOL_GPL(get_pid_task);

struct pid *find_get_pid(pid_t nr)
{
 struct pid *pid;

 rcu_read_lock();
 pid = get_pid(find_vpid(nr));
 rcu_read_unlock();

 return pid;
}
EXPORT_SYMBOL_GPL(find_get_pid);

pid_t pid_nr_ns(struct pid *pid, struct pid_namespace *ns)
{
 struct upid *upid;
 pid_t nr = 0;

 if (pid && ns->level <= pid->level) {
  upid = &pid->numbers[ns->level];
  if (upid->ns == ns)
   nr = upid->nr;
 }
 return nr;
}
EXPORT_SYMBOL_GPL(pid_nr_ns);

pid_t pid_vnr(struct pid *pid)
{
 return pid_nr_ns(pid, task_active_pid_ns(current));
}
EXPORT_SYMBOL_GPL(pid_vnr);

pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,
   struct pid_namespace *ns)
{
 pid_t nr = 0;

 rcu_read_lock();
 if (!ns)
  ns = task_active_pid_ns(current);
 if (likely(pid_alive(task))) {
  if (type != PIDTYPE_PID)
   task = task->group_leader;
  nr = pid_nr_ns(rcu_dereference(task->pids[type].pid), ns);
 }
 rcu_read_unlock();

 return nr;
}
EXPORT_SYMBOL(__task_pid_nr_ns);

pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
{
 return pid_nr_ns(task_tgid(tsk), ns);
}
EXPORT_SYMBOL(task_tgid_nr_ns);

struct pid_namespace *task_active_pid_ns(struct task_struct *tsk)
{
 return ns_of_pid(task_pid(tsk));
}
EXPORT_SYMBOL_GPL(task_active_pid_ns);






struct pid *find_ge_pid(int nr, struct pid_namespace *ns)
{
 struct pid *pid;

 do {
  pid = find_pid_ns(nr, ns);
  if (pid)
   break;
  nr = next_pidmap(ns, nr);
 } while (nr > 0);

 return pid;
}






void __init pidhash_init(void)
{
 unsigned int i, pidhash_size;

 pid_hash = alloc_large_system_hash("PID", sizeof(*pid_hash), 0, 18,
        HASH_EARLY | HASH_SMALL,
        &pidhash_shift, NULL,
        0, 4096);
 pidhash_size = 1U << pidhash_shift;

 for (i = 0; i < pidhash_size; i++)
  INIT_HLIST_HEAD(&pid_hash[i]);
}

void __init pidmap_init(void)
{

 BUILD_BUG_ON(PID_MAX_LIMIT >= PIDNS_HASH_ADDING);


 pid_max = min(pid_max_max, max_t(int, pid_max,
    PIDS_PER_CPU_DEFAULT * num_possible_cpus()));
 pid_max_min = max_t(int, pid_max_min,
    PIDS_PER_CPU_MIN * num_possible_cpus());
 pr_info("pid_max: default: %u minimum: %u\n", pid_max, pid_max_min);

 init_pid_ns.pidmap[0].page = kzalloc(PAGE_SIZE, GFP_KERNEL);

 set_bit(0, init_pid_ns.pidmap[0].page);
 atomic_dec(&init_pid_ns.pidmap[0].nr_free);

 init_pid_ns.pid_cachep = KMEM_CACHE(pid,
   SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT);
}

struct pid_cache {
 int nr_ids;
 char name[16];
 struct kmem_cache *cachep;
 struct list_head list;
};

static LIST_HEAD(pid_caches_lh);
static DEFINE_MUTEX(pid_caches_mutex);
static struct kmem_cache *pid_ns_cachep;






static struct kmem_cache *create_pid_cachep(int nr_ids)
{
 struct pid_cache *pcache;
 struct kmem_cache *cachep;

 mutex_lock(&pid_caches_mutex);
 list_for_each_entry(pcache, &pid_caches_lh, list)
  if (pcache->nr_ids == nr_ids)
   goto out;

 pcache = kmalloc(sizeof(struct pid_cache), GFP_KERNEL);
 if (pcache == NULL)
  goto err_alloc;

 snprintf(pcache->name, sizeof(pcache->name), "pid_%d", nr_ids);
 cachep = kmem_cache_create(pcache->name,
   sizeof(struct pid) + (nr_ids - 1) * sizeof(struct upid),
   0, SLAB_HWCACHE_ALIGN, NULL);
 if (cachep == NULL)
  goto err_cachep;

 pcache->nr_ids = nr_ids;
 pcache->cachep = cachep;
 list_add(&pcache->list, &pid_caches_lh);
out:
 mutex_unlock(&pid_caches_mutex);
 return pcache->cachep;

err_cachep:
 kfree(pcache);
err_alloc:
 mutex_unlock(&pid_caches_mutex);
 return NULL;
}

static void proc_cleanup_work(struct work_struct *work)
{
 struct pid_namespace *ns = container_of(work, struct pid_namespace, proc_work);
 pid_ns_release_proc(ns);
}



static struct pid_namespace *create_pid_namespace(struct user_namespace *user_ns,
 struct pid_namespace *parent_pid_ns)
{
 struct pid_namespace *ns;
 unsigned int level = parent_pid_ns->level + 1;
 int i;
 int err;

 if (level > MAX_PID_NS_LEVEL) {
  err = -EINVAL;
  goto out;
 }

 err = -ENOMEM;
 ns = kmem_cache_zalloc(pid_ns_cachep, GFP_KERNEL);
 if (ns == NULL)
  goto out;

 ns->pidmap[0].page = kzalloc(PAGE_SIZE, GFP_KERNEL);
 if (!ns->pidmap[0].page)
  goto out_free;

 ns->pid_cachep = create_pid_cachep(level + 1);
 if (ns->pid_cachep == NULL)
  goto out_free_map;

 err = ns_alloc_inum(&ns->ns);
 if (err)
  goto out_free_map;
 ns->ns.ops = &pidns_operations;

 kref_init(&ns->kref);
 ns->level = level;
 ns->parent = get_pid_ns(parent_pid_ns);
 ns->user_ns = get_user_ns(user_ns);
 ns->nr_hashed = PIDNS_HASH_ADDING;
 INIT_WORK(&ns->proc_work, proc_cleanup_work);

 set_bit(0, ns->pidmap[0].page);
 atomic_set(&ns->pidmap[0].nr_free, BITS_PER_PAGE - 1);

 for (i = 1; i < PIDMAP_ENTRIES; i++)
  atomic_set(&ns->pidmap[i].nr_free, BITS_PER_PAGE);

 return ns;

out_free_map:
 kfree(ns->pidmap[0].page);
out_free:
 kmem_cache_free(pid_ns_cachep, ns);
out:
 return ERR_PTR(err);
}

static void delayed_free_pidns(struct rcu_head *p)
{
 kmem_cache_free(pid_ns_cachep,
   container_of(p, struct pid_namespace, rcu));
}

static void destroy_pid_namespace(struct pid_namespace *ns)
{
 int i;

 ns_free_inum(&ns->ns);
 for (i = 0; i < PIDMAP_ENTRIES; i++)
  kfree(ns->pidmap[i].page);
 put_user_ns(ns->user_ns);
 call_rcu(&ns->rcu, delayed_free_pidns);
}

struct pid_namespace *copy_pid_ns(unsigned long flags,
 struct user_namespace *user_ns, struct pid_namespace *old_ns)
{
 if (!(flags & CLONE_NEWPID))
  return get_pid_ns(old_ns);
 if (task_active_pid_ns(current) != old_ns)
  return ERR_PTR(-EINVAL);
 return create_pid_namespace(user_ns, old_ns);
}

static void free_pid_ns(struct kref *kref)
{
 struct pid_namespace *ns;

 ns = container_of(kref, struct pid_namespace, kref);
 destroy_pid_namespace(ns);
}

void put_pid_ns(struct pid_namespace *ns)
{
 struct pid_namespace *parent;

 while (ns != &init_pid_ns) {
  parent = ns->parent;
  if (!kref_put(&ns->kref, free_pid_ns))
   break;
  ns = parent;
 }
}
EXPORT_SYMBOL_GPL(put_pid_ns);

void zap_pid_ns_processes(struct pid_namespace *pid_ns)
{
 int nr;
 int rc;
 struct task_struct *task, *me = current;
 int init_pids = thread_group_leader(me) ? 1 : 2;


 disable_pid_allocation(pid_ns);






 spin_lock_irq(&me->sighand->siglock);
 me->sighand->action[SIGCHLD - 1].sa.sa_handler = SIG_IGN;
 spin_unlock_irq(&me->sighand->siglock);
 read_lock(&tasklist_lock);
 nr = next_pidmap(pid_ns, 1);
 while (nr > 0) {
  rcu_read_lock();

  task = pid_task(find_vpid(nr), PIDTYPE_PID);
  if (task && !__fatal_signal_pending(task))
   send_sig_info(SIGKILL, SEND_SIG_FORCED, task);

  rcu_read_unlock();

  nr = next_pidmap(pid_ns, nr);
 }
 read_unlock(&tasklist_lock);






 do {
  clear_thread_flag(TIF_SIGPENDING);
  rc = sys_wait4(-1, NULL, __WALL, NULL);
 } while (rc != -ECHILD);
 for (;;) {
  set_current_state(TASK_UNINTERRUPTIBLE);
  if (pid_ns->nr_hashed == init_pids)
   break;
  schedule();
 }
 __set_current_state(TASK_RUNNING);

 if (pid_ns->reboot)
  current->signal->group_exit_code = pid_ns->reboot;

 acct_exit_ns(pid_ns);
 return;
}

static int pid_ns_ctl_handler(struct ctl_table *table, int write,
  void __user *buffer, size_t *lenp, loff_t *ppos)
{
 struct pid_namespace *pid_ns = task_active_pid_ns(current);
 struct ctl_table tmp = *table;

 if (write && !ns_capable(pid_ns->user_ns, CAP_SYS_ADMIN))
  return -EPERM;







 tmp.data = &pid_ns->last_pid;
 return proc_dointvec_minmax(&tmp, write, buffer, lenp, ppos);
}

extern int pid_max;
static int zero = 0;
static struct ctl_table pid_ns_ctl_table[] = {
 {
  .procname = "ns_last_pid",
  .maxlen = sizeof(int),
  .mode = 0666,
  .proc_handler = pid_ns_ctl_handler,
  .extra1 = &zero,
  .extra2 = &pid_max,
 },
 { }
};
static struct ctl_path kern_path[] = { { .procname = "kernel", }, { } };

int reboot_pid_ns(struct pid_namespace *pid_ns, int cmd)
{
 if (pid_ns == &init_pid_ns)
  return 0;

 switch (cmd) {
 case LINUX_REBOOT_CMD_RESTART2:
 case LINUX_REBOOT_CMD_RESTART:
  pid_ns->reboot = SIGHUP;
  break;

 case LINUX_REBOOT_CMD_POWER_OFF:
 case LINUX_REBOOT_CMD_HALT:
  pid_ns->reboot = SIGINT;
  break;
 default:
  return -EINVAL;
 }

 read_lock(&tasklist_lock);
 force_sig(SIGKILL, pid_ns->child_reaper);
 read_unlock(&tasklist_lock);

 do_exit(0);


 return 0;
}

static inline struct pid_namespace *to_pid_ns(struct ns_common *ns)
{
 return container_of(ns, struct pid_namespace, ns);
}

static struct ns_common *pidns_get(struct task_struct *task)
{
 struct pid_namespace *ns;

 rcu_read_lock();
 ns = task_active_pid_ns(task);
 if (ns)
  get_pid_ns(ns);
 rcu_read_unlock();

 return ns ? &ns->ns : NULL;
}

static void pidns_put(struct ns_common *ns)
{
 put_pid_ns(to_pid_ns(ns));
}

static int pidns_install(struct nsproxy *nsproxy, struct ns_common *ns)
{
 struct pid_namespace *active = task_active_pid_ns(current);
 struct pid_namespace *ancestor, *new = to_pid_ns(ns);

 if (!ns_capable(new->user_ns, CAP_SYS_ADMIN) ||
     !ns_capable(current_user_ns(), CAP_SYS_ADMIN))
  return -EPERM;
 if (new->level < active->level)
  return -EINVAL;

 ancestor = new;
 while (ancestor->level > active->level)
  ancestor = ancestor->parent;
 if (ancestor != active)
  return -EINVAL;

 put_pid_ns(nsproxy->pid_ns_for_children);
 nsproxy->pid_ns_for_children = get_pid_ns(new);
 return 0;
}

const struct proc_ns_operations pidns_operations = {
 .name = "pid",
 .type = CLONE_NEWPID,
 .get = pidns_get,
 .put = pidns_put,
 .install = pidns_install,
};

static __init int pid_namespaces_init(void)
{
 pid_ns_cachep = KMEM_CACHE(pid_namespace, SLAB_PANIC);

 register_sysctl_paths(kern_path, pid_ns_ctl_table);
 return 0;
}

__initcall(pid_namespaces_init);


bool irq_pm_check_wakeup(struct irq_desc *desc)
{
 if (irqd_is_wakeup_armed(&desc->irq_data)) {
  irqd_clear(&desc->irq_data, IRQD_WAKEUP_ARMED);
  desc->istate |= IRQS_SUSPENDED | IRQS_PENDING;
  desc->depth++;
  irq_disable(desc);
  pm_system_irq_wakeup(irq_desc_get_irq(desc));
  return true;
 }
 return false;
}





void irq_pm_install_action(struct irq_desc *desc, struct irqaction *action)
{
 desc->nr_actions++;

 if (action->flags & IRQF_FORCE_RESUME)
  desc->force_resume_depth++;

 WARN_ON_ONCE(desc->force_resume_depth &&
       desc->force_resume_depth != desc->nr_actions);

 if (action->flags & IRQF_NO_SUSPEND)
  desc->no_suspend_depth++;
 else if (action->flags & IRQF_COND_SUSPEND)
  desc->cond_suspend_depth++;

 WARN_ON_ONCE(desc->no_suspend_depth &&
       (desc->no_suspend_depth +
   desc->cond_suspend_depth) != desc->nr_actions);
}





void irq_pm_remove_action(struct irq_desc *desc, struct irqaction *action)
{
 desc->nr_actions--;

 if (action->flags & IRQF_FORCE_RESUME)
  desc->force_resume_depth--;

 if (action->flags & IRQF_NO_SUSPEND)
  desc->no_suspend_depth--;
 else if (action->flags & IRQF_COND_SUSPEND)
  desc->cond_suspend_depth--;
}

static bool suspend_device_irq(struct irq_desc *desc)
{
 if (!desc->action || irq_desc_is_chained(desc) ||
     desc->no_suspend_depth)
  return false;

 if (irqd_is_wakeup_set(&desc->irq_data)) {
  irqd_set(&desc->irq_data, IRQD_WAKEUP_ARMED);






  return true;
 }

 desc->istate |= IRQS_SUSPENDED;
 __disable_irq(desc);







 if (irq_desc_get_chip(desc)->flags & IRQCHIP_MASK_ON_SUSPEND)
  mask_irq(desc);
 return true;
}
void suspend_device_irqs(void)
{
 struct irq_desc *desc;
 int irq;

 for_each_irq_desc(irq, desc) {
  unsigned long flags;
  bool sync;

  if (irq_settings_is_nested_thread(desc))
   continue;
  raw_spin_lock_irqsave(&desc->lock, flags);
  sync = suspend_device_irq(desc);
  raw_spin_unlock_irqrestore(&desc->lock, flags);

  if (sync)
   synchronize_irq(irq);
 }
}
EXPORT_SYMBOL_GPL(suspend_device_irqs);

static void resume_irq(struct irq_desc *desc)
{
 irqd_clear(&desc->irq_data, IRQD_WAKEUP_ARMED);

 if (desc->istate & IRQS_SUSPENDED)
  goto resume;


 if (!desc->force_resume_depth)
  return;


 desc->depth++;
resume:
 desc->istate &= ~IRQS_SUSPENDED;
 __enable_irq(desc);
}

static void resume_irqs(bool want_early)
{
 struct irq_desc *desc;
 int irq;

 for_each_irq_desc(irq, desc) {
  unsigned long flags;
  bool is_early = desc->action &&
   desc->action->flags & IRQF_EARLY_RESUME;

  if (!is_early && want_early)
   continue;
  if (irq_settings_is_nested_thread(desc))
   continue;

  raw_spin_lock_irqsave(&desc->lock, flags);
  resume_irq(desc);
  raw_spin_unlock_irqrestore(&desc->lock, flags);
 }
}






static void irq_pm_syscore_resume(void)
{
 resume_irqs(true);
}

static struct syscore_ops irq_pm_syscore_ops = {
 .resume = irq_pm_syscore_resume,
};

static int __init irq_pm_init_ops(void)
{
 register_syscore_ops(&irq_pm_syscore_ops);
 return 0;
}

device_initcall(irq_pm_init_ops);
void resume_device_irqs(void)
{
 resume_irqs(false);
}
EXPORT_SYMBOL_GPL(resume_device_irqs);

static void delete_clock(struct kref *kref);




static struct posix_clock *get_posix_clock(struct file *fp)
{
 struct posix_clock *clk = fp->private_data;

 down_read(&clk->rwsem);

 if (!clk->zombie)
  return clk;

 up_read(&clk->rwsem);

 return NULL;
}

static void put_posix_clock(struct posix_clock *clk)
{
 up_read(&clk->rwsem);
}

static ssize_t posix_clock_read(struct file *fp, char __user *buf,
    size_t count, loff_t *ppos)
{
 struct posix_clock *clk = get_posix_clock(fp);
 int err = -EINVAL;

 if (!clk)
  return -ENODEV;

 if (clk->ops.read)
  err = clk->ops.read(clk, fp->f_flags, buf, count);

 put_posix_clock(clk);

 return err;
}

static unsigned int posix_clock_poll(struct file *fp, poll_table *wait)
{
 struct posix_clock *clk = get_posix_clock(fp);
 unsigned int result = 0;

 if (!clk)
  return POLLERR;

 if (clk->ops.poll)
  result = clk->ops.poll(clk, fp, wait);

 put_posix_clock(clk);

 return result;
}

static int posix_clock_fasync(int fd, struct file *fp, int on)
{
 struct posix_clock *clk = get_posix_clock(fp);
 int err = 0;

 if (!clk)
  return -ENODEV;

 if (clk->ops.fasync)
  err = clk->ops.fasync(clk, fd, fp, on);

 put_posix_clock(clk);

 return err;
}

static int posix_clock_mmap(struct file *fp, struct vm_area_struct *vma)
{
 struct posix_clock *clk = get_posix_clock(fp);
 int err = -ENODEV;

 if (!clk)
  return -ENODEV;

 if (clk->ops.mmap)
  err = clk->ops.mmap(clk, vma);

 put_posix_clock(clk);

 return err;
}

static long posix_clock_ioctl(struct file *fp,
         unsigned int cmd, unsigned long arg)
{
 struct posix_clock *clk = get_posix_clock(fp);
 int err = -ENOTTY;

 if (!clk)
  return -ENODEV;

 if (clk->ops.ioctl)
  err = clk->ops.ioctl(clk, cmd, arg);

 put_posix_clock(clk);

 return err;
}

static long posix_clock_compat_ioctl(struct file *fp,
         unsigned int cmd, unsigned long arg)
{
 struct posix_clock *clk = get_posix_clock(fp);
 int err = -ENOTTY;

 if (!clk)
  return -ENODEV;

 if (clk->ops.ioctl)
  err = clk->ops.ioctl(clk, cmd, arg);

 put_posix_clock(clk);

 return err;
}

static int posix_clock_open(struct inode *inode, struct file *fp)
{
 int err;
 struct posix_clock *clk =
  container_of(inode->i_cdev, struct posix_clock, cdev);

 down_read(&clk->rwsem);

 if (clk->zombie) {
  err = -ENODEV;
  goto out;
 }
 if (clk->ops.open)
  err = clk->ops.open(clk, fp->f_mode);
 else
  err = 0;

 if (!err) {
  kref_get(&clk->kref);
  fp->private_data = clk;
 }
out:
 up_read(&clk->rwsem);
 return err;
}

static int posix_clock_release(struct inode *inode, struct file *fp)
{
 struct posix_clock *clk = fp->private_data;
 int err = 0;

 if (clk->ops.release)
  err = clk->ops.release(clk);

 kref_put(&clk->kref, delete_clock);

 fp->private_data = NULL;

 return err;
}

static const struct file_operations posix_clock_file_operations = {
 .owner = THIS_MODULE,
 .llseek = no_llseek,
 .read = posix_clock_read,
 .poll = posix_clock_poll,
 .unlocked_ioctl = posix_clock_ioctl,
 .open = posix_clock_open,
 .release = posix_clock_release,
 .fasync = posix_clock_fasync,
 .mmap = posix_clock_mmap,
 .compat_ioctl = posix_clock_compat_ioctl,
};

int posix_clock_register(struct posix_clock *clk, dev_t devid)
{
 int err;

 kref_init(&clk->kref);
 init_rwsem(&clk->rwsem);

 cdev_init(&clk->cdev, &posix_clock_file_operations);
 clk->cdev.owner = clk->ops.owner;
 err = cdev_add(&clk->cdev, devid, 1);

 return err;
}
EXPORT_SYMBOL_GPL(posix_clock_register);

static void delete_clock(struct kref *kref)
{
 struct posix_clock *clk = container_of(kref, struct posix_clock, kref);

 if (clk->release)
  clk->release(clk);
}

void posix_clock_unregister(struct posix_clock *clk)
{
 cdev_del(&clk->cdev);

 down_write(&clk->rwsem);
 clk->zombie = true;
 up_write(&clk->rwsem);

 kref_put(&clk->kref, delete_clock);
}
EXPORT_SYMBOL_GPL(posix_clock_unregister);

struct posix_clock_desc {
 struct file *fp;
 struct posix_clock *clk;
};

static int get_clock_desc(const clockid_t id, struct posix_clock_desc *cd)
{
 struct file *fp = fget(CLOCKID_TO_FD(id));
 int err = -EINVAL;

 if (!fp)
  return err;

 if (fp->f_op->open != posix_clock_open || !fp->private_data)
  goto out;

 cd->fp = fp;
 cd->clk = get_posix_clock(fp);

 err = cd->clk ? 0 : -ENODEV;
out:
 if (err)
  fput(fp);
 return err;
}

static void put_clock_desc(struct posix_clock_desc *cd)
{
 put_posix_clock(cd->clk);
 fput(cd->fp);
}

static int pc_clock_adjtime(clockid_t id, struct timex *tx)
{
 struct posix_clock_desc cd;
 int err;

 err = get_clock_desc(id, &cd);
 if (err)
  return err;

 if ((cd.fp->f_mode & FMODE_WRITE) == 0) {
  err = -EACCES;
  goto out;
 }

 if (cd.clk->ops.clock_adjtime)
  err = cd.clk->ops.clock_adjtime(cd.clk, tx);
 else
  err = -EOPNOTSUPP;
out:
 put_clock_desc(&cd);

 return err;
}

static int pc_clock_gettime(clockid_t id, struct timespec *ts)
{
 struct posix_clock_desc cd;
 int err;

 err = get_clock_desc(id, &cd);
 if (err)
  return err;

 if (cd.clk->ops.clock_gettime)
  err = cd.clk->ops.clock_gettime(cd.clk, ts);
 else
  err = -EOPNOTSUPP;

 put_clock_desc(&cd);

 return err;
}

static int pc_clock_getres(clockid_t id, struct timespec *ts)
{
 struct posix_clock_desc cd;
 int err;

 err = get_clock_desc(id, &cd);
 if (err)
  return err;

 if (cd.clk->ops.clock_getres)
  err = cd.clk->ops.clock_getres(cd.clk, ts);
 else
  err = -EOPNOTSUPP;

 put_clock_desc(&cd);

 return err;
}

static int pc_clock_settime(clockid_t id, const struct timespec *ts)
{
 struct posix_clock_desc cd;
 int err;

 err = get_clock_desc(id, &cd);
 if (err)
  return err;

 if ((cd.fp->f_mode & FMODE_WRITE) == 0) {
  err = -EACCES;
  goto out;
 }

 if (cd.clk->ops.clock_settime)
  err = cd.clk->ops.clock_settime(cd.clk, ts);
 else
  err = -EOPNOTSUPP;
out:
 put_clock_desc(&cd);

 return err;
}

static int pc_timer_create(struct k_itimer *kit)
{
 clockid_t id = kit->it_clock;
 struct posix_clock_desc cd;
 int err;

 err = get_clock_desc(id, &cd);
 if (err)
  return err;

 if (cd.clk->ops.timer_create)
  err = cd.clk->ops.timer_create(cd.clk, kit);
 else
  err = -EOPNOTSUPP;

 put_clock_desc(&cd);

 return err;
}

static int pc_timer_delete(struct k_itimer *kit)
{
 clockid_t id = kit->it_clock;
 struct posix_clock_desc cd;
 int err;

 err = get_clock_desc(id, &cd);
 if (err)
  return err;

 if (cd.clk->ops.timer_delete)
  err = cd.clk->ops.timer_delete(cd.clk, kit);
 else
  err = -EOPNOTSUPP;

 put_clock_desc(&cd);

 return err;
}

static void pc_timer_gettime(struct k_itimer *kit, struct itimerspec *ts)
{
 clockid_t id = kit->it_clock;
 struct posix_clock_desc cd;

 if (get_clock_desc(id, &cd))
  return;

 if (cd.clk->ops.timer_gettime)
  cd.clk->ops.timer_gettime(cd.clk, kit, ts);

 put_clock_desc(&cd);
}

static int pc_timer_settime(struct k_itimer *kit, int flags,
       struct itimerspec *ts, struct itimerspec *old)
{
 clockid_t id = kit->it_clock;
 struct posix_clock_desc cd;
 int err;

 err = get_clock_desc(id, &cd);
 if (err)
  return err;

 if (cd.clk->ops.timer_settime)
  err = cd.clk->ops.timer_settime(cd.clk, kit, flags, ts, old);
 else
  err = -EOPNOTSUPP;

 put_clock_desc(&cd);

 return err;
}

struct k_clock clock_posix_dynamic = {
 .clock_getres = pc_clock_getres,
 .clock_set = pc_clock_settime,
 .clock_get = pc_clock_gettime,
 .clock_adj = pc_clock_adjtime,
 .timer_create = pc_timer_create,
 .timer_set = pc_timer_settime,
 .timer_del = pc_timer_delete,
 .timer_get = pc_timer_gettime,
};











void update_rlimit_cpu(struct task_struct *task, unsigned long rlim_new)
{
 cputime_t cputime = secs_to_cputime(rlim_new);

 spin_lock_irq(&task->sighand->siglock);
 set_process_cpu_timer(task, CPUCLOCK_PROF, &cputime, NULL);
 spin_unlock_irq(&task->sighand->siglock);
}

static int check_clock(const clockid_t which_clock)
{
 int error = 0;
 struct task_struct *p;
 const pid_t pid = CPUCLOCK_PID(which_clock);

 if (CPUCLOCK_WHICH(which_clock) >= CPUCLOCK_MAX)
  return -EINVAL;

 if (pid == 0)
  return 0;

 rcu_read_lock();
 p = find_task_by_vpid(pid);
 if (!p || !(CPUCLOCK_PERTHREAD(which_clock) ?
     same_thread_group(p, current) : has_group_leader_pid(p))) {
  error = -EINVAL;
 }
 rcu_read_unlock();

 return error;
}

static inline unsigned long long
timespec_to_sample(const clockid_t which_clock, const struct timespec *tp)
{
 unsigned long long ret;

 ret = 0;
 if (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {
  ret = (unsigned long long)tp->tv_sec * NSEC_PER_SEC + tp->tv_nsec;
 } else {
  ret = cputime_to_expires(timespec_to_cputime(tp));
 }
 return ret;
}

static void sample_to_timespec(const clockid_t which_clock,
          unsigned long long expires,
          struct timespec *tp)
{
 if (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED)
  *tp = ns_to_timespec(expires);
 else
  cputime_to_timespec((__force cputime_t)expires, tp);
}





static void bump_cpu_timer(struct k_itimer *timer,
      unsigned long long now)
{
 int i;
 unsigned long long delta, incr;

 if (timer->it.cpu.incr == 0)
  return;

 if (now < timer->it.cpu.expires)
  return;

 incr = timer->it.cpu.incr;
 delta = now + incr - timer->it.cpu.expires;


 for (i = 0; incr < delta - incr; i++)
  incr = incr << 1;

 for (; i >= 0; incr >>= 1, i--) {
  if (delta < incr)
   continue;

  timer->it.cpu.expires += incr;
  timer->it_overrun += 1 << i;
  delta -= incr;
 }
}
static inline int task_cputime_zero(const struct task_cputime *cputime)
{
 if (!cputime->utime && !cputime->stime && !cputime->sum_exec_runtime)
  return 1;
 return 0;
}

static inline unsigned long long prof_ticks(struct task_struct *p)
{
 cputime_t utime, stime;

 task_cputime(p, &utime, &stime);

 return cputime_to_expires(utime + stime);
}
static inline unsigned long long virt_ticks(struct task_struct *p)
{
 cputime_t utime;

 task_cputime(p, &utime, NULL);

 return cputime_to_expires(utime);
}

static int
posix_cpu_clock_getres(const clockid_t which_clock, struct timespec *tp)
{
 int error = check_clock(which_clock);
 if (!error) {
  tp->tv_sec = 0;
  tp->tv_nsec = ((NSEC_PER_SEC + HZ - 1) / HZ);
  if (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {





   tp->tv_nsec = 1;
  }
 }
 return error;
}

static int
posix_cpu_clock_set(const clockid_t which_clock, const struct timespec *tp)
{




 int error = check_clock(which_clock);
 if (error == 0) {
  error = -EPERM;
 }
 return error;
}





static int cpu_clock_sample(const clockid_t which_clock, struct task_struct *p,
       unsigned long long *sample)
{
 switch (CPUCLOCK_WHICH(which_clock)) {
 default:
  return -EINVAL;
 case CPUCLOCK_PROF:
  *sample = prof_ticks(p);
  break;
 case CPUCLOCK_VIRT:
  *sample = virt_ticks(p);
  break;
 case CPUCLOCK_SCHED:
  *sample = task_sched_runtime(p);
  break;
 }
 return 0;
}





static inline void __update_gt_cputime(atomic64_t *cputime, u64 sum_cputime)
{
 u64 curr_cputime;
retry:
 curr_cputime = atomic64_read(cputime);
 if (sum_cputime > curr_cputime) {
  if (atomic64_cmpxchg(cputime, curr_cputime, sum_cputime) != curr_cputime)
   goto retry;
 }
}

static void update_gt_cputime(struct task_cputime_atomic *cputime_atomic, struct task_cputime *sum)
{
 __update_gt_cputime(&cputime_atomic->utime, sum->utime);
 __update_gt_cputime(&cputime_atomic->stime, sum->stime);
 __update_gt_cputime(&cputime_atomic->sum_exec_runtime, sum->sum_exec_runtime);
}


static inline void sample_cputime_atomic(struct task_cputime *times,
      struct task_cputime_atomic *atomic_times)
{
 times->utime = atomic64_read(&atomic_times->utime);
 times->stime = atomic64_read(&atomic_times->stime);
 times->sum_exec_runtime = atomic64_read(&atomic_times->sum_exec_runtime);
}

void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times)
{
 struct thread_group_cputimer *cputimer = &tsk->signal->cputimer;
 struct task_cputime sum;


 if (!READ_ONCE(cputimer->running)) {





  thread_group_cputime(tsk, &sum);
  update_gt_cputime(&cputimer->cputime_atomic, &sum);
  WRITE_ONCE(cputimer->running, true);
 }
 sample_cputime_atomic(times, &cputimer->cputime_atomic);
}






static int cpu_clock_sample_group(const clockid_t which_clock,
      struct task_struct *p,
      unsigned long long *sample)
{
 struct task_cputime cputime;

 switch (CPUCLOCK_WHICH(which_clock)) {
 default:
  return -EINVAL;
 case CPUCLOCK_PROF:
  thread_group_cputime(p, &cputime);
  *sample = cputime_to_expires(cputime.utime + cputime.stime);
  break;
 case CPUCLOCK_VIRT:
  thread_group_cputime(p, &cputime);
  *sample = cputime_to_expires(cputime.utime);
  break;
 case CPUCLOCK_SCHED:
  thread_group_cputime(p, &cputime);
  *sample = cputime.sum_exec_runtime;
  break;
 }
 return 0;
}

static int posix_cpu_clock_get_task(struct task_struct *tsk,
        const clockid_t which_clock,
        struct timespec *tp)
{
 int err = -EINVAL;
 unsigned long long rtn;

 if (CPUCLOCK_PERTHREAD(which_clock)) {
  if (same_thread_group(tsk, current))
   err = cpu_clock_sample(which_clock, tsk, &rtn);
 } else {
  if (tsk == current || thread_group_leader(tsk))
   err = cpu_clock_sample_group(which_clock, tsk, &rtn);
 }

 if (!err)
  sample_to_timespec(which_clock, rtn, tp);

 return err;
}


static int posix_cpu_clock_get(const clockid_t which_clock, struct timespec *tp)
{
 const pid_t pid = CPUCLOCK_PID(which_clock);
 int err = -EINVAL;

 if (pid == 0) {




  err = posix_cpu_clock_get_task(current, which_clock, tp);
 } else {




  struct task_struct *p;
  rcu_read_lock();
  p = find_task_by_vpid(pid);
  if (p)
   err = posix_cpu_clock_get_task(p, which_clock, tp);
  rcu_read_unlock();
 }

 return err;
}






static int posix_cpu_timer_create(struct k_itimer *new_timer)
{
 int ret = 0;
 const pid_t pid = CPUCLOCK_PID(new_timer->it_clock);
 struct task_struct *p;

 if (CPUCLOCK_WHICH(new_timer->it_clock) >= CPUCLOCK_MAX)
  return -EINVAL;

 INIT_LIST_HEAD(&new_timer->it.cpu.entry);

 rcu_read_lock();
 if (CPUCLOCK_PERTHREAD(new_timer->it_clock)) {
  if (pid == 0) {
   p = current;
  } else {
   p = find_task_by_vpid(pid);
   if (p && !same_thread_group(p, current))
    p = NULL;
  }
 } else {
  if (pid == 0) {
   p = current->group_leader;
  } else {
   p = find_task_by_vpid(pid);
   if (p && !has_group_leader_pid(p))
    p = NULL;
  }
 }
 new_timer->it.cpu.task = p;
 if (p) {
  get_task_struct(p);
 } else {
  ret = -EINVAL;
 }
 rcu_read_unlock();

 return ret;
}







static int posix_cpu_timer_del(struct k_itimer *timer)
{
 int ret = 0;
 unsigned long flags;
 struct sighand_struct *sighand;
 struct task_struct *p = timer->it.cpu.task;

 WARN_ON_ONCE(p == NULL);





 sighand = lock_task_sighand(p, &flags);
 if (unlikely(sighand == NULL)) {




  WARN_ON_ONCE(!list_empty(&timer->it.cpu.entry));
 } else {
  if (timer->it.cpu.firing)
   ret = TIMER_RETRY;
  else
   list_del(&timer->it.cpu.entry);

  unlock_task_sighand(p, &flags);
 }

 if (!ret)
  put_task_struct(p);

 return ret;
}

static void cleanup_timers_list(struct list_head *head)
{
 struct cpu_timer_list *timer, *next;

 list_for_each_entry_safe(timer, next, head, entry)
  list_del_init(&timer->entry);
}







static void cleanup_timers(struct list_head *head)
{
 cleanup_timers_list(head);
 cleanup_timers_list(++head);
 cleanup_timers_list(++head);
}






void posix_cpu_timers_exit(struct task_struct *tsk)
{
 add_device_randomness((const void*) &tsk->se.sum_exec_runtime,
      sizeof(unsigned long long));
 cleanup_timers(tsk->cpu_timers);

}
void posix_cpu_timers_exit_group(struct task_struct *tsk)
{
 cleanup_timers(tsk->signal->cpu_timers);
}

static inline int expires_gt(cputime_t expires, cputime_t new_exp)
{
 return expires == 0 || expires > new_exp;
}





static void arm_timer(struct k_itimer *timer)
{
 struct task_struct *p = timer->it.cpu.task;
 struct list_head *head, *listpos;
 struct task_cputime *cputime_expires;
 struct cpu_timer_list *const nt = &timer->it.cpu;
 struct cpu_timer_list *next;

 if (CPUCLOCK_PERTHREAD(timer->it_clock)) {
  head = p->cpu_timers;
  cputime_expires = &p->cputime_expires;
 } else {
  head = p->signal->cpu_timers;
  cputime_expires = &p->signal->cputime_expires;
 }
 head += CPUCLOCK_WHICH(timer->it_clock);

 listpos = head;
 list_for_each_entry(next, head, entry) {
  if (nt->expires < next->expires)
   break;
  listpos = &next->entry;
 }
 list_add(&nt->entry, listpos);

 if (listpos == head) {
  unsigned long long exp = nt->expires;
  switch (CPUCLOCK_WHICH(timer->it_clock)) {
  case CPUCLOCK_PROF:
   if (expires_gt(cputime_expires->prof_exp, expires_to_cputime(exp)))
    cputime_expires->prof_exp = expires_to_cputime(exp);
   break;
  case CPUCLOCK_VIRT:
   if (expires_gt(cputime_expires->virt_exp, expires_to_cputime(exp)))
    cputime_expires->virt_exp = expires_to_cputime(exp);
   break;
  case CPUCLOCK_SCHED:
   if (cputime_expires->sched_exp == 0 ||
       cputime_expires->sched_exp > exp)
    cputime_expires->sched_exp = exp;
   break;
  }
  if (CPUCLOCK_PERTHREAD(timer->it_clock))
   tick_dep_set_task(p, TICK_DEP_BIT_POSIX_TIMER);
  else
   tick_dep_set_signal(p->signal, TICK_DEP_BIT_POSIX_TIMER);
 }
}




static void cpu_timer_fire(struct k_itimer *timer)
{
 if ((timer->it_sigev_notify & ~SIGEV_THREAD_ID) == SIGEV_NONE) {



  timer->it.cpu.expires = 0;
 } else if (unlikely(timer->sigq == NULL)) {




  wake_up_process(timer->it_process);
  timer->it.cpu.expires = 0;
 } else if (timer->it.cpu.incr == 0) {



  posix_timer_event(timer, 0);
  timer->it.cpu.expires = 0;
 } else if (posix_timer_event(timer, ++timer->it_requeue_pending)) {






  posix_cpu_timer_schedule(timer);
 }
}






static int cpu_timer_sample_group(const clockid_t which_clock,
      struct task_struct *p,
      unsigned long long *sample)
{
 struct task_cputime cputime;

 thread_group_cputimer(p, &cputime);
 switch (CPUCLOCK_WHICH(which_clock)) {
 default:
  return -EINVAL;
 case CPUCLOCK_PROF:
  *sample = cputime_to_expires(cputime.utime + cputime.stime);
  break;
 case CPUCLOCK_VIRT:
  *sample = cputime_to_expires(cputime.utime);
  break;
 case CPUCLOCK_SCHED:
  *sample = cputime.sum_exec_runtime;
  break;
 }
 return 0;
}







static int posix_cpu_timer_set(struct k_itimer *timer, int timer_flags,
          struct itimerspec *new, struct itimerspec *old)
{
 unsigned long flags;
 struct sighand_struct *sighand;
 struct task_struct *p = timer->it.cpu.task;
 unsigned long long old_expires, new_expires, old_incr, val;
 int ret;

 WARN_ON_ONCE(p == NULL);

 new_expires = timespec_to_sample(timer->it_clock, &new->it_value);





 sighand = lock_task_sighand(p, &flags);




 if (unlikely(sighand == NULL)) {
  return -ESRCH;
 }




 WARN_ON_ONCE(!irqs_disabled());

 ret = 0;
 old_incr = timer->it.cpu.incr;
 old_expires = timer->it.cpu.expires;
 if (unlikely(timer->it.cpu.firing)) {
  timer->it.cpu.firing = -1;
  ret = TIMER_RETRY;
 } else
  list_del_init(&timer->it.cpu.entry);
 if (CPUCLOCK_PERTHREAD(timer->it_clock)) {
  cpu_clock_sample(timer->it_clock, p, &val);
 } else {
  cpu_timer_sample_group(timer->it_clock, p, &val);
 }

 if (old) {
  if (old_expires == 0) {
   old->it_value.tv_sec = 0;
   old->it_value.tv_nsec = 0;
  } else {
   bump_cpu_timer(timer, val);
   if (val < timer->it.cpu.expires) {
    old_expires = timer->it.cpu.expires - val;
    sample_to_timespec(timer->it_clock,
         old_expires,
         &old->it_value);
   } else {
    old->it_value.tv_nsec = 1;
    old->it_value.tv_sec = 0;
   }
  }
 }

 if (unlikely(ret)) {






  unlock_task_sighand(p, &flags);
  goto out;
 }

 if (new_expires != 0 && !(timer_flags & TIMER_ABSTIME)) {
  new_expires += val;
 }






 timer->it.cpu.expires = new_expires;
 if (new_expires != 0 && val < new_expires) {
  arm_timer(timer);
 }

 unlock_task_sighand(p, &flags);




 timer->it.cpu.incr = timespec_to_sample(timer->it_clock,
      &new->it_interval);






 timer->it_requeue_pending = (timer->it_requeue_pending + 2) &
  ~REQUEUE_PENDING;
 timer->it_overrun_last = 0;
 timer->it_overrun = -1;

 if (new_expires != 0 && !(val < new_expires)) {





  cpu_timer_fire(timer);
 }

 ret = 0;
 out:
 if (old) {
  sample_to_timespec(timer->it_clock,
       old_incr, &old->it_interval);
 }

 return ret;
}

static void posix_cpu_timer_get(struct k_itimer *timer, struct itimerspec *itp)
{
 unsigned long long now;
 struct task_struct *p = timer->it.cpu.task;

 WARN_ON_ONCE(p == NULL);




 sample_to_timespec(timer->it_clock,
      timer->it.cpu.incr, &itp->it_interval);

 if (timer->it.cpu.expires == 0) {
  itp->it_value.tv_sec = itp->it_value.tv_nsec = 0;
  return;
 }




 if (CPUCLOCK_PERTHREAD(timer->it_clock)) {
  cpu_clock_sample(timer->it_clock, p, &now);
 } else {
  struct sighand_struct *sighand;
  unsigned long flags;






  sighand = lock_task_sighand(p, &flags);
  if (unlikely(sighand == NULL)) {





   timer->it.cpu.expires = 0;
   sample_to_timespec(timer->it_clock, timer->it.cpu.expires,
        &itp->it_value);
  } else {
   cpu_timer_sample_group(timer->it_clock, p, &now);
   unlock_task_sighand(p, &flags);
  }
 }

 if (now < timer->it.cpu.expires) {
  sample_to_timespec(timer->it_clock,
       timer->it.cpu.expires - now,
       &itp->it_value);
 } else {




  itp->it_value.tv_nsec = 1;
  itp->it_value.tv_sec = 0;
 }
}

static unsigned long long
check_timers_list(struct list_head *timers,
    struct list_head *firing,
    unsigned long long curr)
{
 int maxfire = 20;

 while (!list_empty(timers)) {
  struct cpu_timer_list *t;

  t = list_first_entry(timers, struct cpu_timer_list, entry);

  if (!--maxfire || curr < t->expires)
   return t->expires;

  t->firing = 1;
  list_move_tail(&t->entry, firing);
 }

 return 0;
}






static void check_thread_timers(struct task_struct *tsk,
    struct list_head *firing)
{
 struct list_head *timers = tsk->cpu_timers;
 struct signal_struct *const sig = tsk->signal;
 struct task_cputime *tsk_expires = &tsk->cputime_expires;
 unsigned long long expires;
 unsigned long soft;





 if (task_cputime_zero(&tsk->cputime_expires))
  return;

 expires = check_timers_list(timers, firing, prof_ticks(tsk));
 tsk_expires->prof_exp = expires_to_cputime(expires);

 expires = check_timers_list(++timers, firing, virt_ticks(tsk));
 tsk_expires->virt_exp = expires_to_cputime(expires);

 tsk_expires->sched_exp = check_timers_list(++timers, firing,
         tsk->se.sum_exec_runtime);




 soft = READ_ONCE(sig->rlim[RLIMIT_RTTIME].rlim_cur);
 if (soft != RLIM_INFINITY) {
  unsigned long hard =
   READ_ONCE(sig->rlim[RLIMIT_RTTIME].rlim_max);

  if (hard != RLIM_INFINITY &&
      tsk->rt.timeout > DIV_ROUND_UP(hard, USEC_PER_SEC/HZ)) {




   __group_send_sig_info(SIGKILL, SEND_SIG_PRIV, tsk);
   return;
  }
  if (tsk->rt.timeout > DIV_ROUND_UP(soft, USEC_PER_SEC/HZ)) {



   if (soft < hard) {
    soft += USEC_PER_SEC;
    sig->rlim[RLIMIT_RTTIME].rlim_cur = soft;
   }
   printk(KERN_INFO
    "RT Watchdog Timeout: %s[%d]\n",
    tsk->comm, task_pid_nr(tsk));
   __group_send_sig_info(SIGXCPU, SEND_SIG_PRIV, tsk);
  }
 }
 if (task_cputime_zero(tsk_expires))
  tick_dep_clear_task(tsk, TICK_DEP_BIT_POSIX_TIMER);
}

static inline void stop_process_timers(struct signal_struct *sig)
{
 struct thread_group_cputimer *cputimer = &sig->cputimer;


 WRITE_ONCE(cputimer->running, false);
 tick_dep_clear_signal(sig, TICK_DEP_BIT_POSIX_TIMER);
}

static u32 onecputick;

static void check_cpu_itimer(struct task_struct *tsk, struct cpu_itimer *it,
        unsigned long long *expires,
        unsigned long long cur_time, int signo)
{
 if (!it->expires)
  return;

 if (cur_time >= it->expires) {
  if (it->incr) {
   it->expires += it->incr;
   it->error += it->incr_error;
   if (it->error >= onecputick) {
    it->expires -= cputime_one_jiffy;
    it->error -= onecputick;
   }
  } else {
   it->expires = 0;
  }

  trace_itimer_expire(signo == SIGPROF ?
        ITIMER_PROF : ITIMER_VIRTUAL,
        tsk->signal->leader_pid, cur_time);
  __group_send_sig_info(signo, SEND_SIG_PRIV, tsk);
 }

 if (it->expires && (!*expires || it->expires < *expires)) {
  *expires = it->expires;
 }
}






static void check_process_timers(struct task_struct *tsk,
     struct list_head *firing)
{
 struct signal_struct *const sig = tsk->signal;
 unsigned long long utime, ptime, virt_expires, prof_expires;
 unsigned long long sum_sched_runtime, sched_expires;
 struct list_head *timers = sig->cpu_timers;
 struct task_cputime cputime;
 unsigned long soft;





 if (!READ_ONCE(tsk->signal->cputimer.running))
  return;





 sig->cputimer.checking_timer = true;




 thread_group_cputimer(tsk, &cputime);
 utime = cputime_to_expires(cputime.utime);
 ptime = utime + cputime_to_expires(cputime.stime);
 sum_sched_runtime = cputime.sum_exec_runtime;

 prof_expires = check_timers_list(timers, firing, ptime);
 virt_expires = check_timers_list(++timers, firing, utime);
 sched_expires = check_timers_list(++timers, firing, sum_sched_runtime);




 check_cpu_itimer(tsk, &sig->it[CPUCLOCK_PROF], &prof_expires, ptime,
    SIGPROF);
 check_cpu_itimer(tsk, &sig->it[CPUCLOCK_VIRT], &virt_expires, utime,
    SIGVTALRM);
 soft = READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
 if (soft != RLIM_INFINITY) {
  unsigned long psecs = cputime_to_secs(ptime);
  unsigned long hard =
   READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_max);
  cputime_t x;
  if (psecs >= hard) {




   __group_send_sig_info(SIGKILL, SEND_SIG_PRIV, tsk);
   return;
  }
  if (psecs >= soft) {



   __group_send_sig_info(SIGXCPU, SEND_SIG_PRIV, tsk);
   if (soft < hard) {
    soft++;
    sig->rlim[RLIMIT_CPU].rlim_cur = soft;
   }
  }
  x = secs_to_cputime(soft);
  if (!prof_expires || x < prof_expires) {
   prof_expires = x;
  }
 }

 sig->cputime_expires.prof_exp = expires_to_cputime(prof_expires);
 sig->cputime_expires.virt_exp = expires_to_cputime(virt_expires);
 sig->cputime_expires.sched_exp = sched_expires;
 if (task_cputime_zero(&sig->cputime_expires))
  stop_process_timers(sig);

 sig->cputimer.checking_timer = false;
}





void posix_cpu_timer_schedule(struct k_itimer *timer)
{
 struct sighand_struct *sighand;
 unsigned long flags;
 struct task_struct *p = timer->it.cpu.task;
 unsigned long long now;

 WARN_ON_ONCE(p == NULL);




 if (CPUCLOCK_PERTHREAD(timer->it_clock)) {
  cpu_clock_sample(timer->it_clock, p, &now);
  bump_cpu_timer(timer, now);
  if (unlikely(p->exit_state))
   goto out;


  sighand = lock_task_sighand(p, &flags);
  if (!sighand)
   goto out;
 } else {




  sighand = lock_task_sighand(p, &flags);
  if (unlikely(sighand == NULL)) {




   timer->it.cpu.expires = 0;
   goto out;
  } else if (unlikely(p->exit_state) && thread_group_empty(p)) {
   unlock_task_sighand(p, &flags);

   goto out;
  }
  cpu_timer_sample_group(timer->it_clock, p, &now);
  bump_cpu_timer(timer, now);

 }




 WARN_ON_ONCE(!irqs_disabled());
 arm_timer(timer);
 unlock_task_sighand(p, &flags);

out:
 timer->it_overrun_last = timer->it_overrun;
 timer->it_overrun = -1;
 ++timer->it_requeue_pending;
}
static inline int task_cputime_expired(const struct task_cputime *sample,
     const struct task_cputime *expires)
{
 if (expires->utime && sample->utime >= expires->utime)
  return 1;
 if (expires->stime && sample->utime + sample->stime >= expires->stime)
  return 1;
 if (expires->sum_exec_runtime != 0 &&
     sample->sum_exec_runtime >= expires->sum_exec_runtime)
  return 1;
 return 0;
}
static inline int fastpath_timer_check(struct task_struct *tsk)
{
 struct signal_struct *sig;

 if (!task_cputime_zero(&tsk->cputime_expires)) {
  struct task_cputime task_sample;

  task_cputime(tsk, &task_sample.utime, &task_sample.stime);
  task_sample.sum_exec_runtime = tsk->se.sum_exec_runtime;
  if (task_cputime_expired(&task_sample, &tsk->cputime_expires))
   return 1;
 }

 sig = tsk->signal;
 if (READ_ONCE(sig->cputimer.running) &&
     !READ_ONCE(sig->cputimer.checking_timer)) {
  struct task_cputime group_sample;

  sample_cputime_atomic(&group_sample, &sig->cputimer.cputime_atomic);

  if (task_cputime_expired(&group_sample, &sig->cputime_expires))
   return 1;
 }

 return 0;
}






void run_posix_cpu_timers(struct task_struct *tsk)
{
 LIST_HEAD(firing);
 struct k_itimer *timer, *next;
 unsigned long flags;

 WARN_ON_ONCE(!irqs_disabled());





 if (!fastpath_timer_check(tsk))
  return;

 if (!lock_task_sighand(tsk, &flags))
  return;





 check_thread_timers(tsk, &firing);

 check_process_timers(tsk, &firing);
 unlock_task_sighand(tsk, &flags);







 list_for_each_entry_safe(timer, next, &firing, it.cpu.entry) {
  int cpu_firing;

  spin_lock(&timer->it_lock);
  list_del_init(&timer->it.cpu.entry);
  cpu_firing = timer->it.cpu.firing;
  timer->it.cpu.firing = 0;





  if (likely(cpu_firing >= 0))
   cpu_timer_fire(timer);
  spin_unlock(&timer->it_lock);
 }
}





void set_process_cpu_timer(struct task_struct *tsk, unsigned int clock_idx,
      cputime_t *newval, cputime_t *oldval)
{
 unsigned long long now;

 WARN_ON_ONCE(clock_idx == CPUCLOCK_SCHED);
 cpu_timer_sample_group(clock_idx, tsk, &now);

 if (oldval) {





  if (*oldval) {
   if (*oldval <= now) {

    *oldval = cputime_one_jiffy;
   } else {
    *oldval -= now;
   }
  }

  if (!*newval)
   return;
  *newval += now;
 }





 switch (clock_idx) {
 case CPUCLOCK_PROF:
  if (expires_gt(tsk->signal->cputime_expires.prof_exp, *newval))
   tsk->signal->cputime_expires.prof_exp = *newval;
  break;
 case CPUCLOCK_VIRT:
  if (expires_gt(tsk->signal->cputime_expires.virt_exp, *newval))
   tsk->signal->cputime_expires.virt_exp = *newval;
  break;
 }

 tick_dep_set_signal(tsk->signal, TICK_DEP_BIT_POSIX_TIMER);
}

static int do_cpu_nanosleep(const clockid_t which_clock, int flags,
       struct timespec *rqtp, struct itimerspec *it)
{
 struct k_itimer timer;
 int error;




 memset(&timer, 0, sizeof timer);
 spin_lock_init(&timer.it_lock);
 timer.it_clock = which_clock;
 timer.it_overrun = -1;
 error = posix_cpu_timer_create(&timer);
 timer.it_process = current;
 if (!error) {
  static struct itimerspec zero_it;

  memset(it, 0, sizeof *it);
  it->it_value = *rqtp;

  spin_lock_irq(&timer.it_lock);
  error = posix_cpu_timer_set(&timer, flags, it, NULL);
  if (error) {
   spin_unlock_irq(&timer.it_lock);
   return error;
  }

  while (!signal_pending(current)) {
   if (timer.it.cpu.expires == 0) {




    posix_cpu_timer_del(&timer);
    spin_unlock_irq(&timer.it_lock);
    return 0;
   }




   __set_current_state(TASK_INTERRUPTIBLE);
   spin_unlock_irq(&timer.it_lock);
   schedule();
   spin_lock_irq(&timer.it_lock);
  }




  sample_to_timespec(which_clock, timer.it.cpu.expires, rqtp);
  error = posix_cpu_timer_set(&timer, 0, &zero_it, it);
  if (!error) {



   posix_cpu_timer_del(&timer);
  }
  spin_unlock_irq(&timer.it_lock);

  while (error == TIMER_RETRY) {





   spin_lock_irq(&timer.it_lock);
   error = posix_cpu_timer_del(&timer);
   spin_unlock_irq(&timer.it_lock);
  }

  if ((it->it_value.tv_sec | it->it_value.tv_nsec) == 0) {



   return 0;
  }

  error = -ERESTART_RESTARTBLOCK;
 }

 return error;
}

static long posix_cpu_nsleep_restart(struct restart_block *restart_block);

static int posix_cpu_nsleep(const clockid_t which_clock, int flags,
       struct timespec *rqtp, struct timespec __user *rmtp)
{
 struct restart_block *restart_block = &current->restart_block;
 struct itimerspec it;
 int error;




 if (CPUCLOCK_PERTHREAD(which_clock) &&
     (CPUCLOCK_PID(which_clock) == 0 ||
      CPUCLOCK_PID(which_clock) == current->pid))
  return -EINVAL;

 error = do_cpu_nanosleep(which_clock, flags, rqtp, &it);

 if (error == -ERESTART_RESTARTBLOCK) {

  if (flags & TIMER_ABSTIME)
   return -ERESTARTNOHAND;



  if (rmtp && copy_to_user(rmtp, &it.it_value, sizeof *rmtp))
   return -EFAULT;

  restart_block->fn = posix_cpu_nsleep_restart;
  restart_block->nanosleep.clockid = which_clock;
  restart_block->nanosleep.rmtp = rmtp;
  restart_block->nanosleep.expires = timespec_to_ns(rqtp);
 }
 return error;
}

static long posix_cpu_nsleep_restart(struct restart_block *restart_block)
{
 clockid_t which_clock = restart_block->nanosleep.clockid;
 struct timespec t;
 struct itimerspec it;
 int error;

 t = ns_to_timespec(restart_block->nanosleep.expires);

 error = do_cpu_nanosleep(which_clock, TIMER_ABSTIME, &t, &it);

 if (error == -ERESTART_RESTARTBLOCK) {
  struct timespec __user *rmtp = restart_block->nanosleep.rmtp;



  if (rmtp && copy_to_user(rmtp, &it.it_value, sizeof *rmtp))
   return -EFAULT;

  restart_block->nanosleep.expires = timespec_to_ns(&t);
 }
 return error;

}


static int process_cpu_clock_getres(const clockid_t which_clock,
        struct timespec *tp)
{
 return posix_cpu_clock_getres(PROCESS_CLOCK, tp);
}
static int process_cpu_clock_get(const clockid_t which_clock,
     struct timespec *tp)
{
 return posix_cpu_clock_get(PROCESS_CLOCK, tp);
}
static int process_cpu_timer_create(struct k_itimer *timer)
{
 timer->it_clock = PROCESS_CLOCK;
 return posix_cpu_timer_create(timer);
}
static int process_cpu_nsleep(const clockid_t which_clock, int flags,
         struct timespec *rqtp,
         struct timespec __user *rmtp)
{
 return posix_cpu_nsleep(PROCESS_CLOCK, flags, rqtp, rmtp);
}
static long process_cpu_nsleep_restart(struct restart_block *restart_block)
{
 return -EINVAL;
}
static int thread_cpu_clock_getres(const clockid_t which_clock,
       struct timespec *tp)
{
 return posix_cpu_clock_getres(THREAD_CLOCK, tp);
}
static int thread_cpu_clock_get(const clockid_t which_clock,
    struct timespec *tp)
{
 return posix_cpu_clock_get(THREAD_CLOCK, tp);
}
static int thread_cpu_timer_create(struct k_itimer *timer)
{
 timer->it_clock = THREAD_CLOCK;
 return posix_cpu_timer_create(timer);
}

struct k_clock clock_posix_cpu = {
 .clock_getres = posix_cpu_clock_getres,
 .clock_set = posix_cpu_clock_set,
 .clock_get = posix_cpu_clock_get,
 .timer_create = posix_cpu_timer_create,
 .nsleep = posix_cpu_nsleep,
 .nsleep_restart = posix_cpu_nsleep_restart,
 .timer_set = posix_cpu_timer_set,
 .timer_del = posix_cpu_timer_del,
 .timer_get = posix_cpu_timer_get,
};

static __init int init_posix_cpu_timers(void)
{
 struct k_clock process = {
  .clock_getres = process_cpu_clock_getres,
  .clock_get = process_cpu_clock_get,
  .timer_create = process_cpu_timer_create,
  .nsleep = process_cpu_nsleep,
  .nsleep_restart = process_cpu_nsleep_restart,
 };
 struct k_clock thread = {
  .clock_getres = thread_cpu_clock_getres,
  .clock_get = thread_cpu_clock_get,
  .timer_create = thread_cpu_timer_create,
 };
 struct timespec ts;

 posix_timers_register_clock(CLOCK_PROCESS_CPUTIME_ID, &process);
 posix_timers_register_clock(CLOCK_THREAD_CPUTIME_ID, &thread);

 cputime_to_timespec(cputime_one_jiffy, &ts);
 onecputick = ts.tv_nsec;
 WARN_ON(ts.tv_sec != 0);

 return 0;
}
__initcall(init_posix_cpu_timers);


static struct kmem_cache *posix_timers_cache;

static DEFINE_HASHTABLE(posix_timers_hashtable, 9);
static DEFINE_SPINLOCK(hash_lock);





                       ~(SIGEV_SIGNAL | SIGEV_NONE | SIGEV_THREAD))




static struct k_clock posix_clocks[MAX_CLOCKS];




static int common_nsleep(const clockid_t, int flags, struct timespec *t,
    struct timespec __user *rmtp);
static int common_timer_create(struct k_itimer *new_timer);
static void common_timer_get(struct k_itimer *, struct itimerspec *);
static int common_timer_set(struct k_itimer *, int,
       struct itimerspec *, struct itimerspec *);
static int common_timer_del(struct k_itimer *timer);

static enum hrtimer_restart posix_timer_fn(struct hrtimer *data);

static struct k_itimer *__lock_timer(timer_t timer_id, unsigned long *flags);

({ struct k_itimer *__timr; \
 __cond_lock(&__timr->it_lock, __timr = __lock_timer(tid, flags)); \
 __timr; \
})

static int hash(struct signal_struct *sig, unsigned int nr)
{
 return hash_32(hash32_ptr(sig) ^ nr, HASH_BITS(posix_timers_hashtable));
}

static struct k_itimer *__posix_timers_find(struct hlist_head *head,
         struct signal_struct *sig,
         timer_t id)
{
 struct k_itimer *timer;

 hlist_for_each_entry_rcu(timer, head, t_hash) {
  if ((timer->it_signal == sig) && (timer->it_id == id))
   return timer;
 }
 return NULL;
}

static struct k_itimer *posix_timer_by_id(timer_t id)
{
 struct signal_struct *sig = current->signal;
 struct hlist_head *head = &posix_timers_hashtable[hash(sig, id)];

 return __posix_timers_find(head, sig, id);
}

static int posix_timer_add(struct k_itimer *timer)
{
 struct signal_struct *sig = current->signal;
 int first_free_id = sig->posix_timer_id;
 struct hlist_head *head;
 int ret = -ENOENT;

 do {
  spin_lock(&hash_lock);
  head = &posix_timers_hashtable[hash(sig, sig->posix_timer_id)];
  if (!__posix_timers_find(head, sig, sig->posix_timer_id)) {
   hlist_add_head_rcu(&timer->t_hash, head);
   ret = sig->posix_timer_id;
  }
  if (++sig->posix_timer_id < 0)
   sig->posix_timer_id = 0;
  if ((sig->posix_timer_id == first_free_id) && (ret == -ENOENT))

   ret = -EAGAIN;
  spin_unlock(&hash_lock);
 } while (ret == -ENOENT);
 return ret;
}

static inline void unlock_timer(struct k_itimer *timr, unsigned long flags)
{
 spin_unlock_irqrestore(&timr->it_lock, flags);
}


static int posix_clock_realtime_get(clockid_t which_clock, struct timespec *tp)
{
 ktime_get_real_ts(tp);
 return 0;
}


static int posix_clock_realtime_set(const clockid_t which_clock,
        const struct timespec *tp)
{
 return do_sys_settimeofday(tp, NULL);
}

static int posix_clock_realtime_adj(const clockid_t which_clock,
        struct timex *t)
{
 return do_adjtimex(t);
}




static int posix_ktime_get_ts(clockid_t which_clock, struct timespec *tp)
{
 ktime_get_ts(tp);
 return 0;
}




static int posix_get_monotonic_raw(clockid_t which_clock, struct timespec *tp)
{
 getrawmonotonic(tp);
 return 0;
}


static int posix_get_realtime_coarse(clockid_t which_clock, struct timespec *tp)
{
 *tp = current_kernel_time();
 return 0;
}

static int posix_get_monotonic_coarse(clockid_t which_clock,
      struct timespec *tp)
{
 *tp = get_monotonic_coarse();
 return 0;
}

static int posix_get_coarse_res(const clockid_t which_clock, struct timespec *tp)
{
 *tp = ktime_to_timespec(KTIME_LOW_RES);
 return 0;
}

static int posix_get_boottime(const clockid_t which_clock, struct timespec *tp)
{
 get_monotonic_boottime(tp);
 return 0;
}

static int posix_get_tai(clockid_t which_clock, struct timespec *tp)
{
 timekeeping_clocktai(tp);
 return 0;
}

static int posix_get_hrtimer_res(clockid_t which_clock, struct timespec *tp)
{
 tp->tv_sec = 0;
 tp->tv_nsec = hrtimer_resolution;
 return 0;
}




static __init int init_posix_timers(void)
{
 struct k_clock clock_realtime = {
  .clock_getres = posix_get_hrtimer_res,
  .clock_get = posix_clock_realtime_get,
  .clock_set = posix_clock_realtime_set,
  .clock_adj = posix_clock_realtime_adj,
  .nsleep = common_nsleep,
  .nsleep_restart = hrtimer_nanosleep_restart,
  .timer_create = common_timer_create,
  .timer_set = common_timer_set,
  .timer_get = common_timer_get,
  .timer_del = common_timer_del,
 };
 struct k_clock clock_monotonic = {
  .clock_getres = posix_get_hrtimer_res,
  .clock_get = posix_ktime_get_ts,
  .nsleep = common_nsleep,
  .nsleep_restart = hrtimer_nanosleep_restart,
  .timer_create = common_timer_create,
  .timer_set = common_timer_set,
  .timer_get = common_timer_get,
  .timer_del = common_timer_del,
 };
 struct k_clock clock_monotonic_raw = {
  .clock_getres = posix_get_hrtimer_res,
  .clock_get = posix_get_monotonic_raw,
 };
 struct k_clock clock_realtime_coarse = {
  .clock_getres = posix_get_coarse_res,
  .clock_get = posix_get_realtime_coarse,
 };
 struct k_clock clock_monotonic_coarse = {
  .clock_getres = posix_get_coarse_res,
  .clock_get = posix_get_monotonic_coarse,
 };
 struct k_clock clock_tai = {
  .clock_getres = posix_get_hrtimer_res,
  .clock_get = posix_get_tai,
  .nsleep = common_nsleep,
  .nsleep_restart = hrtimer_nanosleep_restart,
  .timer_create = common_timer_create,
  .timer_set = common_timer_set,
  .timer_get = common_timer_get,
  .timer_del = common_timer_del,
 };
 struct k_clock clock_boottime = {
  .clock_getres = posix_get_hrtimer_res,
  .clock_get = posix_get_boottime,
  .nsleep = common_nsleep,
  .nsleep_restart = hrtimer_nanosleep_restart,
  .timer_create = common_timer_create,
  .timer_set = common_timer_set,
  .timer_get = common_timer_get,
  .timer_del = common_timer_del,
 };

 posix_timers_register_clock(CLOCK_REALTIME, &clock_realtime);
 posix_timers_register_clock(CLOCK_MONOTONIC, &clock_monotonic);
 posix_timers_register_clock(CLOCK_MONOTONIC_RAW, &clock_monotonic_raw);
 posix_timers_register_clock(CLOCK_REALTIME_COARSE, &clock_realtime_coarse);
 posix_timers_register_clock(CLOCK_MONOTONIC_COARSE, &clock_monotonic_coarse);
 posix_timers_register_clock(CLOCK_BOOTTIME, &clock_boottime);
 posix_timers_register_clock(CLOCK_TAI, &clock_tai);

 posix_timers_cache = kmem_cache_create("posix_timers_cache",
     sizeof (struct k_itimer), 0, SLAB_PANIC,
     NULL);
 return 0;
}

__initcall(init_posix_timers);

static void schedule_next_timer(struct k_itimer *timr)
{
 struct hrtimer *timer = &timr->it.real.timer;

 if (timr->it.real.interval.tv64 == 0)
  return;

 timr->it_overrun += (unsigned int) hrtimer_forward(timer,
      timer->base->get_time(),
      timr->it.real.interval);

 timr->it_overrun_last = timr->it_overrun;
 timr->it_overrun = -1;
 ++timr->it_requeue_pending;
 hrtimer_restart(timer);
}
void do_schedule_next_timer(struct siginfo *info)
{
 struct k_itimer *timr;
 unsigned long flags;

 timr = lock_timer(info->si_tid, &flags);

 if (timr && timr->it_requeue_pending == info->si_sys_private) {
  if (timr->it_clock < 0)
   posix_cpu_timer_schedule(timr);
  else
   schedule_next_timer(timr);

  info->si_overrun += timr->it_overrun_last;
 }

 if (timr)
  unlock_timer(timr, flags);
}

int posix_timer_event(struct k_itimer *timr, int si_private)
{
 struct task_struct *task;
 int shared, ret = -1;
 timr->sigq->info.si_sys_private = si_private;

 rcu_read_lock();
 task = pid_task(timr->it_pid, PIDTYPE_PID);
 if (task) {
  shared = !(timr->it_sigev_notify & SIGEV_THREAD_ID);
  ret = send_sigqueue(timr->sigq, task, shared);
 }
 rcu_read_unlock();

 return ret > 0;
}
EXPORT_SYMBOL_GPL(posix_timer_event);
static enum hrtimer_restart posix_timer_fn(struct hrtimer *timer)
{
 struct k_itimer *timr;
 unsigned long flags;
 int si_private = 0;
 enum hrtimer_restart ret = HRTIMER_NORESTART;

 timr = container_of(timer, struct k_itimer, it.real.timer);
 spin_lock_irqsave(&timr->it_lock, flags);

 if (timr->it.real.interval.tv64 != 0)
  si_private = ++timr->it_requeue_pending;

 if (posix_timer_event(timr, si_private)) {





  if (timr->it.real.interval.tv64 != 0) {
   ktime_t now = hrtimer_cb_get_time(timer);
   {
    ktime_t kj = ktime_set(0, NSEC_PER_SEC / HZ);

    if (timr->it.real.interval.tv64 < kj.tv64)
     now = ktime_add(now, kj);
   }
   timr->it_overrun += (unsigned int)
    hrtimer_forward(timer, now,
      timr->it.real.interval);
   ret = HRTIMER_RESTART;
   ++timr->it_requeue_pending;
  }
 }

 unlock_timer(timr, flags);
 return ret;
}

static struct pid *good_sigevent(sigevent_t * event)
{
 struct task_struct *rtn = current->group_leader;

 if ((event->sigev_notify & SIGEV_THREAD_ID ) &&
  (!(rtn = find_task_by_vpid(event->sigev_notify_thread_id)) ||
   !same_thread_group(rtn, current) ||
   (event->sigev_notify & ~SIGEV_THREAD_ID) != SIGEV_SIGNAL))
  return NULL;

 if (((event->sigev_notify & ~SIGEV_THREAD_ID) != SIGEV_NONE) &&
     ((event->sigev_signo <= 0) || (event->sigev_signo > SIGRTMAX)))
  return NULL;

 return task_pid(rtn);
}

void posix_timers_register_clock(const clockid_t clock_id,
     struct k_clock *new_clock)
{
 if ((unsigned) clock_id >= MAX_CLOCKS) {
  printk(KERN_WARNING "POSIX clock register failed for clock_id %d\n",
         clock_id);
  return;
 }

 if (!new_clock->clock_get) {
  printk(KERN_WARNING "POSIX clock id %d lacks clock_get()\n",
         clock_id);
  return;
 }
 if (!new_clock->clock_getres) {
  printk(KERN_WARNING "POSIX clock id %d lacks clock_getres()\n",
         clock_id);
  return;
 }

 posix_clocks[clock_id] = *new_clock;
}
EXPORT_SYMBOL_GPL(posix_timers_register_clock);

static struct k_itimer * alloc_posix_timer(void)
{
 struct k_itimer *tmr;
 tmr = kmem_cache_zalloc(posix_timers_cache, GFP_KERNEL);
 if (!tmr)
  return tmr;
 if (unlikely(!(tmr->sigq = sigqueue_alloc()))) {
  kmem_cache_free(posix_timers_cache, tmr);
  return NULL;
 }
 memset(&tmr->sigq->info, 0, sizeof(siginfo_t));
 return tmr;
}

static void k_itimer_rcu_free(struct rcu_head *head)
{
 struct k_itimer *tmr = container_of(head, struct k_itimer, it.rcu);

 kmem_cache_free(posix_timers_cache, tmr);
}

static void release_posix_timer(struct k_itimer *tmr, int it_id_set)
{
 if (it_id_set) {
  unsigned long flags;
  spin_lock_irqsave(&hash_lock, flags);
  hlist_del_rcu(&tmr->t_hash);
  spin_unlock_irqrestore(&hash_lock, flags);
 }
 put_pid(tmr->it_pid);
 sigqueue_free(tmr->sigq);
 call_rcu(&tmr->it.rcu, k_itimer_rcu_free);
}

static struct k_clock *clockid_to_kclock(const clockid_t id)
{
 if (id < 0)
  return (id & CLOCKFD_MASK) == CLOCKFD ?
   &clock_posix_dynamic : &clock_posix_cpu;

 if (id >= MAX_CLOCKS || !posix_clocks[id].clock_getres)
  return NULL;
 return &posix_clocks[id];
}

static int common_timer_create(struct k_itimer *new_timer)
{
 hrtimer_init(&new_timer->it.real.timer, new_timer->it_clock, 0);
 return 0;
}



SYSCALL_DEFINE3(timer_create, const clockid_t, which_clock,
  struct sigevent __user *, timer_event_spec,
  timer_t __user *, created_timer_id)
{
 struct k_clock *kc = clockid_to_kclock(which_clock);
 struct k_itimer *new_timer;
 int error, new_timer_id;
 sigevent_t event;
 int it_id_set = IT_ID_NOT_SET;

 if (!kc)
  return -EINVAL;
 if (!kc->timer_create)
  return -EOPNOTSUPP;

 new_timer = alloc_posix_timer();
 if (unlikely(!new_timer))
  return -EAGAIN;

 spin_lock_init(&new_timer->it_lock);
 new_timer_id = posix_timer_add(new_timer);
 if (new_timer_id < 0) {
  error = new_timer_id;
  goto out;
 }

 it_id_set = IT_ID_SET;
 new_timer->it_id = (timer_t) new_timer_id;
 new_timer->it_clock = which_clock;
 new_timer->it_overrun = -1;

 if (timer_event_spec) {
  if (copy_from_user(&event, timer_event_spec, sizeof (event))) {
   error = -EFAULT;
   goto out;
  }
  rcu_read_lock();
  new_timer->it_pid = get_pid(good_sigevent(&event));
  rcu_read_unlock();
  if (!new_timer->it_pid) {
   error = -EINVAL;
   goto out;
  }
 } else {
  memset(&event.sigev_value, 0, sizeof(event.sigev_value));
  event.sigev_notify = SIGEV_SIGNAL;
  event.sigev_signo = SIGALRM;
  event.sigev_value.sival_int = new_timer->it_id;
  new_timer->it_pid = get_pid(task_tgid(current));
 }

 new_timer->it_sigev_notify = event.sigev_notify;
 new_timer->sigq->info.si_signo = event.sigev_signo;
 new_timer->sigq->info.si_value = event.sigev_value;
 new_timer->sigq->info.si_tid = new_timer->it_id;
 new_timer->sigq->info.si_code = SI_TIMER;

 if (copy_to_user(created_timer_id,
    &new_timer_id, sizeof (new_timer_id))) {
  error = -EFAULT;
  goto out;
 }

 error = kc->timer_create(new_timer);
 if (error)
  goto out;

 spin_lock_irq(&current->sighand->siglock);
 new_timer->it_signal = current->signal;
 list_add(&new_timer->list, &current->signal->posix_timers);
 spin_unlock_irq(&current->sighand->siglock);

 return 0;






out:
 release_posix_timer(new_timer, it_id_set);
 return error;
}
static struct k_itimer *__lock_timer(timer_t timer_id, unsigned long *flags)
{
 struct k_itimer *timr;





 if ((unsigned long long)timer_id > INT_MAX)
  return NULL;

 rcu_read_lock();
 timr = posix_timer_by_id(timer_id);
 if (timr) {
  spin_lock_irqsave(&timr->it_lock, *flags);
  if (timr->it_signal == current->signal) {
   rcu_read_unlock();
   return timr;
  }
  spin_unlock_irqrestore(&timr->it_lock, *flags);
 }
 rcu_read_unlock();

 return NULL;
}
static void
common_timer_get(struct k_itimer *timr, struct itimerspec *cur_setting)
{
 ktime_t now, remaining, iv;
 struct hrtimer *timer = &timr->it.real.timer;

 memset(cur_setting, 0, sizeof(struct itimerspec));

 iv = timr->it.real.interval;


 if (iv.tv64)
  cur_setting->it_interval = ktime_to_timespec(iv);
 else if (!hrtimer_active(timer) &&
   (timr->it_sigev_notify & ~SIGEV_THREAD_ID) != SIGEV_NONE)
  return;

 now = timer->base->get_time();






 if (iv.tv64 && (timr->it_requeue_pending & REQUEUE_PENDING ||
     (timr->it_sigev_notify & ~SIGEV_THREAD_ID) == SIGEV_NONE))
  timr->it_overrun += (unsigned int) hrtimer_forward(timer, now, iv);

 remaining = __hrtimer_expires_remaining_adjusted(timer, now);

 if (remaining.tv64 <= 0) {




  if ((timr->it_sigev_notify & ~SIGEV_THREAD_ID) != SIGEV_NONE)
   cur_setting->it_value.tv_nsec = 1;
 } else
  cur_setting->it_value = ktime_to_timespec(remaining);
}


SYSCALL_DEFINE2(timer_gettime, timer_t, timer_id,
  struct itimerspec __user *, setting)
{
 struct itimerspec cur_setting;
 struct k_itimer *timr;
 struct k_clock *kc;
 unsigned long flags;
 int ret = 0;

 timr = lock_timer(timer_id, &flags);
 if (!timr)
  return -EINVAL;

 kc = clockid_to_kclock(timr->it_clock);
 if (WARN_ON_ONCE(!kc || !kc->timer_get))
  ret = -EINVAL;
 else
  kc->timer_get(timr, &cur_setting);

 unlock_timer(timr, flags);

 if (!ret && copy_to_user(setting, &cur_setting, sizeof (cur_setting)))
  return -EFAULT;

 return ret;
}
SYSCALL_DEFINE1(timer_getoverrun, timer_t, timer_id)
{
 struct k_itimer *timr;
 int overrun;
 unsigned long flags;

 timr = lock_timer(timer_id, &flags);
 if (!timr)
  return -EINVAL;

 overrun = timr->it_overrun_last;
 unlock_timer(timr, flags);

 return overrun;
}



static int
common_timer_set(struct k_itimer *timr, int flags,
   struct itimerspec *new_setting, struct itimerspec *old_setting)
{
 struct hrtimer *timer = &timr->it.real.timer;
 enum hrtimer_mode mode;

 if (old_setting)
  common_timer_get(timr, old_setting);


 timr->it.real.interval.tv64 = 0;




 if (hrtimer_try_to_cancel(timer) < 0)
  return TIMER_RETRY;

 timr->it_requeue_pending = (timr->it_requeue_pending + 2) &
  ~REQUEUE_PENDING;
 timr->it_overrun_last = 0;


 if (!new_setting->it_value.tv_sec && !new_setting->it_value.tv_nsec)
  return 0;

 mode = flags & TIMER_ABSTIME ? HRTIMER_MODE_ABS : HRTIMER_MODE_REL;
 hrtimer_init(&timr->it.real.timer, timr->it_clock, mode);
 timr->it.real.timer.function = posix_timer_fn;

 hrtimer_set_expires(timer, timespec_to_ktime(new_setting->it_value));


 timr->it.real.interval = timespec_to_ktime(new_setting->it_interval);


 if (((timr->it_sigev_notify & ~SIGEV_THREAD_ID) == SIGEV_NONE)) {

  if (mode == HRTIMER_MODE_REL) {
   hrtimer_add_expires(timer, timer->base->get_time());
  }
  return 0;
 }

 hrtimer_start_expires(timer, mode);
 return 0;
}


SYSCALL_DEFINE4(timer_settime, timer_t, timer_id, int, flags,
  const struct itimerspec __user *, new_setting,
  struct itimerspec __user *, old_setting)
{
 struct k_itimer *timr;
 struct itimerspec new_spec, old_spec;
 int error = 0;
 unsigned long flag;
 struct itimerspec *rtn = old_setting ? &old_spec : NULL;
 struct k_clock *kc;

 if (!new_setting)
  return -EINVAL;

 if (copy_from_user(&new_spec, new_setting, sizeof (new_spec)))
  return -EFAULT;

 if (!timespec_valid(&new_spec.it_interval) ||
     !timespec_valid(&new_spec.it_value))
  return -EINVAL;
retry:
 timr = lock_timer(timer_id, &flag);
 if (!timr)
  return -EINVAL;

 kc = clockid_to_kclock(timr->it_clock);
 if (WARN_ON_ONCE(!kc || !kc->timer_set))
  error = -EINVAL;
 else
  error = kc->timer_set(timr, flags, &new_spec, rtn);

 unlock_timer(timr, flag);
 if (error == TIMER_RETRY) {
  rtn = NULL;
  goto retry;
 }

 if (old_setting && !error &&
     copy_to_user(old_setting, &old_spec, sizeof (old_spec)))
  error = -EFAULT;

 return error;
}

static int common_timer_del(struct k_itimer *timer)
{
 timer->it.real.interval.tv64 = 0;

 if (hrtimer_try_to_cancel(&timer->it.real.timer) < 0)
  return TIMER_RETRY;
 return 0;
}

static inline int timer_delete_hook(struct k_itimer *timer)
{
 struct k_clock *kc = clockid_to_kclock(timer->it_clock);

 if (WARN_ON_ONCE(!kc || !kc->timer_del))
  return -EINVAL;
 return kc->timer_del(timer);
}


SYSCALL_DEFINE1(timer_delete, timer_t, timer_id)
{
 struct k_itimer *timer;
 unsigned long flags;

retry_delete:
 timer = lock_timer(timer_id, &flags);
 if (!timer)
  return -EINVAL;

 if (timer_delete_hook(timer) == TIMER_RETRY) {
  unlock_timer(timer, flags);
  goto retry_delete;
 }

 spin_lock(&current->sighand->siglock);
 list_del(&timer->list);
 spin_unlock(&current->sighand->siglock);




 timer->it_signal = NULL;

 unlock_timer(timer, flags);
 release_posix_timer(timer, IT_ID_SET);
 return 0;
}




static void itimer_delete(struct k_itimer *timer)
{
 unsigned long flags;

retry_delete:
 spin_lock_irqsave(&timer->it_lock, flags);

 if (timer_delete_hook(timer) == TIMER_RETRY) {
  unlock_timer(timer, flags);
  goto retry_delete;
 }
 list_del(&timer->list);




 timer->it_signal = NULL;

 unlock_timer(timer, flags);
 release_posix_timer(timer, IT_ID_SET);
}





void exit_itimers(struct signal_struct *sig)
{
 struct k_itimer *tmr;

 while (!list_empty(&sig->posix_timers)) {
  tmr = list_entry(sig->posix_timers.next, struct k_itimer, list);
  itimer_delete(tmr);
 }
}

SYSCALL_DEFINE2(clock_settime, const clockid_t, which_clock,
  const struct timespec __user *, tp)
{
 struct k_clock *kc = clockid_to_kclock(which_clock);
 struct timespec new_tp;

 if (!kc || !kc->clock_set)
  return -EINVAL;

 if (copy_from_user(&new_tp, tp, sizeof (*tp)))
  return -EFAULT;

 return kc->clock_set(which_clock, &new_tp);
}

SYSCALL_DEFINE2(clock_gettime, const clockid_t, which_clock,
  struct timespec __user *,tp)
{
 struct k_clock *kc = clockid_to_kclock(which_clock);
 struct timespec kernel_tp;
 int error;

 if (!kc)
  return -EINVAL;

 error = kc->clock_get(which_clock, &kernel_tp);

 if (!error && copy_to_user(tp, &kernel_tp, sizeof (kernel_tp)))
  error = -EFAULT;

 return error;
}

SYSCALL_DEFINE2(clock_adjtime, const clockid_t, which_clock,
  struct timex __user *, utx)
{
 struct k_clock *kc = clockid_to_kclock(which_clock);
 struct timex ktx;
 int err;

 if (!kc)
  return -EINVAL;
 if (!kc->clock_adj)
  return -EOPNOTSUPP;

 if (copy_from_user(&ktx, utx, sizeof(ktx)))
  return -EFAULT;

 err = kc->clock_adj(which_clock, &ktx);

 if (err >= 0 && copy_to_user(utx, &ktx, sizeof(ktx)))
  return -EFAULT;

 return err;
}

SYSCALL_DEFINE2(clock_getres, const clockid_t, which_clock,
  struct timespec __user *, tp)
{
 struct k_clock *kc = clockid_to_kclock(which_clock);
 struct timespec rtn_tp;
 int error;

 if (!kc)
  return -EINVAL;

 error = kc->clock_getres(which_clock, &rtn_tp);

 if (!error && tp && copy_to_user(tp, &rtn_tp, sizeof (rtn_tp)))
  error = -EFAULT;

 return error;
}




static int common_nsleep(const clockid_t which_clock, int flags,
    struct timespec *tsave, struct timespec __user *rmtp)
{
 return hrtimer_nanosleep(tsave, rmtp, flags & TIMER_ABSTIME ?
     HRTIMER_MODE_ABS : HRTIMER_MODE_REL,
     which_clock);
}

SYSCALL_DEFINE4(clock_nanosleep, const clockid_t, which_clock, int, flags,
  const struct timespec __user *, rqtp,
  struct timespec __user *, rmtp)
{
 struct k_clock *kc = clockid_to_kclock(which_clock);
 struct timespec t;

 if (!kc)
  return -EINVAL;
 if (!kc->nsleep)
  return -ENANOSLEEP_NOTSUP;

 if (copy_from_user(&t, rqtp, sizeof (struct timespec)))
  return -EFAULT;

 if (!timespec_valid(&t))
  return -EINVAL;

 return kc->nsleep(which_clock, flags, &t, rmtp);
}





long clock_nanosleep_restart(struct restart_block *restart_block)
{
 clockid_t which_clock = restart_block->nanosleep.clockid;
 struct k_clock *kc = clockid_to_kclock(which_clock);

 if (WARN_ON_ONCE(!kc || !kc->nsleep_restart))
  return -EINVAL;

 return kc->nsleep_restart(restart_block);
}












static void do_poweroff(struct work_struct *dummy)
{
 kernel_power_off();
}

static DECLARE_WORK(poweroff_work, do_poweroff);

static void handle_poweroff(int key)
{

 schedule_work_on(cpumask_first(cpu_online_mask), &poweroff_work);
}

static struct sysrq_key_op sysrq_poweroff_op = {
 .handler = handle_poweroff,
 .help_msg = "poweroff(o)",
 .action_msg = "Power Off",
 .enable_mask = SYSRQ_ENABLE_BOOT,
};

static int __init pm_sysrq_init(void)
{
 register_sysrq_key('o', &sysrq_poweroff_op);
 return 0;
}

subsys_initcall(pm_sysrq_init);




int console_printk[4] = {
 CONSOLE_LOGLEVEL_DEFAULT,
 MESSAGE_LOGLEVEL_DEFAULT,
 CONSOLE_LOGLEVEL_MIN,
 CONSOLE_LOGLEVEL_DEFAULT,
};





int oops_in_progress;
EXPORT_SYMBOL(oops_in_progress);






static DEFINE_SEMAPHORE(console_sem);
struct console *console_drivers;
EXPORT_SYMBOL_GPL(console_drivers);

static struct lockdep_map console_lock_dep_map = {
 .name = "console_lock"
};
static int nr_ext_console_drivers;





 down(&console_sem);\
 mutex_acquire(&console_lock_dep_map, 0, 0, _RET_IP_);\
} while (0)

static int __down_trylock_console_sem(unsigned long ip)
{
 if (down_trylock(&console_sem))
  return 1;
 mutex_acquire(&console_lock_dep_map, 0, 1, ip);
 return 0;
}

 mutex_release(&console_lock_dep_map, 1, _RET_IP_);\
 up(&console_sem);\
} while (0)
static int console_locked, console_suspended;




static struct console *exclusive_console;






static struct console_cmdline console_cmdline[MAX_CMDLINECONSOLES];

static int selected_console = -1;
static int preferred_console = -1;
int console_set_on_cmdline;
EXPORT_SYMBOL(console_set_on_cmdline);


static int console_may_schedule;
enum log_flags {
 LOG_NOCONS = 1,
 LOG_NEWLINE = 2,
 LOG_PREFIX = 4,
 LOG_CONT = 8,
};

struct printk_log {
 u64 ts_nsec;
 u16 len;
 u16 text_len;
 u16 dict_len;
 u8 facility;
 u8 flags:5;
 u8 level:3;
}
__packed __aligned(4)
;






DEFINE_RAW_SPINLOCK(logbuf_lock);

DECLARE_WAIT_QUEUE_HEAD(log_wait);

static u64 syslog_seq;
static u32 syslog_idx;
static enum log_flags syslog_prev;
static size_t syslog_partial;


static u64 log_first_seq;
static u32 log_first_idx;


static u64 log_next_seq;
static u32 log_next_idx;


static u64 console_seq;
static u32 console_idx;
static enum log_flags console_prev;


static u64 clear_seq;
static u32 clear_idx;




static char __log_buf[__LOG_BUF_LEN] __aligned(LOG_ALIGN);
static char *log_buf = __log_buf;
static u32 log_buf_len = __LOG_BUF_LEN;


char *log_buf_addr_get(void)
{
 return log_buf;
}


u32 log_buf_len_get(void)
{
 return log_buf_len;
}


static char *log_text(const struct printk_log *msg)
{
 return (char *)msg + sizeof(struct printk_log);
}


static char *log_dict(const struct printk_log *msg)
{
 return (char *)msg + sizeof(struct printk_log) + msg->text_len;
}


static struct printk_log *log_from_idx(u32 idx)
{
 struct printk_log *msg = (struct printk_log *)(log_buf + idx);





 if (!msg->len)
  return (struct printk_log *)log_buf;
 return msg;
}


static u32 log_next(u32 idx)
{
 struct printk_log *msg = (struct printk_log *)(log_buf + idx);







 if (!msg->len) {
  msg = (struct printk_log *)log_buf;
  return msg->len;
 }
 return idx + msg->len;
}
static int logbuf_has_space(u32 msg_size, bool empty)
{
 u32 free;

 if (log_next_idx > log_first_idx || empty)
  free = max(log_buf_len - log_next_idx, log_first_idx);
 else
  free = log_first_idx - log_next_idx;





 return free >= msg_size + sizeof(struct printk_log);
}

static int log_make_free_space(u32 msg_size)
{
 while (log_first_seq < log_next_seq &&
        !logbuf_has_space(msg_size, false)) {

  log_first_idx = log_next(log_first_idx);
  log_first_seq++;
 }

 if (clear_seq < log_first_seq) {
  clear_seq = log_first_seq;
  clear_idx = log_first_idx;
 }


 if (logbuf_has_space(msg_size, log_first_seq == log_next_seq))
  return 0;

 return -ENOMEM;
}


static u32 msg_used_size(u16 text_len, u16 dict_len, u32 *pad_len)
{
 u32 size;

 size = sizeof(struct printk_log) + text_len + dict_len;
 *pad_len = (-size) & (LOG_ALIGN - 1);
 size += *pad_len;

 return size;
}






static const char trunc_msg[] = "<truncated>";

static u32 truncate_msg(u16 *text_len, u16 *trunc_msg_len,
   u16 *dict_len, u32 *pad_len)
{




 u32 max_text_len = log_buf_len / MAX_LOG_TAKE_PART;
 if (*text_len > max_text_len)
  *text_len = max_text_len;

 *trunc_msg_len = strlen(trunc_msg);

 *dict_len = 0;

 return msg_used_size(*text_len + *trunc_msg_len, 0, pad_len);
}


static int log_store(int facility, int level,
       enum log_flags flags, u64 ts_nsec,
       const char *dict, u16 dict_len,
       const char *text, u16 text_len)
{
 struct printk_log *msg;
 u32 size, pad_len;
 u16 trunc_msg_len = 0;


 size = msg_used_size(text_len, dict_len, &pad_len);

 if (log_make_free_space(size)) {

  size = truncate_msg(&text_len, &trunc_msg_len,
        &dict_len, &pad_len);

  if (log_make_free_space(size))
   return 0;
 }

 if (log_next_idx + size + sizeof(struct printk_log) > log_buf_len) {





  memset(log_buf + log_next_idx, 0, sizeof(struct printk_log));
  log_next_idx = 0;
 }


 msg = (struct printk_log *)(log_buf + log_next_idx);
 memcpy(log_text(msg), text, text_len);
 msg->text_len = text_len;
 if (trunc_msg_len) {
  memcpy(log_text(msg) + text_len, trunc_msg, trunc_msg_len);
  msg->text_len += trunc_msg_len;
 }
 memcpy(log_dict(msg), dict, dict_len);
 msg->dict_len = dict_len;
 msg->facility = facility;
 msg->level = level & 7;
 msg->flags = flags & 0x1f;
 if (ts_nsec > 0)
  msg->ts_nsec = ts_nsec;
 else
  msg->ts_nsec = local_clock();
 memset(log_dict(msg) + dict_len, 0, pad_len);
 msg->len = size;


 log_next_idx += msg->len;
 log_next_seq++;

 return msg->text_len;
}

int dmesg_restrict = IS_ENABLED(CONFIG_SECURITY_DMESG_RESTRICT);

static int syslog_action_restricted(int type)
{
 if (dmesg_restrict)
  return 1;




 return type != SYSLOG_ACTION_READ_ALL &&
        type != SYSLOG_ACTION_SIZE_BUFFER;
}

int check_syslog_permissions(int type, int source)
{




 if (source == SYSLOG_FROM_PROC && type != SYSLOG_ACTION_OPEN)
  goto ok;

 if (syslog_action_restricted(type)) {
  if (capable(CAP_SYSLOG))
   goto ok;




  if (capable(CAP_SYS_ADMIN)) {
   pr_warn_once("%s (%d): Attempt to access syslog with "
         "CAP_SYS_ADMIN but no CAP_SYSLOG "
         "(deprecated).\n",
     current->comm, task_pid_nr(current));
   goto ok;
  }
  return -EPERM;
 }
ok:
 return security_syslog(type);
}
EXPORT_SYMBOL_GPL(check_syslog_permissions);

static void append_char(char **pp, char *e, char c)
{
 if (*pp < e)
  *(*pp)++ = c;
}

static ssize_t msg_print_ext_header(char *buf, size_t size,
        struct printk_log *msg, u64 seq,
        enum log_flags prev_flags)
{
 u64 ts_usec = msg->ts_nsec;
 char cont = '-';

 do_div(ts_usec, 1000);
 if (msg->flags & LOG_CONT && !(prev_flags & LOG_CONT))
  cont = 'c';
 else if ((msg->flags & LOG_CONT) ||
   ((prev_flags & LOG_CONT) && !(msg->flags & LOG_PREFIX)))
  cont = '+';

 return scnprintf(buf, size, "%u,%llu,%llu,%c;",
         (msg->facility << 3) | msg->level, seq, ts_usec, cont);
}

static ssize_t msg_print_ext_body(char *buf, size_t size,
      char *dict, size_t dict_len,
      char *text, size_t text_len)
{
 char *p = buf, *e = buf + size;
 size_t i;


 for (i = 0; i < text_len; i++) {
  unsigned char c = text[i];

  if (c < ' ' || c >= 127 || c == '\\')
   p += scnprintf(p, e - p, "\\x%02x", c);
  else
   append_char(&p, e, c);
 }
 append_char(&p, e, '\n');

 if (dict_len) {
  bool line = true;

  for (i = 0; i < dict_len; i++) {
   unsigned char c = dict[i];

   if (line) {
    append_char(&p, e, ' ');
    line = false;
   }

   if (c == '\0') {
    append_char(&p, e, '\n');
    line = true;
    continue;
   }

   if (c < ' ' || c >= 127 || c == '\\') {
    p += scnprintf(p, e - p, "\\x%02x", c);
    continue;
   }

   append_char(&p, e, c);
  }
  append_char(&p, e, '\n');
 }

 return p - buf;
}


struct devkmsg_user {
 u64 seq;
 u32 idx;
 enum log_flags prev;
 struct mutex lock;
 char buf[CONSOLE_EXT_LOG_MAX];
};

static ssize_t devkmsg_write(struct kiocb *iocb, struct iov_iter *from)
{
 char *buf, *line;
 int level = default_message_loglevel;
 int facility = 1;
 size_t len = iov_iter_count(from);
 ssize_t ret = len;

 if (len > LOG_LINE_MAX)
  return -EINVAL;
 buf = kmalloc(len+1, GFP_KERNEL);
 if (buf == NULL)
  return -ENOMEM;

 buf[len] = '\0';
 if (copy_from_iter(buf, len, from) != len) {
  kfree(buf);
  return -EFAULT;
 }
 line = buf;
 if (line[0] == '<') {
  char *endp = NULL;
  unsigned int u;

  u = simple_strtoul(line + 1, &endp, 10);
  if (endp && endp[0] == '>') {
   level = LOG_LEVEL(u);
   if (LOG_FACILITY(u) != 0)
    facility = LOG_FACILITY(u);
   endp++;
   len -= endp - line;
   line = endp;
  }
 }

 printk_emit(facility, level, NULL, 0, "%s", line);
 kfree(buf);
 return ret;
}

static ssize_t devkmsg_read(struct file *file, char __user *buf,
       size_t count, loff_t *ppos)
{
 struct devkmsg_user *user = file->private_data;
 struct printk_log *msg;
 size_t len;
 ssize_t ret;

 if (!user)
  return -EBADF;

 ret = mutex_lock_interruptible(&user->lock);
 if (ret)
  return ret;
 raw_spin_lock_irq(&logbuf_lock);
 while (user->seq == log_next_seq) {
  if (file->f_flags & O_NONBLOCK) {
   ret = -EAGAIN;
   raw_spin_unlock_irq(&logbuf_lock);
   goto out;
  }

  raw_spin_unlock_irq(&logbuf_lock);
  ret = wait_event_interruptible(log_wait,
            user->seq != log_next_seq);
  if (ret)
   goto out;
  raw_spin_lock_irq(&logbuf_lock);
 }

 if (user->seq < log_first_seq) {

  user->idx = log_first_idx;
  user->seq = log_first_seq;
  ret = -EPIPE;
  raw_spin_unlock_irq(&logbuf_lock);
  goto out;
 }

 msg = log_from_idx(user->idx);
 len = msg_print_ext_header(user->buf, sizeof(user->buf),
       msg, user->seq, user->prev);
 len += msg_print_ext_body(user->buf + len, sizeof(user->buf) - len,
      log_dict(msg), msg->dict_len,
      log_text(msg), msg->text_len);

 user->prev = msg->flags;
 user->idx = log_next(user->idx);
 user->seq++;
 raw_spin_unlock_irq(&logbuf_lock);

 if (len > count) {
  ret = -EINVAL;
  goto out;
 }

 if (copy_to_user(buf, user->buf, len)) {
  ret = -EFAULT;
  goto out;
 }
 ret = len;
out:
 mutex_unlock(&user->lock);
 return ret;
}

static loff_t devkmsg_llseek(struct file *file, loff_t offset, int whence)
{
 struct devkmsg_user *user = file->private_data;
 loff_t ret = 0;

 if (!user)
  return -EBADF;
 if (offset)
  return -ESPIPE;

 raw_spin_lock_irq(&logbuf_lock);
 switch (whence) {
 case SEEK_SET:

  user->idx = log_first_idx;
  user->seq = log_first_seq;
  break;
 case SEEK_DATA:





  user->idx = clear_idx;
  user->seq = clear_seq;
  break;
 case SEEK_END:

  user->idx = log_next_idx;
  user->seq = log_next_seq;
  break;
 default:
  ret = -EINVAL;
 }
 raw_spin_unlock_irq(&logbuf_lock);
 return ret;
}

static unsigned int devkmsg_poll(struct file *file, poll_table *wait)
{
 struct devkmsg_user *user = file->private_data;
 int ret = 0;

 if (!user)
  return POLLERR|POLLNVAL;

 poll_wait(file, &log_wait, wait);

 raw_spin_lock_irq(&logbuf_lock);
 if (user->seq < log_next_seq) {

  if (user->seq < log_first_seq)
   ret = POLLIN|POLLRDNORM|POLLERR|POLLPRI;
  else
   ret = POLLIN|POLLRDNORM;
 }
 raw_spin_unlock_irq(&logbuf_lock);

 return ret;
}

static int devkmsg_open(struct inode *inode, struct file *file)
{
 struct devkmsg_user *user;
 int err;


 if ((file->f_flags & O_ACCMODE) == O_WRONLY)
  return 0;

 err = check_syslog_permissions(SYSLOG_ACTION_READ_ALL,
           SYSLOG_FROM_READER);
 if (err)
  return err;

 user = kmalloc(sizeof(struct devkmsg_user), GFP_KERNEL);
 if (!user)
  return -ENOMEM;

 mutex_init(&user->lock);

 raw_spin_lock_irq(&logbuf_lock);
 user->idx = log_first_idx;
 user->seq = log_first_seq;
 raw_spin_unlock_irq(&logbuf_lock);

 file->private_data = user;
 return 0;
}

static int devkmsg_release(struct inode *inode, struct file *file)
{
 struct devkmsg_user *user = file->private_data;

 if (!user)
  return 0;

 mutex_destroy(&user->lock);
 kfree(user);
 return 0;
}

const struct file_operations kmsg_fops = {
 .open = devkmsg_open,
 .read = devkmsg_read,
 .write_iter = devkmsg_write,
 .llseek = devkmsg_llseek,
 .poll = devkmsg_poll,
 .release = devkmsg_release,
};

void log_buf_kexec_setup(void)
{
 VMCOREINFO_SYMBOL(log_buf);
 VMCOREINFO_SYMBOL(log_buf_len);
 VMCOREINFO_SYMBOL(log_first_idx);
 VMCOREINFO_SYMBOL(clear_idx);
 VMCOREINFO_SYMBOL(log_next_idx);




 VMCOREINFO_STRUCT_SIZE(printk_log);
 VMCOREINFO_OFFSET(printk_log, ts_nsec);
 VMCOREINFO_OFFSET(printk_log, len);
 VMCOREINFO_OFFSET(printk_log, text_len);
 VMCOREINFO_OFFSET(printk_log, dict_len);
}


static unsigned long __initdata new_log_buf_len;


static void __init log_buf_len_update(unsigned size)
{
 if (size)
  size = roundup_pow_of_two(size);
 if (size > log_buf_len)
  new_log_buf_len = size;
}


static int __init log_buf_len_setup(char *str)
{
 unsigned size = memparse(str, &str);

 log_buf_len_update(size);

 return 0;
}
early_param("log_buf_len", log_buf_len_setup);


static void __init log_buf_add_cpu(void)
{
 unsigned int cpu_extra;






 if (num_possible_cpus() == 1)
  return;

 cpu_extra = (num_possible_cpus() - 1) * __LOG_CPU_MAX_BUF_LEN;


 if (cpu_extra <= __LOG_BUF_LEN / 2)
  return;

 pr_info("log_buf_len individual max cpu contribution: %d bytes\n",
  __LOG_CPU_MAX_BUF_LEN);
 pr_info("log_buf_len total cpu_extra contributions: %d bytes\n",
  cpu_extra);
 pr_info("log_buf_len min size: %d bytes\n", __LOG_BUF_LEN);

 log_buf_len_update(cpu_extra + __LOG_BUF_LEN);
}
static inline void log_buf_add_cpu(void) {}

void __init setup_log_buf(int early)
{
 unsigned long flags;
 char *new_log_buf;
 int free;

 if (log_buf != __log_buf)
  return;

 if (!early && !new_log_buf_len)
  log_buf_add_cpu();

 if (!new_log_buf_len)
  return;

 if (early) {
  new_log_buf =
   memblock_virt_alloc(new_log_buf_len, LOG_ALIGN);
 } else {
  new_log_buf = memblock_virt_alloc_nopanic(new_log_buf_len,
         LOG_ALIGN);
 }

 if (unlikely(!new_log_buf)) {
  pr_err("log_buf_len: %ld bytes not available\n",
   new_log_buf_len);
  return;
 }

 raw_spin_lock_irqsave(&logbuf_lock, flags);
 log_buf_len = new_log_buf_len;
 log_buf = new_log_buf;
 new_log_buf_len = 0;
 free = __LOG_BUF_LEN - log_next_idx;
 memcpy(log_buf, __log_buf, __LOG_BUF_LEN);
 raw_spin_unlock_irqrestore(&logbuf_lock, flags);

 pr_info("log_buf_len: %d bytes\n", log_buf_len);
 pr_info("early log buf free: %d(%d%%)\n",
  free, (free * 100) / __LOG_BUF_LEN);
}

static bool __read_mostly ignore_loglevel;

static int __init ignore_loglevel_setup(char *str)
{
 ignore_loglevel = true;
 pr_info("debug: ignoring loglevel setting.\n");

 return 0;
}

early_param("ignore_loglevel", ignore_loglevel_setup);
module_param(ignore_loglevel, bool, S_IRUGO | S_IWUSR);
MODULE_PARM_DESC(ignore_loglevel,
   "ignore loglevel setting (prints all kernel messages to the console)");


static int boot_delay;
static unsigned long long loops_per_msec;

static int __init boot_delay_setup(char *str)
{
 unsigned long lpj;

 lpj = preset_lpj ? preset_lpj : 1000000;
 loops_per_msec = (unsigned long long)lpj / 1000 * HZ;

 get_option(&str, &boot_delay);
 if (boot_delay > 10 * 1000)
  boot_delay = 0;

 pr_debug("boot_delay: %u, preset_lpj: %ld, lpj: %lu, "
  "HZ: %d, loops_per_msec: %llu\n",
  boot_delay, preset_lpj, lpj, HZ, loops_per_msec);
 return 0;
}
early_param("boot_delay", boot_delay_setup);

static void boot_delay_msec(int level)
{
 unsigned long long k;
 unsigned long timeout;

 if ((boot_delay == 0 || system_state != SYSTEM_BOOTING)
  || (level >= console_loglevel && !ignore_loglevel)) {
  return;
 }

 k = (unsigned long long)loops_per_msec * boot_delay;

 timeout = jiffies + msecs_to_jiffies(boot_delay);
 while (k) {
  k--;
  cpu_relax();





  if (time_after(jiffies, timeout))
   break;
  touch_nmi_watchdog();
 }
}
static inline void boot_delay_msec(int level)
{
}

static bool printk_time = IS_ENABLED(CONFIG_PRINTK_TIME);
module_param_named(time, printk_time, bool, S_IRUGO | S_IWUSR);

static size_t print_time(u64 ts, char *buf)
{
 unsigned long rem_nsec;

 if (!printk_time)
  return 0;

 rem_nsec = do_div(ts, 1000000000);

 if (!buf)
  return snprintf(NULL, 0, "[%5lu.000000] ", (unsigned long)ts);

 return sprintf(buf, "[%5lu.%06lu] ",
         (unsigned long)ts, rem_nsec / 1000);
}

static size_t print_prefix(const struct printk_log *msg, bool syslog, char *buf)
{
 size_t len = 0;
 unsigned int prefix = (msg->facility << 3) | msg->level;

 if (syslog) {
  if (buf) {
   len += sprintf(buf, "<%u>", prefix);
  } else {
   len += 3;
   if (prefix > 999)
    len += 3;
   else if (prefix > 99)
    len += 2;
   else if (prefix > 9)
    len++;
  }
 }

 len += print_time(msg->ts_nsec, buf ? buf + len : NULL);
 return len;
}

static size_t msg_print_text(const struct printk_log *msg, enum log_flags prev,
        bool syslog, char *buf, size_t size)
{
 const char *text = log_text(msg);
 size_t text_size = msg->text_len;
 bool prefix = true;
 bool newline = true;
 size_t len = 0;

 if ((prev & LOG_CONT) && !(msg->flags & LOG_PREFIX))
  prefix = false;

 if (msg->flags & LOG_CONT) {
  if ((prev & LOG_CONT) && !(prev & LOG_NEWLINE))
   prefix = false;

  if (!(msg->flags & LOG_NEWLINE))
   newline = false;
 }

 do {
  const char *next = memchr(text, '\n', text_size);
  size_t text_len;

  if (next) {
   text_len = next - text;
   next++;
   text_size -= next - text;
  } else {
   text_len = text_size;
  }

  if (buf) {
   if (print_prefix(msg, syslog, NULL) +
       text_len + 1 >= size - len)
    break;

   if (prefix)
    len += print_prefix(msg, syslog, buf + len);
   memcpy(buf + len, text, text_len);
   len += text_len;
   if (next || newline)
    buf[len++] = '\n';
  } else {

   if (prefix)
    len += print_prefix(msg, syslog, NULL);
   len += text_len;
   if (next || newline)
    len++;
  }

  prefix = true;
  text = next;
 } while (text);

 return len;
}

static int syslog_print(char __user *buf, int size)
{
 char *text;
 struct printk_log *msg;
 int len = 0;

 text = kmalloc(LOG_LINE_MAX + PREFIX_MAX, GFP_KERNEL);
 if (!text)
  return -ENOMEM;

 while (size > 0) {
  size_t n;
  size_t skip;

  raw_spin_lock_irq(&logbuf_lock);
  if (syslog_seq < log_first_seq) {

   syslog_seq = log_first_seq;
   syslog_idx = log_first_idx;
   syslog_prev = 0;
   syslog_partial = 0;
  }
  if (syslog_seq == log_next_seq) {
   raw_spin_unlock_irq(&logbuf_lock);
   break;
  }

  skip = syslog_partial;
  msg = log_from_idx(syslog_idx);
  n = msg_print_text(msg, syslog_prev, true, text,
       LOG_LINE_MAX + PREFIX_MAX);
  if (n - syslog_partial <= size) {

   syslog_idx = log_next(syslog_idx);
   syslog_seq++;
   syslog_prev = msg->flags;
   n -= syslog_partial;
   syslog_partial = 0;
  } else if (!len){

   n = size;
   syslog_partial += n;
  } else
   n = 0;
  raw_spin_unlock_irq(&logbuf_lock);

  if (!n)
   break;

  if (copy_to_user(buf, text + skip, n)) {
   if (!len)
    len = -EFAULT;
   break;
  }

  len += n;
  size -= n;
  buf += n;
 }

 kfree(text);
 return len;
}

static int syslog_print_all(char __user *buf, int size, bool clear)
{
 char *text;
 int len = 0;

 text = kmalloc(LOG_LINE_MAX + PREFIX_MAX, GFP_KERNEL);
 if (!text)
  return -ENOMEM;

 raw_spin_lock_irq(&logbuf_lock);
 if (buf) {
  u64 next_seq;
  u64 seq;
  u32 idx;
  enum log_flags prev;





  seq = clear_seq;
  idx = clear_idx;
  prev = 0;
  while (seq < log_next_seq) {
   struct printk_log *msg = log_from_idx(idx);

   len += msg_print_text(msg, prev, true, NULL, 0);
   prev = msg->flags;
   idx = log_next(idx);
   seq++;
  }


  seq = clear_seq;
  idx = clear_idx;
  prev = 0;
  while (len > size && seq < log_next_seq) {
   struct printk_log *msg = log_from_idx(idx);

   len -= msg_print_text(msg, prev, true, NULL, 0);
   prev = msg->flags;
   idx = log_next(idx);
   seq++;
  }


  next_seq = log_next_seq;

  len = 0;
  while (len >= 0 && seq < next_seq) {
   struct printk_log *msg = log_from_idx(idx);
   int textlen;

   textlen = msg_print_text(msg, prev, true, text,
       LOG_LINE_MAX + PREFIX_MAX);
   if (textlen < 0) {
    len = textlen;
    break;
   }
   idx = log_next(idx);
   seq++;
   prev = msg->flags;

   raw_spin_unlock_irq(&logbuf_lock);
   if (copy_to_user(buf + len, text, textlen))
    len = -EFAULT;
   else
    len += textlen;
   raw_spin_lock_irq(&logbuf_lock);

   if (seq < log_first_seq) {

    seq = log_first_seq;
    idx = log_first_idx;
    prev = 0;
   }
  }
 }

 if (clear) {
  clear_seq = log_next_seq;
  clear_idx = log_next_idx;
 }
 raw_spin_unlock_irq(&logbuf_lock);

 kfree(text);
 return len;
}

int do_syslog(int type, char __user *buf, int len, int source)
{
 bool clear = false;
 static int saved_console_loglevel = LOGLEVEL_DEFAULT;
 int error;

 error = check_syslog_permissions(type, source);
 if (error)
  goto out;

 switch (type) {
 case SYSLOG_ACTION_CLOSE:
  break;
 case SYSLOG_ACTION_OPEN:
  break;
 case SYSLOG_ACTION_READ:
  error = -EINVAL;
  if (!buf || len < 0)
   goto out;
  error = 0;
  if (!len)
   goto out;
  if (!access_ok(VERIFY_WRITE, buf, len)) {
   error = -EFAULT;
   goto out;
  }
  error = wait_event_interruptible(log_wait,
       syslog_seq != log_next_seq);
  if (error)
   goto out;
  error = syslog_print(buf, len);
  break;

 case SYSLOG_ACTION_READ_CLEAR:
  clear = true;


 case SYSLOG_ACTION_READ_ALL:
  error = -EINVAL;
  if (!buf || len < 0)
   goto out;
  error = 0;
  if (!len)
   goto out;
  if (!access_ok(VERIFY_WRITE, buf, len)) {
   error = -EFAULT;
   goto out;
  }
  error = syslog_print_all(buf, len, clear);
  break;

 case SYSLOG_ACTION_CLEAR:
  syslog_print_all(NULL, 0, true);
  break;

 case SYSLOG_ACTION_CONSOLE_OFF:
  if (saved_console_loglevel == LOGLEVEL_DEFAULT)
   saved_console_loglevel = console_loglevel;
  console_loglevel = minimum_console_loglevel;
  break;

 case SYSLOG_ACTION_CONSOLE_ON:
  if (saved_console_loglevel != LOGLEVEL_DEFAULT) {
   console_loglevel = saved_console_loglevel;
   saved_console_loglevel = LOGLEVEL_DEFAULT;
  }
  break;

 case SYSLOG_ACTION_CONSOLE_LEVEL:
  error = -EINVAL;
  if (len < 1 || len > 8)
   goto out;
  if (len < minimum_console_loglevel)
   len = minimum_console_loglevel;
  console_loglevel = len;

  saved_console_loglevel = LOGLEVEL_DEFAULT;
  error = 0;
  break;

 case SYSLOG_ACTION_SIZE_UNREAD:
  raw_spin_lock_irq(&logbuf_lock);
  if (syslog_seq < log_first_seq) {

   syslog_seq = log_first_seq;
   syslog_idx = log_first_idx;
   syslog_prev = 0;
   syslog_partial = 0;
  }
  if (source == SYSLOG_FROM_PROC) {





   error = log_next_seq - syslog_seq;
  } else {
   u64 seq = syslog_seq;
   u32 idx = syslog_idx;
   enum log_flags prev = syslog_prev;

   error = 0;
   while (seq < log_next_seq) {
    struct printk_log *msg = log_from_idx(idx);

    error += msg_print_text(msg, prev, true, NULL, 0);
    idx = log_next(idx);
    seq++;
    prev = msg->flags;
   }
   error -= syslog_partial;
  }
  raw_spin_unlock_irq(&logbuf_lock);
  break;

 case SYSLOG_ACTION_SIZE_BUFFER:
  error = log_buf_len;
  break;
 default:
  error = -EINVAL;
  break;
 }
out:
 return error;
}

SYSCALL_DEFINE3(syslog, int, type, char __user *, buf, int, len)
{
 return do_syslog(type, buf, len, SYSLOG_FROM_READER);
}






static void call_console_drivers(int level,
     const char *ext_text, size_t ext_len,
     const char *text, size_t len)
{
 struct console *con;

 trace_console(text, len);

 if (level >= console_loglevel && !ignore_loglevel)
  return;
 if (!console_drivers)
  return;

 for_each_console(con) {
  if (exclusive_console && con != exclusive_console)
   continue;
  if (!(con->flags & CON_ENABLED))
   continue;
  if (!con->write)
   continue;
  if (!cpu_online(smp_processor_id()) &&
      !(con->flags & CON_ANYTIME))
   continue;
  if (con->flags & CON_EXTENDED)
   con->write(con, ext_text, ext_len);
  else
   con->write(con, text, len);
 }
}






static void zap_locks(void)
{
 static unsigned long oops_timestamp;

 if (time_after_eq(jiffies, oops_timestamp) &&
     !time_after(jiffies, oops_timestamp + 30 * HZ))
  return;

 oops_timestamp = jiffies;

 debug_locks_off();

 raw_spin_lock_init(&logbuf_lock);

 sema_init(&console_sem, 1);
}

int printk_delay_msec __read_mostly;

static inline void printk_delay(void)
{
 if (unlikely(printk_delay_msec)) {
  int m = printk_delay_msec;

  while (m--) {
   mdelay(1);
   touch_nmi_watchdog();
  }
 }
}







static struct cont {
 char buf[LOG_LINE_MAX];
 size_t len;
 size_t cons;
 struct task_struct *owner;
 u64 ts_nsec;
 u8 level;
 u8 facility;
 enum log_flags flags;
 bool flushed:1;
} cont;

static void cont_flush(enum log_flags flags)
{
 if (cont.flushed)
  return;
 if (cont.len == 0)
  return;

 if (cont.cons) {





  log_store(cont.facility, cont.level, flags | LOG_NOCONS,
     cont.ts_nsec, NULL, 0, cont.buf, cont.len);
  cont.flags = flags;
  cont.flushed = true;
 } else {




  log_store(cont.facility, cont.level, flags, 0,
     NULL, 0, cont.buf, cont.len);
  cont.len = 0;
 }
}

static bool cont_add(int facility, int level, const char *text, size_t len)
{
 if (cont.len && cont.flushed)
  return false;






 if (nr_ext_console_drivers || cont.len + len > sizeof(cont.buf)) {
  cont_flush(LOG_CONT);
  return false;
 }

 if (!cont.len) {
  cont.facility = facility;
  cont.level = level;
  cont.owner = current;
  cont.ts_nsec = local_clock();
  cont.flags = 0;
  cont.cons = 0;
  cont.flushed = false;
 }

 memcpy(cont.buf + cont.len, text, len);
 cont.len += len;

 if (cont.len > (sizeof(cont.buf) * 80) / 100)
  cont_flush(LOG_CONT);

 return true;
}

static size_t cont_print_text(char *text, size_t size)
{
 size_t textlen = 0;
 size_t len;

 if (cont.cons == 0 && (console_prev & LOG_NEWLINE)) {
  textlen += print_time(cont.ts_nsec, text);
  size -= textlen;
 }

 len = cont.len - cont.cons;
 if (len > 0) {
  if (len+1 > size)
   len = size-1;
  memcpy(text + textlen, cont.buf + cont.cons, len);
  textlen += len;
  cont.cons = cont.len;
 }

 if (cont.flushed) {
  if (cont.flags & LOG_NEWLINE)
   text[textlen++] = '\n';

  cont.len = 0;
 }
 return textlen;
}

asmlinkage int vprintk_emit(int facility, int level,
       const char *dict, size_t dictlen,
       const char *fmt, va_list args)
{
 static bool recursion_bug;
 static char textbuf[LOG_LINE_MAX];
 char *text = textbuf;
 size_t text_len = 0;
 enum log_flags lflags = 0;
 unsigned long flags;
 int this_cpu;
 int printed_len = 0;
 int nmi_message_lost;
 bool in_sched = false;

 static unsigned int logbuf_cpu = UINT_MAX;

 if (level == LOGLEVEL_SCHED) {
  level = LOGLEVEL_DEFAULT;
  in_sched = true;
 }

 boot_delay_msec(level);
 printk_delay();

 local_irq_save(flags);
 this_cpu = smp_processor_id();




 if (unlikely(logbuf_cpu == this_cpu)) {







  if (!oops_in_progress && !lockdep_recursing(current)) {
   recursion_bug = true;
   local_irq_restore(flags);
   return 0;
  }
  zap_locks();
 }

 lockdep_off();

 raw_spin_lock(&logbuf_lock);
 logbuf_cpu = this_cpu;

 if (unlikely(recursion_bug)) {
  static const char recursion_msg[] =
   "BUG: recent printk recursion!";

  recursion_bug = false;

  printed_len += log_store(0, 2, LOG_PREFIX|LOG_NEWLINE, 0,
      NULL, 0, recursion_msg,
      strlen(recursion_msg));
 }

 nmi_message_lost = get_nmi_message_lost();
 if (unlikely(nmi_message_lost)) {
  text_len = scnprintf(textbuf, sizeof(textbuf),
         "BAD LUCK: lost %d message(s) from NMI context!",
         nmi_message_lost);
  printed_len += log_store(0, 2, LOG_PREFIX|LOG_NEWLINE, 0,
      NULL, 0, textbuf, text_len);
 }





 text_len = vscnprintf(text, sizeof(textbuf), fmt, args);


 if (text_len && text[text_len-1] == '\n') {
  text_len--;
  lflags |= LOG_NEWLINE;
 }


 if (facility == 0) {
  int kern_level = printk_get_level(text);

  if (kern_level) {
   const char *end_of_header = printk_skip_level(text);
   switch (kern_level) {
   case '0' ... '7':
    if (level == LOGLEVEL_DEFAULT)
     level = kern_level - '0';

   case 'd':
    lflags |= LOG_PREFIX;
   }





   text_len -= end_of_header - text;
   text = (char *)end_of_header;
  }
 }

 if (level == LOGLEVEL_DEFAULT)
  level = default_message_loglevel;

 if (dict)
  lflags |= LOG_PREFIX|LOG_NEWLINE;

 if (!(lflags & LOG_NEWLINE)) {




  if (cont.len && (lflags & LOG_PREFIX || cont.owner != current))
   cont_flush(LOG_NEWLINE);


  if (cont_add(facility, level, text, text_len))
   printed_len += text_len;
  else
   printed_len += log_store(facility, level,
       lflags | LOG_CONT, 0,
       dict, dictlen, text, text_len);
 } else {
  bool stored = false;
  if (cont.len) {
   if (cont.owner == current && !(lflags & LOG_PREFIX))
    stored = cont_add(facility, level, text,
        text_len);
   cont_flush(LOG_NEWLINE);
  }

  if (stored)
   printed_len += text_len;
  else
   printed_len += log_store(facility, level, lflags, 0,
       dict, dictlen, text, text_len);
 }

 logbuf_cpu = UINT_MAX;
 raw_spin_unlock(&logbuf_lock);
 lockdep_on();
 local_irq_restore(flags);


 if (!in_sched) {
  lockdep_off();





  if (console_trylock())
   console_unlock();
  lockdep_on();
 }

 return printed_len;
}
EXPORT_SYMBOL(vprintk_emit);

asmlinkage int vprintk(const char *fmt, va_list args)
{
 return vprintk_emit(0, LOGLEVEL_DEFAULT, NULL, 0, fmt, args);
}
EXPORT_SYMBOL(vprintk);

asmlinkage int printk_emit(int facility, int level,
      const char *dict, size_t dictlen,
      const char *fmt, ...)
{
 va_list args;
 int r;

 va_start(args, fmt);
 r = vprintk_emit(facility, level, dict, dictlen, fmt, args);
 va_end(args);

 return r;
}
EXPORT_SYMBOL(printk_emit);

int vprintk_default(const char *fmt, va_list args)
{
 int r;

 if (unlikely(kdb_trap_printk)) {
  r = vkdb_printf(KDB_MSGSRC_PRINTK, fmt, args);
  return r;
 }
 r = vprintk_emit(0, LOGLEVEL_DEFAULT, NULL, 0, fmt, args);

 return r;
}
EXPORT_SYMBOL_GPL(vprintk_default);
asmlinkage __visible int printk(const char *fmt, ...)
{
 va_list args;
 int r;

 va_start(args, fmt);
 r = vprintk_func(fmt, args);
 va_end(args);

 return r;
}
EXPORT_SYMBOL(printk);



static u64 syslog_seq;
static u32 syslog_idx;
static u64 console_seq;
static u32 console_idx;
static enum log_flags syslog_prev;
static u64 log_first_seq;
static u32 log_first_idx;
static u64 log_next_seq;
static enum log_flags console_prev;
static struct cont {
 size_t len;
 size_t cons;
 u8 level;
 bool flushed:1;
} cont;
static char *log_text(const struct printk_log *msg) { return NULL; }
static char *log_dict(const struct printk_log *msg) { return NULL; }
static struct printk_log *log_from_idx(u32 idx) { return NULL; }
static u32 log_next(u32 idx) { return 0; }
static ssize_t msg_print_ext_header(char *buf, size_t size,
        struct printk_log *msg, u64 seq,
        enum log_flags prev_flags) { return 0; }
static ssize_t msg_print_ext_body(char *buf, size_t size,
      char *dict, size_t dict_len,
      char *text, size_t text_len) { return 0; }
static void call_console_drivers(int level,
     const char *ext_text, size_t ext_len,
     const char *text, size_t len) {}
static size_t msg_print_text(const struct printk_log *msg, enum log_flags prev,
        bool syslog, char *buf, size_t size) { return 0; }
static size_t cont_print_text(char *text, size_t size) { return 0; }


DEFINE_PER_CPU(printk_func_t, printk_func);


struct console *early_console;

asmlinkage __visible void early_printk(const char *fmt, ...)
{
 va_list ap;
 char buf[512];
 int n;

 if (!early_console)
  return;

 va_start(ap, fmt);
 n = vscnprintf(buf, sizeof(buf), fmt, ap);
 va_end(ap);

 early_console->write(early_console, buf, n);
}

static int __add_preferred_console(char *name, int idx, char *options,
       char *brl_options)
{
 struct console_cmdline *c;
 int i;





 for (i = 0, c = console_cmdline;
      i < MAX_CMDLINECONSOLES && c->name[0];
      i++, c++) {
  if (strcmp(c->name, name) == 0 && c->index == idx) {
   if (!brl_options)
    selected_console = i;
   return 0;
  }
 }
 if (i == MAX_CMDLINECONSOLES)
  return -E2BIG;
 if (!brl_options)
  selected_console = i;
 strlcpy(c->name, name, sizeof(c->name));
 c->options = options;
 braille_set_options(c, brl_options);

 c->index = idx;
 return 0;
}




static int __init console_setup(char *str)
{
 char buf[sizeof(console_cmdline[0].name) + 4];
 char *s, *options, *brl_options = NULL;
 int idx;

 if (_braille_console_setup(&str, &brl_options))
  return 1;




 if (str[0] >= '0' && str[0] <= '9') {
  strcpy(buf, "ttyS");
  strncpy(buf + 4, str, sizeof(buf) - 5);
 } else {
  strncpy(buf, str, sizeof(buf) - 1);
 }
 buf[sizeof(buf) - 1] = 0;
 options = strchr(str, ',');
 if (options)
  *(options++) = 0;
 if (!strcmp(str, "ttya"))
  strcpy(buf, "ttyS0");
 if (!strcmp(str, "ttyb"))
  strcpy(buf, "ttyS1");
 for (s = buf; *s; s++)
  if (isdigit(*s) || *s == ',')
   break;
 idx = simple_strtoul(s, NULL, 10);
 *s = 0;

 __add_preferred_console(buf, idx, options, brl_options);
 console_set_on_cmdline = 1;
 return 1;
}
__setup("console=", console_setup);
int add_preferred_console(char *name, int idx, char *options)
{
 return __add_preferred_console(name, idx, options, NULL);
}

bool console_suspend_enabled = true;
EXPORT_SYMBOL(console_suspend_enabled);

static int __init console_suspend_disable(char *str)
{
 console_suspend_enabled = false;
 return 1;
}
__setup("no_console_suspend", console_suspend_disable);
module_param_named(console_suspend, console_suspend_enabled,
  bool, S_IRUGO | S_IWUSR);
MODULE_PARM_DESC(console_suspend, "suspend console during suspend"
 " and hibernate operations");






void suspend_console(void)
{
 if (!console_suspend_enabled)
  return;
 printk("Suspending console(s) (use no_console_suspend to debug)\n");
 console_lock();
 console_suspended = 1;
 up_console_sem();
}

void resume_console(void)
{
 if (!console_suspend_enabled)
  return;
 down_console_sem();
 console_suspended = 0;
 console_unlock();
}
static int console_cpu_notify(struct notifier_block *self,
 unsigned long action, void *hcpu)
{
 switch (action) {
 case CPU_ONLINE:
 case CPU_DEAD:
 case CPU_DOWN_FAILED:
 case CPU_UP_CANCELED:
  console_lock();
  console_unlock();
 }
 return NOTIFY_OK;
}
void console_lock(void)
{
 might_sleep();

 down_console_sem();
 if (console_suspended)
  return;
 console_locked = 1;
 console_may_schedule = 1;
}
EXPORT_SYMBOL(console_lock);
int console_trylock(void)
{
 if (down_trylock_console_sem())
  return 0;
 if (console_suspended) {
  up_console_sem();
  return 0;
 }
 console_locked = 1;
 console_may_schedule = !oops_in_progress &&
   preemptible() &&
   !rcu_preempt_depth();
 return 1;
}
EXPORT_SYMBOL(console_trylock);

int is_console_locked(void)
{
 return console_locked;
}





static int have_callable_console(void)
{
 struct console *con;

 for_each_console(con)
  if ((con->flags & CON_ENABLED) &&
    (con->flags & CON_ANYTIME))
   return 1;

 return 0;
}
static inline int can_use_console(void)
{
 return cpu_online(raw_smp_processor_id()) || have_callable_console();
}

static void console_cont_flush(char *text, size_t size)
{
 unsigned long flags;
 size_t len;

 raw_spin_lock_irqsave(&logbuf_lock, flags);

 if (!cont.len)
  goto out;






 if (console_seq < log_next_seq && !cont.cons)
  goto out;

 len = cont_print_text(text, size);
 raw_spin_unlock(&logbuf_lock);
 stop_critical_timings();
 call_console_drivers(cont.level, NULL, 0, text, len);
 start_critical_timings();
 local_irq_restore(flags);
 return;
out:
 raw_spin_unlock_irqrestore(&logbuf_lock, flags);
}
void console_unlock(void)
{
 static char ext_text[CONSOLE_EXT_LOG_MAX];
 static char text[LOG_LINE_MAX + PREFIX_MAX];
 static u64 seen_seq;
 unsigned long flags;
 bool wake_klogd = false;
 bool do_cond_resched, retry;

 if (console_suspended) {
  up_console_sem();
  return;
 }
 do_cond_resched = console_may_schedule;
 console_may_schedule = 0;

again:





 if (!can_use_console()) {
  console_locked = 0;
  up_console_sem();
  return;
 }


 console_cont_flush(text, sizeof(text));

 for (;;) {
  struct printk_log *msg;
  size_t ext_len = 0;
  size_t len;
  int level;

  raw_spin_lock_irqsave(&logbuf_lock, flags);
  if (seen_seq != log_next_seq) {
   wake_klogd = true;
   seen_seq = log_next_seq;
  }

  if (console_seq < log_first_seq) {
   len = sprintf(text, "** %u printk messages dropped ** ",
          (unsigned)(log_first_seq - console_seq));


   console_seq = log_first_seq;
   console_idx = log_first_idx;
   console_prev = 0;
  } else {
   len = 0;
  }
skip:
  if (console_seq == log_next_seq)
   break;

  msg = log_from_idx(console_idx);
  if (msg->flags & LOG_NOCONS) {




   console_idx = log_next(console_idx);
   console_seq++;





   msg->flags &= ~LOG_NOCONS;
   console_prev = msg->flags;
   goto skip;
  }

  level = msg->level;
  len += msg_print_text(msg, console_prev, false,
          text + len, sizeof(text) - len);
  if (nr_ext_console_drivers) {
   ext_len = msg_print_ext_header(ext_text,
      sizeof(ext_text),
      msg, console_seq, console_prev);
   ext_len += msg_print_ext_body(ext_text + ext_len,
      sizeof(ext_text) - ext_len,
      log_dict(msg), msg->dict_len,
      log_text(msg), msg->text_len);
  }
  console_idx = log_next(console_idx);
  console_seq++;
  console_prev = msg->flags;
  raw_spin_unlock(&logbuf_lock);

  stop_critical_timings();
  call_console_drivers(level, ext_text, ext_len, text, len);
  start_critical_timings();
  local_irq_restore(flags);

  if (do_cond_resched)
   cond_resched();
 }
 console_locked = 0;


 if (unlikely(exclusive_console))
  exclusive_console = NULL;

 raw_spin_unlock(&logbuf_lock);

 up_console_sem();







 raw_spin_lock(&logbuf_lock);
 retry = console_seq != log_next_seq;
 raw_spin_unlock_irqrestore(&logbuf_lock, flags);

 if (retry && console_trylock())
  goto again;

 if (wake_klogd)
  wake_up_klogd();
}
EXPORT_SYMBOL(console_unlock);
void __sched console_conditional_schedule(void)
{
 if (console_may_schedule)
  cond_resched();
}
EXPORT_SYMBOL(console_conditional_schedule);

void console_unblank(void)
{
 struct console *c;





 if (oops_in_progress) {
  if (down_trylock_console_sem() != 0)
   return;
 } else
  console_lock();

 console_locked = 1;
 console_may_schedule = 0;
 for_each_console(c)
  if ((c->flags & CON_ENABLED) && c->unblank)
   c->unblank();
 console_unlock();
}






void console_flush_on_panic(void)
{







 console_trylock();
 console_may_schedule = 0;
 console_unlock();
}




struct tty_driver *console_device(int *index)
{
 struct console *c;
 struct tty_driver *driver = NULL;

 console_lock();
 for_each_console(c) {
  if (!c->device)
   continue;
  driver = c->device(c, index);
  if (driver)
   break;
 }
 console_unlock();
 return driver;
}






void console_stop(struct console *console)
{
 console_lock();
 console->flags &= ~CON_ENABLED;
 console_unlock();
}
EXPORT_SYMBOL(console_stop);

void console_start(struct console *console)
{
 console_lock();
 console->flags |= CON_ENABLED;
 console_unlock();
}
EXPORT_SYMBOL(console_start);

static int __read_mostly keep_bootcon;

static int __init keep_bootcon_setup(char *str)
{
 keep_bootcon = 1;
 pr_info("debug: skip boot console de-registration.\n");

 return 0;
}

early_param("keep_bootcon", keep_bootcon_setup);
void register_console(struct console *newcon)
{
 int i;
 unsigned long flags;
 struct console *bcon = NULL;
 struct console_cmdline *c;

 if (console_drivers)
  for_each_console(bcon)
   if (WARN(bcon == newcon,
     "console '%s%d' already registered\n",
     bcon->name, bcon->index))
    return;





 if (console_drivers && newcon->flags & CON_BOOT) {

  for_each_console(bcon) {
   if (!(bcon->flags & CON_BOOT)) {
    pr_info("Too late to register bootconsole %s%d\n",
     newcon->name, newcon->index);
    return;
   }
  }
 }

 if (console_drivers && console_drivers->flags & CON_BOOT)
  bcon = console_drivers;

 if (preferred_console < 0 || bcon || !console_drivers)
  preferred_console = selected_console;






 if (preferred_console < 0) {
  if (newcon->index < 0)
   newcon->index = 0;
  if (newcon->setup == NULL ||
      newcon->setup(newcon, NULL) == 0) {
   newcon->flags |= CON_ENABLED;
   if (newcon->device) {
    newcon->flags |= CON_CONSDEV;
    preferred_console = 0;
   }
  }
 }





 for (i = 0, c = console_cmdline;
      i < MAX_CMDLINECONSOLES && c->name[0];
      i++, c++) {
  if (!newcon->match ||
      newcon->match(newcon, c->name, c->index, c->options) != 0) {

   BUILD_BUG_ON(sizeof(c->name) != sizeof(newcon->name));
   if (strcmp(c->name, newcon->name) != 0)
    continue;
   if (newcon->index >= 0 &&
       newcon->index != c->index)
    continue;
   if (newcon->index < 0)
    newcon->index = c->index;

   if (_braille_register_console(newcon, c))
    return;

   if (newcon->setup &&
       newcon->setup(newcon, c->options) != 0)
    break;
  }

  newcon->flags |= CON_ENABLED;
  if (i == selected_console) {
   newcon->flags |= CON_CONSDEV;
   preferred_console = selected_console;
  }
  break;
 }

 if (!(newcon->flags & CON_ENABLED))
  return;







 if (bcon && ((newcon->flags & (CON_CONSDEV | CON_BOOT)) == CON_CONSDEV))
  newcon->flags &= ~CON_PRINTBUFFER;





 console_lock();
 if ((newcon->flags & CON_CONSDEV) || console_drivers == NULL) {
  newcon->next = console_drivers;
  console_drivers = newcon;
  if (newcon->next)
   newcon->next->flags &= ~CON_CONSDEV;
 } else {
  newcon->next = console_drivers->next;
  console_drivers->next = newcon;
 }

 if (newcon->flags & CON_EXTENDED)
  if (!nr_ext_console_drivers++)
   pr_info("printk: continuation disabled due to ext consoles, expect more fragments in /dev/kmsg\n");

 if (newcon->flags & CON_PRINTBUFFER) {




  raw_spin_lock_irqsave(&logbuf_lock, flags);
  console_seq = syslog_seq;
  console_idx = syslog_idx;
  console_prev = syslog_prev;
  raw_spin_unlock_irqrestore(&logbuf_lock, flags);





  exclusive_console = newcon;
 }
 console_unlock();
 console_sysfs_notify();
 pr_info("%sconsole [%s%d] enabled\n",
  (newcon->flags & CON_BOOT) ? "boot" : "" ,
  newcon->name, newcon->index);
 if (bcon &&
     ((newcon->flags & (CON_CONSDEV | CON_BOOT)) == CON_CONSDEV) &&
     !keep_bootcon) {



  for_each_console(bcon)
   if (bcon->flags & CON_BOOT)
    unregister_console(bcon);
 }
}
EXPORT_SYMBOL(register_console);

int unregister_console(struct console *console)
{
        struct console *a, *b;
 int res;

 pr_info("%sconsole [%s%d] disabled\n",
  (console->flags & CON_BOOT) ? "boot" : "" ,
  console->name, console->index);

 res = _braille_unregister_console(console);
 if (res)
  return res;

 res = 1;
 console_lock();
 if (console_drivers == console) {
  console_drivers=console->next;
  res = 0;
 } else if (console_drivers) {
  for (a=console_drivers->next, b=console_drivers ;
       a; b=a, a=b->next) {
   if (a == console) {
    b->next = a->next;
    res = 0;
    break;
   }
  }
 }

 if (!res && (console->flags & CON_EXTENDED))
  nr_ext_console_drivers--;





 if (console_drivers != NULL && console->flags & CON_CONSDEV)
  console_drivers->flags |= CON_CONSDEV;

 console->flags &= ~CON_ENABLED;
 console_unlock();
 console_sysfs_notify();
 return res;
}
EXPORT_SYMBOL(unregister_console);
static int __init printk_late_init(void)
{
 struct console *con;

 for_each_console(con) {
  if (!keep_bootcon && con->flags & CON_BOOT) {







   if (init_section_intersects(con, sizeof(*con)))
    unregister_console(con);
  }
 }
 hotcpu_notifier(console_cpu_notify, 0);
 return 0;
}
late_initcall(printk_late_init);





static DEFINE_PER_CPU(int, printk_pending);

static void wake_up_klogd_work_func(struct irq_work *irq_work)
{
 int pending = __this_cpu_xchg(printk_pending, 0);

 if (pending & PRINTK_PENDING_OUTPUT) {

  if (console_trylock())
   console_unlock();
 }

 if (pending & PRINTK_PENDING_WAKEUP)
  wake_up_interruptible(&log_wait);
}

static DEFINE_PER_CPU(struct irq_work, wake_up_klogd_work) = {
 .func = wake_up_klogd_work_func,
 .flags = IRQ_WORK_LAZY,
};

void wake_up_klogd(void)
{
 preempt_disable();
 if (waitqueue_active(&log_wait)) {
  this_cpu_or(printk_pending, PRINTK_PENDING_WAKEUP);
  irq_work_queue(this_cpu_ptr(&wake_up_klogd_work));
 }
 preempt_enable();
}

int printk_deferred(const char *fmt, ...)
{
 va_list args;
 int r;

 preempt_disable();
 va_start(args, fmt);
 r = vprintk_emit(0, LOGLEVEL_SCHED, NULL, 0, fmt, args);
 va_end(args);

 __this_cpu_or(printk_pending, PRINTK_PENDING_OUTPUT);
 irq_work_queue(this_cpu_ptr(&wake_up_klogd_work));
 preempt_enable();

 return r;
}







DEFINE_RATELIMIT_STATE(printk_ratelimit_state, 5 * HZ, 10);

int __printk_ratelimit(const char *func)
{
 return ___ratelimit(&printk_ratelimit_state, func);
}
EXPORT_SYMBOL(__printk_ratelimit);
bool printk_timed_ratelimit(unsigned long *caller_jiffies,
   unsigned int interval_msecs)
{
 unsigned long elapsed = jiffies - *caller_jiffies;

 if (*caller_jiffies && elapsed <= msecs_to_jiffies(interval_msecs))
  return false;

 *caller_jiffies = jiffies;
 return true;
}
EXPORT_SYMBOL(printk_timed_ratelimit);

static DEFINE_SPINLOCK(dump_list_lock);
static LIST_HEAD(dump_list);
int kmsg_dump_register(struct kmsg_dumper *dumper)
{
 unsigned long flags;
 int err = -EBUSY;


 if (!dumper->dump)
  return -EINVAL;

 spin_lock_irqsave(&dump_list_lock, flags);

 if (!dumper->registered) {
  dumper->registered = 1;
  list_add_tail_rcu(&dumper->list, &dump_list);
  err = 0;
 }
 spin_unlock_irqrestore(&dump_list_lock, flags);

 return err;
}
EXPORT_SYMBOL_GPL(kmsg_dump_register);
int kmsg_dump_unregister(struct kmsg_dumper *dumper)
{
 unsigned long flags;
 int err = -EINVAL;

 spin_lock_irqsave(&dump_list_lock, flags);
 if (dumper->registered) {
  dumper->registered = 0;
  list_del_rcu(&dumper->list);
  err = 0;
 }
 spin_unlock_irqrestore(&dump_list_lock, flags);
 synchronize_rcu();

 return err;
}
EXPORT_SYMBOL_GPL(kmsg_dump_unregister);

static bool always_kmsg_dump;
module_param_named(always_kmsg_dump, always_kmsg_dump, bool, S_IRUGO | S_IWUSR);
void kmsg_dump(enum kmsg_dump_reason reason)
{
 struct kmsg_dumper *dumper;
 unsigned long flags;

 if ((reason > KMSG_DUMP_OOPS) && !always_kmsg_dump)
  return;

 rcu_read_lock();
 list_for_each_entry_rcu(dumper, &dump_list, list) {
  if (dumper->max_reason && reason > dumper->max_reason)
   continue;


  dumper->active = true;

  raw_spin_lock_irqsave(&logbuf_lock, flags);
  dumper->cur_seq = clear_seq;
  dumper->cur_idx = clear_idx;
  dumper->next_seq = log_next_seq;
  dumper->next_idx = log_next_idx;
  raw_spin_unlock_irqrestore(&logbuf_lock, flags);


  dumper->dump(dumper, reason);


  dumper->active = false;
 }
 rcu_read_unlock();
}
bool kmsg_dump_get_line_nolock(struct kmsg_dumper *dumper, bool syslog,
          char *line, size_t size, size_t *len)
{
 struct printk_log *msg;
 size_t l = 0;
 bool ret = false;

 if (!dumper->active)
  goto out;

 if (dumper->cur_seq < log_first_seq) {

  dumper->cur_seq = log_first_seq;
  dumper->cur_idx = log_first_idx;
 }


 if (dumper->cur_seq >= log_next_seq)
  goto out;

 msg = log_from_idx(dumper->cur_idx);
 l = msg_print_text(msg, 0, syslog, line, size);

 dumper->cur_idx = log_next(dumper->cur_idx);
 dumper->cur_seq++;
 ret = true;
out:
 if (len)
  *len = l;
 return ret;
}
bool kmsg_dump_get_line(struct kmsg_dumper *dumper, bool syslog,
   char *line, size_t size, size_t *len)
{
 unsigned long flags;
 bool ret;

 raw_spin_lock_irqsave(&logbuf_lock, flags);
 ret = kmsg_dump_get_line_nolock(dumper, syslog, line, size, len);
 raw_spin_unlock_irqrestore(&logbuf_lock, flags);

 return ret;
}
EXPORT_SYMBOL_GPL(kmsg_dump_get_line);
bool kmsg_dump_get_buffer(struct kmsg_dumper *dumper, bool syslog,
     char *buf, size_t size, size_t *len)
{
 unsigned long flags;
 u64 seq;
 u32 idx;
 u64 next_seq;
 u32 next_idx;
 enum log_flags prev;
 size_t l = 0;
 bool ret = false;

 if (!dumper->active)
  goto out;

 raw_spin_lock_irqsave(&logbuf_lock, flags);
 if (dumper->cur_seq < log_first_seq) {

  dumper->cur_seq = log_first_seq;
  dumper->cur_idx = log_first_idx;
 }


 if (dumper->cur_seq >= dumper->next_seq) {
  raw_spin_unlock_irqrestore(&logbuf_lock, flags);
  goto out;
 }


 seq = dumper->cur_seq;
 idx = dumper->cur_idx;
 prev = 0;
 while (seq < dumper->next_seq) {
  struct printk_log *msg = log_from_idx(idx);

  l += msg_print_text(msg, prev, true, NULL, 0);
  idx = log_next(idx);
  seq++;
  prev = msg->flags;
 }


 seq = dumper->cur_seq;
 idx = dumper->cur_idx;
 prev = 0;
 while (l > size && seq < dumper->next_seq) {
  struct printk_log *msg = log_from_idx(idx);

  l -= msg_print_text(msg, prev, true, NULL, 0);
  idx = log_next(idx);
  seq++;
  prev = msg->flags;
 }


 next_seq = seq;
 next_idx = idx;

 l = 0;
 while (seq < dumper->next_seq) {
  struct printk_log *msg = log_from_idx(idx);

  l += msg_print_text(msg, prev, syslog, buf + l, size - l);
  idx = log_next(idx);
  seq++;
  prev = msg->flags;
 }

 dumper->next_seq = next_seq;
 dumper->next_idx = next_idx;
 ret = true;
 raw_spin_unlock_irqrestore(&logbuf_lock, flags);
out:
 if (len)
  *len = l;
 return ret;
}
EXPORT_SYMBOL_GPL(kmsg_dump_get_buffer);
void kmsg_dump_rewind_nolock(struct kmsg_dumper *dumper)
{
 dumper->cur_seq = clear_seq;
 dumper->cur_idx = clear_idx;
 dumper->next_seq = log_next_seq;
 dumper->next_idx = log_next_idx;
}
void kmsg_dump_rewind(struct kmsg_dumper *dumper)
{
 unsigned long flags;

 raw_spin_lock_irqsave(&logbuf_lock, flags);
 kmsg_dump_rewind_nolock(dumper);
 raw_spin_unlock_irqrestore(&logbuf_lock, flags);
}
EXPORT_SYMBOL_GPL(kmsg_dump_rewind);

static char dump_stack_arch_desc_str[128];
void __init dump_stack_set_arch_desc(const char *fmt, ...)
{
 va_list args;

 va_start(args, fmt);
 vsnprintf(dump_stack_arch_desc_str, sizeof(dump_stack_arch_desc_str),
    fmt, args);
 va_end(args);
}
void dump_stack_print_info(const char *log_lvl)
{
 printk("%sCPU: %d PID: %d Comm: %.20s %s %s %.*s\n",
        log_lvl, raw_smp_processor_id(), current->pid, current->comm,
        print_tainted(), init_utsname()->release,
        (int)strcspn(init_utsname()->version, " "),
        init_utsname()->version);

 if (dump_stack_arch_desc_str[0] != '\0')
  printk("%sHardware name: %s\n",
         log_lvl, dump_stack_arch_desc_str);

 print_worker_info(log_lvl, current);
}
void show_regs_print_info(const char *log_lvl)
{
 dump_stack_print_info(log_lvl);

 printk("%stask: %p ti: %p task.ti: %p\n",
        log_lvl, current, current_thread_info(),
        task_thread_info(current));
}


static struct proc_dir_entry *root_irq_dir;


static int show_irq_affinity(int type, struct seq_file *m, void *v)
{
 struct irq_desc *desc = irq_to_desc((long)m->private);
 const struct cpumask *mask = desc->irq_common_data.affinity;

 if (irqd_is_setaffinity_pending(&desc->irq_data))
  mask = desc->pending_mask;
 if (type)
  seq_printf(m, "%*pbl\n", cpumask_pr_args(mask));
 else
  seq_printf(m, "%*pb\n", cpumask_pr_args(mask));
 return 0;
}

static int irq_affinity_hint_proc_show(struct seq_file *m, void *v)
{
 struct irq_desc *desc = irq_to_desc((long)m->private);
 unsigned long flags;
 cpumask_var_t mask;

 if (!zalloc_cpumask_var(&mask, GFP_KERNEL))
  return -ENOMEM;

 raw_spin_lock_irqsave(&desc->lock, flags);
 if (desc->affinity_hint)
  cpumask_copy(mask, desc->affinity_hint);
 raw_spin_unlock_irqrestore(&desc->lock, flags);

 seq_printf(m, "%*pb\n", cpumask_pr_args(mask));
 free_cpumask_var(mask);

 return 0;
}


int no_irq_affinity;
static int irq_affinity_proc_show(struct seq_file *m, void *v)
{
 return show_irq_affinity(0, m, v);
}

static int irq_affinity_list_proc_show(struct seq_file *m, void *v)
{
 return show_irq_affinity(1, m, v);
}


static ssize_t write_irq_affinity(int type, struct file *file,
  const char __user *buffer, size_t count, loff_t *pos)
{
 unsigned int irq = (int)(long)PDE_DATA(file_inode(file));
 cpumask_var_t new_value;
 int err;

 if (!irq_can_set_affinity(irq) || no_irq_affinity)
  return -EIO;

 if (!alloc_cpumask_var(&new_value, GFP_KERNEL))
  return -ENOMEM;

 if (type)
  err = cpumask_parselist_user(buffer, count, new_value);
 else
  err = cpumask_parse_user(buffer, count, new_value);
 if (err)
  goto free_cpumask;

 if (!is_affinity_mask_valid(new_value)) {
  err = -EINVAL;
  goto free_cpumask;
 }






 if (!cpumask_intersects(new_value, cpu_online_mask)) {


  err = irq_select_affinity_usr(irq, new_value) ? -EINVAL : count;
 } else {
  irq_set_affinity(irq, new_value);
  err = count;
 }

free_cpumask:
 free_cpumask_var(new_value);
 return err;
}

static ssize_t irq_affinity_proc_write(struct file *file,
  const char __user *buffer, size_t count, loff_t *pos)
{
 return write_irq_affinity(0, file, buffer, count, pos);
}

static ssize_t irq_affinity_list_proc_write(struct file *file,
  const char __user *buffer, size_t count, loff_t *pos)
{
 return write_irq_affinity(1, file, buffer, count, pos);
}

static int irq_affinity_proc_open(struct inode *inode, struct file *file)
{
 return single_open(file, irq_affinity_proc_show, PDE_DATA(inode));
}

static int irq_affinity_list_proc_open(struct inode *inode, struct file *file)
{
 return single_open(file, irq_affinity_list_proc_show, PDE_DATA(inode));
}

static int irq_affinity_hint_proc_open(struct inode *inode, struct file *file)
{
 return single_open(file, irq_affinity_hint_proc_show, PDE_DATA(inode));
}

static const struct file_operations irq_affinity_proc_fops = {
 .open = irq_affinity_proc_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
 .write = irq_affinity_proc_write,
};

static const struct file_operations irq_affinity_hint_proc_fops = {
 .open = irq_affinity_hint_proc_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
};

static const struct file_operations irq_affinity_list_proc_fops = {
 .open = irq_affinity_list_proc_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
 .write = irq_affinity_list_proc_write,
};

static int default_affinity_show(struct seq_file *m, void *v)
{
 seq_printf(m, "%*pb\n", cpumask_pr_args(irq_default_affinity));
 return 0;
}

static ssize_t default_affinity_write(struct file *file,
  const char __user *buffer, size_t count, loff_t *ppos)
{
 cpumask_var_t new_value;
 int err;

 if (!alloc_cpumask_var(&new_value, GFP_KERNEL))
  return -ENOMEM;

 err = cpumask_parse_user(buffer, count, new_value);
 if (err)
  goto out;

 if (!is_affinity_mask_valid(new_value)) {
  err = -EINVAL;
  goto out;
 }






 if (!cpumask_intersects(new_value, cpu_online_mask)) {
  err = -EINVAL;
  goto out;
 }

 cpumask_copy(irq_default_affinity, new_value);
 err = count;

out:
 free_cpumask_var(new_value);
 return err;
}

static int default_affinity_open(struct inode *inode, struct file *file)
{
 return single_open(file, default_affinity_show, PDE_DATA(inode));
}

static const struct file_operations default_affinity_proc_fops = {
 .open = default_affinity_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
 .write = default_affinity_write,
};

static int irq_node_proc_show(struct seq_file *m, void *v)
{
 struct irq_desc *desc = irq_to_desc((long) m->private);

 seq_printf(m, "%d\n", irq_desc_get_node(desc));
 return 0;
}

static int irq_node_proc_open(struct inode *inode, struct file *file)
{
 return single_open(file, irq_node_proc_show, PDE_DATA(inode));
}

static const struct file_operations irq_node_proc_fops = {
 .open = irq_node_proc_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
};

static int irq_spurious_proc_show(struct seq_file *m, void *v)
{
 struct irq_desc *desc = irq_to_desc((long) m->private);

 seq_printf(m, "count %u\n" "unhandled %u\n" "last_unhandled %u ms\n",
     desc->irq_count, desc->irqs_unhandled,
     jiffies_to_msecs(desc->last_unhandled));
 return 0;
}

static int irq_spurious_proc_open(struct inode *inode, struct file *file)
{
 return single_open(file, irq_spurious_proc_show, PDE_DATA(inode));
}

static const struct file_operations irq_spurious_proc_fops = {
 .open = irq_spurious_proc_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
};


static int name_unique(unsigned int irq, struct irqaction *new_action)
{
 struct irq_desc *desc = irq_to_desc(irq);
 struct irqaction *action;
 unsigned long flags;
 int ret = 1;

 raw_spin_lock_irqsave(&desc->lock, flags);
 for_each_action_of_desc(desc, action) {
  if ((action != new_action) && action->name &&
    !strcmp(new_action->name, action->name)) {
   ret = 0;
   break;
  }
 }
 raw_spin_unlock_irqrestore(&desc->lock, flags);
 return ret;
}

void register_handler_proc(unsigned int irq, struct irqaction *action)
{
 char name [MAX_NAMELEN];
 struct irq_desc *desc = irq_to_desc(irq);

 if (!desc->dir || action->dir || !action->name ||
     !name_unique(irq, action))
  return;

 memset(name, 0, MAX_NAMELEN);
 snprintf(name, MAX_NAMELEN, "%s", action->name);


 action->dir = proc_mkdir(name, desc->dir);
}



void register_irq_proc(unsigned int irq, struct irq_desc *desc)
{
 static DEFINE_MUTEX(register_lock);
 char name [MAX_NAMELEN];

 if (!root_irq_dir || (desc->irq_data.chip == &no_irq_chip))
  return;






 mutex_lock(&register_lock);

 if (desc->dir)
  goto out_unlock;

 memset(name, 0, MAX_NAMELEN);
 sprintf(name, "%d", irq);


 desc->dir = proc_mkdir(name, root_irq_dir);
 if (!desc->dir)
  goto out_unlock;


 proc_create_data("smp_affinity", 0644, desc->dir,
    &irq_affinity_proc_fops, (void *)(long)irq);


 proc_create_data("affinity_hint", 0444, desc->dir,
    &irq_affinity_hint_proc_fops, (void *)(long)irq);


 proc_create_data("smp_affinity_list", 0644, desc->dir,
    &irq_affinity_list_proc_fops, (void *)(long)irq);

 proc_create_data("node", 0444, desc->dir,
    &irq_node_proc_fops, (void *)(long)irq);

 proc_create_data("spurious", 0444, desc->dir,
    &irq_spurious_proc_fops, (void *)(long)irq);

out_unlock:
 mutex_unlock(&register_lock);
}

void unregister_irq_proc(unsigned int irq, struct irq_desc *desc)
{
 char name [MAX_NAMELEN];

 if (!root_irq_dir || !desc->dir)
  return;
 remove_proc_entry("smp_affinity", desc->dir);
 remove_proc_entry("affinity_hint", desc->dir);
 remove_proc_entry("smp_affinity_list", desc->dir);
 remove_proc_entry("node", desc->dir);
 remove_proc_entry("spurious", desc->dir);

 memset(name, 0, MAX_NAMELEN);
 sprintf(name, "%u", irq);
 remove_proc_entry(name, root_irq_dir);
}


void unregister_handler_proc(unsigned int irq, struct irqaction *action)
{
 proc_remove(action->dir);
}

static void register_default_affinity_proc(void)
{
 proc_create("irq/default_smp_affinity", 0644, NULL,
      &default_affinity_proc_fops);
}

void init_irq_proc(void)
{
 unsigned int irq;
 struct irq_desc *desc;


 root_irq_dir = proc_mkdir("irq", NULL);
 if (!root_irq_dir)
  return;

 register_default_affinity_proc();




 for_each_irq_desc(irq, desc) {
  if (!desc)
   continue;

  register_irq_proc(irq, desc);
 }
}


int __weak arch_show_interrupts(struct seq_file *p, int prec)
{
 return 0;
}


int show_interrupts(struct seq_file *p, void *v)
{
 static int prec;

 unsigned long flags, any_count = 0;
 int i = *(loff_t *) v, j;
 struct irqaction *action;
 struct irq_desc *desc;

 if (i > ACTUAL_NR_IRQS)
  return 0;

 if (i == ACTUAL_NR_IRQS)
  return arch_show_interrupts(p, prec);


 if (i == 0) {
  for (prec = 3, j = 1000; prec < 10 && j <= nr_irqs; ++prec)
   j *= 10;

  seq_printf(p, "%*s", prec + 8, "");
  for_each_online_cpu(j)
   seq_printf(p, "CPU%-8d", j);
  seq_putc(p, '\n');
 }

 irq_lock_sparse();
 desc = irq_to_desc(i);
 if (!desc)
  goto outsparse;

 raw_spin_lock_irqsave(&desc->lock, flags);
 for_each_online_cpu(j)
  any_count |= kstat_irqs_cpu(i, j);
 action = desc->action;
 if ((!action || irq_desc_is_chained(desc)) && !any_count)
  goto out;

 seq_printf(p, "%*d: ", prec, i);
 for_each_online_cpu(j)
  seq_printf(p, "%10u ", kstat_irqs_cpu(i, j));

 if (desc->irq_data.chip) {
  if (desc->irq_data.chip->irq_print_chip)
   desc->irq_data.chip->irq_print_chip(&desc->irq_data, p);
  else if (desc->irq_data.chip->name)
   seq_printf(p, " %8s", desc->irq_data.chip->name);
  else
   seq_printf(p, " %8s", "-");
 } else {
  seq_printf(p, " %8s", "None");
 }
 if (desc->irq_data.domain)
  seq_printf(p, " %*d", prec, (int) desc->irq_data.hwirq);
 seq_printf(p, " %-8s", irqd_is_level_type(&desc->irq_data) ? "Level" : "Edge");
 if (desc->name)
  seq_printf(p, "-%-8s", desc->name);

 if (action) {
  seq_printf(p, "  %s", action->name);
  while ((action = action->next) != NULL)
   seq_printf(p, ", %s", action->name);
 }

 seq_putc(p, '\n');
out:
 raw_spin_unlock_irqrestore(&desc->lock, flags);
outsparse:
 irq_unlock_sparse();
 return 0;
}





unsigned int __read_mostly freeze_timeout_msecs = 20 * MSEC_PER_SEC;

static int try_to_freeze_tasks(bool user_only)
{
 struct task_struct *g, *p;
 unsigned long end_time;
 unsigned int todo;
 bool wq_busy = false;
 ktime_t start, end, elapsed;
 unsigned int elapsed_msecs;
 bool wakeup = false;
 int sleep_usecs = USEC_PER_MSEC;

 start = ktime_get_boottime();

 end_time = jiffies + msecs_to_jiffies(freeze_timeout_msecs);

 if (!user_only)
  freeze_workqueues_begin();

 while (true) {
  todo = 0;
  read_lock(&tasklist_lock);
  for_each_process_thread(g, p) {
   if (p == current || !freeze_task(p))
    continue;

   if (!freezer_should_skip(p))
    todo++;
  }
  read_unlock(&tasklist_lock);

  if (!user_only) {
   wq_busy = freeze_workqueues_busy();
   todo += wq_busy;
  }

  if (!todo || time_after(jiffies, end_time))
   break;

  if (pm_wakeup_pending()) {
   wakeup = true;
   break;
  }






  usleep_range(sleep_usecs / 2, sleep_usecs);
  if (sleep_usecs < 8 * USEC_PER_MSEC)
   sleep_usecs *= 2;
 }

 end = ktime_get_boottime();
 elapsed = ktime_sub(end, start);
 elapsed_msecs = ktime_to_ms(elapsed);

 if (todo) {
  pr_cont("\n");
  pr_err("Freezing of tasks %s after %d.%03d seconds "
         "(%d tasks refusing to freeze, wq_busy=%d):\n",
         wakeup ? "aborted" : "failed",
         elapsed_msecs / 1000, elapsed_msecs % 1000,
         todo - wq_busy, wq_busy);

  if (!wakeup) {
   read_lock(&tasklist_lock);
   for_each_process_thread(g, p) {
    if (p != current && !freezer_should_skip(p)
        && freezing(p) && !frozen(p))
     sched_show_task(p);
   }
   read_unlock(&tasklist_lock);
  }
 } else {
  pr_cont("(elapsed %d.%03d seconds) ", elapsed_msecs / 1000,
   elapsed_msecs % 1000);
 }

 return todo ? -EBUSY : 0;
}
int freeze_processes(void)
{
 int error;

 error = __usermodehelper_disable(UMH_FREEZING);
 if (error)
  return error;


 current->flags |= PF_SUSPEND_TASK;

 if (!pm_freezing)
  atomic_inc(&system_freezing_cnt);

 pm_wakeup_clear();
 pr_info("Freezing user space processes ... ");
 pm_freezing = true;
 error = try_to_freeze_tasks(true);
 if (!error) {
  __usermodehelper_set_disable_depth(UMH_DISABLED);
  pr_cont("done.");
 }
 pr_cont("\n");
 BUG_ON(in_atomic());






 if (!error && !oom_killer_disable())
  error = -EBUSY;







 if (!error) {
  pr_info("Double checking all user space processes after OOM killer disable... ");
  error = try_to_freeze_tasks(true);
  pr_cont("\n");
 }

 if (error)
  thaw_processes();
 return error;
}
int freeze_kernel_threads(void)
{
 int error;

 pr_info("Freezing remaining freezable tasks ... ");

 pm_nosig_freezing = true;
 error = try_to_freeze_tasks(false);
 if (!error)
  pr_cont("done.");

 pr_cont("\n");
 BUG_ON(in_atomic());

 if (error)
  thaw_kernel_threads();
 return error;
}

void thaw_processes(void)
{
 struct task_struct *g, *p;
 struct task_struct *curr = current;

 trace_suspend_resume(TPS("thaw_processes"), 0, true);
 if (pm_freezing)
  atomic_dec(&system_freezing_cnt);
 pm_freezing = false;
 pm_nosig_freezing = false;

 oom_killer_enable();

 pr_info("Restarting tasks ... ");

 __usermodehelper_set_disable_depth(UMH_FREEZING);
 thaw_workqueues();

 read_lock(&tasklist_lock);
 for_each_process_thread(g, p) {

  WARN_ON((p != curr) && (p->flags & PF_SUSPEND_TASK));
  __thaw_task(p);
 }
 read_unlock(&tasklist_lock);

 WARN_ON(!(curr->flags & PF_SUSPEND_TASK));
 curr->flags &= ~PF_SUSPEND_TASK;

 usermodehelper_enable();

 schedule();
 pr_cont("done.\n");
 trace_suspend_resume(TPS("thaw_processes"), 0, false);
}

void thaw_kernel_threads(void)
{
 struct task_struct *g, *p;

 pm_nosig_freezing = false;
 pr_info("Restarting kernel threads ... ");

 thaw_workqueues();

 read_lock(&tasklist_lock);
 for_each_process_thread(g, p) {
  if (p->flags & (PF_KTHREAD | PF_WQ_WORKER))
   __thaw_task(p);
 }
 read_unlock(&tasklist_lock);

 schedule();
 pr_cont("done.\n");
}

struct profile_hit {
 u32 pc, hits;
};

static atomic_t *prof_buffer;
static unsigned long prof_len, prof_shift;

int prof_on __read_mostly;
EXPORT_SYMBOL_GPL(prof_on);

static cpumask_var_t prof_cpu_mask;
static DEFINE_PER_CPU(struct profile_hit *[2], cpu_profile_hits);
static DEFINE_PER_CPU(int, cpu_profile_flip);
static DEFINE_MUTEX(profile_flip_mutex);

int profile_setup(char *str)
{
 static const char schedstr[] = "schedule";
 static const char sleepstr[] = "sleep";
 static const char kvmstr[] = "kvm";
 int par;

 if (!strncmp(str, sleepstr, strlen(sleepstr))) {
  force_schedstat_enabled();
  prof_on = SLEEP_PROFILING;
  if (str[strlen(sleepstr)] == ',')
   str += strlen(sleepstr) + 1;
  if (get_option(&str, &par))
   prof_shift = par;
  pr_info("kernel sleep profiling enabled (shift: %ld)\n",
   prof_shift);
  pr_warn("kernel sleep profiling requires CONFIG_SCHEDSTATS\n");
 } else if (!strncmp(str, schedstr, strlen(schedstr))) {
  prof_on = SCHED_PROFILING;
  if (str[strlen(schedstr)] == ',')
   str += strlen(schedstr) + 1;
  if (get_option(&str, &par))
   prof_shift = par;
  pr_info("kernel schedule profiling enabled (shift: %ld)\n",
   prof_shift);
 } else if (!strncmp(str, kvmstr, strlen(kvmstr))) {
  prof_on = KVM_PROFILING;
  if (str[strlen(kvmstr)] == ',')
   str += strlen(kvmstr) + 1;
  if (get_option(&str, &par))
   prof_shift = par;
  pr_info("kernel KVM profiling enabled (shift: %ld)\n",
   prof_shift);
 } else if (get_option(&str, &par)) {
  prof_shift = par;
  prof_on = CPU_PROFILING;
  pr_info("kernel profiling enabled (shift: %ld)\n",
   prof_shift);
 }
 return 1;
}
__setup("profile=", profile_setup);


int __ref profile_init(void)
{
 int buffer_bytes;
 if (!prof_on)
  return 0;


 prof_len = (_etext - _stext) >> prof_shift;
 buffer_bytes = prof_len*sizeof(atomic_t);

 if (!alloc_cpumask_var(&prof_cpu_mask, GFP_KERNEL))
  return -ENOMEM;

 cpumask_copy(prof_cpu_mask, cpu_possible_mask);

 prof_buffer = kzalloc(buffer_bytes, GFP_KERNEL|__GFP_NOWARN);
 if (prof_buffer)
  return 0;

 prof_buffer = alloc_pages_exact(buffer_bytes,
     GFP_KERNEL|__GFP_ZERO|__GFP_NOWARN);
 if (prof_buffer)
  return 0;

 prof_buffer = vzalloc(buffer_bytes);
 if (prof_buffer)
  return 0;

 free_cpumask_var(prof_cpu_mask);
 return -ENOMEM;
}



static BLOCKING_NOTIFIER_HEAD(task_exit_notifier);
static ATOMIC_NOTIFIER_HEAD(task_free_notifier);
static BLOCKING_NOTIFIER_HEAD(munmap_notifier);

void profile_task_exit(struct task_struct *task)
{
 blocking_notifier_call_chain(&task_exit_notifier, 0, task);
}

int profile_handoff_task(struct task_struct *task)
{
 int ret;
 ret = atomic_notifier_call_chain(&task_free_notifier, 0, task);
 return (ret == NOTIFY_OK) ? 1 : 0;
}

void profile_munmap(unsigned long addr)
{
 blocking_notifier_call_chain(&munmap_notifier, 0, (void *)addr);
}

int task_handoff_register(struct notifier_block *n)
{
 return atomic_notifier_chain_register(&task_free_notifier, n);
}
EXPORT_SYMBOL_GPL(task_handoff_register);

int task_handoff_unregister(struct notifier_block *n)
{
 return atomic_notifier_chain_unregister(&task_free_notifier, n);
}
EXPORT_SYMBOL_GPL(task_handoff_unregister);

int profile_event_register(enum profile_type type, struct notifier_block *n)
{
 int err = -EINVAL;

 switch (type) {
 case PROFILE_TASK_EXIT:
  err = blocking_notifier_chain_register(
    &task_exit_notifier, n);
  break;
 case PROFILE_MUNMAP:
  err = blocking_notifier_chain_register(
    &munmap_notifier, n);
  break;
 }

 return err;
}
EXPORT_SYMBOL_GPL(profile_event_register);

int profile_event_unregister(enum profile_type type, struct notifier_block *n)
{
 int err = -EINVAL;

 switch (type) {
 case PROFILE_TASK_EXIT:
  err = blocking_notifier_chain_unregister(
    &task_exit_notifier, n);
  break;
 case PROFILE_MUNMAP:
  err = blocking_notifier_chain_unregister(
    &munmap_notifier, n);
  break;
 }

 return err;
}
EXPORT_SYMBOL_GPL(profile_event_unregister);

static void __profile_flip_buffers(void *unused)
{
 int cpu = smp_processor_id();

 per_cpu(cpu_profile_flip, cpu) = !per_cpu(cpu_profile_flip, cpu);
}

static void profile_flip_buffers(void)
{
 int i, j, cpu;

 mutex_lock(&profile_flip_mutex);
 j = per_cpu(cpu_profile_flip, get_cpu());
 put_cpu();
 on_each_cpu(__profile_flip_buffers, NULL, 1);
 for_each_online_cpu(cpu) {
  struct profile_hit *hits = per_cpu(cpu_profile_hits, cpu)[j];
  for (i = 0; i < NR_PROFILE_HIT; ++i) {
   if (!hits[i].hits) {
    if (hits[i].pc)
     hits[i].pc = 0;
    continue;
   }
   atomic_add(hits[i].hits, &prof_buffer[hits[i].pc]);
   hits[i].hits = hits[i].pc = 0;
  }
 }
 mutex_unlock(&profile_flip_mutex);
}

static void profile_discard_flip_buffers(void)
{
 int i, cpu;

 mutex_lock(&profile_flip_mutex);
 i = per_cpu(cpu_profile_flip, get_cpu());
 put_cpu();
 on_each_cpu(__profile_flip_buffers, NULL, 1);
 for_each_online_cpu(cpu) {
  struct profile_hit *hits = per_cpu(cpu_profile_hits, cpu)[i];
  memset(hits, 0, NR_PROFILE_HIT*sizeof(struct profile_hit));
 }
 mutex_unlock(&profile_flip_mutex);
}

static void do_profile_hits(int type, void *__pc, unsigned int nr_hits)
{
 unsigned long primary, secondary, flags, pc = (unsigned long)__pc;
 int i, j, cpu;
 struct profile_hit *hits;

 pc = min((pc - (unsigned long)_stext) >> prof_shift, prof_len - 1);
 i = primary = (pc & (NR_PROFILE_GRP - 1)) << PROFILE_GRPSHIFT;
 secondary = (~(pc << 1) & (NR_PROFILE_GRP - 1)) << PROFILE_GRPSHIFT;
 cpu = get_cpu();
 hits = per_cpu(cpu_profile_hits, cpu)[per_cpu(cpu_profile_flip, cpu)];
 if (!hits) {
  put_cpu();
  return;
 }





 local_irq_save(flags);
 do {
  for (j = 0; j < PROFILE_GRPSZ; ++j) {
   if (hits[i + j].pc == pc) {
    hits[i + j].hits += nr_hits;
    goto out;
   } else if (!hits[i + j].hits) {
    hits[i + j].pc = pc;
    hits[i + j].hits = nr_hits;
    goto out;
   }
  }
  i = (i + secondary) & (NR_PROFILE_HIT - 1);
 } while (i != primary);





 atomic_add(nr_hits, &prof_buffer[pc]);
 for (i = 0; i < NR_PROFILE_HIT; ++i) {
  atomic_add(hits[i].hits, &prof_buffer[hits[i].pc]);
  hits[i].pc = hits[i].hits = 0;
 }
out:
 local_irq_restore(flags);
 put_cpu();
}

static int profile_cpu_callback(struct notifier_block *info,
     unsigned long action, void *__cpu)
{
 int node, cpu = (unsigned long)__cpu;
 struct page *page;

 switch (action) {
 case CPU_UP_PREPARE:
 case CPU_UP_PREPARE_FROZEN:
  node = cpu_to_mem(cpu);
  per_cpu(cpu_profile_flip, cpu) = 0;
  if (!per_cpu(cpu_profile_hits, cpu)[1]) {
   page = __alloc_pages_node(node,
     GFP_KERNEL | __GFP_ZERO,
     0);
   if (!page)
    return notifier_from_errno(-ENOMEM);
   per_cpu(cpu_profile_hits, cpu)[1] = page_address(page);
  }
  if (!per_cpu(cpu_profile_hits, cpu)[0]) {
   page = __alloc_pages_node(node,
     GFP_KERNEL | __GFP_ZERO,
     0);
   if (!page)
    goto out_free;
   per_cpu(cpu_profile_hits, cpu)[0] = page_address(page);
  }
  break;
out_free:
  page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[1]);
  per_cpu(cpu_profile_hits, cpu)[1] = NULL;
  __free_page(page);
  return notifier_from_errno(-ENOMEM);
 case CPU_ONLINE:
 case CPU_ONLINE_FROZEN:
  if (prof_cpu_mask != NULL)
   cpumask_set_cpu(cpu, prof_cpu_mask);
  break;
 case CPU_UP_CANCELED:
 case CPU_UP_CANCELED_FROZEN:
 case CPU_DEAD:
 case CPU_DEAD_FROZEN:
  if (prof_cpu_mask != NULL)
   cpumask_clear_cpu(cpu, prof_cpu_mask);
  if (per_cpu(cpu_profile_hits, cpu)[0]) {
   page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[0]);
   per_cpu(cpu_profile_hits, cpu)[0] = NULL;
   __free_page(page);
  }
  if (per_cpu(cpu_profile_hits, cpu)[1]) {
   page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[1]);
   per_cpu(cpu_profile_hits, cpu)[1] = NULL;
   __free_page(page);
  }
  break;
 }
 return NOTIFY_OK;
}

static void do_profile_hits(int type, void *__pc, unsigned int nr_hits)
{
 unsigned long pc;
 pc = ((unsigned long)__pc - (unsigned long)_stext) >> prof_shift;
 atomic_add(nr_hits, &prof_buffer[min(pc, prof_len - 1)]);
}

void profile_hits(int type, void *__pc, unsigned int nr_hits)
{
 if (prof_on != type || !prof_buffer)
  return;
 do_profile_hits(type, __pc, nr_hits);
}
EXPORT_SYMBOL_GPL(profile_hits);

void profile_tick(int type)
{
 struct pt_regs *regs = get_irq_regs();

 if (!user_mode(regs) && prof_cpu_mask != NULL &&
     cpumask_test_cpu(smp_processor_id(), prof_cpu_mask))
  profile_hit(type, (void *)profile_pc(regs));
}


static int prof_cpu_mask_proc_show(struct seq_file *m, void *v)
{
 seq_printf(m, "%*pb\n", cpumask_pr_args(prof_cpu_mask));
 return 0;
}

static int prof_cpu_mask_proc_open(struct inode *inode, struct file *file)
{
 return single_open(file, prof_cpu_mask_proc_show, NULL);
}

static ssize_t prof_cpu_mask_proc_write(struct file *file,
 const char __user *buffer, size_t count, loff_t *pos)
{
 cpumask_var_t new_value;
 int err;

 if (!alloc_cpumask_var(&new_value, GFP_KERNEL))
  return -ENOMEM;

 err = cpumask_parse_user(buffer, count, new_value);
 if (!err) {
  cpumask_copy(prof_cpu_mask, new_value);
  err = count;
 }
 free_cpumask_var(new_value);
 return err;
}

static const struct file_operations prof_cpu_mask_proc_fops = {
 .open = prof_cpu_mask_proc_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
 .write = prof_cpu_mask_proc_write,
};

void create_prof_cpu_mask(void)
{

 proc_create("irq/prof_cpu_mask", 0600, NULL, &prof_cpu_mask_proc_fops);
}







static ssize_t
read_profile(struct file *file, char __user *buf, size_t count, loff_t *ppos)
{
 unsigned long p = *ppos;
 ssize_t read;
 char *pnt;
 unsigned int sample_step = 1 << prof_shift;

 profile_flip_buffers();
 if (p >= (prof_len+1)*sizeof(unsigned int))
  return 0;
 if (count > (prof_len+1)*sizeof(unsigned int) - p)
  count = (prof_len+1)*sizeof(unsigned int) - p;
 read = 0;

 while (p < sizeof(unsigned int) && count > 0) {
  if (put_user(*((char *)(&sample_step)+p), buf))
   return -EFAULT;
  buf++; p++; count--; read++;
 }
 pnt = (char *)prof_buffer + p - sizeof(atomic_t);
 if (copy_to_user(buf, (void *)pnt, count))
  return -EFAULT;
 read += count;
 *ppos += read;
 return read;
}







static ssize_t write_profile(struct file *file, const char __user *buf,
        size_t count, loff_t *ppos)
{
 extern int setup_profiling_timer(unsigned int multiplier);

 if (count == sizeof(int)) {
  unsigned int multiplier;

  if (copy_from_user(&multiplier, buf, sizeof(int)))
   return -EFAULT;

  if (setup_profiling_timer(multiplier))
   return -EINVAL;
 }
 profile_discard_flip_buffers();
 memset(prof_buffer, 0, prof_len * sizeof(atomic_t));
 return count;
}

static const struct file_operations proc_profile_operations = {
 .read = read_profile,
 .write = write_profile,
 .llseek = default_llseek,
};

static void profile_nop(void *unused)
{
}

static int create_hash_tables(void)
{
 int cpu;

 for_each_online_cpu(cpu) {
  int node = cpu_to_mem(cpu);
  struct page *page;

  page = __alloc_pages_node(node,
    GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
    0);
  if (!page)
   goto out_cleanup;
  per_cpu(cpu_profile_hits, cpu)[1]
    = (struct profile_hit *)page_address(page);
  page = __alloc_pages_node(node,
    GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
    0);
  if (!page)
   goto out_cleanup;
  per_cpu(cpu_profile_hits, cpu)[0]
    = (struct profile_hit *)page_address(page);
 }
 return 0;
out_cleanup:
 prof_on = 0;
 smp_mb();
 on_each_cpu(profile_nop, NULL, 1);
 for_each_online_cpu(cpu) {
  struct page *page;

  if (per_cpu(cpu_profile_hits, cpu)[0]) {
   page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[0]);
   per_cpu(cpu_profile_hits, cpu)[0] = NULL;
   __free_page(page);
  }
  if (per_cpu(cpu_profile_hits, cpu)[1]) {
   page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[1]);
   per_cpu(cpu_profile_hits, cpu)[1] = NULL;
   __free_page(page);
  }
 }
 return -1;
}

int __ref create_proc_profile(void)
{
 struct proc_dir_entry *entry;
 int err = 0;

 if (!prof_on)
  return 0;

 cpu_notifier_register_begin();

 if (create_hash_tables()) {
  err = -ENOMEM;
  goto out;
 }

 entry = proc_create("profile", S_IWUSR | S_IRUGO,
       NULL, &proc_profile_operations);
 if (!entry)
  goto out;
 proc_set_size(entry, (1 + prof_len) * sizeof(atomic_t));
 __hotcpu_notifier(profile_cpu_callback, 0);

out:
 cpu_notifier_register_done();
 return err;
}
subsys_initcall(create_proc_profile);
void __ptrace_link(struct task_struct *child, struct task_struct *new_parent)
{
 BUG_ON(!list_empty(&child->ptrace_entry));
 list_add(&child->ptrace_entry, &new_parent->ptraced);
 child->parent = new_parent;
}
void __ptrace_unlink(struct task_struct *child)
{
 BUG_ON(!child->ptrace);

 child->parent = child->real_parent;
 list_del_init(&child->ptrace_entry);

 spin_lock(&child->sighand->siglock);
 child->ptrace = 0;




 task_clear_jobctl_pending(child, JOBCTL_TRAP_MASK);
 task_clear_jobctl_trapping(child);





 if (!(child->flags & PF_EXITING) &&
     (child->signal->flags & SIGNAL_STOP_STOPPED ||
      child->signal->group_stop_count)) {
  child->jobctl |= JOBCTL_STOP_PENDING;
  if (!(child->jobctl & JOBCTL_STOP_SIGMASK))
   child->jobctl |= SIGSTOP;
 }







 if (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))
  ptrace_signal_wake_up(child, true);

 spin_unlock(&child->sighand->siglock);
}


static bool ptrace_freeze_traced(struct task_struct *task)
{
 bool ret = false;


 if (task->jobctl & JOBCTL_LISTENING)
  return ret;

 spin_lock_irq(&task->sighand->siglock);
 if (task_is_traced(task) && !__fatal_signal_pending(task)) {
  task->state = __TASK_TRACED;
  ret = true;
 }
 spin_unlock_irq(&task->sighand->siglock);

 return ret;
}

static void ptrace_unfreeze_traced(struct task_struct *task)
{
 if (task->state != __TASK_TRACED)
  return;

 WARN_ON(!task->ptrace || task->parent != current);

 spin_lock_irq(&task->sighand->siglock);
 if (__fatal_signal_pending(task))
  wake_up_state(task, __TASK_TRACED);
 else
  task->state = TASK_TRACED;
 spin_unlock_irq(&task->sighand->siglock);
}
static int ptrace_check_attach(struct task_struct *child, bool ignore_state)
{
 int ret = -ESRCH;
 read_lock(&tasklist_lock);
 if (child->ptrace && child->parent == current) {
  WARN_ON(child->state == __TASK_TRACED);




  if (ignore_state || ptrace_freeze_traced(child))
   ret = 0;
 }
 read_unlock(&tasklist_lock);

 if (!ret && !ignore_state) {
  if (!wait_task_inactive(child, __TASK_TRACED)) {





   WARN_ON(child->state == __TASK_TRACED);
   ret = -ESRCH;
  }
 }

 return ret;
}

static int ptrace_has_cap(struct user_namespace *ns, unsigned int mode)
{
 if (mode & PTRACE_MODE_NOAUDIT)
  return has_ns_capability_noaudit(current, ns, CAP_SYS_PTRACE);
 else
  return has_ns_capability(current, ns, CAP_SYS_PTRACE);
}


static int __ptrace_may_access(struct task_struct *task, unsigned int mode)
{
 const struct cred *cred = current_cred(), *tcred;
 int dumpable = 0;
 kuid_t caller_uid;
 kgid_t caller_gid;

 if (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {
  WARN(1, "denying ptrace access check without PTRACE_MODE_*CREDS\n");
  return -EPERM;
 }
 if (same_thread_group(task, current))
  return 0;
 rcu_read_lock();
 if (mode & PTRACE_MODE_FSCREDS) {
  caller_uid = cred->fsuid;
  caller_gid = cred->fsgid;
 } else {
  caller_uid = cred->uid;
  caller_gid = cred->gid;
 }
 tcred = __task_cred(task);
 if (uid_eq(caller_uid, tcred->euid) &&
     uid_eq(caller_uid, tcred->suid) &&
     uid_eq(caller_uid, tcred->uid) &&
     gid_eq(caller_gid, tcred->egid) &&
     gid_eq(caller_gid, tcred->sgid) &&
     gid_eq(caller_gid, tcred->gid))
  goto ok;
 if (ptrace_has_cap(tcred->user_ns, mode))
  goto ok;
 rcu_read_unlock();
 return -EPERM;
ok:
 rcu_read_unlock();
 smp_rmb();
 if (task->mm)
  dumpable = get_dumpable(task->mm);
 rcu_read_lock();
 if (dumpable != SUID_DUMP_USER &&
     !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {
  rcu_read_unlock();
  return -EPERM;
 }
 rcu_read_unlock();

 return security_ptrace_access_check(task, mode);
}

bool ptrace_may_access(struct task_struct *task, unsigned int mode)
{
 int err;
 task_lock(task);
 err = __ptrace_may_access(task, mode);
 task_unlock(task);
 return !err;
}

static int ptrace_attach(struct task_struct *task, long request,
    unsigned long addr,
    unsigned long flags)
{
 bool seize = (request == PTRACE_SEIZE);
 int retval;

 retval = -EIO;
 if (seize) {
  if (addr != 0)
   goto out;
  if (flags & ~(unsigned long)PTRACE_O_MASK)
   goto out;
  flags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);
 } else {
  flags = PT_PTRACED;
 }

 audit_ptrace(task);

 retval = -EPERM;
 if (unlikely(task->flags & PF_KTHREAD))
  goto out;
 if (same_thread_group(task, current))
  goto out;






 retval = -ERESTARTNOINTR;
 if (mutex_lock_interruptible(&task->signal->cred_guard_mutex))
  goto out;

 task_lock(task);
 retval = __ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS);
 task_unlock(task);
 if (retval)
  goto unlock_creds;

 write_lock_irq(&tasklist_lock);
 retval = -EPERM;
 if (unlikely(task->exit_state))
  goto unlock_tasklist;
 if (task->ptrace)
  goto unlock_tasklist;

 if (seize)
  flags |= PT_SEIZED;
 rcu_read_lock();
 if (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))
  flags |= PT_PTRACE_CAP;
 rcu_read_unlock();
 task->ptrace = flags;

 __ptrace_link(task, current);


 if (!seize)
  send_sig_info(SIGSTOP, SEND_SIG_FORCED, task);

 spin_lock(&task->sighand->siglock);
 if (task_is_stopped(task) &&
     task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))
  signal_wake_up_state(task, __TASK_STOPPED);

 spin_unlock(&task->sighand->siglock);

 retval = 0;
unlock_tasklist:
 write_unlock_irq(&tasklist_lock);
unlock_creds:
 mutex_unlock(&task->signal->cred_guard_mutex);
out:
 if (!retval) {







  wait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT, TASK_KILLABLE);
  proc_ptrace_connector(task, PTRACE_ATTACH);
 }

 return retval;
}







static int ptrace_traceme(void)
{
 int ret = -EPERM;

 write_lock_irq(&tasklist_lock);

 if (!current->ptrace) {
  ret = security_ptrace_traceme(current->parent);





  if (!ret && !(current->real_parent->flags & PF_EXITING)) {
   current->ptrace = PT_PTRACED;
   __ptrace_link(current, current->real_parent);
  }
 }
 write_unlock_irq(&tasklist_lock);

 return ret;
}




static int ignoring_children(struct sighand_struct *sigh)
{
 int ret;
 spin_lock(&sigh->siglock);
 ret = (sigh->action[SIGCHLD-1].sa.sa_handler == SIG_IGN) ||
       (sigh->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT);
 spin_unlock(&sigh->siglock);
 return ret;
}
static bool __ptrace_detach(struct task_struct *tracer, struct task_struct *p)
{
 bool dead;

 __ptrace_unlink(p);

 if (p->exit_state != EXIT_ZOMBIE)
  return false;

 dead = !thread_group_leader(p);

 if (!dead && thread_group_empty(p)) {
  if (!same_thread_group(p->real_parent, tracer))
   dead = do_notify_parent(p, p->exit_signal);
  else if (ignoring_children(tracer->sighand)) {
   __wake_up_parent(p, tracer);
   dead = true;
  }
 }

 if (dead)
  p->exit_state = EXIT_DEAD;
 return dead;
}

static int ptrace_detach(struct task_struct *child, unsigned int data)
{
 if (!valid_signal(data))
  return -EIO;


 ptrace_disable(child);
 clear_tsk_thread_flag(child, TIF_SYSCALL_TRACE);

 write_lock_irq(&tasklist_lock);




 WARN_ON(!child->ptrace || child->exit_state);




 child->exit_code = data;
 __ptrace_detach(current, child);
 write_unlock_irq(&tasklist_lock);

 proc_ptrace_connector(child, PTRACE_DETACH);

 return 0;
}





void exit_ptrace(struct task_struct *tracer, struct list_head *dead)
{
 struct task_struct *p, *n;

 list_for_each_entry_safe(p, n, &tracer->ptraced, ptrace_entry) {
  if (unlikely(p->ptrace & PT_EXITKILL))
   send_sig_info(SIGKILL, SEND_SIG_FORCED, p);

  if (__ptrace_detach(tracer, p))
   list_add(&p->ptrace_entry, dead);
 }
}

int ptrace_readdata(struct task_struct *tsk, unsigned long src, char __user *dst, int len)
{
 int copied = 0;

 while (len > 0) {
  char buf[128];
  int this_len, retval;

  this_len = (len > sizeof(buf)) ? sizeof(buf) : len;
  retval = access_process_vm(tsk, src, buf, this_len, 0);
  if (!retval) {
   if (copied)
    break;
   return -EIO;
  }
  if (copy_to_user(dst, buf, retval))
   return -EFAULT;
  copied += retval;
  src += retval;
  dst += retval;
  len -= retval;
 }
 return copied;
}

int ptrace_writedata(struct task_struct *tsk, char __user *src, unsigned long dst, int len)
{
 int copied = 0;

 while (len > 0) {
  char buf[128];
  int this_len, retval;

  this_len = (len > sizeof(buf)) ? sizeof(buf) : len;
  if (copy_from_user(buf, src, this_len))
   return -EFAULT;
  retval = access_process_vm(tsk, dst, buf, this_len, 1);
  if (!retval) {
   if (copied)
    break;
   return -EIO;
  }
  copied += retval;
  src += retval;
  dst += retval;
  len -= retval;
 }
 return copied;
}

static int ptrace_setoptions(struct task_struct *child, unsigned long data)
{
 unsigned flags;

 if (data & ~(unsigned long)PTRACE_O_MASK)
  return -EINVAL;

 if (unlikely(data & PTRACE_O_SUSPEND_SECCOMP)) {
  if (!config_enabled(CONFIG_CHECKPOINT_RESTORE) ||
      !config_enabled(CONFIG_SECCOMP))
   return -EINVAL;

  if (!capable(CAP_SYS_ADMIN))
   return -EPERM;

  if (seccomp_mode(&current->seccomp) != SECCOMP_MODE_DISABLED ||
      current->ptrace & PT_SUSPEND_SECCOMP)
   return -EPERM;
 }


 flags = child->ptrace;
 flags &= ~(PTRACE_O_MASK << PT_OPT_FLAG_SHIFT);
 flags |= (data << PT_OPT_FLAG_SHIFT);
 child->ptrace = flags;

 return 0;
}

static int ptrace_getsiginfo(struct task_struct *child, siginfo_t *info)
{
 unsigned long flags;
 int error = -ESRCH;

 if (lock_task_sighand(child, &flags)) {
  error = -EINVAL;
  if (likely(child->last_siginfo != NULL)) {
   *info = *child->last_siginfo;
   error = 0;
  }
  unlock_task_sighand(child, &flags);
 }
 return error;
}

static int ptrace_setsiginfo(struct task_struct *child, const siginfo_t *info)
{
 unsigned long flags;
 int error = -ESRCH;

 if (lock_task_sighand(child, &flags)) {
  error = -EINVAL;
  if (likely(child->last_siginfo != NULL)) {
   *child->last_siginfo = *info;
   error = 0;
  }
  unlock_task_sighand(child, &flags);
 }
 return error;
}

static int ptrace_peek_siginfo(struct task_struct *child,
    unsigned long addr,
    unsigned long data)
{
 struct ptrace_peeksiginfo_args arg;
 struct sigpending *pending;
 struct sigqueue *q;
 int ret, i;

 ret = copy_from_user(&arg, (void __user *) addr,
    sizeof(struct ptrace_peeksiginfo_args));
 if (ret)
  return -EFAULT;

 if (arg.flags & ~PTRACE_PEEKSIGINFO_SHARED)
  return -EINVAL;

 if (arg.nr < 0)
  return -EINVAL;

 if (arg.flags & PTRACE_PEEKSIGINFO_SHARED)
  pending = &child->signal->shared_pending;
 else
  pending = &child->pending;

 for (i = 0; i < arg.nr; ) {
  siginfo_t info;
  s32 off = arg.off + i;

  spin_lock_irq(&child->sighand->siglock);
  list_for_each_entry(q, &pending->list, list) {
   if (!off--) {
    copy_siginfo(&info, &q->info);
    break;
   }
  }
  spin_unlock_irq(&child->sighand->siglock);

  if (off >= 0)
   break;

  if (unlikely(in_compat_syscall())) {
   compat_siginfo_t __user *uinfo = compat_ptr(data);

   if (copy_siginfo_to_user32(uinfo, &info) ||
       __put_user(info.si_code, &uinfo->si_code)) {
    ret = -EFAULT;
    break;
   }

  } else
  {
   siginfo_t __user *uinfo = (siginfo_t __user *) data;

   if (copy_siginfo_to_user(uinfo, &info) ||
       __put_user(info.si_code, &uinfo->si_code)) {
    ret = -EFAULT;
    break;
   }
  }

  data += sizeof(siginfo_t);
  i++;

  if (signal_pending(current))
   break;

  cond_resched();
 }

 if (i > 0)
  return i;

 return ret;
}




static int ptrace_resume(struct task_struct *child, long request,
    unsigned long data)
{
 bool need_siglock;

 if (!valid_signal(data))
  return -EIO;

 if (request == PTRACE_SYSCALL)
  set_tsk_thread_flag(child, TIF_SYSCALL_TRACE);
 else
  clear_tsk_thread_flag(child, TIF_SYSCALL_TRACE);

 if (request == PTRACE_SYSEMU || request == PTRACE_SYSEMU_SINGLESTEP)
  set_tsk_thread_flag(child, TIF_SYSCALL_EMU);
 else
  clear_tsk_thread_flag(child, TIF_SYSCALL_EMU);

 if (is_singleblock(request)) {
  if (unlikely(!arch_has_block_step()))
   return -EIO;
  user_enable_block_step(child);
 } else if (is_singlestep(request) || is_sysemu_singlestep(request)) {
  if (unlikely(!arch_has_single_step()))
   return -EIO;
  user_enable_single_step(child);
 } else {
  user_disable_single_step(child);
 }
 need_siglock = data && !thread_group_empty(current);
 if (need_siglock)
  spin_lock_irq(&child->sighand->siglock);
 child->exit_code = data;
 wake_up_state(child, __TASK_TRACED);
 if (need_siglock)
  spin_unlock_irq(&child->sighand->siglock);

 return 0;
}


static const struct user_regset *
find_regset(const struct user_regset_view *view, unsigned int type)
{
 const struct user_regset *regset;
 int n;

 for (n = 0; n < view->n; ++n) {
  regset = view->regsets + n;
  if (regset->core_note_type == type)
   return regset;
 }

 return NULL;
}

static int ptrace_regset(struct task_struct *task, int req, unsigned int type,
    struct iovec *kiov)
{
 const struct user_regset_view *view = task_user_regset_view(task);
 const struct user_regset *regset = find_regset(view, type);
 int regset_no;

 if (!regset || (kiov->iov_len % regset->size) != 0)
  return -EINVAL;

 regset_no = regset - view->regsets;
 kiov->iov_len = min(kiov->iov_len,
       (__kernel_size_t) (regset->n * regset->size));

 if (req == PTRACE_GETREGSET)
  return copy_regset_to_user(task, view, regset_no, 0,
        kiov->iov_len, kiov->iov_base);
 else
  return copy_regset_from_user(task, view, regset_no, 0,
          kiov->iov_len, kiov->iov_base);
}






EXPORT_SYMBOL_GPL(task_user_regset_view);

int ptrace_request(struct task_struct *child, long request,
     unsigned long addr, unsigned long data)
{
 bool seized = child->ptrace & PT_SEIZED;
 int ret = -EIO;
 siginfo_t siginfo, *si;
 void __user *datavp = (void __user *) data;
 unsigned long __user *datalp = datavp;
 unsigned long flags;

 switch (request) {
 case PTRACE_PEEKTEXT:
 case PTRACE_PEEKDATA:
  return generic_ptrace_peekdata(child, addr, data);
 case PTRACE_POKETEXT:
 case PTRACE_POKEDATA:
  return generic_ptrace_pokedata(child, addr, data);

 case PTRACE_OLDSETOPTIONS:
 case PTRACE_SETOPTIONS:
  ret = ptrace_setoptions(child, data);
  break;
 case PTRACE_GETEVENTMSG:
  ret = put_user(child->ptrace_message, datalp);
  break;

 case PTRACE_PEEKSIGINFO:
  ret = ptrace_peek_siginfo(child, addr, data);
  break;

 case PTRACE_GETSIGINFO:
  ret = ptrace_getsiginfo(child, &siginfo);
  if (!ret)
   ret = copy_siginfo_to_user(datavp, &siginfo);
  break;

 case PTRACE_SETSIGINFO:
  if (copy_from_user(&siginfo, datavp, sizeof siginfo))
   ret = -EFAULT;
  else
   ret = ptrace_setsiginfo(child, &siginfo);
  break;

 case PTRACE_GETSIGMASK:
  if (addr != sizeof(sigset_t)) {
   ret = -EINVAL;
   break;
  }

  if (copy_to_user(datavp, &child->blocked, sizeof(sigset_t)))
   ret = -EFAULT;
  else
   ret = 0;

  break;

 case PTRACE_SETSIGMASK: {
  sigset_t new_set;

  if (addr != sizeof(sigset_t)) {
   ret = -EINVAL;
   break;
  }

  if (copy_from_user(&new_set, datavp, sizeof(sigset_t))) {
   ret = -EFAULT;
   break;
  }

  sigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));






  spin_lock_irq(&child->sighand->siglock);
  child->blocked = new_set;
  spin_unlock_irq(&child->sighand->siglock);

  ret = 0;
  break;
 }

 case PTRACE_INTERRUPT:
  if (unlikely(!seized || !lock_task_sighand(child, &flags)))
   break;







  if (likely(task_set_jobctl_pending(child, JOBCTL_TRAP_STOP)))
   ptrace_signal_wake_up(child, child->jobctl & JOBCTL_LISTENING);

  unlock_task_sighand(child, &flags);
  ret = 0;
  break;

 case PTRACE_LISTEN:
  if (unlikely(!seized || !lock_task_sighand(child, &flags)))
   break;

  si = child->last_siginfo;
  if (likely(si && (si->si_code >> 8) == PTRACE_EVENT_STOP)) {
   child->jobctl |= JOBCTL_LISTENING;




   if (child->jobctl & JOBCTL_TRAP_NOTIFY)
    ptrace_signal_wake_up(child, true);
   ret = 0;
  }
  unlock_task_sighand(child, &flags);
  break;

 case PTRACE_DETACH:
  ret = ptrace_detach(child, data);
  break;

 case PTRACE_GETFDPIC: {
  struct mm_struct *mm = get_task_mm(child);
  unsigned long tmp = 0;

  ret = -ESRCH;
  if (!mm)
   break;

  switch (addr) {
  case PTRACE_GETFDPIC_EXEC:
   tmp = mm->context.exec_fdpic_loadmap;
   break;
  case PTRACE_GETFDPIC_INTERP:
   tmp = mm->context.interp_fdpic_loadmap;
   break;
  default:
   break;
  }
  mmput(mm);

  ret = put_user(tmp, datalp);
  break;
 }

 case PTRACE_SINGLESTEP:
 case PTRACE_SINGLEBLOCK:
 case PTRACE_SYSEMU:
 case PTRACE_SYSEMU_SINGLESTEP:
 case PTRACE_SYSCALL:
 case PTRACE_CONT:
  return ptrace_resume(child, request, data);

 case PTRACE_KILL:
  if (child->exit_state)
   return 0;
  return ptrace_resume(child, request, SIGKILL);

 case PTRACE_GETREGSET:
 case PTRACE_SETREGSET: {
  struct iovec kiov;
  struct iovec __user *uiov = datavp;

  if (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))
   return -EFAULT;

  if (__get_user(kiov.iov_base, &uiov->iov_base) ||
      __get_user(kiov.iov_len, &uiov->iov_len))
   return -EFAULT;

  ret = ptrace_regset(child, request, addr, &kiov);
  if (!ret)
   ret = __put_user(kiov.iov_len, &uiov->iov_len);
  break;
 }

 case PTRACE_SECCOMP_GET_FILTER:
  ret = seccomp_get_filter(child, addr, datavp);
  break;

 default:
  break;
 }

 return ret;
}

static struct task_struct *ptrace_get_task_struct(pid_t pid)
{
 struct task_struct *child;

 rcu_read_lock();
 child = find_task_by_vpid(pid);
 if (child)
  get_task_struct(child);
 rcu_read_unlock();

 if (!child)
  return ERR_PTR(-ESRCH);
 return child;
}


SYSCALL_DEFINE4(ptrace, long, request, long, pid, unsigned long, addr,
  unsigned long, data)
{
 struct task_struct *child;
 long ret;

 if (request == PTRACE_TRACEME) {
  ret = ptrace_traceme();
  if (!ret)
   arch_ptrace_attach(current);
  goto out;
 }

 child = ptrace_get_task_struct(pid);
 if (IS_ERR(child)) {
  ret = PTR_ERR(child);
  goto out;
 }

 if (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {
  ret = ptrace_attach(child, request, addr, data);




  if (!ret)
   arch_ptrace_attach(child);
  goto out_put_task_struct;
 }

 ret = ptrace_check_attach(child, request == PTRACE_KILL ||
      request == PTRACE_INTERRUPT);
 if (ret < 0)
  goto out_put_task_struct;

 ret = arch_ptrace(child, request, addr, data);
 if (ret || request != PTRACE_DETACH)
  ptrace_unfreeze_traced(child);

 out_put_task_struct:
 put_task_struct(child);
 out:
 return ret;
}

int generic_ptrace_peekdata(struct task_struct *tsk, unsigned long addr,
       unsigned long data)
{
 unsigned long tmp;
 int copied;

 copied = access_process_vm(tsk, addr, &tmp, sizeof(tmp), 0);
 if (copied != sizeof(tmp))
  return -EIO;
 return put_user(tmp, (unsigned long __user *)data);
}

int generic_ptrace_pokedata(struct task_struct *tsk, unsigned long addr,
       unsigned long data)
{
 int copied;

 copied = access_process_vm(tsk, addr, &data, sizeof(data), 1);
 return (copied == sizeof(data)) ? 0 : -EIO;
}


int compat_ptrace_request(struct task_struct *child, compat_long_t request,
     compat_ulong_t addr, compat_ulong_t data)
{
 compat_ulong_t __user *datap = compat_ptr(data);
 compat_ulong_t word;
 siginfo_t siginfo;
 int ret;

 switch (request) {
 case PTRACE_PEEKTEXT:
 case PTRACE_PEEKDATA:
  ret = access_process_vm(child, addr, &word, sizeof(word), 0);
  if (ret != sizeof(word))
   ret = -EIO;
  else
   ret = put_user(word, datap);
  break;

 case PTRACE_POKETEXT:
 case PTRACE_POKEDATA:
  ret = access_process_vm(child, addr, &data, sizeof(data), 1);
  ret = (ret != sizeof(data) ? -EIO : 0);
  break;

 case PTRACE_GETEVENTMSG:
  ret = put_user((compat_ulong_t) child->ptrace_message, datap);
  break;

 case PTRACE_GETSIGINFO:
  ret = ptrace_getsiginfo(child, &siginfo);
  if (!ret)
   ret = copy_siginfo_to_user32(
    (struct compat_siginfo __user *) datap,
    &siginfo);
  break;

 case PTRACE_SETSIGINFO:
  memset(&siginfo, 0, sizeof siginfo);
  if (copy_siginfo_from_user32(
       &siginfo, (struct compat_siginfo __user *) datap))
   ret = -EFAULT;
  else
   ret = ptrace_setsiginfo(child, &siginfo);
  break;
 case PTRACE_GETREGSET:
 case PTRACE_SETREGSET:
 {
  struct iovec kiov;
  struct compat_iovec __user *uiov =
   (struct compat_iovec __user *) datap;
  compat_uptr_t ptr;
  compat_size_t len;

  if (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))
   return -EFAULT;

  if (__get_user(ptr, &uiov->iov_base) ||
      __get_user(len, &uiov->iov_len))
   return -EFAULT;

  kiov.iov_base = compat_ptr(ptr);
  kiov.iov_len = len;

  ret = ptrace_regset(child, request, addr, &kiov);
  if (!ret)
   ret = __put_user(kiov.iov_len, &uiov->iov_len);
  break;
 }

 default:
  ret = ptrace_request(child, request, addr, data);
 }

 return ret;
}

COMPAT_SYSCALL_DEFINE4(ptrace, compat_long_t, request, compat_long_t, pid,
         compat_long_t, addr, compat_long_t, data)
{
 struct task_struct *child;
 long ret;

 if (request == PTRACE_TRACEME) {
  ret = ptrace_traceme();
  goto out;
 }

 child = ptrace_get_task_struct(pid);
 if (IS_ERR(child)) {
  ret = PTR_ERR(child);
  goto out;
 }

 if (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {
  ret = ptrace_attach(child, request, addr, data);




  if (!ret)
   arch_ptrace_attach(child);
  goto out_put_task_struct;
 }

 ret = ptrace_check_attach(child, request == PTRACE_KILL ||
      request == PTRACE_INTERRUPT);
 if (!ret) {
  ret = compat_arch_ptrace(child, request, addr, data);
  if (ret || request != PTRACE_DETACH)
   ptrace_unfreeze_traced(child);
 }

 out_put_task_struct:
 put_task_struct(child);
 out:
 return ret;
}







struct pm_qos_object {
 struct pm_qos_constraints *constraints;
 struct miscdevice pm_qos_power_miscdev;
 char *name;
};

static DEFINE_SPINLOCK(pm_qos_lock);

static struct pm_qos_object null_pm_qos;

static BLOCKING_NOTIFIER_HEAD(cpu_dma_lat_notifier);
static struct pm_qos_constraints cpu_dma_constraints = {
 .list = PLIST_HEAD_INIT(cpu_dma_constraints.list),
 .target_value = PM_QOS_CPU_DMA_LAT_DEFAULT_VALUE,
 .default_value = PM_QOS_CPU_DMA_LAT_DEFAULT_VALUE,
 .no_constraint_value = PM_QOS_CPU_DMA_LAT_DEFAULT_VALUE,
 .type = PM_QOS_MIN,
 .notifiers = &cpu_dma_lat_notifier,
};
static struct pm_qos_object cpu_dma_pm_qos = {
 .constraints = &cpu_dma_constraints,
 .name = "cpu_dma_latency",
};

static BLOCKING_NOTIFIER_HEAD(network_lat_notifier);
static struct pm_qos_constraints network_lat_constraints = {
 .list = PLIST_HEAD_INIT(network_lat_constraints.list),
 .target_value = PM_QOS_NETWORK_LAT_DEFAULT_VALUE,
 .default_value = PM_QOS_NETWORK_LAT_DEFAULT_VALUE,
 .no_constraint_value = PM_QOS_NETWORK_LAT_DEFAULT_VALUE,
 .type = PM_QOS_MIN,
 .notifiers = &network_lat_notifier,
};
static struct pm_qos_object network_lat_pm_qos = {
 .constraints = &network_lat_constraints,
 .name = "network_latency",
};


static BLOCKING_NOTIFIER_HEAD(network_throughput_notifier);
static struct pm_qos_constraints network_tput_constraints = {
 .list = PLIST_HEAD_INIT(network_tput_constraints.list),
 .target_value = PM_QOS_NETWORK_THROUGHPUT_DEFAULT_VALUE,
 .default_value = PM_QOS_NETWORK_THROUGHPUT_DEFAULT_VALUE,
 .no_constraint_value = PM_QOS_NETWORK_THROUGHPUT_DEFAULT_VALUE,
 .type = PM_QOS_MAX,
 .notifiers = &network_throughput_notifier,
};
static struct pm_qos_object network_throughput_pm_qos = {
 .constraints = &network_tput_constraints,
 .name = "network_throughput",
};


static BLOCKING_NOTIFIER_HEAD(memory_bandwidth_notifier);
static struct pm_qos_constraints memory_bw_constraints = {
 .list = PLIST_HEAD_INIT(memory_bw_constraints.list),
 .target_value = PM_QOS_MEMORY_BANDWIDTH_DEFAULT_VALUE,
 .default_value = PM_QOS_MEMORY_BANDWIDTH_DEFAULT_VALUE,
 .no_constraint_value = PM_QOS_MEMORY_BANDWIDTH_DEFAULT_VALUE,
 .type = PM_QOS_SUM,
 .notifiers = &memory_bandwidth_notifier,
};
static struct pm_qos_object memory_bandwidth_pm_qos = {
 .constraints = &memory_bw_constraints,
 .name = "memory_bandwidth",
};


static struct pm_qos_object *pm_qos_array[] = {
 &null_pm_qos,
 &cpu_dma_pm_qos,
 &network_lat_pm_qos,
 &network_throughput_pm_qos,
 &memory_bandwidth_pm_qos,
};

static ssize_t pm_qos_power_write(struct file *filp, const char __user *buf,
  size_t count, loff_t *f_pos);
static ssize_t pm_qos_power_read(struct file *filp, char __user *buf,
  size_t count, loff_t *f_pos);
static int pm_qos_power_open(struct inode *inode, struct file *filp);
static int pm_qos_power_release(struct inode *inode, struct file *filp);

static const struct file_operations pm_qos_power_fops = {
 .write = pm_qos_power_write,
 .read = pm_qos_power_read,
 .open = pm_qos_power_open,
 .release = pm_qos_power_release,
 .llseek = noop_llseek,
};


static inline int pm_qos_get_value(struct pm_qos_constraints *c)
{
 struct plist_node *node;
 int total_value = 0;

 if (plist_head_empty(&c->list))
  return c->no_constraint_value;

 switch (c->type) {
 case PM_QOS_MIN:
  return plist_first(&c->list)->prio;

 case PM_QOS_MAX:
  return plist_last(&c->list)->prio;

 case PM_QOS_SUM:
  plist_for_each(node, &c->list)
   total_value += node->prio;

  return total_value;

 default:

  BUG();
  return PM_QOS_DEFAULT_VALUE;
 }
}

s32 pm_qos_read_value(struct pm_qos_constraints *c)
{
 return c->target_value;
}

static inline void pm_qos_set_value(struct pm_qos_constraints *c, s32 value)
{
 c->target_value = value;
}

static inline int pm_qos_get_value(struct pm_qos_constraints *c);
static int pm_qos_dbg_show_requests(struct seq_file *s, void *unused)
{
 struct pm_qos_object *qos = (struct pm_qos_object *)s->private;
 struct pm_qos_constraints *c;
 struct pm_qos_request *req;
 char *type;
 unsigned long flags;
 int tot_reqs = 0;
 int active_reqs = 0;

 if (IS_ERR_OR_NULL(qos)) {
  pr_err("%s: bad qos param!\n", __func__);
  return -EINVAL;
 }
 c = qos->constraints;
 if (IS_ERR_OR_NULL(c)) {
  pr_err("%s: Bad constraints on qos?\n", __func__);
  return -EINVAL;
 }


 spin_lock_irqsave(&pm_qos_lock, flags);
 if (plist_head_empty(&c->list)) {
  seq_puts(s, "Empty!\n");
  goto out;
 }

 switch (c->type) {
 case PM_QOS_MIN:
  type = "Minimum";
  break;
 case PM_QOS_MAX:
  type = "Maximum";
  break;
 case PM_QOS_SUM:
  type = "Sum";
  break;
 default:
  type = "Unknown";
 }

 plist_for_each_entry(req, &c->list, node) {
  char *state = "Default";

  if ((req->node).prio != c->default_value) {
   active_reqs++;
   state = "Active";
  }
  tot_reqs++;
  seq_printf(s, "%d: %d: %s\n", tot_reqs,
      (req->node).prio, state);
 }

 seq_printf(s, "Type=%s, Value=%d, Requests: active=%d / total=%d\n",
     type, pm_qos_get_value(c), active_reqs, tot_reqs);

out:
 spin_unlock_irqrestore(&pm_qos_lock, flags);
 return 0;
}

static int pm_qos_dbg_open(struct inode *inode, struct file *file)
{
 return single_open(file, pm_qos_dbg_show_requests,
      inode->i_private);
}

static const struct file_operations pm_qos_debug_fops = {
 .open = pm_qos_dbg_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
};
int pm_qos_update_target(struct pm_qos_constraints *c, struct plist_node *node,
    enum pm_qos_req_action action, int value)
{
 unsigned long flags;
 int prev_value, curr_value, new_value;
 int ret;

 spin_lock_irqsave(&pm_qos_lock, flags);
 prev_value = pm_qos_get_value(c);
 if (value == PM_QOS_DEFAULT_VALUE)
  new_value = c->default_value;
 else
  new_value = value;

 switch (action) {
 case PM_QOS_REMOVE_REQ:
  plist_del(node, &c->list);
  break;
 case PM_QOS_UPDATE_REQ:





  plist_del(node, &c->list);
 case PM_QOS_ADD_REQ:
  plist_node_init(node, new_value);
  plist_add(node, &c->list);
  break;
 default:

  ;
 }

 curr_value = pm_qos_get_value(c);
 pm_qos_set_value(c, curr_value);

 spin_unlock_irqrestore(&pm_qos_lock, flags);

 trace_pm_qos_update_target(action, prev_value, curr_value);
 if (prev_value != curr_value) {
  ret = 1;
  if (c->notifiers)
   blocking_notifier_call_chain(c->notifiers,
           (unsigned long)curr_value,
           NULL);
 } else {
  ret = 0;
 }
 return ret;
}






static void pm_qos_flags_remove_req(struct pm_qos_flags *pqf,
        struct pm_qos_flags_request *req)
{
 s32 val = 0;

 list_del(&req->node);
 list_for_each_entry(req, &pqf->list, node)
  val |= req->flags;

 pqf->effective_flags = val;
}
bool pm_qos_update_flags(struct pm_qos_flags *pqf,
    struct pm_qos_flags_request *req,
    enum pm_qos_req_action action, s32 val)
{
 unsigned long irqflags;
 s32 prev_value, curr_value;

 spin_lock_irqsave(&pm_qos_lock, irqflags);

 prev_value = list_empty(&pqf->list) ? 0 : pqf->effective_flags;

 switch (action) {
 case PM_QOS_REMOVE_REQ:
  pm_qos_flags_remove_req(pqf, req);
  break;
 case PM_QOS_UPDATE_REQ:
  pm_qos_flags_remove_req(pqf, req);
 case PM_QOS_ADD_REQ:
  req->flags = val;
  INIT_LIST_HEAD(&req->node);
  list_add_tail(&req->node, &pqf->list);
  pqf->effective_flags |= val;
  break;
 default:

  ;
 }

 curr_value = list_empty(&pqf->list) ? 0 : pqf->effective_flags;

 spin_unlock_irqrestore(&pm_qos_lock, irqflags);

 trace_pm_qos_update_flags(action, prev_value, curr_value);
 return prev_value != curr_value;
}







int pm_qos_request(int pm_qos_class)
{
 return pm_qos_read_value(pm_qos_array[pm_qos_class]->constraints);
}
EXPORT_SYMBOL_GPL(pm_qos_request);

int pm_qos_request_active(struct pm_qos_request *req)
{
 return req->pm_qos_class != 0;
}
EXPORT_SYMBOL_GPL(pm_qos_request_active);

static void __pm_qos_update_request(struct pm_qos_request *req,
      s32 new_value)
{
 trace_pm_qos_update_request(req->pm_qos_class, new_value);

 if (new_value != req->node.prio)
  pm_qos_update_target(
   pm_qos_array[req->pm_qos_class]->constraints,
   &req->node, PM_QOS_UPDATE_REQ, new_value);
}







static void pm_qos_work_fn(struct work_struct *work)
{
 struct pm_qos_request *req = container_of(to_delayed_work(work),
        struct pm_qos_request,
        work);

 __pm_qos_update_request(req, PM_QOS_DEFAULT_VALUE);
}
void pm_qos_add_request(struct pm_qos_request *req,
   int pm_qos_class, s32 value)
{
 if (!req)
  return;

 if (pm_qos_request_active(req)) {
  WARN(1, KERN_ERR "pm_qos_add_request() called for already added request\n");
  return;
 }
 req->pm_qos_class = pm_qos_class;
 INIT_DELAYED_WORK(&req->work, pm_qos_work_fn);
 trace_pm_qos_add_request(pm_qos_class, value);
 pm_qos_update_target(pm_qos_array[pm_qos_class]->constraints,
        &req->node, PM_QOS_ADD_REQ, value);
}
EXPORT_SYMBOL_GPL(pm_qos_add_request);
void pm_qos_update_request(struct pm_qos_request *req,
      s32 new_value)
{
 if (!req)
  return;

 if (!pm_qos_request_active(req)) {
  WARN(1, KERN_ERR "pm_qos_update_request() called for unknown object\n");
  return;
 }

 cancel_delayed_work_sync(&req->work);
 __pm_qos_update_request(req, new_value);
}
EXPORT_SYMBOL_GPL(pm_qos_update_request);
void pm_qos_update_request_timeout(struct pm_qos_request *req, s32 new_value,
       unsigned long timeout_us)
{
 if (!req)
  return;
 if (WARN(!pm_qos_request_active(req),
   "%s called for unknown object.", __func__))
  return;

 cancel_delayed_work_sync(&req->work);

 trace_pm_qos_update_request_timeout(req->pm_qos_class,
         new_value, timeout_us);
 if (new_value != req->node.prio)
  pm_qos_update_target(
   pm_qos_array[req->pm_qos_class]->constraints,
   &req->node, PM_QOS_UPDATE_REQ, new_value);

 schedule_delayed_work(&req->work, usecs_to_jiffies(timeout_us));
}
void pm_qos_remove_request(struct pm_qos_request *req)
{
 if (!req)
  return;


 if (!pm_qos_request_active(req)) {
  WARN(1, KERN_ERR "pm_qos_remove_request() called for unknown object\n");
  return;
 }

 cancel_delayed_work_sync(&req->work);

 trace_pm_qos_remove_request(req->pm_qos_class, PM_QOS_DEFAULT_VALUE);
 pm_qos_update_target(pm_qos_array[req->pm_qos_class]->constraints,
        &req->node, PM_QOS_REMOVE_REQ,
        PM_QOS_DEFAULT_VALUE);
 memset(req, 0, sizeof(*req));
}
EXPORT_SYMBOL_GPL(pm_qos_remove_request);
int pm_qos_add_notifier(int pm_qos_class, struct notifier_block *notifier)
{
 int retval;

 retval = blocking_notifier_chain_register(
   pm_qos_array[pm_qos_class]->constraints->notifiers,
   notifier);

 return retval;
}
EXPORT_SYMBOL_GPL(pm_qos_add_notifier);
int pm_qos_remove_notifier(int pm_qos_class, struct notifier_block *notifier)
{
 int retval;

 retval = blocking_notifier_chain_unregister(
   pm_qos_array[pm_qos_class]->constraints->notifiers,
   notifier);

 return retval;
}
EXPORT_SYMBOL_GPL(pm_qos_remove_notifier);


static int register_pm_qos_misc(struct pm_qos_object *qos, struct dentry *d)
{
 qos->pm_qos_power_miscdev.minor = MISC_DYNAMIC_MINOR;
 qos->pm_qos_power_miscdev.name = qos->name;
 qos->pm_qos_power_miscdev.fops = &pm_qos_power_fops;

 if (d) {
  (void)debugfs_create_file(qos->name, S_IRUGO, d,
       (void *)qos, &pm_qos_debug_fops);
 }

 return misc_register(&qos->pm_qos_power_miscdev);
}

static int find_pm_qos_object_by_minor(int minor)
{
 int pm_qos_class;

 for (pm_qos_class = PM_QOS_CPU_DMA_LATENCY;
  pm_qos_class < PM_QOS_NUM_CLASSES; pm_qos_class++) {
  if (minor ==
   pm_qos_array[pm_qos_class]->pm_qos_power_miscdev.minor)
   return pm_qos_class;
 }
 return -1;
}

static int pm_qos_power_open(struct inode *inode, struct file *filp)
{
 long pm_qos_class;

 pm_qos_class = find_pm_qos_object_by_minor(iminor(inode));
 if (pm_qos_class >= PM_QOS_CPU_DMA_LATENCY) {
  struct pm_qos_request *req = kzalloc(sizeof(*req), GFP_KERNEL);
  if (!req)
   return -ENOMEM;

  pm_qos_add_request(req, pm_qos_class, PM_QOS_DEFAULT_VALUE);
  filp->private_data = req;

  return 0;
 }
 return -EPERM;
}

static int pm_qos_power_release(struct inode *inode, struct file *filp)
{
 struct pm_qos_request *req;

 req = filp->private_data;
 pm_qos_remove_request(req);
 kfree(req);

 return 0;
}


static ssize_t pm_qos_power_read(struct file *filp, char __user *buf,
  size_t count, loff_t *f_pos)
{
 s32 value;
 unsigned long flags;
 struct pm_qos_request *req = filp->private_data;

 if (!req)
  return -EINVAL;
 if (!pm_qos_request_active(req))
  return -EINVAL;

 spin_lock_irqsave(&pm_qos_lock, flags);
 value = pm_qos_get_value(pm_qos_array[req->pm_qos_class]->constraints);
 spin_unlock_irqrestore(&pm_qos_lock, flags);

 return simple_read_from_buffer(buf, count, f_pos, &value, sizeof(s32));
}

static ssize_t pm_qos_power_write(struct file *filp, const char __user *buf,
  size_t count, loff_t *f_pos)
{
 s32 value;
 struct pm_qos_request *req;

 if (count == sizeof(s32)) {
  if (copy_from_user(&value, buf, sizeof(s32)))
   return -EFAULT;
 } else {
  int ret;

  ret = kstrtos32_from_user(buf, count, 16, &value);
  if (ret)
   return ret;
 }

 req = filp->private_data;
 pm_qos_update_request(req, value);

 return count;
}


static int __init pm_qos_power_init(void)
{
 int ret = 0;
 int i;
 struct dentry *d;

 BUILD_BUG_ON(ARRAY_SIZE(pm_qos_array) != PM_QOS_NUM_CLASSES);

 d = debugfs_create_dir("pm_qos", NULL);
 if (IS_ERR_OR_NULL(d))
  d = NULL;

 for (i = PM_QOS_CPU_DMA_LATENCY; i < PM_QOS_NUM_CLASSES; i++) {
  ret = register_pm_qos_misc(pm_qos_array[i], d);
  if (ret < 0) {
   printk(KERN_ERR "pm_qos_param: %s setup failed\n",
          pm_qos_array[i]->name);
   return ret;
  }
 }

 return ret;
}

late_initcall(pm_qos_power_init);




int add_range(struct range *range, int az, int nr_range, u64 start, u64 end)
{
 if (start >= end)
  return nr_range;


 if (nr_range >= az)
  return nr_range;

 range[nr_range].start = start;
 range[nr_range].end = end;

 nr_range++;

 return nr_range;
}

int add_range_with_merge(struct range *range, int az, int nr_range,
       u64 start, u64 end)
{
 int i;

 if (start >= end)
  return nr_range;


 for (i = 0; i < nr_range; i++) {
  u64 common_start, common_end;

  if (!range[i].end)
   continue;

  common_start = max(range[i].start, start);
  common_end = min(range[i].end, end);
  if (common_start > common_end)
   continue;


  start = min(range[i].start, start);
  end = max(range[i].end, end);

  memmove(&range[i], &range[i + 1],
   (nr_range - (i + 1)) * sizeof(range[i]));
  range[nr_range - 1].start = 0;
  range[nr_range - 1].end = 0;
  nr_range--;
  i--;
 }


 return add_range(range, az, nr_range, start, end);
}

void subtract_range(struct range *range, int az, u64 start, u64 end)
{
 int i, j;

 if (start >= end)
  return;

 for (j = 0; j < az; j++) {
  if (!range[j].end)
   continue;

  if (start <= range[j].start && end >= range[j].end) {
   range[j].start = 0;
   range[j].end = 0;
   continue;
  }

  if (start <= range[j].start && end < range[j].end &&
      range[j].start < end) {
   range[j].start = end;
   continue;
  }


  if (start > range[j].start && end >= range[j].end &&
      range[j].end > start) {
   range[j].end = start;
   continue;
  }

  if (start > range[j].start && end < range[j].end) {

   for (i = 0; i < az; i++) {
    if (range[i].end == 0)
     break;
   }
   if (i < az) {
    range[i].end = range[j].end;
    range[i].start = end;
   } else {
    pr_err("%s: run out of slot in ranges\n",
     __func__);
   }
   range[j].end = start;
   continue;
  }
 }
}

static int cmp_range(const void *x1, const void *x2)
{
 const struct range *r1 = x1;
 const struct range *r2 = x2;

 if (r1->start < r2->start)
  return -1;
 if (r1->start > r2->start)
  return 1;
 return 0;
}

int clean_sort_range(struct range *range, int az)
{
 int i, j, k = az - 1, nr_range = az;

 for (i = 0; i < k; i++) {
  if (range[i].end)
   continue;
  for (j = k; j > i; j--) {
   if (range[j].end) {
    k = j;
    break;
   }
  }
  if (j == i)
   break;
  range[i].start = range[k].start;
  range[i].end = range[k].end;
  range[k].start = 0;
  range[k].end = 0;
  k--;
 }

 for (i = 0; i < az; i++) {
  if (!range[i].end) {
   nr_range = i;
   break;
  }
 }


 sort(range, nr_range, sizeof(struct range), cmp_range, NULL);

 return nr_range;
}

void sort_range(struct range *range, int nr_range)
{

 sort(range, nr_range, sizeof(struct range), cmp_range, NULL);
}












int C_A_D = 1;
struct pid *cad_pid;
EXPORT_SYMBOL(cad_pid);

enum reboot_mode reboot_mode DEFAULT_REBOOT_MODE;
int reboot_default = 1;
int reboot_cpu;
enum reboot_type reboot_type = BOOT_ACPI;
int reboot_force;





void (*pm_power_off_prepare)(void);
void emergency_restart(void)
{
 kmsg_dump(KMSG_DUMP_EMERG);
 machine_emergency_restart();
}
EXPORT_SYMBOL_GPL(emergency_restart);

void kernel_restart_prepare(char *cmd)
{
 blocking_notifier_call_chain(&reboot_notifier_list, SYS_RESTART, cmd);
 system_state = SYSTEM_RESTART;
 usermodehelper_disable();
 device_shutdown();
}
int register_reboot_notifier(struct notifier_block *nb)
{
 return blocking_notifier_chain_register(&reboot_notifier_list, nb);
}
EXPORT_SYMBOL(register_reboot_notifier);
int unregister_reboot_notifier(struct notifier_block *nb)
{
 return blocking_notifier_chain_unregister(&reboot_notifier_list, nb);
}
EXPORT_SYMBOL(unregister_reboot_notifier);





static ATOMIC_NOTIFIER_HEAD(restart_handler_list);
int register_restart_handler(struct notifier_block *nb)
{
 return atomic_notifier_chain_register(&restart_handler_list, nb);
}
EXPORT_SYMBOL(register_restart_handler);
int unregister_restart_handler(struct notifier_block *nb)
{
 return atomic_notifier_chain_unregister(&restart_handler_list, nb);
}
EXPORT_SYMBOL(unregister_restart_handler);
void do_kernel_restart(char *cmd)
{
 atomic_notifier_call_chain(&restart_handler_list, reboot_mode, cmd);
}

void migrate_to_reboot_cpu(void)
{

 int cpu = reboot_cpu;

 cpu_hotplug_disable();


 if (!cpu_online(cpu))
  cpu = cpumask_first(cpu_online_mask);


 current->flags |= PF_NO_SETAFFINITY;


 set_cpus_allowed_ptr(current, cpumask_of(cpu));
}
void kernel_restart(char *cmd)
{
 kernel_restart_prepare(cmd);
 migrate_to_reboot_cpu();
 syscore_shutdown();
 if (!cmd)
  pr_emerg("Restarting system\n");
 else
  pr_emerg("Restarting system with command '%s'\n", cmd);
 kmsg_dump(KMSG_DUMP_RESTART);
 machine_restart(cmd);
}
EXPORT_SYMBOL_GPL(kernel_restart);

static void kernel_shutdown_prepare(enum system_states state)
{
 blocking_notifier_call_chain(&reboot_notifier_list,
  (state == SYSTEM_HALT) ? SYS_HALT : SYS_POWER_OFF, NULL);
 system_state = state;
 usermodehelper_disable();
 device_shutdown();
}





void kernel_halt(void)
{
 kernel_shutdown_prepare(SYSTEM_HALT);
 migrate_to_reboot_cpu();
 syscore_shutdown();
 pr_emerg("System halted\n");
 kmsg_dump(KMSG_DUMP_HALT);
 machine_halt();
}
EXPORT_SYMBOL_GPL(kernel_halt);






void kernel_power_off(void)
{
 kernel_shutdown_prepare(SYSTEM_POWER_OFF);
 if (pm_power_off_prepare)
  pm_power_off_prepare();
 migrate_to_reboot_cpu();
 syscore_shutdown();
 pr_emerg("Power down\n");
 kmsg_dump(KMSG_DUMP_POWEROFF);
 machine_power_off();
}
EXPORT_SYMBOL_GPL(kernel_power_off);

static DEFINE_MUTEX(reboot_mutex);
SYSCALL_DEFINE4(reboot, int, magic1, int, magic2, unsigned int, cmd,
  void __user *, arg)
{
 struct pid_namespace *pid_ns = task_active_pid_ns(current);
 char buffer[256];
 int ret = 0;


 if (!ns_capable(pid_ns->user_ns, CAP_SYS_BOOT))
  return -EPERM;


 if (magic1 != LINUX_REBOOT_MAGIC1 ||
   (magic2 != LINUX_REBOOT_MAGIC2 &&
   magic2 != LINUX_REBOOT_MAGIC2A &&
   magic2 != LINUX_REBOOT_MAGIC2B &&
   magic2 != LINUX_REBOOT_MAGIC2C))
  return -EINVAL;






 ret = reboot_pid_ns(pid_ns, cmd);
 if (ret)
  return ret;




 if ((cmd == LINUX_REBOOT_CMD_POWER_OFF) && !pm_power_off)
  cmd = LINUX_REBOOT_CMD_HALT;

 mutex_lock(&reboot_mutex);
 switch (cmd) {
 case LINUX_REBOOT_CMD_RESTART:
  kernel_restart(NULL);
  break;

 case LINUX_REBOOT_CMD_CAD_ON:
  C_A_D = 1;
  break;

 case LINUX_REBOOT_CMD_CAD_OFF:
  C_A_D = 0;
  break;

 case LINUX_REBOOT_CMD_HALT:
  kernel_halt();
  do_exit(0);
  panic("cannot halt");

 case LINUX_REBOOT_CMD_POWER_OFF:
  kernel_power_off();
  do_exit(0);
  break;

 case LINUX_REBOOT_CMD_RESTART2:
  ret = strncpy_from_user(&buffer[0], arg, sizeof(buffer) - 1);
  if (ret < 0) {
   ret = -EFAULT;
   break;
  }
  buffer[sizeof(buffer) - 1] = '\0';

  kernel_restart(buffer);
  break;

 case LINUX_REBOOT_CMD_KEXEC:
  ret = kernel_kexec();
  break;

 case LINUX_REBOOT_CMD_SW_SUSPEND:
  ret = hibernate();
  break;

 default:
  ret = -EINVAL;
  break;
 }
 mutex_unlock(&reboot_mutex);
 return ret;
}

static void deferred_cad(struct work_struct *dummy)
{
 kernel_restart(NULL);
}






void ctrl_alt_del(void)
{
 static DECLARE_WORK(cad_work, deferred_cad);

 if (C_A_D)
  schedule_work(&cad_work);
 else
  kill_cad_pid(SIGINT, 1);
}

char poweroff_cmd[POWEROFF_CMD_PATH_LEN] = "/sbin/poweroff";
static const char reboot_cmd[] = "/sbin/reboot";

static int run_cmd(const char *cmd)
{
 char **argv;
 static char *envp[] = {
  "HOME=/",
  "PATH=/sbin:/bin:/usr/sbin:/usr/bin",
  NULL
 };
 int ret;
 argv = argv_split(GFP_KERNEL, cmd, NULL);
 if (argv) {
  ret = call_usermodehelper(argv[0], argv, envp, UMH_WAIT_EXEC);
  argv_free(argv);
 } else {
  ret = -ENOMEM;
 }

 return ret;
}

static int __orderly_reboot(void)
{
 int ret;

 ret = run_cmd(reboot_cmd);

 if (ret) {
  pr_warn("Failed to start orderly reboot: forcing the issue\n");
  emergency_sync();
  kernel_restart(NULL);
 }

 return ret;
}

static int __orderly_poweroff(bool force)
{
 int ret;

 ret = run_cmd(poweroff_cmd);

 if (ret && force) {
  pr_warn("Failed to start orderly shutdown: forcing the issue\n");






  emergency_sync();
  kernel_power_off();
 }

 return ret;
}

static bool poweroff_force;

static void poweroff_work_func(struct work_struct *work)
{
 __orderly_poweroff(poweroff_force);
}

static DECLARE_WORK(poweroff_work, poweroff_work_func);
void orderly_poweroff(bool force)
{
 if (force)
  poweroff_force = true;
 schedule_work(&poweroff_work);
}
EXPORT_SYMBOL_GPL(orderly_poweroff);

static void reboot_work_func(struct work_struct *work)
{
 __orderly_reboot();
}

static DECLARE_WORK(reboot_work, reboot_work_func);







void orderly_reboot(void)
{
 schedule_work(&reboot_work);
}
EXPORT_SYMBOL_GPL(orderly_reboot);

static int __init reboot_setup(char *str)
{
 for (;;) {





  reboot_default = 0;

  switch (*str) {
  case 'w':
   reboot_mode = REBOOT_WARM;
   break;

  case 'c':
   reboot_mode = REBOOT_COLD;
   break;

  case 'h':
   reboot_mode = REBOOT_HARD;
   break;

  case 's':
  {
   int rc;

   if (isdigit(*(str+1))) {
    rc = kstrtoint(str+1, 0, &reboot_cpu);
    if (rc)
     return rc;
   } else if (str[1] == 'm' && str[2] == 'p' &&
       isdigit(*(str+3))) {
    rc = kstrtoint(str+3, 0, &reboot_cpu);
    if (rc)
     return rc;
   } else
    reboot_mode = REBOOT_SOFT;
   break;
  }
  case 'g':
   reboot_mode = REBOOT_GPIO;
   break;

  case 'b':
  case 'a':
  case 'k':
  case 't':
  case 'e':
  case 'p':
   reboot_type = *str;
   break;

  case 'f':
   reboot_force = 1;
   break;
  }

  str = strchr(str, ',');
  if (str)
   str++;
  else
   break;
 }
 return 1;
}
__setup("reboot=", reboot_setup);


static DEFINE_MUTEX(relay_channels_mutex);
static LIST_HEAD(relay_channels);




static void relay_file_mmap_close(struct vm_area_struct *vma)
{
 struct rchan_buf *buf = vma->vm_private_data;
 buf->chan->cb->buf_unmapped(buf, vma->vm_file);
}




static int relay_buf_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
{
 struct page *page;
 struct rchan_buf *buf = vma->vm_private_data;
 pgoff_t pgoff = vmf->pgoff;

 if (!buf)
  return VM_FAULT_OOM;

 page = vmalloc_to_page(buf->start + (pgoff << PAGE_SHIFT));
 if (!page)
  return VM_FAULT_SIGBUS;
 get_page(page);
 vmf->page = page;

 return 0;
}




static const struct vm_operations_struct relay_file_mmap_ops = {
 .fault = relay_buf_fault,
 .close = relay_file_mmap_close,
};




static struct page **relay_alloc_page_array(unsigned int n_pages)
{
 const size_t pa_size = n_pages * sizeof(struct page *);
 if (pa_size > PAGE_SIZE)
  return vzalloc(pa_size);
 return kzalloc(pa_size, GFP_KERNEL);
}




static void relay_free_page_array(struct page **array)
{
 kvfree(array);
}
static int relay_mmap_buf(struct rchan_buf *buf, struct vm_area_struct *vma)
{
 unsigned long length = vma->vm_end - vma->vm_start;
 struct file *filp = vma->vm_file;

 if (!buf)
  return -EBADF;

 if (length != (unsigned long)buf->chan->alloc_size)
  return -EINVAL;

 vma->vm_ops = &relay_file_mmap_ops;
 vma->vm_flags |= VM_DONTEXPAND;
 vma->vm_private_data = buf;
 buf->chan->cb->buf_mapped(buf, filp);

 return 0;
}
static void *relay_alloc_buf(struct rchan_buf *buf, size_t *size)
{
 void *mem;
 unsigned int i, j, n_pages;

 *size = PAGE_ALIGN(*size);
 n_pages = *size >> PAGE_SHIFT;

 buf->page_array = relay_alloc_page_array(n_pages);
 if (!buf->page_array)
  return NULL;

 for (i = 0; i < n_pages; i++) {
  buf->page_array[i] = alloc_page(GFP_KERNEL);
  if (unlikely(!buf->page_array[i]))
   goto depopulate;
  set_page_private(buf->page_array[i], (unsigned long)buf);
 }
 mem = vmap(buf->page_array, n_pages, VM_MAP, PAGE_KERNEL);
 if (!mem)
  goto depopulate;

 memset(mem, 0, *size);
 buf->page_count = n_pages;
 return mem;

depopulate:
 for (j = 0; j < i; j++)
  __free_page(buf->page_array[j]);
 relay_free_page_array(buf->page_array);
 return NULL;
}







static struct rchan_buf *relay_create_buf(struct rchan *chan)
{
 struct rchan_buf *buf;

 if (chan->n_subbufs > UINT_MAX / sizeof(size_t *))
  return NULL;

 buf = kzalloc(sizeof(struct rchan_buf), GFP_KERNEL);
 if (!buf)
  return NULL;
 buf->padding = kmalloc(chan->n_subbufs * sizeof(size_t *), GFP_KERNEL);
 if (!buf->padding)
  goto free_buf;

 buf->start = relay_alloc_buf(buf, &chan->alloc_size);
 if (!buf->start)
  goto free_buf;

 buf->chan = chan;
 kref_get(&buf->chan->kref);
 return buf;

free_buf:
 kfree(buf->padding);
 kfree(buf);
 return NULL;
}







static void relay_destroy_channel(struct kref *kref)
{
 struct rchan *chan = container_of(kref, struct rchan, kref);
 kfree(chan);
}





static void relay_destroy_buf(struct rchan_buf *buf)
{
 struct rchan *chan = buf->chan;
 unsigned int i;

 if (likely(buf->start)) {
  vunmap(buf->start);
  for (i = 0; i < buf->page_count; i++)
   __free_page(buf->page_array[i]);
  relay_free_page_array(buf->page_array);
 }
 chan->buf[buf->cpu] = NULL;
 kfree(buf->padding);
 kfree(buf);
 kref_put(&chan->kref, relay_destroy_channel);
}
static void relay_remove_buf(struct kref *kref)
{
 struct rchan_buf *buf = container_of(kref, struct rchan_buf, kref);
 relay_destroy_buf(buf);
}







static int relay_buf_empty(struct rchan_buf *buf)
{
 return (buf->subbufs_produced - buf->subbufs_consumed) ? 0 : 1;
}







int relay_buf_full(struct rchan_buf *buf)
{
 size_t ready = buf->subbufs_produced - buf->subbufs_consumed;
 return (ready >= buf->chan->n_subbufs) ? 1 : 0;
}
EXPORT_SYMBOL_GPL(relay_buf_full);
static int subbuf_start_default_callback (struct rchan_buf *buf,
       void *subbuf,
       void *prev_subbuf,
       size_t prev_padding)
{
 if (relay_buf_full(buf))
  return 0;

 return 1;
}




static void buf_mapped_default_callback(struct rchan_buf *buf,
     struct file *filp)
{
}




static void buf_unmapped_default_callback(struct rchan_buf *buf,
       struct file *filp)
{
}




static struct dentry *create_buf_file_default_callback(const char *filename,
             struct dentry *parent,
             umode_t mode,
             struct rchan_buf *buf,
             int *is_global)
{
 return NULL;
}




static int remove_buf_file_default_callback(struct dentry *dentry)
{
 return -EINVAL;
}


static struct rchan_callbacks default_channel_callbacks = {
 .subbuf_start = subbuf_start_default_callback,
 .buf_mapped = buf_mapped_default_callback,
 .buf_unmapped = buf_unmapped_default_callback,
 .create_buf_file = create_buf_file_default_callback,
 .remove_buf_file = remove_buf_file_default_callback,
};







static void wakeup_readers(unsigned long data)
{
 struct rchan_buf *buf = (struct rchan_buf *)data;
 wake_up_interruptible(&buf->read_wait);
}
static void __relay_reset(struct rchan_buf *buf, unsigned int init)
{
 size_t i;

 if (init) {
  init_waitqueue_head(&buf->read_wait);
  kref_init(&buf->kref);
  setup_timer(&buf->timer, wakeup_readers, (unsigned long)buf);
 } else
  del_timer_sync(&buf->timer);

 buf->subbufs_produced = 0;
 buf->subbufs_consumed = 0;
 buf->bytes_consumed = 0;
 buf->finalized = 0;
 buf->data = buf->start;
 buf->offset = 0;

 for (i = 0; i < buf->chan->n_subbufs; i++)
  buf->padding[i] = 0;

 buf->chan->cb->subbuf_start(buf, buf->data, NULL, 0);
}
void relay_reset(struct rchan *chan)
{
 unsigned int i;

 if (!chan)
  return;

 if (chan->is_global && chan->buf[0]) {
  __relay_reset(chan->buf[0], 0);
  return;
 }

 mutex_lock(&relay_channels_mutex);
 for_each_possible_cpu(i)
  if (chan->buf[i])
   __relay_reset(chan->buf[i], 0);
 mutex_unlock(&relay_channels_mutex);
}
EXPORT_SYMBOL_GPL(relay_reset);

static inline void relay_set_buf_dentry(struct rchan_buf *buf,
     struct dentry *dentry)
{
 buf->dentry = dentry;
 d_inode(buf->dentry)->i_size = buf->early_bytes;
}

static struct dentry *relay_create_buf_file(struct rchan *chan,
         struct rchan_buf *buf,
         unsigned int cpu)
{
 struct dentry *dentry;
 char *tmpname;

 tmpname = kzalloc(NAME_MAX + 1, GFP_KERNEL);
 if (!tmpname)
  return NULL;
 snprintf(tmpname, NAME_MAX, "%s%d", chan->base_filename, cpu);


 dentry = chan->cb->create_buf_file(tmpname, chan->parent,
        S_IRUSR, buf,
        &chan->is_global);

 kfree(tmpname);

 return dentry;
}






static struct rchan_buf *relay_open_buf(struct rchan *chan, unsigned int cpu)
{
  struct rchan_buf *buf = NULL;
 struct dentry *dentry;

  if (chan->is_global)
  return chan->buf[0];

 buf = relay_create_buf(chan);
 if (!buf)
  return NULL;

 if (chan->has_base_filename) {
  dentry = relay_create_buf_file(chan, buf, cpu);
  if (!dentry)
   goto free_buf;
  relay_set_buf_dentry(buf, dentry);
 }

  buf->cpu = cpu;
  __relay_reset(buf, 1);

  if(chan->is_global) {
   chan->buf[0] = buf;
   buf->cpu = 0;
   }

 return buf;

free_buf:
  relay_destroy_buf(buf);
 return NULL;
}
static void relay_close_buf(struct rchan_buf *buf)
{
 buf->finalized = 1;
 del_timer_sync(&buf->timer);
 buf->chan->cb->remove_buf_file(buf->dentry);
 kref_put(&buf->kref, relay_remove_buf);
}

static void setup_callbacks(struct rchan *chan,
       struct rchan_callbacks *cb)
{
 if (!cb) {
  chan->cb = &default_channel_callbacks;
  return;
 }

 if (!cb->subbuf_start)
  cb->subbuf_start = subbuf_start_default_callback;
 if (!cb->buf_mapped)
  cb->buf_mapped = buf_mapped_default_callback;
 if (!cb->buf_unmapped)
  cb->buf_unmapped = buf_unmapped_default_callback;
 if (!cb->create_buf_file)
  cb->create_buf_file = create_buf_file_default_callback;
 if (!cb->remove_buf_file)
  cb->remove_buf_file = remove_buf_file_default_callback;
 chan->cb = cb;
}
static int relay_hotcpu_callback(struct notifier_block *nb,
    unsigned long action,
    void *hcpu)
{
 unsigned int hotcpu = (unsigned long)hcpu;
 struct rchan *chan;

 switch(action) {
 case CPU_UP_PREPARE:
 case CPU_UP_PREPARE_FROZEN:
  mutex_lock(&relay_channels_mutex);
  list_for_each_entry(chan, &relay_channels, list) {
   if (chan->buf[hotcpu])
    continue;
   chan->buf[hotcpu] = relay_open_buf(chan, hotcpu);
   if(!chan->buf[hotcpu]) {
    printk(KERN_ERR
     "relay_hotcpu_callback: cpu %d buffer "
     "creation failed\n", hotcpu);
    mutex_unlock(&relay_channels_mutex);
    return notifier_from_errno(-ENOMEM);
   }
  }
  mutex_unlock(&relay_channels_mutex);
  break;
 case CPU_DEAD:
 case CPU_DEAD_FROZEN:


  break;
 }
 return NOTIFY_OK;
}
struct rchan *relay_open(const char *base_filename,
    struct dentry *parent,
    size_t subbuf_size,
    size_t n_subbufs,
    struct rchan_callbacks *cb,
    void *private_data)
{
 unsigned int i;
 struct rchan *chan;

 if (!(subbuf_size && n_subbufs))
  return NULL;
 if (subbuf_size > UINT_MAX / n_subbufs)
  return NULL;

 chan = kzalloc(sizeof(struct rchan), GFP_KERNEL);
 if (!chan)
  return NULL;

 chan->version = RELAYFS_CHANNEL_VERSION;
 chan->n_subbufs = n_subbufs;
 chan->subbuf_size = subbuf_size;
 chan->alloc_size = PAGE_ALIGN(subbuf_size * n_subbufs);
 chan->parent = parent;
 chan->private_data = private_data;
 if (base_filename) {
  chan->has_base_filename = 1;
  strlcpy(chan->base_filename, base_filename, NAME_MAX);
 }
 setup_callbacks(chan, cb);
 kref_init(&chan->kref);

 mutex_lock(&relay_channels_mutex);
 for_each_online_cpu(i) {
  chan->buf[i] = relay_open_buf(chan, i);
  if (!chan->buf[i])
   goto free_bufs;
 }
 list_add(&chan->list, &relay_channels);
 mutex_unlock(&relay_channels_mutex);

 return chan;

free_bufs:
 for_each_possible_cpu(i) {
  if (chan->buf[i])
   relay_close_buf(chan->buf[i]);
 }

 kref_put(&chan->kref, relay_destroy_channel);
 mutex_unlock(&relay_channels_mutex);
 kfree(chan);
 return NULL;
}
EXPORT_SYMBOL_GPL(relay_open);

struct rchan_percpu_buf_dispatcher {
 struct rchan_buf *buf;
 struct dentry *dentry;
};


static void __relay_set_buf_dentry(void *info)
{
 struct rchan_percpu_buf_dispatcher *p = info;

 relay_set_buf_dentry(p->buf, p->dentry);
}
int relay_late_setup_files(struct rchan *chan,
      const char *base_filename,
      struct dentry *parent)
{
 int err = 0;
 unsigned int i, curr_cpu;
 unsigned long flags;
 struct dentry *dentry;
 struct rchan_percpu_buf_dispatcher disp;

 if (!chan || !base_filename)
  return -EINVAL;

 strlcpy(chan->base_filename, base_filename, NAME_MAX);

 mutex_lock(&relay_channels_mutex);

 if (unlikely(chan->has_base_filename)) {
  mutex_unlock(&relay_channels_mutex);
  return -EEXIST;
 }
 chan->has_base_filename = 1;
 chan->parent = parent;
 curr_cpu = get_cpu();





 for_each_online_cpu(i) {
  if (unlikely(!chan->buf[i])) {
   WARN_ONCE(1, KERN_ERR "CPU has no buffer!\n");
   err = -EINVAL;
   break;
  }

  dentry = relay_create_buf_file(chan, chan->buf[i], i);
  if (unlikely(!dentry)) {
   err = -EINVAL;
   break;
  }

  if (curr_cpu == i) {
   local_irq_save(flags);
   relay_set_buf_dentry(chan->buf[i], dentry);
   local_irq_restore(flags);
  } else {
   disp.buf = chan->buf[i];
   disp.dentry = dentry;
   smp_mb();

   err = smp_call_function_single(i,
             __relay_set_buf_dentry,
             &disp, 1);
  }
  if (unlikely(err))
   break;
 }
 put_cpu();
 mutex_unlock(&relay_channels_mutex);

 return err;
}
size_t relay_switch_subbuf(struct rchan_buf *buf, size_t length)
{
 void *old, *new;
 size_t old_subbuf, new_subbuf;

 if (unlikely(length > buf->chan->subbuf_size))
  goto toobig;

 if (buf->offset != buf->chan->subbuf_size + 1) {
  buf->prev_padding = buf->chan->subbuf_size - buf->offset;
  old_subbuf = buf->subbufs_produced % buf->chan->n_subbufs;
  buf->padding[old_subbuf] = buf->prev_padding;
  buf->subbufs_produced++;
  if (buf->dentry)
   d_inode(buf->dentry)->i_size +=
    buf->chan->subbuf_size -
    buf->padding[old_subbuf];
  else
   buf->early_bytes += buf->chan->subbuf_size -
         buf->padding[old_subbuf];
  smp_mb();
  if (waitqueue_active(&buf->read_wait))






   mod_timer(&buf->timer, jiffies + 1);
 }

 old = buf->data;
 new_subbuf = buf->subbufs_produced % buf->chan->n_subbufs;
 new = buf->start + new_subbuf * buf->chan->subbuf_size;
 buf->offset = 0;
 if (!buf->chan->cb->subbuf_start(buf, new, old, buf->prev_padding)) {
  buf->offset = buf->chan->subbuf_size + 1;
  return 0;
 }
 buf->data = new;
 buf->padding[new_subbuf] = 0;

 if (unlikely(length + buf->offset > buf->chan->subbuf_size))
  goto toobig;

 return length;

toobig:
 buf->chan->last_toobig = length;
 return 0;
}
EXPORT_SYMBOL_GPL(relay_switch_subbuf);
void relay_subbufs_consumed(struct rchan *chan,
       unsigned int cpu,
       size_t subbufs_consumed)
{
 struct rchan_buf *buf;

 if (!chan)
  return;

 if (cpu >= NR_CPUS || !chan->buf[cpu] ||
     subbufs_consumed > chan->n_subbufs)
  return;

 buf = chan->buf[cpu];
 if (subbufs_consumed > buf->subbufs_produced - buf->subbufs_consumed)
  buf->subbufs_consumed = buf->subbufs_produced;
 else
  buf->subbufs_consumed += subbufs_consumed;
}
EXPORT_SYMBOL_GPL(relay_subbufs_consumed);







void relay_close(struct rchan *chan)
{
 unsigned int i;

 if (!chan)
  return;

 mutex_lock(&relay_channels_mutex);
 if (chan->is_global && chan->buf[0])
  relay_close_buf(chan->buf[0]);
 else
  for_each_possible_cpu(i)
   if (chan->buf[i])
    relay_close_buf(chan->buf[i]);

 if (chan->last_toobig)
  printk(KERN_WARNING "relay: one or more items not logged "
         "[item size (%Zd) > sub-buffer size (%Zd)]\n",
         chan->last_toobig, chan->subbuf_size);

 list_del(&chan->list);
 kref_put(&chan->kref, relay_destroy_channel);
 mutex_unlock(&relay_channels_mutex);
}
EXPORT_SYMBOL_GPL(relay_close);







void relay_flush(struct rchan *chan)
{
 unsigned int i;

 if (!chan)
  return;

 if (chan->is_global && chan->buf[0]) {
  relay_switch_subbuf(chan->buf[0], 0);
  return;
 }

 mutex_lock(&relay_channels_mutex);
 for_each_possible_cpu(i)
  if (chan->buf[i])
   relay_switch_subbuf(chan->buf[i], 0);
 mutex_unlock(&relay_channels_mutex);
}
EXPORT_SYMBOL_GPL(relay_flush);
static int relay_file_open(struct inode *inode, struct file *filp)
{
 struct rchan_buf *buf = inode->i_private;
 kref_get(&buf->kref);
 filp->private_data = buf;

 return nonseekable_open(inode, filp);
}
static int relay_file_mmap(struct file *filp, struct vm_area_struct *vma)
{
 struct rchan_buf *buf = filp->private_data;
 return relay_mmap_buf(buf, vma);
}
static unsigned int relay_file_poll(struct file *filp, poll_table *wait)
{
 unsigned int mask = 0;
 struct rchan_buf *buf = filp->private_data;

 if (buf->finalized)
  return POLLERR;

 if (filp->f_mode & FMODE_READ) {
  poll_wait(filp, &buf->read_wait, wait);
  if (!relay_buf_empty(buf))
   mask |= POLLIN | POLLRDNORM;
 }

 return mask;
}
static int relay_file_release(struct inode *inode, struct file *filp)
{
 struct rchan_buf *buf = filp->private_data;
 kref_put(&buf->kref, relay_remove_buf);

 return 0;
}




static void relay_file_read_consume(struct rchan_buf *buf,
        size_t read_pos,
        size_t bytes_consumed)
{
 size_t subbuf_size = buf->chan->subbuf_size;
 size_t n_subbufs = buf->chan->n_subbufs;
 size_t read_subbuf;

 if (buf->subbufs_produced == buf->subbufs_consumed &&
     buf->offset == buf->bytes_consumed)
  return;

 if (buf->bytes_consumed + bytes_consumed > subbuf_size) {
  relay_subbufs_consumed(buf->chan, buf->cpu, 1);
  buf->bytes_consumed = 0;
 }

 buf->bytes_consumed += bytes_consumed;
 if (!read_pos)
  read_subbuf = buf->subbufs_consumed % n_subbufs;
 else
  read_subbuf = read_pos / buf->chan->subbuf_size;
 if (buf->bytes_consumed + buf->padding[read_subbuf] == subbuf_size) {
  if ((read_subbuf == buf->subbufs_produced % n_subbufs) &&
      (buf->offset == subbuf_size))
   return;
  relay_subbufs_consumed(buf->chan, buf->cpu, 1);
  buf->bytes_consumed = 0;
 }
}




static int relay_file_read_avail(struct rchan_buf *buf, size_t read_pos)
{
 size_t subbuf_size = buf->chan->subbuf_size;
 size_t n_subbufs = buf->chan->n_subbufs;
 size_t produced = buf->subbufs_produced;
 size_t consumed = buf->subbufs_consumed;

 relay_file_read_consume(buf, read_pos, 0);

 consumed = buf->subbufs_consumed;

 if (unlikely(buf->offset > subbuf_size)) {
  if (produced == consumed)
   return 0;
  return 1;
 }

 if (unlikely(produced - consumed >= n_subbufs)) {
  consumed = produced - n_subbufs + 1;
  buf->subbufs_consumed = consumed;
  buf->bytes_consumed = 0;
 }

 produced = (produced % n_subbufs) * subbuf_size + buf->offset;
 consumed = (consumed % n_subbufs) * subbuf_size + buf->bytes_consumed;

 if (consumed > produced)
  produced += n_subbufs * subbuf_size;

 if (consumed == produced) {
  if (buf->offset == subbuf_size &&
      buf->subbufs_produced > buf->subbufs_consumed)
   return 1;
  return 0;
 }

 return 1;
}






static size_t relay_file_read_subbuf_avail(size_t read_pos,
        struct rchan_buf *buf)
{
 size_t padding, avail = 0;
 size_t read_subbuf, read_offset, write_subbuf, write_offset;
 size_t subbuf_size = buf->chan->subbuf_size;

 write_subbuf = (buf->data - buf->start) / subbuf_size;
 write_offset = buf->offset > subbuf_size ? subbuf_size : buf->offset;
 read_subbuf = read_pos / subbuf_size;
 read_offset = read_pos % subbuf_size;
 padding = buf->padding[read_subbuf];

 if (read_subbuf == write_subbuf) {
  if (read_offset + padding < write_offset)
   avail = write_offset - (read_offset + padding);
 } else
  avail = (subbuf_size - padding) - read_offset;

 return avail;
}
static size_t relay_file_read_start_pos(size_t read_pos,
     struct rchan_buf *buf)
{
 size_t read_subbuf, padding, padding_start, padding_end;
 size_t subbuf_size = buf->chan->subbuf_size;
 size_t n_subbufs = buf->chan->n_subbufs;
 size_t consumed = buf->subbufs_consumed % n_subbufs;

 if (!read_pos)
  read_pos = consumed * subbuf_size + buf->bytes_consumed;
 read_subbuf = read_pos / subbuf_size;
 padding = buf->padding[read_subbuf];
 padding_start = (read_subbuf + 1) * subbuf_size - padding;
 padding_end = (read_subbuf + 1) * subbuf_size;
 if (read_pos >= padding_start && read_pos < padding_end) {
  read_subbuf = (read_subbuf + 1) % n_subbufs;
  read_pos = read_subbuf * subbuf_size;
 }

 return read_pos;
}







static size_t relay_file_read_end_pos(struct rchan_buf *buf,
          size_t read_pos,
          size_t count)
{
 size_t read_subbuf, padding, end_pos;
 size_t subbuf_size = buf->chan->subbuf_size;
 size_t n_subbufs = buf->chan->n_subbufs;

 read_subbuf = read_pos / subbuf_size;
 padding = buf->padding[read_subbuf];
 if (read_pos % subbuf_size + count + padding == subbuf_size)
  end_pos = (read_subbuf + 1) * subbuf_size;
 else
  end_pos = read_pos + count;
 if (end_pos >= subbuf_size * n_subbufs)
  end_pos = 0;

 return end_pos;
}




static int subbuf_read_actor(size_t read_start,
        struct rchan_buf *buf,
        size_t avail,
        read_descriptor_t *desc)
{
 void *from;
 int ret = 0;

 from = buf->start + read_start;
 ret = avail;
 if (copy_to_user(desc->arg.buf, from, avail)) {
  desc->error = -EFAULT;
  ret = 0;
 }
 desc->arg.data += ret;
 desc->written += ret;
 desc->count -= ret;

 return ret;
}

typedef int (*subbuf_actor_t) (size_t read_start,
          struct rchan_buf *buf,
          size_t avail,
          read_descriptor_t *desc);




static ssize_t relay_file_read_subbufs(struct file *filp, loff_t *ppos,
     subbuf_actor_t subbuf_actor,
     read_descriptor_t *desc)
{
 struct rchan_buf *buf = filp->private_data;
 size_t read_start, avail;
 int ret;

 if (!desc->count)
  return 0;

 inode_lock(file_inode(filp));
 do {
  if (!relay_file_read_avail(buf, *ppos))
   break;

  read_start = relay_file_read_start_pos(*ppos, buf);
  avail = relay_file_read_subbuf_avail(read_start, buf);
  if (!avail)
   break;

  avail = min(desc->count, avail);
  ret = subbuf_actor(read_start, buf, avail, desc);
  if (desc->error < 0)
   break;

  if (ret) {
   relay_file_read_consume(buf, read_start, ret);
   *ppos = relay_file_read_end_pos(buf, read_start, ret);
  }
 } while (desc->count && ret);
 inode_unlock(file_inode(filp));

 return desc->written;
}

static ssize_t relay_file_read(struct file *filp,
          char __user *buffer,
          size_t count,
          loff_t *ppos)
{
 read_descriptor_t desc;
 desc.written = 0;
 desc.count = count;
 desc.arg.buf = buffer;
 desc.error = 0;
 return relay_file_read_subbufs(filp, ppos, subbuf_read_actor, &desc);
}

static void relay_consume_bytes(struct rchan_buf *rbuf, int bytes_consumed)
{
 rbuf->bytes_consumed += bytes_consumed;

 if (rbuf->bytes_consumed >= rbuf->chan->subbuf_size) {
  relay_subbufs_consumed(rbuf->chan, rbuf->cpu, 1);
  rbuf->bytes_consumed %= rbuf->chan->subbuf_size;
 }
}

static void relay_pipe_buf_release(struct pipe_inode_info *pipe,
       struct pipe_buffer *buf)
{
 struct rchan_buf *rbuf;

 rbuf = (struct rchan_buf *)page_private(buf->page);
 relay_consume_bytes(rbuf, buf->private);
}

static const struct pipe_buf_operations relay_pipe_buf_ops = {
 .can_merge = 0,
 .confirm = generic_pipe_buf_confirm,
 .release = relay_pipe_buf_release,
 .steal = generic_pipe_buf_steal,
 .get = generic_pipe_buf_get,
};

static void relay_page_release(struct splice_pipe_desc *spd, unsigned int i)
{
}




static ssize_t subbuf_splice_actor(struct file *in,
          loff_t *ppos,
          struct pipe_inode_info *pipe,
          size_t len,
          unsigned int flags,
          int *nonpad_ret)
{
 unsigned int pidx, poff, total_len, subbuf_pages, nr_pages;
 struct rchan_buf *rbuf = in->private_data;
 unsigned int subbuf_size = rbuf->chan->subbuf_size;
 uint64_t pos = (uint64_t) *ppos;
 uint32_t alloc_size = (uint32_t) rbuf->chan->alloc_size;
 size_t read_start = (size_t) do_div(pos, alloc_size);
 size_t read_subbuf = read_start / subbuf_size;
 size_t padding = rbuf->padding[read_subbuf];
 size_t nonpad_end = read_subbuf * subbuf_size + subbuf_size - padding;
 struct page *pages[PIPE_DEF_BUFFERS];
 struct partial_page partial[PIPE_DEF_BUFFERS];
 struct splice_pipe_desc spd = {
  .pages = pages,
  .nr_pages = 0,
  .nr_pages_max = PIPE_DEF_BUFFERS,
  .partial = partial,
  .flags = flags,
  .ops = &relay_pipe_buf_ops,
  .spd_release = relay_page_release,
 };
 ssize_t ret;

 if (rbuf->subbufs_produced == rbuf->subbufs_consumed)
  return 0;
 if (splice_grow_spd(pipe, &spd))
  return -ENOMEM;




 if (len > (subbuf_size - read_start % subbuf_size))
  len = subbuf_size - read_start % subbuf_size;

 subbuf_pages = rbuf->chan->alloc_size >> PAGE_SHIFT;
 pidx = (read_start / PAGE_SIZE) % subbuf_pages;
 poff = read_start & ~PAGE_MASK;
 nr_pages = min_t(unsigned int, subbuf_pages, spd.nr_pages_max);

 for (total_len = 0; spd.nr_pages < nr_pages; spd.nr_pages++) {
  unsigned int this_len, this_end, private;
  unsigned int cur_pos = read_start + total_len;

  if (!len)
   break;

  this_len = min_t(unsigned long, len, PAGE_SIZE - poff);
  private = this_len;

  spd.pages[spd.nr_pages] = rbuf->page_array[pidx];
  spd.partial[spd.nr_pages].offset = poff;

  this_end = cur_pos + this_len;
  if (this_end >= nonpad_end) {
   this_len = nonpad_end - cur_pos;
   private = this_len + padding;
  }
  spd.partial[spd.nr_pages].len = this_len;
  spd.partial[spd.nr_pages].private = private;

  len -= this_len;
  total_len += this_len;
  poff = 0;
  pidx = (pidx + 1) % subbuf_pages;

  if (this_end >= nonpad_end) {
   spd.nr_pages++;
   break;
  }
 }

 ret = 0;
 if (!spd.nr_pages)
  goto out;

 ret = *nonpad_ret = splice_to_pipe(pipe, &spd);
 if (ret < 0 || ret < total_len)
  goto out;

        if (read_start + ret == nonpad_end)
                ret += padding;

out:
 splice_shrink_spd(&spd);
 return ret;
}

static ssize_t relay_file_splice_read(struct file *in,
          loff_t *ppos,
          struct pipe_inode_info *pipe,
          size_t len,
          unsigned int flags)
{
 ssize_t spliced;
 int ret;
 int nonpad_ret = 0;

 ret = 0;
 spliced = 0;

 while (len && !spliced) {
  ret = subbuf_splice_actor(in, ppos, pipe, len, flags, &nonpad_ret);
  if (ret < 0)
   break;
  else if (!ret) {
   if (flags & SPLICE_F_NONBLOCK)
    ret = -EAGAIN;
   break;
  }

  *ppos += ret;
  if (ret > len)
   len = 0;
  else
   len -= ret;
  spliced += nonpad_ret;
  nonpad_ret = 0;
 }

 if (spliced)
  return spliced;

 return ret;
}

const struct file_operations relay_file_operations = {
 .open = relay_file_open,
 .poll = relay_file_poll,
 .mmap = relay_file_mmap,
 .read = relay_file_read,
 .llseek = no_llseek,
 .release = relay_file_release,
 .splice_read = relay_file_splice_read,
};
EXPORT_SYMBOL_GPL(relay_file_operations);

static __init int relay_init(void)
{

 hotcpu_notifier(relay_hotcpu_callback, 0);
 return 0;
}

early_initcall(relay_init);




static DECLARE_BITMAP(irqs_resend, IRQ_BITMAP_BITS);




static void resend_irqs(unsigned long arg)
{
 struct irq_desc *desc;
 int irq;

 while (!bitmap_empty(irqs_resend, nr_irqs)) {
  irq = find_first_bit(irqs_resend, nr_irqs);
  clear_bit(irq, irqs_resend);
  desc = irq_to_desc(irq);
  local_irq_disable();
  desc->handle_irq(desc);
  local_irq_enable();
 }
}


static DECLARE_TASKLET(resend_tasklet, resend_irqs, 0);







void check_irq_resend(struct irq_desc *desc)
{






 if (irq_settings_is_level(desc)) {
  desc->istate &= ~IRQS_PENDING;
  return;
 }
 if (desc->istate & IRQS_REPLAY)
  return;
 if (desc->istate & IRQS_PENDING) {
  desc->istate &= ~IRQS_PENDING;
  desc->istate |= IRQS_REPLAY;

  if (!desc->irq_data.chip->irq_retrigger ||
      !desc->irq_data.chip->irq_retrigger(&desc->irq_data)) {
   unsigned int irq = irq_desc_get_irq(desc);







   if (irq_settings_is_nested_thread(desc)) {





    if (!desc->parent_irq)
     return;
    irq = desc->parent_irq;
   }

   set_bit(irq, irqs_resend);
   tasklet_schedule(&resend_tasklet);
  }
 }
}



struct resource ioport_resource = {
 .name = "PCI IO",
 .start = 0,
 .end = IO_SPACE_LIMIT,
 .flags = IORESOURCE_IO,
};
EXPORT_SYMBOL(ioport_resource);

struct resource iomem_resource = {
 .name = "PCI mem",
 .start = 0,
 .end = -1,
 .flags = IORESOURCE_MEM,
};
EXPORT_SYMBOL(iomem_resource);


struct resource_constraint {
 resource_size_t min, max, align;
 resource_size_t (*alignf)(void *, const struct resource *,
   resource_size_t, resource_size_t);
 void *alignf_data;
};

static DEFINE_RWLOCK(resource_lock);






static struct resource *bootmem_resource_free;
static DEFINE_SPINLOCK(bootmem_resource_lock);

static struct resource *next_resource(struct resource *p, bool sibling_only)
{

 if (sibling_only)
  return p->sibling;

 if (p->child)
  return p->child;
 while (!p->sibling && p->parent)
  p = p->parent;
 return p->sibling;
}

static void *r_next(struct seq_file *m, void *v, loff_t *pos)
{
 struct resource *p = v;
 (*pos)++;
 return (void *)next_resource(p, false);
}


enum { MAX_IORES_LEVEL = 5 };

static void *r_start(struct seq_file *m, loff_t *pos)
 __acquires(resource_lock)
{
 struct resource *p = m->private;
 loff_t l = 0;
 read_lock(&resource_lock);
 for (p = p->child; p && l < *pos; p = r_next(m, p, &l))
  ;
 return p;
}

static void r_stop(struct seq_file *m, void *v)
 __releases(resource_lock)
{
 read_unlock(&resource_lock);
}

static int r_show(struct seq_file *m, void *v)
{
 struct resource *root = m->private;
 struct resource *r = v, *p;
 unsigned long long start, end;
 int width = root->end < 0x10000 ? 4 : 8;
 int depth;

 for (depth = 0, p = r; depth < MAX_IORES_LEVEL; depth++, p = p->parent)
  if (p->parent == root)
   break;

 if (file_ns_capable(m->file, &init_user_ns, CAP_SYS_ADMIN)) {
  start = r->start;
  end = r->end;
 } else {
  start = end = 0;
 }

 seq_printf(m, "%*s%0*llx-%0*llx : %s\n",
   depth * 2, "",
   width, start,
   width, end,
   r->name ? r->name : "<BAD>");
 return 0;
}

static const struct seq_operations resource_op = {
 .start = r_start,
 .next = r_next,
 .stop = r_stop,
 .show = r_show,
};

static int ioports_open(struct inode *inode, struct file *file)
{
 int res = seq_open(file, &resource_op);
 if (!res) {
  struct seq_file *m = file->private_data;
  m->private = &ioport_resource;
 }
 return res;
}

static int iomem_open(struct inode *inode, struct file *file)
{
 int res = seq_open(file, &resource_op);
 if (!res) {
  struct seq_file *m = file->private_data;
  m->private = &iomem_resource;
 }
 return res;
}

static const struct file_operations proc_ioports_operations = {
 .open = ioports_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = seq_release,
};

static const struct file_operations proc_iomem_operations = {
 .open = iomem_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = seq_release,
};

static int __init ioresources_init(void)
{
 proc_create("ioports", 0, NULL, &proc_ioports_operations);
 proc_create("iomem", 0, NULL, &proc_iomem_operations);
 return 0;
}
__initcall(ioresources_init);


static void free_resource(struct resource *res)
{
 if (!res)
  return;

 if (!PageSlab(virt_to_head_page(res))) {
  spin_lock(&bootmem_resource_lock);
  res->sibling = bootmem_resource_free;
  bootmem_resource_free = res;
  spin_unlock(&bootmem_resource_lock);
 } else {
  kfree(res);
 }
}

static struct resource *alloc_resource(gfp_t flags)
{
 struct resource *res = NULL;

 spin_lock(&bootmem_resource_lock);
 if (bootmem_resource_free) {
  res = bootmem_resource_free;
  bootmem_resource_free = res->sibling;
 }
 spin_unlock(&bootmem_resource_lock);

 if (res)
  memset(res, 0, sizeof(struct resource));
 else
  res = kzalloc(sizeof(struct resource), flags);

 return res;
}


static struct resource * __request_resource(struct resource *root, struct resource *new)
{
 resource_size_t start = new->start;
 resource_size_t end = new->end;
 struct resource *tmp, **p;

 if (end < start)
  return root;
 if (start < root->start)
  return root;
 if (end > root->end)
  return root;
 p = &root->child;
 for (;;) {
  tmp = *p;
  if (!tmp || tmp->start > end) {
   new->sibling = tmp;
   *p = new;
   new->parent = root;
   return NULL;
  }
  p = &tmp->sibling;
  if (tmp->end < start)
   continue;
  return tmp;
 }
}

static int __release_resource(struct resource *old, bool release_child)
{
 struct resource *tmp, **p, *chd;

 p = &old->parent->child;
 for (;;) {
  tmp = *p;
  if (!tmp)
   break;
  if (tmp == old) {
   if (release_child || !(tmp->child)) {
    *p = tmp->sibling;
   } else {
    for (chd = tmp->child;; chd = chd->sibling) {
     chd->parent = tmp->parent;
     if (!(chd->sibling))
      break;
    }
    *p = tmp->child;
    chd->sibling = tmp->sibling;
   }
   old->parent = NULL;
   return 0;
  }
  p = &tmp->sibling;
 }
 return -EINVAL;
}

static void __release_child_resources(struct resource *r)
{
 struct resource *tmp, *p;
 resource_size_t size;

 p = r->child;
 r->child = NULL;
 while (p) {
  tmp = p;
  p = p->sibling;

  tmp->parent = NULL;
  tmp->sibling = NULL;
  __release_child_resources(tmp);

  printk(KERN_DEBUG "release child resource %pR\n", tmp);

  size = resource_size(tmp);
  tmp->start = 0;
  tmp->end = size - 1;
 }
}

void release_child_resources(struct resource *r)
{
 write_lock(&resource_lock);
 __release_child_resources(r);
 write_unlock(&resource_lock);
}
struct resource *request_resource_conflict(struct resource *root, struct resource *new)
{
 struct resource *conflict;

 write_lock(&resource_lock);
 conflict = __request_resource(root, new);
 write_unlock(&resource_lock);
 return conflict;
}
int request_resource(struct resource *root, struct resource *new)
{
 struct resource *conflict;

 conflict = request_resource_conflict(root, new);
 return conflict ? -EBUSY : 0;
}

EXPORT_SYMBOL(request_resource);





int release_resource(struct resource *old)
{
 int retval;

 write_lock(&resource_lock);
 retval = __release_resource(old, true);
 write_unlock(&resource_lock);
 return retval;
}

EXPORT_SYMBOL(release_resource);
static int find_next_iomem_res(struct resource *res, unsigned long desc,
          bool first_level_children_only)
{
 resource_size_t start, end;
 struct resource *p;
 bool sibling_only = false;

 BUG_ON(!res);

 start = res->start;
 end = res->end;
 BUG_ON(start >= end);

 if (first_level_children_only)
  sibling_only = true;

 read_lock(&resource_lock);

 for (p = iomem_resource.child; p; p = next_resource(p, sibling_only)) {
  if ((p->flags & res->flags) != res->flags)
   continue;
  if ((desc != IORES_DESC_NONE) && (desc != p->desc))
   continue;
  if (p->start > end) {
   p = NULL;
   break;
  }
  if ((p->end >= start) && (p->start < end))
   break;
 }

 read_unlock(&resource_lock);
 if (!p)
  return -1;

 if (res->start < p->start)
  res->start = p->start;
 if (res->end > p->end)
  res->end = p->end;
 return 0;
}
int walk_iomem_res_desc(unsigned long desc, unsigned long flags, u64 start,
  u64 end, void *arg, int (*func)(u64, u64, void *))
{
 struct resource res;
 u64 orig_end;
 int ret = -1;

 res.start = start;
 res.end = end;
 res.flags = flags;
 orig_end = res.end;

 while ((res.start < res.end) &&
  (!find_next_iomem_res(&res, desc, false))) {

  ret = (*func)(res.start, res.end, arg);
  if (ret)
   break;

  res.start = res.end + 1;
  res.end = orig_end;
 }

 return ret;
}
int walk_system_ram_res(u64 start, u64 end, void *arg,
    int (*func)(u64, u64, void *))
{
 struct resource res;
 u64 orig_end;
 int ret = -1;

 res.start = start;
 res.end = end;
 res.flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;
 orig_end = res.end;
 while ((res.start < res.end) &&
  (!find_next_iomem_res(&res, IORES_DESC_NONE, true))) {
  ret = (*func)(res.start, res.end, arg);
  if (ret)
   break;
  res.start = res.end + 1;
  res.end = orig_end;
 }
 return ret;
}







int walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
  void *arg, int (*func)(unsigned long, unsigned long, void *))
{
 struct resource res;
 unsigned long pfn, end_pfn;
 u64 orig_end;
 int ret = -1;

 res.start = (u64) start_pfn << PAGE_SHIFT;
 res.end = ((u64)(start_pfn + nr_pages) << PAGE_SHIFT) - 1;
 res.flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;
 orig_end = res.end;
 while ((res.start < res.end) &&
  (find_next_iomem_res(&res, IORES_DESC_NONE, true) >= 0)) {
  pfn = (res.start + PAGE_SIZE - 1) >> PAGE_SHIFT;
  end_pfn = (res.end + 1) >> PAGE_SHIFT;
  if (end_pfn > pfn)
   ret = (*func)(pfn, end_pfn - pfn, arg);
  if (ret)
   break;
  res.start = res.end + 1;
  res.end = orig_end;
 }
 return ret;
}


static int __is_ram(unsigned long pfn, unsigned long nr_pages, void *arg)
{
 return 1;
}




int __weak page_is_ram(unsigned long pfn)
{
 return walk_system_ram_range(pfn, 1, NULL, __is_ram) == 1;
}
EXPORT_SYMBOL_GPL(page_is_ram);
int region_intersects(resource_size_t start, size_t size, unsigned long flags,
        unsigned long desc)
{
 resource_size_t end = start + size - 1;
 int type = 0; int other = 0;
 struct resource *p;

 read_lock(&resource_lock);
 for (p = iomem_resource.child; p ; p = p->sibling) {
  bool is_type = (((p->flags & flags) == flags) &&
    ((desc == IORES_DESC_NONE) ||
     (desc == p->desc)));

  if (start >= p->start && start <= p->end)
   is_type ? type++ : other++;
  if (end >= p->start && end <= p->end)
   is_type ? type++ : other++;
  if (p->start >= start && p->end <= end)
   is_type ? type++ : other++;
 }
 read_unlock(&resource_lock);

 if (other == 0)
  return type ? REGION_INTERSECTS : REGION_DISJOINT;

 if (type)
  return REGION_MIXED;

 return REGION_DISJOINT;
}
EXPORT_SYMBOL_GPL(region_intersects);

void __weak arch_remove_reservations(struct resource *avail)
{
}

static resource_size_t simple_align_resource(void *data,
          const struct resource *avail,
          resource_size_t size,
          resource_size_t align)
{
 return avail->start;
}

static void resource_clip(struct resource *res, resource_size_t min,
     resource_size_t max)
{
 if (res->start < min)
  res->start = min;
 if (res->end > max)
  res->end = max;
}





static int __find_resource(struct resource *root, struct resource *old,
    struct resource *new,
    resource_size_t size,
    struct resource_constraint *constraint)
{
 struct resource *this = root->child;
 struct resource tmp = *new, avail, alloc;

 tmp.start = root->start;




 if (this && this->start == root->start) {
  tmp.start = (this == old) ? old->start : this->end + 1;
  this = this->sibling;
 }
 for(;;) {
  if (this)
   tmp.end = (this == old) ? this->end : this->start - 1;
  else
   tmp.end = root->end;

  if (tmp.end < tmp.start)
   goto next;

  resource_clip(&tmp, constraint->min, constraint->max);
  arch_remove_reservations(&tmp);


  avail.start = ALIGN(tmp.start, constraint->align);
  avail.end = tmp.end;
  avail.flags = new->flags & ~IORESOURCE_UNSET;
  if (avail.start >= tmp.start) {
   alloc.flags = avail.flags;
   alloc.start = constraint->alignf(constraint->alignf_data, &avail,
     size, constraint->align);
   alloc.end = alloc.start + size - 1;
   if (resource_contains(&avail, &alloc)) {
    new->start = alloc.start;
    new->end = alloc.end;
    return 0;
   }
  }

next: if (!this || this->end == root->end)
   break;

  if (this != old)
   tmp.start = this->end + 1;
  this = this->sibling;
 }
 return -EBUSY;
}




static int find_resource(struct resource *root, struct resource *new,
   resource_size_t size,
   struct resource_constraint *constraint)
{
 return __find_resource(root, NULL, new, size, constraint);
}
static int reallocate_resource(struct resource *root, struct resource *old,
   resource_size_t newsize,
   struct resource_constraint *constraint)
{
 int err=0;
 struct resource new = *old;
 struct resource *conflict;

 write_lock(&resource_lock);

 if ((err = __find_resource(root, old, &new, newsize, constraint)))
  goto out;

 if (resource_contains(&new, old)) {
  old->start = new.start;
  old->end = new.end;
  goto out;
 }

 if (old->child) {
  err = -EBUSY;
  goto out;
 }

 if (resource_contains(old, &new)) {
  old->start = new.start;
  old->end = new.end;
 } else {
  __release_resource(old, true);
  *old = new;
  conflict = __request_resource(root, old);
  BUG_ON(conflict);
 }
out:
 write_unlock(&resource_lock);
 return err;
}
int allocate_resource(struct resource *root, struct resource *new,
        resource_size_t size, resource_size_t min,
        resource_size_t max, resource_size_t align,
        resource_size_t (*alignf)(void *,
      const struct resource *,
      resource_size_t,
      resource_size_t),
        void *alignf_data)
{
 int err;
 struct resource_constraint constraint;

 if (!alignf)
  alignf = simple_align_resource;

 constraint.min = min;
 constraint.max = max;
 constraint.align = align;
 constraint.alignf = alignf;
 constraint.alignf_data = alignf_data;

 if ( new->parent ) {


  return reallocate_resource(root, new, size, &constraint);
 }

 write_lock(&resource_lock);
 err = find_resource(root, new, size, &constraint);
 if (err >= 0 && __request_resource(root, new))
  err = -EBUSY;
 write_unlock(&resource_lock);
 return err;
}

EXPORT_SYMBOL(allocate_resource);
struct resource *lookup_resource(struct resource *root, resource_size_t start)
{
 struct resource *res;

 read_lock(&resource_lock);
 for (res = root->child; res; res = res->sibling) {
  if (res->start == start)
   break;
 }
 read_unlock(&resource_lock);

 return res;
}





static struct resource * __insert_resource(struct resource *parent, struct resource *new)
{
 struct resource *first, *next;

 for (;; parent = first) {
  first = __request_resource(parent, new);
  if (!first)
   return first;

  if (first == parent)
   return first;
  if (WARN_ON(first == new))
   return first;

  if ((first->start > new->start) || (first->end < new->end))
   break;
  if ((first->start == new->start) && (first->end == new->end))
   break;
 }

 for (next = first; ; next = next->sibling) {

  if (next->start < new->start || next->end > new->end)
   return next;
  if (!next->sibling)
   break;
  if (next->sibling->start > new->end)
   break;
 }

 new->parent = parent;
 new->sibling = next->sibling;
 new->child = first;

 next->sibling = NULL;
 for (next = first; next; next = next->sibling)
  next->parent = new;

 if (parent->child == first) {
  parent->child = new;
 } else {
  next = parent->child;
  while (next->sibling != first)
   next = next->sibling;
  next->sibling = new;
 }
 return NULL;
}
struct resource *insert_resource_conflict(struct resource *parent, struct resource *new)
{
 struct resource *conflict;

 write_lock(&resource_lock);
 conflict = __insert_resource(parent, new);
 write_unlock(&resource_lock);
 return conflict;
}
int insert_resource(struct resource *parent, struct resource *new)
{
 struct resource *conflict;

 conflict = insert_resource_conflict(parent, new);
 return conflict ? -EBUSY : 0;
}
EXPORT_SYMBOL_GPL(insert_resource);
void insert_resource_expand_to_fit(struct resource *root, struct resource *new)
{
 if (new->parent)
  return;

 write_lock(&resource_lock);
 for (;;) {
  struct resource *conflict;

  conflict = __insert_resource(root, new);
  if (!conflict)
   break;
  if (conflict == root)
   break;


  if (conflict->start < new->start)
   new->start = conflict->start;
  if (conflict->end > new->end)
   new->end = conflict->end;

  printk("Expanded resource %s due to conflict with %s\n", new->name, conflict->name);
 }
 write_unlock(&resource_lock);
}
int remove_resource(struct resource *old)
{
 int retval;

 write_lock(&resource_lock);
 retval = __release_resource(old, false);
 write_unlock(&resource_lock);
 return retval;
}
EXPORT_SYMBOL_GPL(remove_resource);

static int __adjust_resource(struct resource *res, resource_size_t start,
    resource_size_t size)
{
 struct resource *tmp, *parent = res->parent;
 resource_size_t end = start + size - 1;
 int result = -EBUSY;

 if (!parent)
  goto skip;

 if ((start < parent->start) || (end > parent->end))
  goto out;

 if (res->sibling && (res->sibling->start <= end))
  goto out;

 tmp = parent->child;
 if (tmp != res) {
  while (tmp->sibling != res)
   tmp = tmp->sibling;
  if (start <= tmp->end)
   goto out;
 }

skip:
 for (tmp = res->child; tmp; tmp = tmp->sibling)
  if ((tmp->start < start) || (tmp->end > end))
   goto out;

 res->start = start;
 res->end = end;
 result = 0;

 out:
 return result;
}
int adjust_resource(struct resource *res, resource_size_t start,
   resource_size_t size)
{
 int result;

 write_lock(&resource_lock);
 result = __adjust_resource(res, start, size);
 write_unlock(&resource_lock);
 return result;
}
EXPORT_SYMBOL(adjust_resource);

static void __init __reserve_region_with_split(struct resource *root,
  resource_size_t start, resource_size_t end,
  const char *name)
{
 struct resource *parent = root;
 struct resource *conflict;
 struct resource *res = alloc_resource(GFP_ATOMIC);
 struct resource *next_res = NULL;

 if (!res)
  return;

 res->name = name;
 res->start = start;
 res->end = end;
 res->flags = IORESOURCE_BUSY;
 res->desc = IORES_DESC_NONE;

 while (1) {

  conflict = __request_resource(parent, res);
  if (!conflict) {
   if (!next_res)
    break;
   res = next_res;
   next_res = NULL;
   continue;
  }


  if (conflict->start <= res->start &&
    conflict->end >= res->end) {
   free_resource(res);
   WARN_ON(next_res);
   break;
  }


  if (conflict->start > res->start) {
   end = res->end;
   res->end = conflict->start - 1;
   if (conflict->end < end) {
    next_res = alloc_resource(GFP_ATOMIC);
    if (!next_res) {
     free_resource(res);
     break;
    }
    next_res->name = name;
    next_res->start = conflict->end + 1;
    next_res->end = end;
    next_res->flags = IORESOURCE_BUSY;
    next_res->desc = IORES_DESC_NONE;
   }
  } else {
   res->start = conflict->end + 1;
  }
 }

}

void __init reserve_region_with_split(struct resource *root,
  resource_size_t start, resource_size_t end,
  const char *name)
{
 int abort = 0;

 write_lock(&resource_lock);
 if (root->start > start || root->end < end) {
  pr_err("requested range [0x%llx-0x%llx] not in root %pr\n",
         (unsigned long long)start, (unsigned long long)end,
         root);
  if (start > root->end || end < root->start)
   abort = 1;
  else {
   if (end > root->end)
    end = root->end;
   if (start < root->start)
    start = root->start;
   pr_err("fixing request to [0x%llx-0x%llx]\n",
          (unsigned long long)start,
          (unsigned long long)end);
  }
  dump_stack();
 }
 if (!abort)
  __reserve_region_with_split(root, start, end, name);
 write_unlock(&resource_lock);
}







resource_size_t resource_alignment(struct resource *res)
{
 switch (res->flags & (IORESOURCE_SIZEALIGN | IORESOURCE_STARTALIGN)) {
 case IORESOURCE_SIZEALIGN:
  return resource_size(res);
 case IORESOURCE_STARTALIGN:
  return res->start;
 default:
  return 0;
 }
}
static DECLARE_WAIT_QUEUE_HEAD(muxed_resource_wait);
struct resource * __request_region(struct resource *parent,
       resource_size_t start, resource_size_t n,
       const char *name, int flags)
{
 DECLARE_WAITQUEUE(wait, current);
 struct resource *res = alloc_resource(GFP_KERNEL);

 if (!res)
  return NULL;

 res->name = name;
 res->start = start;
 res->end = start + n - 1;

 write_lock(&resource_lock);

 for (;;) {
  struct resource *conflict;

  res->flags = resource_type(parent) | resource_ext_type(parent);
  res->flags |= IORESOURCE_BUSY | flags;
  res->desc = parent->desc;

  conflict = __request_resource(parent, res);
  if (!conflict)
   break;
  if (conflict != parent) {
   if (!(conflict->flags & IORESOURCE_BUSY)) {
    parent = conflict;
    continue;
   }
  }
  if (conflict->flags & flags & IORESOURCE_MUXED) {
   add_wait_queue(&muxed_resource_wait, &wait);
   write_unlock(&resource_lock);
   set_current_state(TASK_UNINTERRUPTIBLE);
   schedule();
   remove_wait_queue(&muxed_resource_wait, &wait);
   write_lock(&resource_lock);
   continue;
  }

  free_resource(res);
  res = NULL;
  break;
 }
 write_unlock(&resource_lock);
 return res;
}
EXPORT_SYMBOL(__request_region);
void __release_region(struct resource *parent, resource_size_t start,
   resource_size_t n)
{
 struct resource **p;
 resource_size_t end;

 p = &parent->child;
 end = start + n - 1;

 write_lock(&resource_lock);

 for (;;) {
  struct resource *res = *p;

  if (!res)
   break;
  if (res->start <= start && res->end >= end) {
   if (!(res->flags & IORESOURCE_BUSY)) {
    p = &res->child;
    continue;
   }
   if (res->start != start || res->end != end)
    break;
   *p = res->sibling;
   write_unlock(&resource_lock);
   if (res->flags & IORESOURCE_MUXED)
    wake_up(&muxed_resource_wait);
   free_resource(res);
   return;
  }
  p = &res->sibling;
 }

 write_unlock(&resource_lock);

 printk(KERN_WARNING "Trying to free nonexistent resource "
  "<%016llx-%016llx>\n", (unsigned long long)start,
  (unsigned long long)end);
}
EXPORT_SYMBOL(__release_region);

int release_mem_region_adjustable(struct resource *parent,
   resource_size_t start, resource_size_t size)
{
 struct resource **p;
 struct resource *res;
 struct resource *new_res;
 resource_size_t end;
 int ret = -EINVAL;

 end = start + size - 1;
 if ((start < parent->start) || (end > parent->end))
  return ret;


 new_res = alloc_resource(GFP_KERNEL);

 p = &parent->child;
 write_lock(&resource_lock);

 while ((res = *p)) {
  if (res->start >= end)
   break;


  if (res->start > start || res->end < end) {
   p = &res->sibling;
   continue;
  }

  if (!(res->flags & IORESOURCE_MEM))
   break;

  if (!(res->flags & IORESOURCE_BUSY)) {
   p = &res->child;
   continue;
  }


  if (res->start == start && res->end == end) {

   *p = res->sibling;
   free_resource(res);
   ret = 0;
  } else if (res->start == start && res->end != end) {

   ret = __adjust_resource(res, end + 1,
      res->end - end);
  } else if (res->start != start && res->end == end) {

   ret = __adjust_resource(res, res->start,
      start - res->start);
  } else {

   if (!new_res) {
    ret = -ENOMEM;
    break;
   }
   new_res->name = res->name;
   new_res->start = end + 1;
   new_res->end = res->end;
   new_res->flags = res->flags;
   new_res->desc = res->desc;
   new_res->parent = res->parent;
   new_res->sibling = res->sibling;
   new_res->child = NULL;

   ret = __adjust_resource(res, res->start,
      start - res->start);
   if (ret)
    break;
   res->sibling = new_res;
   new_res = NULL;
  }

  break;
 }

 write_unlock(&resource_lock);
 free_resource(new_res);
 return ret;
}




static void devm_resource_release(struct device *dev, void *ptr)
{
 struct resource **r = ptr;

 release_resource(*r);
}
int devm_request_resource(struct device *dev, struct resource *root,
     struct resource *new)
{
 struct resource *conflict, **ptr;

 ptr = devres_alloc(devm_resource_release, sizeof(*ptr), GFP_KERNEL);
 if (!ptr)
  return -ENOMEM;

 *ptr = new;

 conflict = request_resource_conflict(root, new);
 if (conflict) {
  dev_err(dev, "resource collision: %pR conflicts with %s %pR\n",
   new, conflict->name, conflict);
  devres_free(ptr);
  return -EBUSY;
 }

 devres_add(dev, ptr);
 return 0;
}
EXPORT_SYMBOL(devm_request_resource);

static int devm_resource_match(struct device *dev, void *res, void *data)
{
 struct resource **ptr = res;

 return *ptr == data;
}
void devm_release_resource(struct device *dev, struct resource *new)
{
 WARN_ON(devres_release(dev, devm_resource_release, devm_resource_match,
          new));
}
EXPORT_SYMBOL(devm_release_resource);

struct region_devres {
 struct resource *parent;
 resource_size_t start;
 resource_size_t n;
};

static void devm_region_release(struct device *dev, void *res)
{
 struct region_devres *this = res;

 __release_region(this->parent, this->start, this->n);
}

static int devm_region_match(struct device *dev, void *res, void *match_data)
{
 struct region_devres *this = res, *match = match_data;

 return this->parent == match->parent &&
  this->start == match->start && this->n == match->n;
}

struct resource * __devm_request_region(struct device *dev,
    struct resource *parent, resource_size_t start,
    resource_size_t n, const char *name)
{
 struct region_devres *dr = NULL;
 struct resource *res;

 dr = devres_alloc(devm_region_release, sizeof(struct region_devres),
     GFP_KERNEL);
 if (!dr)
  return NULL;

 dr->parent = parent;
 dr->start = start;
 dr->n = n;

 res = __request_region(parent, start, n, name, 0);
 if (res)
  devres_add(dev, dr);
 else
  devres_free(dr);

 return res;
}
EXPORT_SYMBOL(__devm_request_region);

void __devm_release_region(struct device *dev, struct resource *parent,
      resource_size_t start, resource_size_t n)
{
 struct region_devres match_data = { parent, start, n };

 __release_region(parent, start, n);
 WARN_ON(devres_destroy(dev, devm_region_release, devm_region_match,
          &match_data));
}
EXPORT_SYMBOL(__devm_release_region);




static int __init reserve_setup(char *str)
{
 static int reserved;
 static struct resource reserve[MAXRESERVE];

 for (;;) {
  unsigned int io_start, io_num;
  int x = reserved;

  if (get_option (&str, &io_start) != 2)
   break;
  if (get_option (&str, &io_num) == 0)
   break;
  if (x < MAXRESERVE) {
   struct resource *res = reserve + x;
   res->name = "reserved";
   res->start = io_start;
   res->end = io_start + io_num - 1;
   res->flags = IORESOURCE_BUSY;
   res->desc = IORES_DESC_NONE;
   res->child = NULL;
   if (request_resource(res->start >= 0x10000 ? &iomem_resource : &ioport_resource, res) == 0)
    reserved = x+1;
  }
 }
 return 1;
}

__setup("reserve=", reserve_setup);





int iomem_map_sanity_check(resource_size_t addr, unsigned long size)
{
 struct resource *p = &iomem_resource;
 int err = 0;
 loff_t l;

 read_lock(&resource_lock);
 for (p = p->child; p ; p = r_next(NULL, p, &l)) {




  if (p->start >= addr + size)
   continue;
  if (p->end < addr)
   continue;
  if (PFN_DOWN(p->start) <= PFN_DOWN(addr) &&
      PFN_DOWN(p->end) >= PFN_DOWN(addr + size - 1))
   continue;






  if (p->flags & IORESOURCE_BUSY)
   continue;

  printk(KERN_WARNING "resource sanity check: requesting [mem %#010llx-%#010llx], which spans more than %s %pR\n",
         (unsigned long long)addr,
         (unsigned long long)(addr + size - 1),
         p->name, p);
  err = -1;
  break;
 }
 read_unlock(&resource_lock);

 return err;
}

static int strict_iomem_checks = 1;
static int strict_iomem_checks;





int iomem_is_exclusive(u64 addr)
{
 struct resource *p = &iomem_resource;
 int err = 0;
 loff_t l;
 int size = PAGE_SIZE;

 if (!strict_iomem_checks)
  return 0;

 addr = addr & PAGE_MASK;

 read_lock(&resource_lock);
 for (p = p->child; p ; p = r_next(NULL, p, &l)) {




  if (p->start >= addr + size)
   break;
  if (p->end < addr)
   continue;





  if ((p->flags & IORESOURCE_BUSY) == 0)
   continue;
  if (IS_ENABLED(CONFIG_IO_STRICT_DEVMEM)
    || p->flags & IORESOURCE_EXCLUSIVE) {
   err = 1;
   break;
  }
 }
 read_unlock(&resource_lock);

 return err;
}

struct resource_entry *resource_list_create_entry(struct resource *res,
        size_t extra_size)
{
 struct resource_entry *entry;

 entry = kzalloc(sizeof(*entry) + extra_size, GFP_KERNEL);
 if (entry) {
  INIT_LIST_HEAD(&entry->node);
  entry->res = res ? res : &entry->__res;
 }

 return entry;
}
EXPORT_SYMBOL(resource_list_create_entry);

void resource_list_free(struct list_head *head)
{
 struct resource_entry *entry, *tmp;

 list_for_each_entry_safe(entry, tmp, head, node)
  resource_list_destroy_entry(entry);
}
EXPORT_SYMBOL(resource_list_free);

static int __init strict_iomem(char *str)
{
 if (strstr(str, "relaxed"))
  strict_iomem_checks = 0;
 if (strstr(str, "strict"))
  strict_iomem_checks = 1;
 return 1;
}

__setup("iomem=", strict_iomem);


static void perf_output_wakeup(struct perf_output_handle *handle)
{
 atomic_set(&handle->rb->poll, POLLIN);

 handle->event->pending_wakeup = 1;
 irq_work_queue(&handle->event->pending);
}
static void perf_output_get_handle(struct perf_output_handle *handle)
{
 struct ring_buffer *rb = handle->rb;

 preempt_disable();
 local_inc(&rb->nest);
 handle->wakeup = local_read(&rb->wakeup);
}

static void perf_output_put_handle(struct perf_output_handle *handle)
{
 struct ring_buffer *rb = handle->rb;
 unsigned long head;

again:
 head = local_read(&rb->head);





 if (!local_dec_and_test(&rb->nest))
  goto out;
 smp_wmb();
 rb->user_page->data_head = head;





 if (unlikely(head != local_read(&rb->head))) {
  local_inc(&rb->nest);
  goto again;
 }

 if (handle->wakeup != local_read(&rb->wakeup))
  perf_output_wakeup(handle);

out:
 preempt_enable();
}

static bool __always_inline
ring_buffer_has_space(unsigned long head, unsigned long tail,
        unsigned long data_size, unsigned int size,
        bool backward)
{
 if (!backward)
  return CIRC_SPACE(head, tail, data_size) >= size;
 else
  return CIRC_SPACE(tail, head, data_size) >= size;
}

static int __always_inline
__perf_output_begin(struct perf_output_handle *handle,
      struct perf_event *event, unsigned int size,
      bool backward)
{
 struct ring_buffer *rb;
 unsigned long tail, offset, head;
 int have_lost, page_shift;
 struct {
  struct perf_event_header header;
  u64 id;
  u64 lost;
 } lost_event;

 rcu_read_lock();



 if (event->parent)
  event = event->parent;

 rb = rcu_dereference(event->rb);
 if (unlikely(!rb))
  goto out;

 if (unlikely(rb->paused)) {
  if (rb->nr_pages)
   local_inc(&rb->lost);
  goto out;
 }

 handle->rb = rb;
 handle->event = event;

 have_lost = local_read(&rb->lost);
 if (unlikely(have_lost)) {
  size += sizeof(lost_event);
  if (event->attr.sample_id_all)
   size += event->id_header_size;
 }

 perf_output_get_handle(handle);

 do {
  tail = READ_ONCE(rb->user_page->data_tail);
  offset = head = local_read(&rb->head);
  if (!rb->overwrite) {
   if (unlikely(!ring_buffer_has_space(head, tail,
           perf_data_size(rb),
           size, backward)))
    goto fail;
  }
  if (!backward)
   head += size;
  else
   head -= size;
 } while (local_cmpxchg(&rb->head, offset, head) != offset);

 if (backward) {
  offset = head;
  head = (u64)(-head);
 }






 if (unlikely(head - local_read(&rb->wakeup) > rb->watermark))
  local_add(rb->watermark, &rb->wakeup);

 page_shift = PAGE_SHIFT + page_order(rb);

 handle->page = (offset >> page_shift) & (rb->nr_pages - 1);
 offset &= (1UL << page_shift) - 1;
 handle->addr = rb->data_pages[handle->page] + offset;
 handle->size = (1UL << page_shift) - offset;

 if (unlikely(have_lost)) {
  struct perf_sample_data sample_data;

  lost_event.header.size = sizeof(lost_event);
  lost_event.header.type = PERF_RECORD_LOST;
  lost_event.header.misc = 0;
  lost_event.id = event->id;
  lost_event.lost = local_xchg(&rb->lost, 0);

  perf_event_header__init_id(&lost_event.header,
        &sample_data, event);
  perf_output_put(handle, lost_event);
  perf_event__output_id_sample(event, handle, &sample_data);
 }

 return 0;

fail:
 local_inc(&rb->lost);
 perf_output_put_handle(handle);
out:
 rcu_read_unlock();

 return -ENOSPC;
}

int perf_output_begin_forward(struct perf_output_handle *handle,
        struct perf_event *event, unsigned int size)
{
 return __perf_output_begin(handle, event, size, false);
}

int perf_output_begin_backward(struct perf_output_handle *handle,
          struct perf_event *event, unsigned int size)
{
 return __perf_output_begin(handle, event, size, true);
}

int perf_output_begin(struct perf_output_handle *handle,
        struct perf_event *event, unsigned int size)
{

 return __perf_output_begin(handle, event, size,
       unlikely(is_write_backward(event)));
}

unsigned int perf_output_copy(struct perf_output_handle *handle,
        const void *buf, unsigned int len)
{
 return __output_copy(handle, buf, len);
}

unsigned int perf_output_skip(struct perf_output_handle *handle,
         unsigned int len)
{
 return __output_skip(handle, NULL, len);
}

void perf_output_end(struct perf_output_handle *handle)
{
 perf_output_put_handle(handle);
 rcu_read_unlock();
}

static void
ring_buffer_init(struct ring_buffer *rb, long watermark, int flags)
{
 long max_size = perf_data_size(rb);

 if (watermark)
  rb->watermark = min(max_size, watermark);

 if (!rb->watermark)
  rb->watermark = max_size / 2;

 if (flags & RING_BUFFER_WRITABLE)
  rb->overwrite = 0;
 else
  rb->overwrite = 1;

 atomic_set(&rb->refcount, 1);

 INIT_LIST_HEAD(&rb->event_list);
 spin_lock_init(&rb->event_lock);





 if (!rb->nr_pages)
  rb->paused = 1;
}
void *perf_aux_output_begin(struct perf_output_handle *handle,
       struct perf_event *event)
{
 struct perf_event *output_event = event;
 unsigned long aux_head, aux_tail;
 struct ring_buffer *rb;

 if (output_event->parent)
  output_event = output_event->parent;






 rb = ring_buffer_get(output_event);
 if (!rb)
  return NULL;

 if (!rb_has_aux(rb) || !atomic_inc_not_zero(&rb->aux_refcount))
  goto err;





 if (!atomic_read(&rb->aux_mmap_count))
  goto err_put;





 if (WARN_ON_ONCE(local_xchg(&rb->aux_nest, 1)))
  goto err_put;

 aux_head = local_read(&rb->aux_head);

 handle->rb = rb;
 handle->event = event;
 handle->head = aux_head;
 handle->size = 0;






 if (!rb->aux_overwrite) {
  aux_tail = ACCESS_ONCE(rb->user_page->aux_tail);
  handle->wakeup = local_read(&rb->aux_wakeup) + rb->aux_watermark;
  if (aux_head - aux_tail < perf_aux_size(rb))
   handle->size = CIRC_SPACE(aux_head, aux_tail, perf_aux_size(rb));






  if (!handle->size) {
   event->pending_disable = 1;
   perf_output_wakeup(handle);
   local_set(&rb->aux_nest, 0);
   goto err_put;
  }
 }

 return handle->rb->aux_priv;

err_put:

 rb_free_aux(rb);

err:
 ring_buffer_put(rb);
 handle->event = NULL;

 return NULL;
}
void perf_aux_output_end(struct perf_output_handle *handle, unsigned long size,
    bool truncated)
{
 struct ring_buffer *rb = handle->rb;
 bool wakeup = truncated;
 unsigned long aux_head;
 u64 flags = 0;

 if (truncated)
  flags |= PERF_AUX_FLAG_TRUNCATED;


 if (rb->aux_overwrite) {
  flags |= PERF_AUX_FLAG_OVERWRITE;

  aux_head = handle->head;
  local_set(&rb->aux_head, aux_head);
 } else {
  aux_head = local_read(&rb->aux_head);
  local_add(size, &rb->aux_head);
 }

 if (size || flags) {




  perf_event_aux_event(handle->event, aux_head, size, flags);
 }

 aux_head = rb->user_page->aux_head = local_read(&rb->aux_head);

 if (aux_head - local_read(&rb->aux_wakeup) >= rb->aux_watermark) {
  wakeup = true;
  local_add(rb->aux_watermark, &rb->aux_wakeup);
 }

 if (wakeup) {
  if (truncated)
   handle->event->pending_disable = 1;
  perf_output_wakeup(handle);
 }

 handle->event = NULL;

 local_set(&rb->aux_nest, 0);

 rb_free_aux(rb);
 ring_buffer_put(rb);
}





int perf_aux_output_skip(struct perf_output_handle *handle, unsigned long size)
{
 struct ring_buffer *rb = handle->rb;
 unsigned long aux_head;

 if (size > handle->size)
  return -ENOSPC;

 local_add(size, &rb->aux_head);

 aux_head = rb->user_page->aux_head = local_read(&rb->aux_head);
 if (aux_head - local_read(&rb->aux_wakeup) >= rb->aux_watermark) {
  perf_output_wakeup(handle);
  local_add(rb->aux_watermark, &rb->aux_wakeup);
  handle->wakeup = local_read(&rb->aux_wakeup) +
     rb->aux_watermark;
 }

 handle->head = aux_head;
 handle->size -= size;

 return 0;
}

void *perf_get_aux(struct perf_output_handle *handle)
{

 if (!handle->event)
  return NULL;

 return handle->rb->aux_priv;
}


static struct page *rb_alloc_aux_page(int node, int order)
{
 struct page *page;

 if (order > MAX_ORDER)
  order = MAX_ORDER;

 do {
  page = alloc_pages_node(node, PERF_AUX_GFP, order);
 } while (!page && order--);

 if (page && order) {






  split_page(page, order);
  SetPagePrivate(page);
  set_page_private(page, order);
 }

 return page;
}

static void rb_free_aux_page(struct ring_buffer *rb, int idx)
{
 struct page *page = virt_to_page(rb->aux_pages[idx]);

 ClearPagePrivate(page);
 page->mapping = NULL;
 __free_page(page);
}

static void __rb_free_aux(struct ring_buffer *rb)
{
 int pg;







 WARN_ON_ONCE(in_atomic());

 if (rb->aux_priv) {
  rb->free_aux(rb->aux_priv);
  rb->free_aux = NULL;
  rb->aux_priv = NULL;
 }

 if (rb->aux_nr_pages) {
  for (pg = 0; pg < rb->aux_nr_pages; pg++)
   rb_free_aux_page(rb, pg);

  kfree(rb->aux_pages);
  rb->aux_nr_pages = 0;
 }
}

int rb_alloc_aux(struct ring_buffer *rb, struct perf_event *event,
   pgoff_t pgoff, int nr_pages, long watermark, int flags)
{
 bool overwrite = !(flags & RING_BUFFER_WRITABLE);
 int node = (event->cpu == -1) ? -1 : cpu_to_node(event->cpu);
 int ret = -ENOMEM, max_order = 0;

 if (!has_aux(event))
  return -ENOTSUPP;

 if (event->pmu->capabilities & PERF_PMU_CAP_AUX_NO_SG) {




  max_order = ilog2(nr_pages);





  if ((event->pmu->capabilities & PERF_PMU_CAP_AUX_SW_DOUBLEBUF) &&
      !overwrite) {
   if (!max_order)
    return -EINVAL;

   max_order--;
  }
 }

 rb->aux_pages = kzalloc_node(nr_pages * sizeof(void *), GFP_KERNEL, node);
 if (!rb->aux_pages)
  return -ENOMEM;

 rb->free_aux = event->pmu->free_aux;
 for (rb->aux_nr_pages = 0; rb->aux_nr_pages < nr_pages;) {
  struct page *page;
  int last, order;

  order = min(max_order, ilog2(nr_pages - rb->aux_nr_pages));
  page = rb_alloc_aux_page(node, order);
  if (!page)
   goto out;

  for (last = rb->aux_nr_pages + (1 << page_private(page));
       last > rb->aux_nr_pages; rb->aux_nr_pages++)
   rb->aux_pages[rb->aux_nr_pages] = page_address(page++);
 }







 if ((event->pmu->capabilities & PERF_PMU_CAP_AUX_NO_SG) &&
     overwrite) {
  struct page *page = virt_to_page(rb->aux_pages[0]);

  if (page_private(page) != max_order)
   goto out;
 }

 rb->aux_priv = event->pmu->setup_aux(event->cpu, rb->aux_pages, nr_pages,
          overwrite);
 if (!rb->aux_priv)
  goto out;

 ret = 0;







 atomic_set(&rb->aux_refcount, 1);

 rb->aux_overwrite = overwrite;
 rb->aux_watermark = watermark;

 if (!rb->aux_watermark && !rb->aux_overwrite)
  rb->aux_watermark = nr_pages << (PAGE_SHIFT - 1);

out:
 if (!ret)
  rb->aux_pgoff = pgoff;
 else
  __rb_free_aux(rb);

 return ret;
}

void rb_free_aux(struct ring_buffer *rb)
{
 if (atomic_dec_and_test(&rb->aux_refcount))
  __rb_free_aux(rb);
}






static struct page *
__perf_mmap_to_page(struct ring_buffer *rb, unsigned long pgoff)
{
 if (pgoff > rb->nr_pages)
  return NULL;

 if (pgoff == 0)
  return virt_to_page(rb->user_page);

 return virt_to_page(rb->data_pages[pgoff - 1]);
}

static void *perf_mmap_alloc_page(int cpu)
{
 struct page *page;
 int node;

 node = (cpu == -1) ? cpu : cpu_to_node(cpu);
 page = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);
 if (!page)
  return NULL;

 return page_address(page);
}

struct ring_buffer *rb_alloc(int nr_pages, long watermark, int cpu, int flags)
{
 struct ring_buffer *rb;
 unsigned long size;
 int i;

 size = sizeof(struct ring_buffer);
 size += nr_pages * sizeof(void *);

 rb = kzalloc(size, GFP_KERNEL);
 if (!rb)
  goto fail;

 rb->user_page = perf_mmap_alloc_page(cpu);
 if (!rb->user_page)
  goto fail_user_page;

 for (i = 0; i < nr_pages; i++) {
  rb->data_pages[i] = perf_mmap_alloc_page(cpu);
  if (!rb->data_pages[i])
   goto fail_data_pages;
 }

 rb->nr_pages = nr_pages;

 ring_buffer_init(rb, watermark, flags);

 return rb;

fail_data_pages:
 for (i--; i >= 0; i--)
  free_page((unsigned long)rb->data_pages[i]);

 free_page((unsigned long)rb->user_page);

fail_user_page:
 kfree(rb);

fail:
 return NULL;
}

static void perf_mmap_free_page(unsigned long addr)
{
 struct page *page = virt_to_page((void *)addr);

 page->mapping = NULL;
 __free_page(page);
}

void rb_free(struct ring_buffer *rb)
{
 int i;

 perf_mmap_free_page((unsigned long)rb->user_page);
 for (i = 0; i < rb->nr_pages; i++)
  perf_mmap_free_page((unsigned long)rb->data_pages[i]);
 kfree(rb);
}

static int data_page_nr(struct ring_buffer *rb)
{
 return rb->nr_pages << page_order(rb);
}

static struct page *
__perf_mmap_to_page(struct ring_buffer *rb, unsigned long pgoff)
{

 if (pgoff > data_page_nr(rb))
  return NULL;

 return vmalloc_to_page((void *)rb->user_page + pgoff * PAGE_SIZE);
}

static void perf_mmap_unmark_page(void *addr)
{
 struct page *page = vmalloc_to_page(addr);

 page->mapping = NULL;
}

static void rb_free_work(struct work_struct *work)
{
 struct ring_buffer *rb;
 void *base;
 int i, nr;

 rb = container_of(work, struct ring_buffer, work);
 nr = data_page_nr(rb);

 base = rb->user_page;

 for (i = 0; i <= nr; i++)
  perf_mmap_unmark_page(base + (i * PAGE_SIZE));

 vfree(base);
 kfree(rb);
}

void rb_free(struct ring_buffer *rb)
{
 schedule_work(&rb->work);
}

struct ring_buffer *rb_alloc(int nr_pages, long watermark, int cpu, int flags)
{
 struct ring_buffer *rb;
 unsigned long size;
 void *all_buf;

 size = sizeof(struct ring_buffer);
 size += sizeof(void *);

 rb = kzalloc(size, GFP_KERNEL);
 if (!rb)
  goto fail;

 INIT_WORK(&rb->work, rb_free_work);

 all_buf = vmalloc_user((nr_pages + 1) * PAGE_SIZE);
 if (!all_buf)
  goto fail_all_buf;

 rb->user_page = all_buf;
 rb->data_pages[0] = all_buf + PAGE_SIZE;
 if (nr_pages) {
  rb->nr_pages = 1;
  rb->page_order = ilog2(nr_pages);
 }

 ring_buffer_init(rb, watermark, flags);

 return rb;

fail_all_buf:
 kfree(rb);

fail:
 return NULL;
}


struct page *
perf_mmap_to_page(struct ring_buffer *rb, unsigned long pgoff)
{
 if (rb->aux_nr_pages) {

  if (pgoff > rb->aux_pgoff + rb->aux_nr_pages)
   return NULL;


  if (pgoff >= rb->aux_pgoff)
   return virt_to_page(rb->aux_pages[pgoff - rb->aux_pgoff]);
 }

 return __perf_mmap_to_page(rb, pgoff);
}
struct clock_read_data {
 u64 epoch_ns;
 u64 epoch_cyc;
 u64 sched_clock_mask;
 u64 (*read_sched_clock)(void);
 u32 mult;
 u32 shift;
};
struct clock_data {
 seqcount_t seq;
 struct clock_read_data read_data[2];
 ktime_t wrap_kt;
 unsigned long rate;

 u64 (*actual_read_sched_clock)(void);
};

static struct hrtimer sched_clock_timer;
static int irqtime = -1;

core_param(irqtime, irqtime, int, 0400);

static u64 notrace jiffy_sched_clock_read(void)
{




 return (u64)(jiffies - INITIAL_JIFFIES);
}

static struct clock_data cd ____cacheline_aligned = {
 .read_data[0] = { .mult = NSEC_PER_SEC / HZ,
     .read_sched_clock = jiffy_sched_clock_read, },
 .actual_read_sched_clock = jiffy_sched_clock_read,
};

static inline u64 notrace cyc_to_ns(u64 cyc, u32 mult, u32 shift)
{
 return (cyc * mult) >> shift;
}

unsigned long long notrace sched_clock(void)
{
 u64 cyc, res;
 unsigned long seq;
 struct clock_read_data *rd;

 do {
  seq = raw_read_seqcount(&cd.seq);
  rd = cd.read_data + (seq & 1);

  cyc = (rd->read_sched_clock() - rd->epoch_cyc) &
        rd->sched_clock_mask;
  res = rd->epoch_ns + cyc_to_ns(cyc, rd->mult, rd->shift);
 } while (read_seqcount_retry(&cd.seq, seq));

 return res;
}
static void update_clock_read_data(struct clock_read_data *rd)
{

 cd.read_data[1] = *rd;


 raw_write_seqcount_latch(&cd.seq);


 cd.read_data[0] = *rd;


 raw_write_seqcount_latch(&cd.seq);
}




static void update_sched_clock(void)
{
 u64 cyc;
 u64 ns;
 struct clock_read_data rd;

 rd = cd.read_data[0];

 cyc = cd.actual_read_sched_clock();
 ns = rd.epoch_ns + cyc_to_ns((cyc - rd.epoch_cyc) & rd.sched_clock_mask, rd.mult, rd.shift);

 rd.epoch_ns = ns;
 rd.epoch_cyc = cyc;

 update_clock_read_data(&rd);
}

static enum hrtimer_restart sched_clock_poll(struct hrtimer *hrt)
{
 update_sched_clock();
 hrtimer_forward_now(hrt, cd.wrap_kt);

 return HRTIMER_RESTART;
}

void __init
sched_clock_register(u64 (*read)(void), int bits, unsigned long rate)
{
 u64 res, wrap, new_mask, new_epoch, cyc, ns;
 u32 new_mult, new_shift;
 unsigned long r;
 char r_unit;
 struct clock_read_data rd;

 if (cd.rate > rate)
  return;

 WARN_ON(!irqs_disabled());


 clocks_calc_mult_shift(&new_mult, &new_shift, rate, NSEC_PER_SEC, 3600);

 new_mask = CLOCKSOURCE_MASK(bits);
 cd.rate = rate;


 wrap = clocks_calc_max_nsecs(new_mult, new_shift, 0, new_mask, NULL);
 cd.wrap_kt = ns_to_ktime(wrap);

 rd = cd.read_data[0];


 new_epoch = read();
 cyc = cd.actual_read_sched_clock();
 ns = rd.epoch_ns + cyc_to_ns((cyc - rd.epoch_cyc) & rd.sched_clock_mask, rd.mult, rd.shift);
 cd.actual_read_sched_clock = read;

 rd.read_sched_clock = read;
 rd.sched_clock_mask = new_mask;
 rd.mult = new_mult;
 rd.shift = new_shift;
 rd.epoch_cyc = new_epoch;
 rd.epoch_ns = ns;

 update_clock_read_data(&rd);

 r = rate;
 if (r >= 4000000) {
  r /= 1000000;
  r_unit = 'M';
 } else {
  if (r >= 1000) {
   r /= 1000;
   r_unit = 'k';
  } else {
   r_unit = ' ';
  }
 }


 res = cyc_to_ns(1ULL, new_mult, new_shift);

 pr_info("sched_clock: %u bits at %lu%cHz, resolution %lluns, wraps every %lluns\n",
  bits, r, r_unit, res, wrap);


 if (irqtime > 0 || (irqtime == -1 && rate >= 1000000))
  enable_sched_clock_irqtime();

 pr_debug("Registered %pF as sched_clock source\n", read);
}

void __init sched_clock_postinit(void)
{




 if (cd.actual_read_sched_clock == jiffy_sched_clock_read)
  sched_clock_register(jiffy_sched_clock_read, BITS_PER_LONG, HZ);

 update_sched_clock();





 hrtimer_init(&sched_clock_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 sched_clock_timer.function = sched_clock_poll;
 hrtimer_start(&sched_clock_timer, cd.wrap_kt, HRTIMER_MODE_REL);
}
static u64 notrace suspended_sched_clock_read(void)
{
 unsigned long seq = raw_read_seqcount(&cd.seq);

 return cd.read_data[seq & 1].epoch_cyc;
}

static int sched_clock_suspend(void)
{
 struct clock_read_data *rd = &cd.read_data[0];

 update_sched_clock();
 hrtimer_cancel(&sched_clock_timer);
 rd->read_sched_clock = suspended_sched_clock_read;

 return 0;
}

static void sched_clock_resume(void)
{
 struct clock_read_data *rd = &cd.read_data[0];

 rd->epoch_cyc = cd.actual_read_sched_clock();
 hrtimer_start(&sched_clock_timer, cd.wrap_kt, HRTIMER_MODE_REL);
 rd->read_sched_clock = cd.actual_read_sched_clock;
}

static struct syscore_ops sched_clock_ops = {
 .suspend = sched_clock_suspend,
 .resume = sched_clock_resume,
};

static int __init sched_clock_syscore_init(void)
{
 register_syscore_ops(&sched_clock_ops);

 return 0;
}
device_initcall(sched_clock_syscore_init);


struct seccomp_filter {
 atomic_t usage;
 struct seccomp_filter *prev;
 struct bpf_prog *prog;
};







static void populate_seccomp_data(struct seccomp_data *sd)
{
 struct task_struct *task = current;
 struct pt_regs *regs = task_pt_regs(task);
 unsigned long args[6];

 sd->nr = syscall_get_nr(task, regs);
 sd->arch = syscall_get_arch();
 syscall_get_arguments(task, regs, 0, 6, args);
 sd->args[0] = args[0];
 sd->args[1] = args[1];
 sd->args[2] = args[2];
 sd->args[3] = args[3];
 sd->args[4] = args[4];
 sd->args[5] = args[5];
 sd->instruction_pointer = KSTK_EIP(task);
}
static int seccomp_check_filter(struct sock_filter *filter, unsigned int flen)
{
 int pc;
 for (pc = 0; pc < flen; pc++) {
  struct sock_filter *ftest = &filter[pc];
  u16 code = ftest->code;
  u32 k = ftest->k;

  switch (code) {
  case BPF_LD | BPF_W | BPF_ABS:
   ftest->code = BPF_LDX | BPF_W | BPF_ABS;

   if (k >= sizeof(struct seccomp_data) || k & 3)
    return -EINVAL;
   continue;
  case BPF_LD | BPF_W | BPF_LEN:
   ftest->code = BPF_LD | BPF_IMM;
   ftest->k = sizeof(struct seccomp_data);
   continue;
  case BPF_LDX | BPF_W | BPF_LEN:
   ftest->code = BPF_LDX | BPF_IMM;
   ftest->k = sizeof(struct seccomp_data);
   continue;

  case BPF_RET | BPF_K:
  case BPF_RET | BPF_A:
  case BPF_ALU | BPF_ADD | BPF_K:
  case BPF_ALU | BPF_ADD | BPF_X:
  case BPF_ALU | BPF_SUB | BPF_K:
  case BPF_ALU | BPF_SUB | BPF_X:
  case BPF_ALU | BPF_MUL | BPF_K:
  case BPF_ALU | BPF_MUL | BPF_X:
  case BPF_ALU | BPF_DIV | BPF_K:
  case BPF_ALU | BPF_DIV | BPF_X:
  case BPF_ALU | BPF_AND | BPF_K:
  case BPF_ALU | BPF_AND | BPF_X:
  case BPF_ALU | BPF_OR | BPF_K:
  case BPF_ALU | BPF_OR | BPF_X:
  case BPF_ALU | BPF_XOR | BPF_K:
  case BPF_ALU | BPF_XOR | BPF_X:
  case BPF_ALU | BPF_LSH | BPF_K:
  case BPF_ALU | BPF_LSH | BPF_X:
  case BPF_ALU | BPF_RSH | BPF_K:
  case BPF_ALU | BPF_RSH | BPF_X:
  case BPF_ALU | BPF_NEG:
  case BPF_LD | BPF_IMM:
  case BPF_LDX | BPF_IMM:
  case BPF_MISC | BPF_TAX:
  case BPF_MISC | BPF_TXA:
  case BPF_LD | BPF_MEM:
  case BPF_LDX | BPF_MEM:
  case BPF_ST:
  case BPF_STX:
  case BPF_JMP | BPF_JA:
  case BPF_JMP | BPF_JEQ | BPF_K:
  case BPF_JMP | BPF_JEQ | BPF_X:
  case BPF_JMP | BPF_JGE | BPF_K:
  case BPF_JMP | BPF_JGE | BPF_X:
  case BPF_JMP | BPF_JGT | BPF_K:
  case BPF_JMP | BPF_JGT | BPF_X:
  case BPF_JMP | BPF_JSET | BPF_K:
  case BPF_JMP | BPF_JSET | BPF_X:
   continue;
  default:
   return -EINVAL;
  }
 }
 return 0;
}







static u32 seccomp_run_filters(struct seccomp_data *sd)
{
 struct seccomp_data sd_local;
 u32 ret = SECCOMP_RET_ALLOW;

 struct seccomp_filter *f =
   lockless_dereference(current->seccomp.filter);


 if (unlikely(WARN_ON(f == NULL)))
  return SECCOMP_RET_KILL;

 if (!sd) {
  populate_seccomp_data(&sd_local);
  sd = &sd_local;
 }





 for (; f; f = f->prev) {
  u32 cur_ret = BPF_PROG_RUN(f->prog, (void *)sd);

  if ((cur_ret & SECCOMP_RET_ACTION) < (ret & SECCOMP_RET_ACTION))
   ret = cur_ret;
 }
 return ret;
}

static inline bool seccomp_may_assign_mode(unsigned long seccomp_mode)
{
 assert_spin_locked(&current->sighand->siglock);

 if (current->seccomp.mode && current->seccomp.mode != seccomp_mode)
  return false;

 return true;
}

static inline void seccomp_assign_mode(struct task_struct *task,
           unsigned long seccomp_mode)
{
 assert_spin_locked(&task->sighand->siglock);

 task->seccomp.mode = seccomp_mode;




 smp_mb__before_atomic();
 set_tsk_thread_flag(task, TIF_SECCOMP);
}


static int is_ancestor(struct seccomp_filter *parent,
         struct seccomp_filter *child)
{

 if (parent == NULL)
  return 1;
 for (; child; child = child->prev)
  if (child == parent)
   return 1;
 return 0;
}
static inline pid_t seccomp_can_sync_threads(void)
{
 struct task_struct *thread, *caller;

 BUG_ON(!mutex_is_locked(&current->signal->cred_guard_mutex));
 assert_spin_locked(&current->sighand->siglock);


 caller = current;
 for_each_thread(caller, thread) {
  pid_t failed;


  if (thread == caller)
   continue;

  if (thread->seccomp.mode == SECCOMP_MODE_DISABLED ||
      (thread->seccomp.mode == SECCOMP_MODE_FILTER &&
       is_ancestor(thread->seccomp.filter,
     caller->seccomp.filter)))
   continue;


  failed = task_pid_vnr(thread);

  if (unlikely(WARN_ON(failed == 0)))
   failed = -ESRCH;
  return failed;
 }

 return 0;
}
static inline void seccomp_sync_threads(void)
{
 struct task_struct *thread, *caller;

 BUG_ON(!mutex_is_locked(&current->signal->cred_guard_mutex));
 assert_spin_locked(&current->sighand->siglock);


 caller = current;
 for_each_thread(caller, thread) {

  if (thread == caller)
   continue;


  get_seccomp_filter(caller);





  put_seccomp_filter(thread);
  smp_store_release(&thread->seccomp.filter,
      caller->seccomp.filter);







  if (task_no_new_privs(caller))
   task_set_no_new_privs(thread);







  if (thread->seccomp.mode == SECCOMP_MODE_DISABLED)
   seccomp_assign_mode(thread, SECCOMP_MODE_FILTER);
 }
}







static struct seccomp_filter *seccomp_prepare_filter(struct sock_fprog *fprog)
{
 struct seccomp_filter *sfilter;
 int ret;
 const bool save_orig = config_enabled(CONFIG_CHECKPOINT_RESTORE);

 if (fprog->len == 0 || fprog->len > BPF_MAXINSNS)
  return ERR_PTR(-EINVAL);

 BUG_ON(INT_MAX / fprog->len < sizeof(struct sock_filter));







 if (!task_no_new_privs(current) &&
     security_capable_noaudit(current_cred(), current_user_ns(),
         CAP_SYS_ADMIN) != 0)
  return ERR_PTR(-EACCES);


 sfilter = kzalloc(sizeof(*sfilter), GFP_KERNEL | __GFP_NOWARN);
 if (!sfilter)
  return ERR_PTR(-ENOMEM);

 ret = bpf_prog_create_from_user(&sfilter->prog, fprog,
     seccomp_check_filter, save_orig);
 if (ret < 0) {
  kfree(sfilter);
  return ERR_PTR(ret);
 }

 atomic_set(&sfilter->usage, 1);

 return sfilter;
}







static struct seccomp_filter *
seccomp_prepare_user_filter(const char __user *user_filter)
{
 struct sock_fprog fprog;
 struct seccomp_filter *filter = ERR_PTR(-EFAULT);

 if (in_compat_syscall()) {
  struct compat_sock_fprog fprog32;
  if (copy_from_user(&fprog32, user_filter, sizeof(fprog32)))
   goto out;
  fprog.len = fprog32.len;
  fprog.filter = compat_ptr(fprog32.filter);
 } else
 if (copy_from_user(&fprog, user_filter, sizeof(fprog)))
  goto out;
 filter = seccomp_prepare_filter(&fprog);
out:
 return filter;
}
static long seccomp_attach_filter(unsigned int flags,
      struct seccomp_filter *filter)
{
 unsigned long total_insns;
 struct seccomp_filter *walker;

 assert_spin_locked(&current->sighand->siglock);


 total_insns = filter->prog->len;
 for (walker = current->seccomp.filter; walker; walker = walker->prev)
  total_insns += walker->prog->len + 4;
 if (total_insns > MAX_INSNS_PER_PATH)
  return -ENOMEM;


 if (flags & SECCOMP_FILTER_FLAG_TSYNC) {
  int ret;

  ret = seccomp_can_sync_threads();
  if (ret)
   return ret;
 }





 filter->prev = current->seccomp.filter;
 current->seccomp.filter = filter;


 if (flags & SECCOMP_FILTER_FLAG_TSYNC)
  seccomp_sync_threads();

 return 0;
}


void get_seccomp_filter(struct task_struct *tsk)
{
 struct seccomp_filter *orig = tsk->seccomp.filter;
 if (!orig)
  return;

 atomic_inc(&orig->usage);
}

static inline void seccomp_filter_free(struct seccomp_filter *filter)
{
 if (filter) {
  bpf_prog_destroy(filter->prog);
  kfree(filter);
 }
}


void put_seccomp_filter(struct task_struct *tsk)
{
 struct seccomp_filter *orig = tsk->seccomp.filter;

 while (orig && atomic_dec_and_test(&orig->usage)) {
  struct seccomp_filter *freeme = orig;
  orig = orig->prev;
  seccomp_filter_free(freeme);
 }
}
static void seccomp_send_sigsys(int syscall, int reason)
{
 struct siginfo info;
 memset(&info, 0, sizeof(info));
 info.si_signo = SIGSYS;
 info.si_code = SYS_SECCOMP;
 info.si_call_addr = (void __user *)KSTK_EIP(current);
 info.si_errno = reason;
 info.si_arch = syscall_get_arch();
 info.si_syscall = syscall;
 force_sig_info(SIGSYS, &info, current);
}






static const int mode1_syscalls[] = {
 __NR_seccomp_read, __NR_seccomp_write, __NR_seccomp_exit, __NR_seccomp_sigreturn,
 0,
};

static void __secure_computing_strict(int this_syscall)
{
 const int *syscall_whitelist = mode1_syscalls;
 if (in_compat_syscall())
  syscall_whitelist = get_compat_mode1_syscalls();
 do {
  if (*syscall_whitelist == this_syscall)
   return;
 } while (*++syscall_whitelist);

 dump_stack();
 audit_seccomp(this_syscall, SIGKILL, SECCOMP_RET_KILL);
 do_exit(SIGKILL);
}

void secure_computing_strict(int this_syscall)
{
 int mode = current->seccomp.mode;

 if (config_enabled(CONFIG_CHECKPOINT_RESTORE) &&
     unlikely(current->ptrace & PT_SUSPEND_SECCOMP))
  return;

 if (mode == SECCOMP_MODE_DISABLED)
  return;
 else if (mode == SECCOMP_MODE_STRICT)
  __secure_computing_strict(this_syscall);
 else
  BUG();
}
int __secure_computing(void)
{
 u32 phase1_result = seccomp_phase1(NULL);

 if (likely(phase1_result == SECCOMP_PHASE1_OK))
  return 0;
 else if (likely(phase1_result == SECCOMP_PHASE1_SKIP))
  return -1;
 else
  return seccomp_phase2(phase1_result);
}

static u32 __seccomp_phase1_filter(int this_syscall, struct seccomp_data *sd)
{
 u32 filter_ret, action;
 int data;





 rmb();

 filter_ret = seccomp_run_filters(sd);
 data = filter_ret & SECCOMP_RET_DATA;
 action = filter_ret & SECCOMP_RET_ACTION;

 switch (action) {
 case SECCOMP_RET_ERRNO:

  if (data > MAX_ERRNO)
   data = MAX_ERRNO;
  syscall_set_return_value(current, task_pt_regs(current),
      -data, 0);
  goto skip;

 case SECCOMP_RET_TRAP:

  syscall_rollback(current, task_pt_regs(current));

  seccomp_send_sigsys(this_syscall, data);
  goto skip;

 case SECCOMP_RET_TRACE:
  return filter_ret;

 case SECCOMP_RET_ALLOW:
  return SECCOMP_PHASE1_OK;

 case SECCOMP_RET_KILL:
 default:
  audit_seccomp(this_syscall, SIGSYS, action);
  do_exit(SIGSYS);
 }

 unreachable();

skip:
 audit_seccomp(this_syscall, 0, action);
 return SECCOMP_PHASE1_SKIP;
}
u32 seccomp_phase1(struct seccomp_data *sd)
{
 int mode = current->seccomp.mode;
 int this_syscall = sd ? sd->nr :
  syscall_get_nr(current, task_pt_regs(current));

 if (config_enabled(CONFIG_CHECKPOINT_RESTORE) &&
     unlikely(current->ptrace & PT_SUSPEND_SECCOMP))
  return SECCOMP_PHASE1_OK;

 switch (mode) {
 case SECCOMP_MODE_STRICT:
  __secure_computing_strict(this_syscall);
  return SECCOMP_PHASE1_OK;
 case SECCOMP_MODE_FILTER:
  return __seccomp_phase1_filter(this_syscall, sd);
 default:
  BUG();
 }
}
int seccomp_phase2(u32 phase1_result)
{
 struct pt_regs *regs = task_pt_regs(current);
 u32 action = phase1_result & SECCOMP_RET_ACTION;
 int data = phase1_result & SECCOMP_RET_DATA;

 BUG_ON(action != SECCOMP_RET_TRACE);

 audit_seccomp(syscall_get_nr(current, regs), 0, action);


 if (!ptrace_event_enabled(current, PTRACE_EVENT_SECCOMP)) {
  syscall_set_return_value(current, regs,
      -ENOSYS, 0);
  return -1;
 }


 ptrace_event(PTRACE_EVENT_SECCOMP, data);






 if (fatal_signal_pending(current))
  do_exit(SIGSYS);
 if (syscall_get_nr(current, regs) < 0)
  return -1;

 return 0;
}

long prctl_get_seccomp(void)
{
 return current->seccomp.mode;
}
static long seccomp_set_mode_strict(void)
{
 const unsigned long seccomp_mode = SECCOMP_MODE_STRICT;
 long ret = -EINVAL;

 spin_lock_irq(&current->sighand->siglock);

 if (!seccomp_may_assign_mode(seccomp_mode))
  goto out;

 disable_TSC();
 seccomp_assign_mode(current, seccomp_mode);
 ret = 0;

out:
 spin_unlock_irq(&current->sighand->siglock);

 return ret;
}

static long seccomp_set_mode_filter(unsigned int flags,
        const char __user *filter)
{
 const unsigned long seccomp_mode = SECCOMP_MODE_FILTER;
 struct seccomp_filter *prepared = NULL;
 long ret = -EINVAL;


 if (flags & ~SECCOMP_FILTER_FLAG_MASK)
  return -EINVAL;


 prepared = seccomp_prepare_user_filter(filter);
 if (IS_ERR(prepared))
  return PTR_ERR(prepared);





 if (flags & SECCOMP_FILTER_FLAG_TSYNC &&
     mutex_lock_killable(&current->signal->cred_guard_mutex))
  goto out_free;

 spin_lock_irq(&current->sighand->siglock);

 if (!seccomp_may_assign_mode(seccomp_mode))
  goto out;

 ret = seccomp_attach_filter(flags, prepared);
 if (ret)
  goto out;

 prepared = NULL;

 seccomp_assign_mode(current, seccomp_mode);
out:
 spin_unlock_irq(&current->sighand->siglock);
 if (flags & SECCOMP_FILTER_FLAG_TSYNC)
  mutex_unlock(&current->signal->cred_guard_mutex);
out_free:
 seccomp_filter_free(prepared);
 return ret;
}
static inline long seccomp_set_mode_filter(unsigned int flags,
        const char __user *filter)
{
 return -EINVAL;
}


static long do_seccomp(unsigned int op, unsigned int flags,
         const char __user *uargs)
{
 switch (op) {
 case SECCOMP_SET_MODE_STRICT:
  if (flags != 0 || uargs != NULL)
   return -EINVAL;
  return seccomp_set_mode_strict();
 case SECCOMP_SET_MODE_FILTER:
  return seccomp_set_mode_filter(flags, uargs);
 default:
  return -EINVAL;
 }
}

SYSCALL_DEFINE3(seccomp, unsigned int, op, unsigned int, flags,
    const char __user *, uargs)
{
 return do_seccomp(op, flags, uargs);
}
long prctl_set_seccomp(unsigned long seccomp_mode, char __user *filter)
{
 unsigned int op;
 char __user *uargs;

 switch (seccomp_mode) {
 case SECCOMP_MODE_STRICT:
  op = SECCOMP_SET_MODE_STRICT;





  uargs = NULL;
  break;
 case SECCOMP_MODE_FILTER:
  op = SECCOMP_SET_MODE_FILTER;
  uargs = filter;
  break;
 default:
  return -EINVAL;
 }


 return do_seccomp(op, 0, uargs);
}

long seccomp_get_filter(struct task_struct *task, unsigned long filter_off,
   void __user *data)
{
 struct seccomp_filter *filter;
 struct sock_fprog_kern *fprog;
 long ret;
 unsigned long count = 0;

 if (!capable(CAP_SYS_ADMIN) ||
     current->seccomp.mode != SECCOMP_MODE_DISABLED) {
  return -EACCES;
 }

 spin_lock_irq(&task->sighand->siglock);
 if (task->seccomp.mode != SECCOMP_MODE_FILTER) {
  ret = -EINVAL;
  goto out;
 }

 filter = task->seccomp.filter;
 while (filter) {
  filter = filter->prev;
  count++;
 }

 if (filter_off >= count) {
  ret = -ENOENT;
  goto out;
 }
 count -= filter_off;

 filter = task->seccomp.filter;
 while (filter && count > 1) {
  filter = filter->prev;
  count--;
 }

 if (WARN_ON(count != 1 || !filter)) {

  ret = -ENOENT;
  goto out;
 }

 fprog = filter->prog->orig_prog;
 if (!fprog) {




  ret = -EMEDIUMTYPE;
  goto out;
 }

 ret = fprog->len;
 if (!data)
  goto out;

 get_seccomp_filter(task);
 spin_unlock_irq(&task->sighand->siglock);

 if (copy_to_user(data, fprog->filter, bpf_classic_proglen(fprog)))
  ret = -EFAULT;

 put_seccomp_filter(task);
 return ret;

out:
 spin_unlock_irq(&task->sighand->siglock);
 return ret;
}







static struct kmem_cache *sigqueue_cachep;

int print_fatal_signals __read_mostly;

static void __user *sig_handler(struct task_struct *t, int sig)
{
 return t->sighand->action[sig - 1].sa.sa_handler;
}

static int sig_handler_ignored(void __user *handler, int sig)
{

 return handler == SIG_IGN ||
  (handler == SIG_DFL && sig_kernel_ignore(sig));
}

static int sig_task_ignored(struct task_struct *t, int sig, bool force)
{
 void __user *handler;

 handler = sig_handler(t, sig);

 if (unlikely(t->signal->flags & SIGNAL_UNKILLABLE) &&
   handler == SIG_DFL && !force)
  return 1;

 return sig_handler_ignored(handler, sig);
}

static int sig_ignored(struct task_struct *t, int sig, bool force)
{





 if (sigismember(&t->blocked, sig) || sigismember(&t->real_blocked, sig))
  return 0;

 if (!sig_task_ignored(t, sig, force))
  return 0;




 return !t->ptrace;
}





static inline int has_pending_signals(sigset_t *signal, sigset_t *blocked)
{
 unsigned long ready;
 long i;

 switch (_NSIG_WORDS) {
 default:
  for (i = _NSIG_WORDS, ready = 0; --i >= 0 ;)
   ready |= signal->sig[i] &~ blocked->sig[i];
  break;

 case 4: ready = signal->sig[3] &~ blocked->sig[3];
  ready |= signal->sig[2] &~ blocked->sig[2];
  ready |= signal->sig[1] &~ blocked->sig[1];
  ready |= signal->sig[0] &~ blocked->sig[0];
  break;

 case 2: ready = signal->sig[1] &~ blocked->sig[1];
  ready |= signal->sig[0] &~ blocked->sig[0];
  break;

 case 1: ready = signal->sig[0] &~ blocked->sig[0];
 }
 return ready != 0;
}


static int recalc_sigpending_tsk(struct task_struct *t)
{
 if ((t->jobctl & JOBCTL_PENDING_MASK) ||
     PENDING(&t->pending, &t->blocked) ||
     PENDING(&t->signal->shared_pending, &t->blocked)) {
  set_tsk_thread_flag(t, TIF_SIGPENDING);
  return 1;
 }





 return 0;
}





void recalc_sigpending_and_wake(struct task_struct *t)
{
 if (recalc_sigpending_tsk(t))
  signal_wake_up(t, 0);
}

void recalc_sigpending(void)
{
 if (!recalc_sigpending_tsk(current) && !freezing(current))
  clear_thread_flag(TIF_SIGPENDING);

}



 (sigmask(SIGSEGV) | sigmask(SIGBUS) | sigmask(SIGILL) | \
  sigmask(SIGTRAP) | sigmask(SIGFPE) | sigmask(SIGSYS))

int next_signal(struct sigpending *pending, sigset_t *mask)
{
 unsigned long i, *s, *m, x;
 int sig = 0;

 s = pending->signal.sig;
 m = mask->sig;





 x = *s &~ *m;
 if (x) {
  if (x & SYNCHRONOUS_MASK)
   x &= SYNCHRONOUS_MASK;
  sig = ffz(~x) + 1;
  return sig;
 }

 switch (_NSIG_WORDS) {
 default:
  for (i = 1; i < _NSIG_WORDS; ++i) {
   x = *++s &~ *++m;
   if (!x)
    continue;
   sig = ffz(~x) + i*_NSIG_BPW + 1;
   break;
  }
  break;

 case 2:
  x = s[1] &~ m[1];
  if (!x)
   break;
  sig = ffz(~x) + _NSIG_BPW + 1;
  break;

 case 1:

  break;
 }

 return sig;
}

static inline void print_dropped_signal(int sig)
{
 static DEFINE_RATELIMIT_STATE(ratelimit_state, 5 * HZ, 10);

 if (!print_fatal_signals)
  return;

 if (!__ratelimit(&ratelimit_state))
  return;

 pr_info("%s/%d: reached RLIMIT_SIGPENDING, dropped signal %d\n",
    current->comm, current->pid, sig);
}
bool task_set_jobctl_pending(struct task_struct *task, unsigned long mask)
{
 BUG_ON(mask & ~(JOBCTL_PENDING_MASK | JOBCTL_STOP_CONSUME |
   JOBCTL_STOP_SIGMASK | JOBCTL_TRAPPING));
 BUG_ON((mask & JOBCTL_TRAPPING) && !(mask & JOBCTL_PENDING_MASK));

 if (unlikely(fatal_signal_pending(task) || (task->flags & PF_EXITING)))
  return false;

 if (mask & JOBCTL_STOP_SIGMASK)
  task->jobctl &= ~JOBCTL_STOP_SIGMASK;

 task->jobctl |= mask;
 return true;
}
void task_clear_jobctl_trapping(struct task_struct *task)
{
 if (unlikely(task->jobctl & JOBCTL_TRAPPING)) {
  task->jobctl &= ~JOBCTL_TRAPPING;
  smp_mb();
  wake_up_bit(&task->jobctl, JOBCTL_TRAPPING_BIT);
 }
}
void task_clear_jobctl_pending(struct task_struct *task, unsigned long mask)
{
 BUG_ON(mask & ~JOBCTL_PENDING_MASK);

 if (mask & JOBCTL_STOP_PENDING)
  mask |= JOBCTL_STOP_CONSUME | JOBCTL_STOP_DEQUEUED;

 task->jobctl &= ~mask;

 if (!(task->jobctl & JOBCTL_PENDING_MASK))
  task_clear_jobctl_trapping(task);
}
static bool task_participate_group_stop(struct task_struct *task)
{
 struct signal_struct *sig = task->signal;
 bool consume = task->jobctl & JOBCTL_STOP_CONSUME;

 WARN_ON_ONCE(!(task->jobctl & JOBCTL_STOP_PENDING));

 task_clear_jobctl_pending(task, JOBCTL_STOP_PENDING);

 if (!consume)
  return false;

 if (!WARN_ON_ONCE(sig->group_stop_count == 0))
  sig->group_stop_count--;





 if (!sig->group_stop_count && !(sig->flags & SIGNAL_STOP_STOPPED)) {
  sig->flags = SIGNAL_STOP_STOPPED;
  return true;
 }
 return false;
}






static struct sigqueue *
__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimit)
{
 struct sigqueue *q = NULL;
 struct user_struct *user;





 rcu_read_lock();
 user = get_uid(__task_cred(t)->user);
 atomic_inc(&user->sigpending);
 rcu_read_unlock();

 if (override_rlimit ||
     atomic_read(&user->sigpending) <=
   task_rlimit(t, RLIMIT_SIGPENDING)) {
  q = kmem_cache_alloc(sigqueue_cachep, flags);
 } else {
  print_dropped_signal(sig);
 }

 if (unlikely(q == NULL)) {
  atomic_dec(&user->sigpending);
  free_uid(user);
 } else {
  INIT_LIST_HEAD(&q->list);
  q->flags = 0;
  q->user = user;
 }

 return q;
}

static void __sigqueue_free(struct sigqueue *q)
{
 if (q->flags & SIGQUEUE_PREALLOC)
  return;
 atomic_dec(&q->user->sigpending);
 free_uid(q->user);
 kmem_cache_free(sigqueue_cachep, q);
}

void flush_sigqueue(struct sigpending *queue)
{
 struct sigqueue *q;

 sigemptyset(&queue->signal);
 while (!list_empty(&queue->list)) {
  q = list_entry(queue->list.next, struct sigqueue , list);
  list_del_init(&q->list);
  __sigqueue_free(q);
 }
}




void flush_signals(struct task_struct *t)
{
 unsigned long flags;

 spin_lock_irqsave(&t->sighand->siglock, flags);
 clear_tsk_thread_flag(t, TIF_SIGPENDING);
 flush_sigqueue(&t->pending);
 flush_sigqueue(&t->signal->shared_pending);
 spin_unlock_irqrestore(&t->sighand->siglock, flags);
}

static void __flush_itimer_signals(struct sigpending *pending)
{
 sigset_t signal, retain;
 struct sigqueue *q, *n;

 signal = pending->signal;
 sigemptyset(&retain);

 list_for_each_entry_safe(q, n, &pending->list, list) {
  int sig = q->info.si_signo;

  if (likely(q->info.si_code != SI_TIMER)) {
   sigaddset(&retain, sig);
  } else {
   sigdelset(&signal, sig);
   list_del_init(&q->list);
   __sigqueue_free(q);
  }
 }

 sigorsets(&pending->signal, &signal, &retain);
}

void flush_itimer_signals(void)
{
 struct task_struct *tsk = current;
 unsigned long flags;

 spin_lock_irqsave(&tsk->sighand->siglock, flags);
 __flush_itimer_signals(&tsk->pending);
 __flush_itimer_signals(&tsk->signal->shared_pending);
 spin_unlock_irqrestore(&tsk->sighand->siglock, flags);
}

void ignore_signals(struct task_struct *t)
{
 int i;

 for (i = 0; i < _NSIG; ++i)
  t->sighand->action[i].sa.sa_handler = SIG_IGN;

 flush_signals(t);
}





void
flush_signal_handlers(struct task_struct *t, int force_default)
{
 int i;
 struct k_sigaction *ka = &t->sighand->action[0];
 for (i = _NSIG ; i != 0 ; i--) {
  if (force_default || ka->sa.sa_handler != SIG_IGN)
   ka->sa.sa_handler = SIG_DFL;
  ka->sa.sa_flags = 0;
  ka->sa.sa_restorer = NULL;
  sigemptyset(&ka->sa.sa_mask);
  ka++;
 }
}

int unhandled_signal(struct task_struct *tsk, int sig)
{
 void __user *handler = tsk->sighand->action[sig-1].sa.sa_handler;
 if (is_global_init(tsk))
  return 1;
 if (handler != SIG_IGN && handler != SIG_DFL)
  return 0;

 return !tsk->ptrace;
}

static void collect_signal(int sig, struct sigpending *list, siginfo_t *info)
{
 struct sigqueue *q, *first = NULL;





 list_for_each_entry(q, &list->list, list) {
  if (q->info.si_signo == sig) {
   if (first)
    goto still_pending;
   first = q;
  }
 }

 sigdelset(&list->signal, sig);

 if (first) {
still_pending:
  list_del_init(&first->list);
  copy_siginfo(info, &first->info);
  __sigqueue_free(first);
 } else {





  info->si_signo = sig;
  info->si_errno = 0;
  info->si_code = SI_USER;
  info->si_pid = 0;
  info->si_uid = 0;
 }
}

static int __dequeue_signal(struct sigpending *pending, sigset_t *mask,
   siginfo_t *info)
{
 int sig = next_signal(pending, mask);

 if (sig)
  collect_signal(sig, pending, info);
 return sig;
}







int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)
{
 int signr;




 signr = __dequeue_signal(&tsk->pending, mask, info);
 if (!signr) {
  signr = __dequeue_signal(&tsk->signal->shared_pending,
      mask, info);
  if (unlikely(signr == SIGALRM)) {
   struct hrtimer *tmr = &tsk->signal->real_timer;

   if (!hrtimer_is_queued(tmr) &&
       tsk->signal->it_real_incr.tv64 != 0) {
    hrtimer_forward(tmr, tmr->base->get_time(),
      tsk->signal->it_real_incr);
    hrtimer_restart(tmr);
   }
  }
 }

 recalc_sigpending();
 if (!signr)
  return 0;

 if (unlikely(sig_kernel_stop(signr))) {
  current->jobctl |= JOBCTL_STOP_DEQUEUED;
 }
 if ((info->si_code & __SI_MASK) == __SI_TIMER && info->si_sys_private) {






  spin_unlock(&tsk->sighand->siglock);
  do_schedule_next_timer(info);
  spin_lock(&tsk->sighand->siglock);
 }
 return signr;
}
void signal_wake_up_state(struct task_struct *t, unsigned int state)
{
 set_tsk_thread_flag(t, TIF_SIGPENDING);







 if (!wake_up_state(t, state | TASK_INTERRUPTIBLE))
  kick_process(t);
}







static int flush_sigqueue_mask(sigset_t *mask, struct sigpending *s)
{
 struct sigqueue *q, *n;
 sigset_t m;

 sigandsets(&m, mask, &s->signal);
 if (sigisemptyset(&m))
  return 0;

 sigandnsets(&s->signal, &s->signal, mask);
 list_for_each_entry_safe(q, n, &s->list, list) {
  if (sigismember(mask, q->info.si_signo)) {
   list_del_init(&q->list);
   __sigqueue_free(q);
  }
 }
 return 1;
}

static inline int is_si_special(const struct siginfo *info)
{
 return info <= SEND_SIG_FORCED;
}

static inline bool si_fromuser(const struct siginfo *info)
{
 return info == SEND_SIG_NOINFO ||
  (!is_si_special(info) && SI_FROMUSER(info));
}




static int kill_ok_by_cred(struct task_struct *t)
{
 const struct cred *cred = current_cred();
 const struct cred *tcred = __task_cred(t);

 if (uid_eq(cred->euid, tcred->suid) ||
     uid_eq(cred->euid, tcred->uid) ||
     uid_eq(cred->uid, tcred->suid) ||
     uid_eq(cred->uid, tcred->uid))
  return 1;

 if (ns_capable(tcred->user_ns, CAP_KILL))
  return 1;

 return 0;
}





static int check_kill_permission(int sig, struct siginfo *info,
     struct task_struct *t)
{
 struct pid *sid;
 int error;

 if (!valid_signal(sig))
  return -EINVAL;

 if (!si_fromuser(info))
  return 0;

 error = audit_signal_info(sig, t);
 if (error)
  return error;

 if (!same_thread_group(current, t) &&
     !kill_ok_by_cred(t)) {
  switch (sig) {
  case SIGCONT:
   sid = task_session(t);




   if (!sid || sid == task_session(current))
    break;
  default:
   return -EPERM;
  }
 }

 return security_task_kill(t, info, sig, 0);
}
static void ptrace_trap_notify(struct task_struct *t)
{
 WARN_ON_ONCE(!(t->ptrace & PT_SEIZED));
 assert_spin_locked(&t->sighand->siglock);

 task_set_jobctl_pending(t, JOBCTL_TRAP_NOTIFY);
 ptrace_signal_wake_up(t, t->jobctl & JOBCTL_LISTENING);
}
static bool prepare_signal(int sig, struct task_struct *p, bool force)
{
 struct signal_struct *signal = p->signal;
 struct task_struct *t;
 sigset_t flush;

 if (signal->flags & (SIGNAL_GROUP_EXIT | SIGNAL_GROUP_COREDUMP)) {
  if (!(signal->flags & SIGNAL_GROUP_EXIT))
   return sig == SIGKILL;



 } else if (sig_kernel_stop(sig)) {



  siginitset(&flush, sigmask(SIGCONT));
  flush_sigqueue_mask(&flush, &signal->shared_pending);
  for_each_thread(p, t)
   flush_sigqueue_mask(&flush, &t->pending);
 } else if (sig == SIGCONT) {
  unsigned int why;



  siginitset(&flush, SIG_KERNEL_STOP_MASK);
  flush_sigqueue_mask(&flush, &signal->shared_pending);
  for_each_thread(p, t) {
   flush_sigqueue_mask(&flush, &t->pending);
   task_clear_jobctl_pending(t, JOBCTL_STOP_PENDING);
   if (likely(!(t->ptrace & PT_SEIZED)))
    wake_up_state(t, __TASK_STOPPED);
   else
    ptrace_trap_notify(t);
  }
  why = 0;
  if (signal->flags & SIGNAL_STOP_STOPPED)
   why |= SIGNAL_CLD_CONTINUED;
  else if (signal->group_stop_count)
   why |= SIGNAL_CLD_STOPPED;

  if (why) {





   signal->flags = why | SIGNAL_STOP_CONTINUED;
   signal->group_stop_count = 0;
   signal->group_exit_code = 0;
  }
 }

 return !sig_ignored(p, sig, force);
}
static inline int wants_signal(int sig, struct task_struct *p)
{
 if (sigismember(&p->blocked, sig))
  return 0;
 if (p->flags & PF_EXITING)
  return 0;
 if (sig == SIGKILL)
  return 1;
 if (task_is_stopped_or_traced(p))
  return 0;
 return task_curr(p) || !signal_pending(p);
}

static void complete_signal(int sig, struct task_struct *p, int group)
{
 struct signal_struct *signal = p->signal;
 struct task_struct *t;







 if (wants_signal(sig, p))
  t = p;
 else if (!group || thread_group_empty(p))




  return;
 else {



  t = signal->curr_target;
  while (!wants_signal(sig, t)) {
   t = next_thread(t);
   if (t == signal->curr_target)





    return;
  }
  signal->curr_target = t;
 }





 if (sig_fatal(p, sig) &&
     !(signal->flags & (SIGNAL_UNKILLABLE | SIGNAL_GROUP_EXIT)) &&
     !sigismember(&t->real_blocked, sig) &&
     (sig == SIGKILL || !t->ptrace)) {



  if (!sig_kernel_coredump(sig)) {






   signal->flags = SIGNAL_GROUP_EXIT;
   signal->group_exit_code = sig;
   signal->group_stop_count = 0;
   t = p;
   do {
    task_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);
    sigaddset(&t->pending.signal, SIGKILL);
    signal_wake_up(t, 1);
   } while_each_thread(p, t);
   return;
  }
 }





 signal_wake_up(t, sig == SIGKILL);
 return;
}

static inline int legacy_queue(struct sigpending *signals, int sig)
{
 return (sig < SIGRTMIN) && sigismember(&signals->signal, sig);
}

static inline void userns_fixup_signal_uid(struct siginfo *info, struct task_struct *t)
{
 if (current_user_ns() == task_cred_xxx(t, user_ns))
  return;

 if (SI_FROMKERNEL(info))
  return;

 rcu_read_lock();
 info->si_uid = from_kuid_munged(task_cred_xxx(t, user_ns),
     make_kuid(current_user_ns(), info->si_uid));
 rcu_read_unlock();
}
static inline void userns_fixup_signal_uid(struct siginfo *info, struct task_struct *t)
{
 return;
}

static int __send_signal(int sig, struct siginfo *info, struct task_struct *t,
   int group, int from_ancestor_ns)
{
 struct sigpending *pending;
 struct sigqueue *q;
 int override_rlimit;
 int ret = 0, result;

 assert_spin_locked(&t->sighand->siglock);

 result = TRACE_SIGNAL_IGNORED;
 if (!prepare_signal(sig, t,
   from_ancestor_ns || (info == SEND_SIG_FORCED)))
  goto ret;

 pending = group ? &t->signal->shared_pending : &t->pending;





 result = TRACE_SIGNAL_ALREADY_PENDING;
 if (legacy_queue(pending, sig))
  goto ret;

 result = TRACE_SIGNAL_DELIVERED;




 if (info == SEND_SIG_FORCED)
  goto out_set;
 if (sig < SIGRTMIN)
  override_rlimit = (is_si_special(info) || info->si_code >= 0);
 else
  override_rlimit = 0;

 q = __sigqueue_alloc(sig, t, GFP_ATOMIC | __GFP_NOTRACK_FALSE_POSITIVE,
  override_rlimit);
 if (q) {
  list_add_tail(&q->list, &pending->list);
  switch ((unsigned long) info) {
  case (unsigned long) SEND_SIG_NOINFO:
   q->info.si_signo = sig;
   q->info.si_errno = 0;
   q->info.si_code = SI_USER;
   q->info.si_pid = task_tgid_nr_ns(current,
       task_active_pid_ns(t));
   q->info.si_uid = from_kuid_munged(current_user_ns(), current_uid());
   break;
  case (unsigned long) SEND_SIG_PRIV:
   q->info.si_signo = sig;
   q->info.si_errno = 0;
   q->info.si_code = SI_KERNEL;
   q->info.si_pid = 0;
   q->info.si_uid = 0;
   break;
  default:
   copy_siginfo(&q->info, info);
   if (from_ancestor_ns)
    q->info.si_pid = 0;
   break;
  }

  userns_fixup_signal_uid(&q->info, t);

 } else if (!is_si_special(info)) {
  if (sig >= SIGRTMIN && info->si_code != SI_USER) {





   result = TRACE_SIGNAL_OVERFLOW_FAIL;
   ret = -EAGAIN;
   goto ret;
  } else {




   result = TRACE_SIGNAL_LOSE_INFO;
  }
 }

out_set:
 signalfd_notify(t, sig);
 sigaddset(&pending->signal, sig);
 complete_signal(sig, t, group);
ret:
 trace_signal_generate(sig, info, t, group, result);
 return ret;
}

static int send_signal(int sig, struct siginfo *info, struct task_struct *t,
   int group)
{
 int from_ancestor_ns = 0;

 from_ancestor_ns = si_fromuser(info) &&
      !task_pid_nr_ns(current, task_active_pid_ns(t));

 return __send_signal(sig, info, t, group, from_ancestor_ns);
}

static void print_fatal_signal(int signr)
{
 struct pt_regs *regs = signal_pt_regs();
 pr_info("potentially unexpected fatal signal %d.\n", signr);

 pr_info("code at %08lx: ", regs->ip);
 {
  int i;
  for (i = 0; i < 16; i++) {
   unsigned char insn;

   if (get_user(insn, (unsigned char *)(regs->ip + i)))
    break;
   pr_cont("%02x ", insn);
  }
 }
 pr_cont("\n");
 preempt_disable();
 show_regs(regs);
 preempt_enable();
}

static int __init setup_print_fatal_signals(char *str)
{
 get_option (&str, &print_fatal_signals);

 return 1;
}

__setup("print-fatal-signals=", setup_print_fatal_signals);

int
__group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)
{
 return send_signal(sig, info, p, 1);
}

static int
specific_send_sig_info(int sig, struct siginfo *info, struct task_struct *t)
{
 return send_signal(sig, info, t, 0);
}

int do_send_sig_info(int sig, struct siginfo *info, struct task_struct *p,
   bool group)
{
 unsigned long flags;
 int ret = -ESRCH;

 if (lock_task_sighand(p, &flags)) {
  ret = send_signal(sig, info, p, group);
  unlock_task_sighand(p, &flags);
 }

 return ret;
}
int
force_sig_info(int sig, struct siginfo *info, struct task_struct *t)
{
 unsigned long int flags;
 int ret, blocked, ignored;
 struct k_sigaction *action;

 spin_lock_irqsave(&t->sighand->siglock, flags);
 action = &t->sighand->action[sig-1];
 ignored = action->sa.sa_handler == SIG_IGN;
 blocked = sigismember(&t->blocked, sig);
 if (blocked || ignored) {
  action->sa.sa_handler = SIG_DFL;
  if (blocked) {
   sigdelset(&t->blocked, sig);
   recalc_sigpending_and_wake(t);
  }
 }
 if (action->sa.sa_handler == SIG_DFL)
  t->signal->flags &= ~SIGNAL_UNKILLABLE;
 ret = specific_send_sig_info(sig, info, t);
 spin_unlock_irqrestore(&t->sighand->siglock, flags);

 return ret;
}




int zap_other_threads(struct task_struct *p)
{
 struct task_struct *t = p;
 int count = 0;

 p->signal->group_stop_count = 0;

 while_each_thread(p, t) {
  task_clear_jobctl_pending(t, JOBCTL_PENDING_MASK);
  count++;


  if (t->exit_state)
   continue;
  sigaddset(&t->pending.signal, SIGKILL);
  signal_wake_up(t, 1);
 }

 return count;
}

struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,
        unsigned long *flags)
{
 struct sighand_struct *sighand;

 for (;;) {




  local_irq_save(*flags);
  rcu_read_lock();
  sighand = rcu_dereference(tsk->sighand);
  if (unlikely(sighand == NULL)) {
   rcu_read_unlock();
   local_irq_restore(*flags);
   break;
  }
  spin_lock(&sighand->siglock);
  if (likely(sighand == tsk->sighand)) {
   rcu_read_unlock();
   break;
  }
  spin_unlock(&sighand->siglock);
  rcu_read_unlock();
  local_irq_restore(*flags);
 }

 return sighand;
}




int group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)
{
 int ret;

 rcu_read_lock();
 ret = check_kill_permission(sig, info, p);
 rcu_read_unlock();

 if (!ret && sig)
  ret = do_send_sig_info(sig, info, p, true);

 return ret;
}






int __kill_pgrp_info(int sig, struct siginfo *info, struct pid *pgrp)
{
 struct task_struct *p = NULL;
 int retval, success;

 success = 0;
 retval = -ESRCH;
 do_each_pid_task(pgrp, PIDTYPE_PGID, p) {
  int err = group_send_sig_info(sig, info, p);
  success |= !err;
  retval = err;
 } while_each_pid_task(pgrp, PIDTYPE_PGID, p);
 return success ? 0 : retval;
}

int kill_pid_info(int sig, struct siginfo *info, struct pid *pid)
{
 int error = -ESRCH;
 struct task_struct *p;

 for (;;) {
  rcu_read_lock();
  p = pid_task(pid, PIDTYPE_PID);
  if (p)
   error = group_send_sig_info(sig, info, p);
  rcu_read_unlock();
  if (likely(!p || error != -ESRCH))
   return error;






 }
}

int kill_proc_info(int sig, struct siginfo *info, pid_t pid)
{
 int error;
 rcu_read_lock();
 error = kill_pid_info(sig, info, find_vpid(pid));
 rcu_read_unlock();
 return error;
}

static int kill_as_cred_perm(const struct cred *cred,
        struct task_struct *target)
{
 const struct cred *pcred = __task_cred(target);
 if (!uid_eq(cred->euid, pcred->suid) && !uid_eq(cred->euid, pcred->uid) &&
     !uid_eq(cred->uid, pcred->suid) && !uid_eq(cred->uid, pcred->uid))
  return 0;
 return 1;
}


int kill_pid_info_as_cred(int sig, struct siginfo *info, struct pid *pid,
    const struct cred *cred, u32 secid)
{
 int ret = -EINVAL;
 struct task_struct *p;
 unsigned long flags;

 if (!valid_signal(sig))
  return ret;

 rcu_read_lock();
 p = pid_task(pid, PIDTYPE_PID);
 if (!p) {
  ret = -ESRCH;
  goto out_unlock;
 }
 if (si_fromuser(info) && !kill_as_cred_perm(cred, p)) {
  ret = -EPERM;
  goto out_unlock;
 }
 ret = security_task_kill(p, info, sig, secid);
 if (ret)
  goto out_unlock;

 if (sig) {
  if (lock_task_sighand(p, &flags)) {
   ret = __send_signal(sig, info, p, 1, 0);
   unlock_task_sighand(p, &flags);
  } else
   ret = -ESRCH;
 }
out_unlock:
 rcu_read_unlock();
 return ret;
}
EXPORT_SYMBOL_GPL(kill_pid_info_as_cred);
static int kill_something_info(int sig, struct siginfo *info, pid_t pid)
{
 int ret;

 if (pid > 0) {
  rcu_read_lock();
  ret = kill_pid_info(sig, info, find_vpid(pid));
  rcu_read_unlock();
  return ret;
 }

 read_lock(&tasklist_lock);
 if (pid != -1) {
  ret = __kill_pgrp_info(sig, info,
    pid ? find_vpid(-pid) : task_pgrp(current));
 } else {
  int retval = 0, count = 0;
  struct task_struct * p;

  for_each_process(p) {
   if (task_pid_vnr(p) > 1 &&
     !same_thread_group(p, current)) {
    int err = group_send_sig_info(sig, info, p);
    ++count;
    if (err != -EPERM)
     retval = err;
   }
  }
  ret = count ? retval : -ESRCH;
 }
 read_unlock(&tasklist_lock);

 return ret;
}





int send_sig_info(int sig, struct siginfo *info, struct task_struct *p)
{




 if (!valid_signal(sig))
  return -EINVAL;

 return do_send_sig_info(sig, info, p, false);
}

 ((priv) ? SEND_SIG_PRIV : SEND_SIG_NOINFO)

int
send_sig(int sig, struct task_struct *p, int priv)
{
 return send_sig_info(sig, __si_special(priv), p);
}

void
force_sig(int sig, struct task_struct *p)
{
 force_sig_info(sig, SEND_SIG_PRIV, p);
}







int
force_sigsegv(int sig, struct task_struct *p)
{
 if (sig == SIGSEGV) {
  unsigned long flags;
  spin_lock_irqsave(&p->sighand->siglock, flags);
  p->sighand->action[sig - 1].sa.sa_handler = SIG_DFL;
  spin_unlock_irqrestore(&p->sighand->siglock, flags);
 }
 force_sig(SIGSEGV, p);
 return 0;
}

int kill_pgrp(struct pid *pid, int sig, int priv)
{
 int ret;

 read_lock(&tasklist_lock);
 ret = __kill_pgrp_info(sig, __si_special(priv), pid);
 read_unlock(&tasklist_lock);

 return ret;
}
EXPORT_SYMBOL(kill_pgrp);

int kill_pid(struct pid *pid, int sig, int priv)
{
 return kill_pid_info(sig, __si_special(priv), pid);
}
EXPORT_SYMBOL(kill_pid);
struct sigqueue *sigqueue_alloc(void)
{
 struct sigqueue *q = __sigqueue_alloc(-1, current, GFP_KERNEL, 0);

 if (q)
  q->flags |= SIGQUEUE_PREALLOC;

 return q;
}

void sigqueue_free(struct sigqueue *q)
{
 unsigned long flags;
 spinlock_t *lock = &current->sighand->siglock;

 BUG_ON(!(q->flags & SIGQUEUE_PREALLOC));





 spin_lock_irqsave(lock, flags);
 q->flags &= ~SIGQUEUE_PREALLOC;




 if (!list_empty(&q->list))
  q = NULL;
 spin_unlock_irqrestore(lock, flags);

 if (q)
  __sigqueue_free(q);
}

int send_sigqueue(struct sigqueue *q, struct task_struct *t, int group)
{
 int sig = q->info.si_signo;
 struct sigpending *pending;
 unsigned long flags;
 int ret, result;

 BUG_ON(!(q->flags & SIGQUEUE_PREALLOC));

 ret = -1;
 if (!likely(lock_task_sighand(t, &flags)))
  goto ret;

 ret = 1;
 result = TRACE_SIGNAL_IGNORED;
 if (!prepare_signal(sig, t, false))
  goto out;

 ret = 0;
 if (unlikely(!list_empty(&q->list))) {




  BUG_ON(q->info.si_code != SI_TIMER);
  q->info.si_overrun++;
  result = TRACE_SIGNAL_ALREADY_PENDING;
  goto out;
 }
 q->info.si_overrun = 0;

 signalfd_notify(t, sig);
 pending = group ? &t->signal->shared_pending : &t->pending;
 list_add_tail(&q->list, &pending->list);
 sigaddset(&pending->signal, sig);
 complete_signal(sig, t, group);
 result = TRACE_SIGNAL_DELIVERED;
out:
 trace_signal_generate(sig, &q->info, t, group, result);
 unlock_task_sighand(t, &flags);
ret:
 return ret;
}
bool do_notify_parent(struct task_struct *tsk, int sig)
{
 struct siginfo info;
 unsigned long flags;
 struct sighand_struct *psig;
 bool autoreap = false;
 cputime_t utime, stime;

 BUG_ON(sig == -1);


  BUG_ON(task_is_stopped_or_traced(tsk));

 BUG_ON(!tsk->ptrace &&
        (tsk->group_leader != tsk || !thread_group_empty(tsk)));

 if (sig != SIGCHLD) {




  if (tsk->parent_exec_id != tsk->parent->self_exec_id)
   sig = SIGCHLD;
 }

 info.si_signo = sig;
 info.si_errno = 0;
 rcu_read_lock();
 info.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(tsk->parent));
 info.si_uid = from_kuid_munged(task_cred_xxx(tsk->parent, user_ns),
           task_uid(tsk));
 rcu_read_unlock();

 task_cputime(tsk, &utime, &stime);
 info.si_utime = cputime_to_clock_t(utime + tsk->signal->utime);
 info.si_stime = cputime_to_clock_t(stime + tsk->signal->stime);

 info.si_status = tsk->exit_code & 0x7f;
 if (tsk->exit_code & 0x80)
  info.si_code = CLD_DUMPED;
 else if (tsk->exit_code & 0x7f)
  info.si_code = CLD_KILLED;
 else {
  info.si_code = CLD_EXITED;
  info.si_status = tsk->exit_code >> 8;
 }

 psig = tsk->parent->sighand;
 spin_lock_irqsave(&psig->siglock, flags);
 if (!tsk->ptrace && sig == SIGCHLD &&
     (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN ||
      (psig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT))) {
  autoreap = true;
  if (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN)
   sig = 0;
 }
 if (valid_signal(sig) && sig)
  __group_send_sig_info(sig, &info, tsk->parent);
 __wake_up_parent(tsk, tsk->parent);
 spin_unlock_irqrestore(&psig->siglock, flags);

 return autoreap;
}
static void do_notify_parent_cldstop(struct task_struct *tsk,
         bool for_ptracer, int why)
{
 struct siginfo info;
 unsigned long flags;
 struct task_struct *parent;
 struct sighand_struct *sighand;
 cputime_t utime, stime;

 if (for_ptracer) {
  parent = tsk->parent;
 } else {
  tsk = tsk->group_leader;
  parent = tsk->real_parent;
 }

 info.si_signo = SIGCHLD;
 info.si_errno = 0;



 rcu_read_lock();
 info.si_pid = task_pid_nr_ns(tsk, task_active_pid_ns(parent));
 info.si_uid = from_kuid_munged(task_cred_xxx(parent, user_ns), task_uid(tsk));
 rcu_read_unlock();

 task_cputime(tsk, &utime, &stime);
 info.si_utime = cputime_to_clock_t(utime);
 info.si_stime = cputime_to_clock_t(stime);

  info.si_code = why;
  switch (why) {
  case CLD_CONTINUED:
   info.si_status = SIGCONT;
   break;
  case CLD_STOPPED:
   info.si_status = tsk->signal->group_exit_code & 0x7f;
   break;
  case CLD_TRAPPED:
   info.si_status = tsk->exit_code & 0x7f;
   break;
  default:
   BUG();
  }

 sighand = parent->sighand;
 spin_lock_irqsave(&sighand->siglock, flags);
 if (sighand->action[SIGCHLD-1].sa.sa_handler != SIG_IGN &&
     !(sighand->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDSTOP))
  __group_send_sig_info(SIGCHLD, &info, parent);



 __wake_up_parent(tsk, parent);
 spin_unlock_irqrestore(&sighand->siglock, flags);
}

static inline int may_ptrace_stop(void)
{
 if (!likely(current->ptrace))
  return 0;
 if (unlikely(current->mm->core_state) &&
     unlikely(current->mm == current->parent->mm))
  return 0;

 return 1;
}





static int sigkill_pending(struct task_struct *tsk)
{
 return sigismember(&tsk->pending.signal, SIGKILL) ||
  sigismember(&tsk->signal->shared_pending.signal, SIGKILL);
}
static void ptrace_stop(int exit_code, int why, int clear_code, siginfo_t *info)
 __releases(&current->sighand->siglock)
 __acquires(&current->sighand->siglock)
{
 bool gstop_done = false;

 if (arch_ptrace_stop_needed(exit_code, info)) {
  spin_unlock_irq(&current->sighand->siglock);
  arch_ptrace_stop(exit_code, info);
  spin_lock_irq(&current->sighand->siglock);
  if (sigkill_pending(current))
   return;
 }
 set_current_state(TASK_TRACED);

 current->last_siginfo = info;
 current->exit_code = exit_code;
 if (why == CLD_STOPPED && (current->jobctl & JOBCTL_STOP_PENDING))
  gstop_done = task_participate_group_stop(current);


 task_clear_jobctl_pending(current, JOBCTL_TRAP_STOP);
 if (info && info->si_code >> 8 == PTRACE_EVENT_STOP)
  task_clear_jobctl_pending(current, JOBCTL_TRAP_NOTIFY);


 task_clear_jobctl_trapping(current);

 spin_unlock_irq(&current->sighand->siglock);
 read_lock(&tasklist_lock);
 if (may_ptrace_stop()) {
  do_notify_parent_cldstop(current, true, why);
  if (gstop_done && ptrace_reparented(current))
   do_notify_parent_cldstop(current, false, why);







  preempt_disable();
  read_unlock(&tasklist_lock);
  preempt_enable_no_resched();
  freezable_schedule();
 } else {
  if (gstop_done)
   do_notify_parent_cldstop(current, false, why);


  __set_current_state(TASK_RUNNING);
  if (clear_code)
   current->exit_code = 0;
  read_unlock(&tasklist_lock);
 }






 spin_lock_irq(&current->sighand->siglock);
 current->last_siginfo = NULL;


 current->jobctl &= ~JOBCTL_LISTENING;






 recalc_sigpending_tsk(current);
}

static void ptrace_do_notify(int signr, int exit_code, int why)
{
 siginfo_t info;

 memset(&info, 0, sizeof info);
 info.si_signo = signr;
 info.si_code = exit_code;
 info.si_pid = task_pid_vnr(current);
 info.si_uid = from_kuid_munged(current_user_ns(), current_uid());


 ptrace_stop(exit_code, why, 1, &info);
}

void ptrace_notify(int exit_code)
{
 BUG_ON((exit_code & (0x7f | ~0xffff)) != SIGTRAP);
 if (unlikely(current->task_works))
  task_work_run();

 spin_lock_irq(&current->sighand->siglock);
 ptrace_do_notify(SIGTRAP, exit_code, CLD_TRAPPED);
 spin_unlock_irq(&current->sighand->siglock);
}
static bool do_signal_stop(int signr)
 __releases(&current->sighand->siglock)
{
 struct signal_struct *sig = current->signal;

 if (!(current->jobctl & JOBCTL_STOP_PENDING)) {
  unsigned long gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;
  struct task_struct *t;


  WARN_ON_ONCE(signr & ~JOBCTL_STOP_SIGMASK);

  if (!likely(current->jobctl & JOBCTL_STOP_DEQUEUED) ||
      unlikely(signal_group_exit(sig)))
   return false;
  if (!(sig->flags & SIGNAL_STOP_STOPPED))
   sig->group_exit_code = signr;

  sig->group_stop_count = 0;

  if (task_set_jobctl_pending(current, signr | gstop))
   sig->group_stop_count++;

  t = current;
  while_each_thread(current, t) {





   if (!task_is_stopped(t) &&
       task_set_jobctl_pending(t, signr | gstop)) {
    sig->group_stop_count++;
    if (likely(!(t->ptrace & PT_SEIZED)))
     signal_wake_up(t, 0);
    else
     ptrace_trap_notify(t);
   }
  }
 }

 if (likely(!current->ptrace)) {
  int notify = 0;






  if (task_participate_group_stop(current))
   notify = CLD_STOPPED;

  __set_current_state(TASK_STOPPED);
  spin_unlock_irq(&current->sighand->siglock);
  if (notify) {
   read_lock(&tasklist_lock);
   do_notify_parent_cldstop(current, false, notify);
   read_unlock(&tasklist_lock);
  }


  freezable_schedule();
  return true;
 } else {




  task_set_jobctl_pending(current, JOBCTL_TRAP_STOP);
  return false;
 }
}
static void do_jobctl_trap(void)
{
 struct signal_struct *signal = current->signal;
 int signr = current->jobctl & JOBCTL_STOP_SIGMASK;

 if (current->ptrace & PT_SEIZED) {
  if (!signal->group_stop_count &&
      !(signal->flags & SIGNAL_STOP_STOPPED))
   signr = SIGTRAP;
  WARN_ON_ONCE(!signr);
  ptrace_do_notify(signr, signr | (PTRACE_EVENT_STOP << 8),
     CLD_STOPPED);
 } else {
  WARN_ON_ONCE(!signr);
  ptrace_stop(signr, CLD_STOPPED, 0, NULL);
  current->exit_code = 0;
 }
}

static int ptrace_signal(int signr, siginfo_t *info)
{
 ptrace_signal_deliver();
 current->jobctl |= JOBCTL_STOP_DEQUEUED;
 ptrace_stop(signr, CLD_TRAPPED, 0, info);


 signr = current->exit_code;
 if (signr == 0)
  return signr;

 current->exit_code = 0;







 if (signr != info->si_signo) {
  info->si_signo = signr;
  info->si_errno = 0;
  info->si_code = SI_USER;
  rcu_read_lock();
  info->si_pid = task_pid_vnr(current->parent);
  info->si_uid = from_kuid_munged(current_user_ns(),
      task_uid(current->parent));
  rcu_read_unlock();
 }


 if (sigismember(&current->blocked, signr)) {
  specific_send_sig_info(signr, info, current);
  signr = 0;
 }

 return signr;
}

int get_signal(struct ksignal *ksig)
{
 struct sighand_struct *sighand = current->sighand;
 struct signal_struct *signal = current->signal;
 int signr;

 if (unlikely(current->task_works))
  task_work_run();

 if (unlikely(uprobe_deny_signal()))
  return 0;






 try_to_freeze();

relock:
 spin_lock_irq(&sighand->siglock);





 if (unlikely(signal->flags & SIGNAL_CLD_MASK)) {
  int why;

  if (signal->flags & SIGNAL_CLD_CONTINUED)
   why = CLD_CONTINUED;
  else
   why = CLD_STOPPED;

  signal->flags &= ~SIGNAL_CLD_MASK;

  spin_unlock_irq(&sighand->siglock);
  read_lock(&tasklist_lock);
  do_notify_parent_cldstop(current, false, why);

  if (ptrace_reparented(current->group_leader))
   do_notify_parent_cldstop(current->group_leader,
      true, why);
  read_unlock(&tasklist_lock);

  goto relock;
 }

 for (;;) {
  struct k_sigaction *ka;

  if (unlikely(current->jobctl & JOBCTL_STOP_PENDING) &&
      do_signal_stop(0))
   goto relock;

  if (unlikely(current->jobctl & JOBCTL_TRAP_MASK)) {
   do_jobctl_trap();
   spin_unlock_irq(&sighand->siglock);
   goto relock;
  }

  signr = dequeue_signal(current, &current->blocked, &ksig->info);

  if (!signr)
   break;

  if (unlikely(current->ptrace) && signr != SIGKILL) {
   signr = ptrace_signal(signr, &ksig->info);
   if (!signr)
    continue;
  }

  ka = &sighand->action[signr-1];


  trace_signal_deliver(signr, &ksig->info, ka);

  if (ka->sa.sa_handler == SIG_IGN)
   continue;
  if (ka->sa.sa_handler != SIG_DFL) {

   ksig->ka = *ka;

   if (ka->sa.sa_flags & SA_ONESHOT)
    ka->sa.sa_handler = SIG_DFL;

   break;
  }




  if (sig_kernel_ignore(signr))
   continue;
  if (unlikely(signal->flags & SIGNAL_UNKILLABLE) &&
    !sig_kernel_only(signr))
   continue;

  if (sig_kernel_stop(signr)) {
   if (signr != SIGSTOP) {
    spin_unlock_irq(&sighand->siglock);



    if (is_current_pgrp_orphaned())
     goto relock;

    spin_lock_irq(&sighand->siglock);
   }

   if (likely(do_signal_stop(ksig->info.si_signo))) {

    goto relock;
   }





   continue;
  }

  spin_unlock_irq(&sighand->siglock);




  current->flags |= PF_SIGNALED;

  if (sig_kernel_coredump(signr)) {
   if (print_fatal_signals)
    print_fatal_signal(ksig->info.si_signo);
   proc_coredump_connector(current);
   do_coredump(&ksig->info);
  }




  do_group_exit(ksig->info.si_signo);

 }
 spin_unlock_irq(&sighand->siglock);

 ksig->sig = signr;
 return ksig->sig > 0;
}
static void signal_delivered(struct ksignal *ksig, int stepping)
{
 sigset_t blocked;





 clear_restore_sigmask();

 sigorsets(&blocked, &current->blocked, &ksig->ka.sa.sa_mask);
 if (!(ksig->ka.sa.sa_flags & SA_NODEFER))
  sigaddset(&blocked, ksig->sig);
 set_current_blocked(&blocked);
 tracehook_signal_handler(stepping);
}

void signal_setup_done(int failed, struct ksignal *ksig, int stepping)
{
 if (failed)
  force_sigsegv(ksig->sig, current);
 else
  signal_delivered(ksig, stepping);
}






static void retarget_shared_pending(struct task_struct *tsk, sigset_t *which)
{
 sigset_t retarget;
 struct task_struct *t;

 sigandsets(&retarget, &tsk->signal->shared_pending.signal, which);
 if (sigisemptyset(&retarget))
  return;

 t = tsk;
 while_each_thread(tsk, t) {
  if (t->flags & PF_EXITING)
   continue;

  if (!has_pending_signals(&retarget, &t->blocked))
   continue;

  sigandsets(&retarget, &retarget, &t->blocked);

  if (!signal_pending(t))
   signal_wake_up(t, 0);

  if (sigisemptyset(&retarget))
   break;
 }
}

void exit_signals(struct task_struct *tsk)
{
 int group_stop = 0;
 sigset_t unblocked;





 threadgroup_change_begin(tsk);

 if (thread_group_empty(tsk) || signal_group_exit(tsk->signal)) {
  tsk->flags |= PF_EXITING;
  threadgroup_change_end(tsk);
  return;
 }

 spin_lock_irq(&tsk->sighand->siglock);




 tsk->flags |= PF_EXITING;

 threadgroup_change_end(tsk);

 if (!signal_pending(tsk))
  goto out;

 unblocked = tsk->blocked;
 signotset(&unblocked);
 retarget_shared_pending(tsk, &unblocked);

 if (unlikely(tsk->jobctl & JOBCTL_STOP_PENDING) &&
     task_participate_group_stop(tsk))
  group_stop = CLD_STOPPED;
out:
 spin_unlock_irq(&tsk->sighand->siglock);





 if (unlikely(group_stop)) {
  read_lock(&tasklist_lock);
  do_notify_parent_cldstop(tsk, false, group_stop);
  read_unlock(&tasklist_lock);
 }
}

EXPORT_SYMBOL(recalc_sigpending);
EXPORT_SYMBOL_GPL(dequeue_signal);
EXPORT_SYMBOL(flush_signals);
EXPORT_SYMBOL(force_sig);
EXPORT_SYMBOL(send_sig);
EXPORT_SYMBOL(send_sig_info);
EXPORT_SYMBOL(sigprocmask);
SYSCALL_DEFINE0(restart_syscall)
{
 struct restart_block *restart = &current->restart_block;
 return restart->fn(restart);
}

long do_no_restart_syscall(struct restart_block *param)
{
 return -EINTR;
}

static void __set_task_blocked(struct task_struct *tsk, const sigset_t *newset)
{
 if (signal_pending(tsk) && !thread_group_empty(tsk)) {
  sigset_t newblocked;

  sigandnsets(&newblocked, newset, &current->blocked);
  retarget_shared_pending(tsk, &newblocked);
 }
 tsk->blocked = *newset;
 recalc_sigpending();
}
void set_current_blocked(sigset_t *newset)
{
 sigdelsetmask(newset, sigmask(SIGKILL) | sigmask(SIGSTOP));
 __set_current_blocked(newset);
}

void __set_current_blocked(const sigset_t *newset)
{
 struct task_struct *tsk = current;

 spin_lock_irq(&tsk->sighand->siglock);
 __set_task_blocked(tsk, newset);
 spin_unlock_irq(&tsk->sighand->siglock);
}
int sigprocmask(int how, sigset_t *set, sigset_t *oldset)
{
 struct task_struct *tsk = current;
 sigset_t newset;


 if (oldset)
  *oldset = tsk->blocked;

 switch (how) {
 case SIG_BLOCK:
  sigorsets(&newset, &tsk->blocked, set);
  break;
 case SIG_UNBLOCK:
  sigandnsets(&newset, &tsk->blocked, set);
  break;
 case SIG_SETMASK:
  newset = *set;
  break;
 default:
  return -EINVAL;
 }

 __set_current_blocked(&newset);
 return 0;
}
SYSCALL_DEFINE4(rt_sigprocmask, int, how, sigset_t __user *, nset,
  sigset_t __user *, oset, size_t, sigsetsize)
{
 sigset_t old_set, new_set;
 int error;


 if (sigsetsize != sizeof(sigset_t))
  return -EINVAL;

 old_set = current->blocked;

 if (nset) {
  if (copy_from_user(&new_set, nset, sizeof(sigset_t)))
   return -EFAULT;
  sigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));

  error = sigprocmask(how, &new_set, NULL);
  if (error)
   return error;
 }

 if (oset) {
  if (copy_to_user(oset, &old_set, sizeof(sigset_t)))
   return -EFAULT;
 }

 return 0;
}

COMPAT_SYSCALL_DEFINE4(rt_sigprocmask, int, how, compat_sigset_t __user *, nset,
  compat_sigset_t __user *, oset, compat_size_t, sigsetsize)
{
 sigset_t old_set = current->blocked;


 if (sigsetsize != sizeof(sigset_t))
  return -EINVAL;

 if (nset) {
  compat_sigset_t new32;
  sigset_t new_set;
  int error;
  if (copy_from_user(&new32, nset, sizeof(compat_sigset_t)))
   return -EFAULT;

  sigset_from_compat(&new_set, &new32);
  sigdelsetmask(&new_set, sigmask(SIGKILL)|sigmask(SIGSTOP));

  error = sigprocmask(how, &new_set, NULL);
  if (error)
   return error;
 }
 if (oset) {
  compat_sigset_t old32;
  sigset_to_compat(&old32, &old_set);
  if (copy_to_user(oset, &old32, sizeof(compat_sigset_t)))
   return -EFAULT;
 }
 return 0;
 return sys_rt_sigprocmask(how, (sigset_t __user *)nset,
      (sigset_t __user *)oset, sigsetsize);
}

static int do_sigpending(void *set, unsigned long sigsetsize)
{
 if (sigsetsize > sizeof(sigset_t))
  return -EINVAL;

 spin_lock_irq(&current->sighand->siglock);
 sigorsets(set, &current->pending.signal,
    &current->signal->shared_pending.signal);
 spin_unlock_irq(&current->sighand->siglock);


 sigandsets(set, &current->blocked, set);
 return 0;
}







SYSCALL_DEFINE2(rt_sigpending, sigset_t __user *, uset, size_t, sigsetsize)
{
 sigset_t set;
 int err = do_sigpending(&set, sigsetsize);
 if (!err && copy_to_user(uset, &set, sigsetsize))
  err = -EFAULT;
 return err;
}

COMPAT_SYSCALL_DEFINE2(rt_sigpending, compat_sigset_t __user *, uset,
  compat_size_t, sigsetsize)
{
 sigset_t set;
 int err = do_sigpending(&set, sigsetsize);
 if (!err) {
  compat_sigset_t set32;
  sigset_to_compat(&set32, &set);

  if (copy_to_user(uset, &set32, sigsetsize))
   err = -EFAULT;
 }
 return err;
 return sys_rt_sigpending((sigset_t __user *)uset, sigsetsize);
}


int copy_siginfo_to_user(siginfo_t __user *to, const siginfo_t *from)
{
 int err;

 if (!access_ok (VERIFY_WRITE, to, sizeof(siginfo_t)))
  return -EFAULT;
 if (from->si_code < 0)
  return __copy_to_user(to, from, sizeof(siginfo_t))
   ? -EFAULT : 0;
 err = __put_user(from->si_signo, &to->si_signo);
 err |= __put_user(from->si_errno, &to->si_errno);
 err |= __put_user((short)from->si_code, &to->si_code);
 switch (from->si_code & __SI_MASK) {
 case __SI_KILL:
  err |= __put_user(from->si_pid, &to->si_pid);
  err |= __put_user(from->si_uid, &to->si_uid);
  break;
 case __SI_TIMER:
   err |= __put_user(from->si_tid, &to->si_tid);
   err |= __put_user(from->si_overrun, &to->si_overrun);
   err |= __put_user(from->si_ptr, &to->si_ptr);
  break;
 case __SI_POLL:
  err |= __put_user(from->si_band, &to->si_band);
  err |= __put_user(from->si_fd, &to->si_fd);
  break;
 case __SI_FAULT:
  err |= __put_user(from->si_addr, &to->si_addr);
  err |= __put_user(from->si_trapno, &to->si_trapno);




  if (from->si_signo == SIGBUS &&
      (from->si_code == BUS_MCEERR_AR || from->si_code == BUS_MCEERR_AO))
   err |= __put_user(from->si_addr_lsb, &to->si_addr_lsb);
  if (from->si_signo == SIGSEGV && from->si_code == SEGV_BNDERR) {
   err |= __put_user(from->si_lower, &to->si_lower);
   err |= __put_user(from->si_upper, &to->si_upper);
  }
  if (from->si_signo == SIGSEGV && from->si_code == SEGV_PKUERR)
   err |= __put_user(from->si_pkey, &to->si_pkey);
  break;
 case __SI_CHLD:
  err |= __put_user(from->si_pid, &to->si_pid);
  err |= __put_user(from->si_uid, &to->si_uid);
  err |= __put_user(from->si_status, &to->si_status);
  err |= __put_user(from->si_utime, &to->si_utime);
  err |= __put_user(from->si_stime, &to->si_stime);
  break;
 case __SI_RT:
 case __SI_MESGQ:
  err |= __put_user(from->si_pid, &to->si_pid);
  err |= __put_user(from->si_uid, &to->si_uid);
  err |= __put_user(from->si_ptr, &to->si_ptr);
  break;
 case __SI_SYS:
  err |= __put_user(from->si_call_addr, &to->si_call_addr);
  err |= __put_user(from->si_syscall, &to->si_syscall);
  err |= __put_user(from->si_arch, &to->si_arch);
  break;
 default:
  err |= __put_user(from->si_pid, &to->si_pid);
  err |= __put_user(from->si_uid, &to->si_uid);
  break;
 }
 return err;
}








int do_sigtimedwait(const sigset_t *which, siginfo_t *info,
   const struct timespec *ts)
{
 struct task_struct *tsk = current;
 long timeout = MAX_SCHEDULE_TIMEOUT;
 sigset_t mask = *which;
 int sig;

 if (ts) {
  if (!timespec_valid(ts))
   return -EINVAL;
  timeout = timespec_to_jiffies(ts);




  if (ts->tv_sec || ts->tv_nsec)
   timeout++;
 }




 sigdelsetmask(&mask, sigmask(SIGKILL) | sigmask(SIGSTOP));
 signotset(&mask);

 spin_lock_irq(&tsk->sighand->siglock);
 sig = dequeue_signal(tsk, &mask, info);
 if (!sig && timeout) {






  tsk->real_blocked = tsk->blocked;
  sigandsets(&tsk->blocked, &tsk->blocked, &mask);
  recalc_sigpending();
  spin_unlock_irq(&tsk->sighand->siglock);

  timeout = freezable_schedule_timeout_interruptible(timeout);

  spin_lock_irq(&tsk->sighand->siglock);
  __set_task_blocked(tsk, &tsk->real_blocked);
  sigemptyset(&tsk->real_blocked);
  sig = dequeue_signal(tsk, &mask, info);
 }
 spin_unlock_irq(&tsk->sighand->siglock);

 if (sig)
  return sig;
 return timeout ? -EINTR : -EAGAIN;
}
SYSCALL_DEFINE4(rt_sigtimedwait, const sigset_t __user *, uthese,
  siginfo_t __user *, uinfo, const struct timespec __user *, uts,
  size_t, sigsetsize)
{
 sigset_t these;
 struct timespec ts;
 siginfo_t info;
 int ret;


 if (sigsetsize != sizeof(sigset_t))
  return -EINVAL;

 if (copy_from_user(&these, uthese, sizeof(these)))
  return -EFAULT;

 if (uts) {
  if (copy_from_user(&ts, uts, sizeof(ts)))
   return -EFAULT;
 }

 ret = do_sigtimedwait(&these, &info, uts ? &ts : NULL);

 if (ret > 0 && uinfo) {
  if (copy_siginfo_to_user(uinfo, &info))
   ret = -EFAULT;
 }

 return ret;
}






SYSCALL_DEFINE2(kill, pid_t, pid, int, sig)
{
 struct siginfo info;

 info.si_signo = sig;
 info.si_errno = 0;
 info.si_code = SI_USER;
 info.si_pid = task_tgid_vnr(current);
 info.si_uid = from_kuid_munged(current_user_ns(), current_uid());

 return kill_something_info(sig, &info, pid);
}

static int
do_send_specific(pid_t tgid, pid_t pid, int sig, struct siginfo *info)
{
 struct task_struct *p;
 int error = -ESRCH;

 rcu_read_lock();
 p = find_task_by_vpid(pid);
 if (p && (tgid <= 0 || task_tgid_vnr(p) == tgid)) {
  error = check_kill_permission(sig, info, p);




  if (!error && sig) {
   error = do_send_sig_info(sig, info, p, false);





   if (unlikely(error == -ESRCH))
    error = 0;
  }
 }
 rcu_read_unlock();

 return error;
}

static int do_tkill(pid_t tgid, pid_t pid, int sig)
{
 struct siginfo info = {};

 info.si_signo = sig;
 info.si_errno = 0;
 info.si_code = SI_TKILL;
 info.si_pid = task_tgid_vnr(current);
 info.si_uid = from_kuid_munged(current_user_ns(), current_uid());

 return do_send_specific(tgid, pid, sig, &info);
}
SYSCALL_DEFINE3(tgkill, pid_t, tgid, pid_t, pid, int, sig)
{

 if (pid <= 0 || tgid <= 0)
  return -EINVAL;

 return do_tkill(tgid, pid, sig);
}
SYSCALL_DEFINE2(tkill, pid_t, pid, int, sig)
{

 if (pid <= 0)
  return -EINVAL;

 return do_tkill(0, pid, sig);
}

static int do_rt_sigqueueinfo(pid_t pid, int sig, siginfo_t *info)
{



 if ((info->si_code >= 0 || info->si_code == SI_TKILL) &&
     (task_pid_vnr(current) != pid))
  return -EPERM;

 info->si_signo = sig;


 return kill_proc_info(sig, info, pid);
}







SYSCALL_DEFINE3(rt_sigqueueinfo, pid_t, pid, int, sig,
  siginfo_t __user *, uinfo)
{
 siginfo_t info;
 if (copy_from_user(&info, uinfo, sizeof(siginfo_t)))
  return -EFAULT;
 return do_rt_sigqueueinfo(pid, sig, &info);
}

COMPAT_SYSCALL_DEFINE3(rt_sigqueueinfo,
   compat_pid_t, pid,
   int, sig,
   struct compat_siginfo __user *, uinfo)
{
 siginfo_t info = {};
 int ret = copy_siginfo_from_user32(&info, uinfo);
 if (unlikely(ret))
  return ret;
 return do_rt_sigqueueinfo(pid, sig, &info);
}

static int do_rt_tgsigqueueinfo(pid_t tgid, pid_t pid, int sig, siginfo_t *info)
{

 if (pid <= 0 || tgid <= 0)
  return -EINVAL;




 if ((info->si_code >= 0 || info->si_code == SI_TKILL) &&
     (task_pid_vnr(current) != pid))
  return -EPERM;

 info->si_signo = sig;

 return do_send_specific(tgid, pid, sig, info);
}

SYSCALL_DEFINE4(rt_tgsigqueueinfo, pid_t, tgid, pid_t, pid, int, sig,
  siginfo_t __user *, uinfo)
{
 siginfo_t info;

 if (copy_from_user(&info, uinfo, sizeof(siginfo_t)))
  return -EFAULT;

 return do_rt_tgsigqueueinfo(tgid, pid, sig, &info);
}

COMPAT_SYSCALL_DEFINE4(rt_tgsigqueueinfo,
   compat_pid_t, tgid,
   compat_pid_t, pid,
   int, sig,
   struct compat_siginfo __user *, uinfo)
{
 siginfo_t info = {};

 if (copy_siginfo_from_user32(&info, uinfo))
  return -EFAULT;
 return do_rt_tgsigqueueinfo(tgid, pid, sig, &info);
}




void kernel_sigaction(int sig, __sighandler_t action)
{
 spin_lock_irq(&current->sighand->siglock);
 current->sighand->action[sig - 1].sa.sa_handler = action;
 if (action == SIG_IGN) {
  sigset_t mask;

  sigemptyset(&mask);
  sigaddset(&mask, sig);

  flush_sigqueue_mask(&mask, &current->signal->shared_pending);
  flush_sigqueue_mask(&mask, &current->pending);
  recalc_sigpending();
 }
 spin_unlock_irq(&current->sighand->siglock);
}
EXPORT_SYMBOL(kernel_sigaction);

int do_sigaction(int sig, struct k_sigaction *act, struct k_sigaction *oact)
{
 struct task_struct *p = current, *t;
 struct k_sigaction *k;
 sigset_t mask;

 if (!valid_signal(sig) || sig < 1 || (act && sig_kernel_only(sig)))
  return -EINVAL;

 k = &p->sighand->action[sig-1];

 spin_lock_irq(&p->sighand->siglock);
 if (oact)
  *oact = *k;

 if (act) {
  sigdelsetmask(&act->sa.sa_mask,
         sigmask(SIGKILL) | sigmask(SIGSTOP));
  *k = *act;
  if (sig_handler_ignored(sig_handler(p, sig), sig)) {
   sigemptyset(&mask);
   sigaddset(&mask, sig);
   flush_sigqueue_mask(&mask, &p->signal->shared_pending);
   for_each_thread(p, t)
    flush_sigqueue_mask(&mask, &t->pending);
  }
 }

 spin_unlock_irq(&p->sighand->siglock);
 return 0;
}

static int
do_sigaltstack (const stack_t __user *uss, stack_t __user *uoss, unsigned long sp)
{
 stack_t oss;
 int error;

 oss.ss_sp = (void __user *) current->sas_ss_sp;
 oss.ss_size = current->sas_ss_size;
 oss.ss_flags = sas_ss_flags(sp) |
  (current->sas_ss_flags & SS_FLAG_BITS);

 if (uss) {
  void __user *ss_sp;
  size_t ss_size;
  unsigned ss_flags;
  int ss_mode;

  error = -EFAULT;
  if (!access_ok(VERIFY_READ, uss, sizeof(*uss)))
   goto out;
  error = __get_user(ss_sp, &uss->ss_sp) |
   __get_user(ss_flags, &uss->ss_flags) |
   __get_user(ss_size, &uss->ss_size);
  if (error)
   goto out;

  error = -EPERM;
  if (on_sig_stack(sp))
   goto out;

  ss_mode = ss_flags & ~SS_FLAG_BITS;
  error = -EINVAL;
  if (ss_mode != SS_DISABLE && ss_mode != SS_ONSTACK &&
    ss_mode != 0)
   goto out;

  if (ss_mode == SS_DISABLE) {
   ss_size = 0;
   ss_sp = NULL;
  } else {
   error = -ENOMEM;
   if (ss_size < MINSIGSTKSZ)
    goto out;
  }

  current->sas_ss_sp = (unsigned long) ss_sp;
  current->sas_ss_size = ss_size;
  current->sas_ss_flags = ss_flags;
 }

 error = 0;
 if (uoss) {
  error = -EFAULT;
  if (!access_ok(VERIFY_WRITE, uoss, sizeof(*uoss)))
   goto out;
  error = __put_user(oss.ss_sp, &uoss->ss_sp) |
   __put_user(oss.ss_size, &uoss->ss_size) |
   __put_user(oss.ss_flags, &uoss->ss_flags);
 }

out:
 return error;
}
SYSCALL_DEFINE2(sigaltstack,const stack_t __user *,uss, stack_t __user *,uoss)
{
 return do_sigaltstack(uss, uoss, current_user_stack_pointer());
}

int restore_altstack(const stack_t __user *uss)
{
 int err = do_sigaltstack(uss, NULL, current_user_stack_pointer());

 return err == -EFAULT ? err : 0;
}

int __save_altstack(stack_t __user *uss, unsigned long sp)
{
 struct task_struct *t = current;
 int err = __put_user((void __user *)t->sas_ss_sp, &uss->ss_sp) |
  __put_user(t->sas_ss_flags, &uss->ss_flags) |
  __put_user(t->sas_ss_size, &uss->ss_size);
 if (err)
  return err;
 if (t->sas_ss_flags & SS_AUTODISARM)
  sas_ss_reset(t);
 return 0;
}

COMPAT_SYSCALL_DEFINE2(sigaltstack,
   const compat_stack_t __user *, uss_ptr,
   compat_stack_t __user *, uoss_ptr)
{
 stack_t uss, uoss;
 int ret;
 mm_segment_t seg;

 if (uss_ptr) {
  compat_stack_t uss32;

  memset(&uss, 0, sizeof(stack_t));
  if (copy_from_user(&uss32, uss_ptr, sizeof(compat_stack_t)))
   return -EFAULT;
  uss.ss_sp = compat_ptr(uss32.ss_sp);
  uss.ss_flags = uss32.ss_flags;
  uss.ss_size = uss32.ss_size;
 }
 seg = get_fs();
 set_fs(KERNEL_DS);
 ret = do_sigaltstack((stack_t __force __user *) (uss_ptr ? &uss : NULL),
        (stack_t __force __user *) &uoss,
        compat_user_stack_pointer());
 set_fs(seg);
 if (ret >= 0 && uoss_ptr) {
  if (!access_ok(VERIFY_WRITE, uoss_ptr, sizeof(compat_stack_t)) ||
      __put_user(ptr_to_compat(uoss.ss_sp), &uoss_ptr->ss_sp) ||
      __put_user(uoss.ss_flags, &uoss_ptr->ss_flags) ||
      __put_user(uoss.ss_size, &uoss_ptr->ss_size))
   ret = -EFAULT;
 }
 return ret;
}

int compat_restore_altstack(const compat_stack_t __user *uss)
{
 int err = compat_sys_sigaltstack(uss, NULL);

 return err == -EFAULT ? err : 0;
}

int __compat_save_altstack(compat_stack_t __user *uss, unsigned long sp)
{
 struct task_struct *t = current;
 return __put_user(ptr_to_compat((void __user *)t->sas_ss_sp), &uss->ss_sp) |
  __put_user(sas_ss_flags(sp), &uss->ss_flags) |
  __put_user(t->sas_ss_size, &uss->ss_size);
}






SYSCALL_DEFINE1(sigpending, old_sigset_t __user *, set)
{
 return sys_rt_sigpending((sigset_t __user *)set, sizeof(old_sigset_t));
}


SYSCALL_DEFINE3(sigprocmask, int, how, old_sigset_t __user *, nset,
  old_sigset_t __user *, oset)
{
 old_sigset_t old_set, new_set;
 sigset_t new_blocked;

 old_set = current->blocked.sig[0];

 if (nset) {
  if (copy_from_user(&new_set, nset, sizeof(*nset)))
   return -EFAULT;

  new_blocked = current->blocked;

  switch (how) {
  case SIG_BLOCK:
   sigaddsetmask(&new_blocked, new_set);
   break;
  case SIG_UNBLOCK:
   sigdelsetmask(&new_blocked, new_set);
   break;
  case SIG_SETMASK:
   new_blocked.sig[0] = new_set;
   break;
  default:
   return -EINVAL;
  }

  set_current_blocked(&new_blocked);
 }

 if (oset) {
  if (copy_to_user(oset, &old_set, sizeof(*oset)))
   return -EFAULT;
 }

 return 0;
}








SYSCALL_DEFINE4(rt_sigaction, int, sig,
  const struct sigaction __user *, act,
  struct sigaction __user *, oact,
  size_t, sigsetsize)
{
 struct k_sigaction new_sa, old_sa;
 int ret = -EINVAL;


 if (sigsetsize != sizeof(sigset_t))
  goto out;

 if (act) {
  if (copy_from_user(&new_sa.sa, act, sizeof(new_sa.sa)))
   return -EFAULT;
 }

 ret = do_sigaction(sig, act ? &new_sa : NULL, oact ? &old_sa : NULL);

 if (!ret && oact) {
  if (copy_to_user(oact, &old_sa.sa, sizeof(old_sa.sa)))
   return -EFAULT;
 }
out:
 return ret;
}
COMPAT_SYSCALL_DEFINE4(rt_sigaction, int, sig,
  const struct compat_sigaction __user *, act,
  struct compat_sigaction __user *, oact,
  compat_size_t, sigsetsize)
{
 struct k_sigaction new_ka, old_ka;
 compat_sigset_t mask;
 compat_uptr_t restorer;
 int ret;


 if (sigsetsize != sizeof(compat_sigset_t))
  return -EINVAL;

 if (act) {
  compat_uptr_t handler;
  ret = get_user(handler, &act->sa_handler);
  new_ka.sa.sa_handler = compat_ptr(handler);
  ret |= get_user(restorer, &act->sa_restorer);
  new_ka.sa.sa_restorer = compat_ptr(restorer);
  ret |= copy_from_user(&mask, &act->sa_mask, sizeof(mask));
  ret |= get_user(new_ka.sa.sa_flags, &act->sa_flags);
  if (ret)
   return -EFAULT;
  sigset_from_compat(&new_ka.sa.sa_mask, &mask);
 }

 ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);
 if (!ret && oact) {
  sigset_to_compat(&mask, &old_ka.sa.sa_mask);
  ret = put_user(ptr_to_compat(old_ka.sa.sa_handler),
          &oact->sa_handler);
  ret |= copy_to_user(&oact->sa_mask, &mask, sizeof(mask));
  ret |= put_user(old_ka.sa.sa_flags, &oact->sa_flags);
  ret |= put_user(ptr_to_compat(old_ka.sa.sa_restorer),
    &oact->sa_restorer);
 }
 return ret;
}

SYSCALL_DEFINE3(sigaction, int, sig,
  const struct old_sigaction __user *, act,
         struct old_sigaction __user *, oact)
{
 struct k_sigaction new_ka, old_ka;
 int ret;

 if (act) {
  old_sigset_t mask;
  if (!access_ok(VERIFY_READ, act, sizeof(*act)) ||
      __get_user(new_ka.sa.sa_handler, &act->sa_handler) ||
      __get_user(new_ka.sa.sa_restorer, &act->sa_restorer) ||
      __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||
      __get_user(mask, &act->sa_mask))
   return -EFAULT;
  new_ka.ka_restorer = NULL;
  siginitset(&new_ka.sa.sa_mask, mask);
 }

 ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);

 if (!ret && oact) {
  if (!access_ok(VERIFY_WRITE, oact, sizeof(*oact)) ||
      __put_user(old_ka.sa.sa_handler, &oact->sa_handler) ||
      __put_user(old_ka.sa.sa_restorer, &oact->sa_restorer) ||
      __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||
      __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))
   return -EFAULT;
 }

 return ret;
}
COMPAT_SYSCALL_DEFINE3(sigaction, int, sig,
  const struct compat_old_sigaction __user *, act,
         struct compat_old_sigaction __user *, oact)
{
 struct k_sigaction new_ka, old_ka;
 int ret;
 compat_old_sigset_t mask;
 compat_uptr_t handler, restorer;

 if (act) {
  if (!access_ok(VERIFY_READ, act, sizeof(*act)) ||
      __get_user(handler, &act->sa_handler) ||
      __get_user(restorer, &act->sa_restorer) ||
      __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||
      __get_user(mask, &act->sa_mask))
   return -EFAULT;

  new_ka.ka_restorer = NULL;
  new_ka.sa.sa_handler = compat_ptr(handler);
  new_ka.sa.sa_restorer = compat_ptr(restorer);
  siginitset(&new_ka.sa.sa_mask, mask);
 }

 ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);

 if (!ret && oact) {
  if (!access_ok(VERIFY_WRITE, oact, sizeof(*oact)) ||
      __put_user(ptr_to_compat(old_ka.sa.sa_handler),
          &oact->sa_handler) ||
      __put_user(ptr_to_compat(old_ka.sa.sa_restorer),
          &oact->sa_restorer) ||
      __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||
      __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))
   return -EFAULT;
 }
 return ret;
}





SYSCALL_DEFINE0(sgetmask)
{

 return current->blocked.sig[0];
}

SYSCALL_DEFINE1(ssetmask, int, newmask)
{
 int old = current->blocked.sig[0];
 sigset_t newset;

 siginitset(&newset, newmask);
 set_current_blocked(&newset);

 return old;
}




SYSCALL_DEFINE2(signal, int, sig, __sighandler_t, handler)
{
 struct k_sigaction new_sa, old_sa;
 int ret;

 new_sa.sa.sa_handler = handler;
 new_sa.sa.sa_flags = SA_ONESHOT | SA_NOMASK;
 sigemptyset(&new_sa.sa.sa_mask);

 ret = do_sigaction(sig, &new_sa, &old_sa);

 return ret ? ret : (unsigned long)old_sa.sa.sa_handler;
}


SYSCALL_DEFINE0(pause)
{
 while (!signal_pending(current)) {
  __set_current_state(TASK_INTERRUPTIBLE);
  schedule();
 }
 return -ERESTARTNOHAND;
}


static int sigsuspend(sigset_t *set)
{
 current->saved_sigmask = current->blocked;
 set_current_blocked(set);

 while (!signal_pending(current)) {
  __set_current_state(TASK_INTERRUPTIBLE);
  schedule();
 }
 set_restore_sigmask();
 return -ERESTARTNOHAND;
}







SYSCALL_DEFINE2(rt_sigsuspend, sigset_t __user *, unewset, size_t, sigsetsize)
{
 sigset_t newset;


 if (sigsetsize != sizeof(sigset_t))
  return -EINVAL;

 if (copy_from_user(&newset, unewset, sizeof(newset)))
  return -EFAULT;
 return sigsuspend(&newset);
}

COMPAT_SYSCALL_DEFINE2(rt_sigsuspend, compat_sigset_t __user *, unewset, compat_size_t, sigsetsize)
{
 sigset_t newset;
 compat_sigset_t newset32;


 if (sigsetsize != sizeof(sigset_t))
  return -EINVAL;

 if (copy_from_user(&newset32, unewset, sizeof(compat_sigset_t)))
  return -EFAULT;
 sigset_from_compat(&newset, &newset32);
 return sigsuspend(&newset);

 return sys_rt_sigsuspend((sigset_t __user *)unewset, sigsetsize);
}

SYSCALL_DEFINE1(sigsuspend, old_sigset_t, mask)
{
 sigset_t blocked;
 siginitset(&blocked, mask);
 return sigsuspend(&blocked);
}
SYSCALL_DEFINE3(sigsuspend, int, unused1, int, unused2, old_sigset_t, mask)
{
 sigset_t blocked;
 siginitset(&blocked, mask);
 return sigsuspend(&blocked);
}

__weak const char *arch_vma_name(struct vm_area_struct *vma)
{
 return NULL;
}

void __init signals_init(void)
{

 BUILD_BUG_ON(__ARCH_SI_PREAMBLE_SIZE
  != offsetof(struct siginfo, _sifields._pad));

 sigqueue_cachep = KMEM_CACHE(sigqueue, SLAB_PANIC);
}







void
kdb_send_sig_info(struct task_struct *t, struct siginfo *info)
{
 static struct task_struct *kdb_prev_t;
 int sig, new_t;
 if (!spin_trylock(&t->sighand->siglock)) {
  kdb_printf("Can't do kill command now.\n"
      "The sigmask lock is held somewhere else in "
      "kernel, try again later\n");
  return;
 }
 spin_unlock(&t->sighand->siglock);
 new_t = kdb_prev_t != t;
 kdb_prev_t = t;
 if (t->state != TASK_RUNNING && new_t) {
  kdb_printf("Process is not RUNNING, sending a signal from "
      "kdb risks deadlock\n"
      "on the run queue locks. "
      "The signal has _not_ been sent.\n"
      "Reissue the kill command if you want to risk "
      "the deadlock.\n");
  return;
 }
 sig = info->si_signo;
 if (send_sig_info(sig, info, t))
  kdb_printf("Fail to deliver Signal %d to process %d.\n",
      sig, t->pid);
 else
  kdb_printf("Signal %d is sent to process %d.\n", sig, t->pid);
}










static DEFINE_PER_CPU(struct task_struct *, idle_threads);

struct task_struct *idle_thread_get(unsigned int cpu)
{
 struct task_struct *tsk = per_cpu(idle_threads, cpu);

 if (!tsk)
  return ERR_PTR(-ENOMEM);
 init_idle(tsk, cpu);
 return tsk;
}

void __init idle_thread_set_boot_cpu(void)
{
 per_cpu(idle_threads, smp_processor_id()) = current;
}







static inline void idle_init(unsigned int cpu)
{
 struct task_struct *tsk = per_cpu(idle_threads, cpu);

 if (!tsk) {
  tsk = fork_idle(cpu);
  if (IS_ERR(tsk))
   pr_err("SMP: fork_idle() failed for CPU %u\n", cpu);
  else
   per_cpu(idle_threads, cpu) = tsk;
 }
}




void __init idle_threads_init(void)
{
 unsigned int cpu, boot_cpu;

 boot_cpu = smp_processor_id();

 for_each_possible_cpu(cpu) {
  if (cpu != boot_cpu)
   idle_init(cpu);
 }
}


static LIST_HEAD(hotplug_threads);
static DEFINE_MUTEX(smpboot_threads_lock);

struct smpboot_thread_data {
 unsigned int cpu;
 unsigned int status;
 struct smp_hotplug_thread *ht;
};

enum {
 HP_THREAD_NONE = 0,
 HP_THREAD_ACTIVE,
 HP_THREAD_PARKED,
};
static int smpboot_thread_fn(void *data)
{
 struct smpboot_thread_data *td = data;
 struct smp_hotplug_thread *ht = td->ht;

 while (1) {
  set_current_state(TASK_INTERRUPTIBLE);
  preempt_disable();
  if (kthread_should_stop()) {
   __set_current_state(TASK_RUNNING);
   preempt_enable();

   if (ht->cleanup && td->status != HP_THREAD_NONE)
    ht->cleanup(td->cpu, cpu_online(td->cpu));
   kfree(td);
   return 0;
  }

  if (kthread_should_park()) {
   __set_current_state(TASK_RUNNING);
   preempt_enable();
   if (ht->park && td->status == HP_THREAD_ACTIVE) {
    BUG_ON(td->cpu != smp_processor_id());
    ht->park(td->cpu);
    td->status = HP_THREAD_PARKED;
   }
   kthread_parkme();

   continue;
  }

  BUG_ON(td->cpu != smp_processor_id());


  switch (td->status) {
  case HP_THREAD_NONE:
   __set_current_state(TASK_RUNNING);
   preempt_enable();
   if (ht->setup)
    ht->setup(td->cpu);
   td->status = HP_THREAD_ACTIVE;
   continue;

  case HP_THREAD_PARKED:
   __set_current_state(TASK_RUNNING);
   preempt_enable();
   if (ht->unpark)
    ht->unpark(td->cpu);
   td->status = HP_THREAD_ACTIVE;
   continue;
  }

  if (!ht->thread_should_run(td->cpu)) {
   preempt_enable_no_resched();
   schedule();
  } else {
   __set_current_state(TASK_RUNNING);
   preempt_enable();
   ht->thread_fn(td->cpu);
  }
 }
}

static int
__smpboot_create_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
{
 struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);
 struct smpboot_thread_data *td;

 if (tsk)
  return 0;

 td = kzalloc_node(sizeof(*td), GFP_KERNEL, cpu_to_node(cpu));
 if (!td)
  return -ENOMEM;
 td->cpu = cpu;
 td->ht = ht;

 tsk = kthread_create_on_cpu(smpboot_thread_fn, td, cpu,
        ht->thread_comm);
 if (IS_ERR(tsk)) {
  kfree(td);
  return PTR_ERR(tsk);
 }
 get_task_struct(tsk);
 *per_cpu_ptr(ht->store, cpu) = tsk;
 if (ht->create) {






  if (!wait_task_inactive(tsk, TASK_PARKED))
   WARN_ON(1);
  else
   ht->create(cpu);
 }
 return 0;
}

int smpboot_create_threads(unsigned int cpu)
{
 struct smp_hotplug_thread *cur;
 int ret = 0;

 mutex_lock(&smpboot_threads_lock);
 list_for_each_entry(cur, &hotplug_threads, list) {
  ret = __smpboot_create_thread(cur, cpu);
  if (ret)
   break;
 }
 mutex_unlock(&smpboot_threads_lock);
 return ret;
}

static void smpboot_unpark_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
{
 struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);

 if (!ht->selfparking)
  kthread_unpark(tsk);
}

int smpboot_unpark_threads(unsigned int cpu)
{
 struct smp_hotplug_thread *cur;

 mutex_lock(&smpboot_threads_lock);
 list_for_each_entry(cur, &hotplug_threads, list)
  if (cpumask_test_cpu(cpu, cur->cpumask))
   smpboot_unpark_thread(cur, cpu);
 mutex_unlock(&smpboot_threads_lock);
 return 0;
}

static void smpboot_park_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
{
 struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);

 if (tsk && !ht->selfparking)
  kthread_park(tsk);
}

int smpboot_park_threads(unsigned int cpu)
{
 struct smp_hotplug_thread *cur;

 mutex_lock(&smpboot_threads_lock);
 list_for_each_entry_reverse(cur, &hotplug_threads, list)
  smpboot_park_thread(cur, cpu);
 mutex_unlock(&smpboot_threads_lock);
 return 0;
}

static void smpboot_destroy_threads(struct smp_hotplug_thread *ht)
{
 unsigned int cpu;


 for_each_possible_cpu(cpu) {
  struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);

  if (tsk) {
   kthread_stop(tsk);
   put_task_struct(tsk);
   *per_cpu_ptr(ht->store, cpu) = NULL;
  }
 }
}
int smpboot_register_percpu_thread_cpumask(struct smp_hotplug_thread *plug_thread,
        const struct cpumask *cpumask)
{
 unsigned int cpu;
 int ret = 0;

 if (!alloc_cpumask_var(&plug_thread->cpumask, GFP_KERNEL))
  return -ENOMEM;
 cpumask_copy(plug_thread->cpumask, cpumask);

 get_online_cpus();
 mutex_lock(&smpboot_threads_lock);
 for_each_online_cpu(cpu) {
  ret = __smpboot_create_thread(plug_thread, cpu);
  if (ret) {
   smpboot_destroy_threads(plug_thread);
   free_cpumask_var(plug_thread->cpumask);
   goto out;
  }
  if (cpumask_test_cpu(cpu, cpumask))
   smpboot_unpark_thread(plug_thread, cpu);
 }
 list_add(&plug_thread->list, &hotplug_threads);
out:
 mutex_unlock(&smpboot_threads_lock);
 put_online_cpus();
 return ret;
}
EXPORT_SYMBOL_GPL(smpboot_register_percpu_thread_cpumask);







void smpboot_unregister_percpu_thread(struct smp_hotplug_thread *plug_thread)
{
 get_online_cpus();
 mutex_lock(&smpboot_threads_lock);
 list_del(&plug_thread->list);
 smpboot_destroy_threads(plug_thread);
 mutex_unlock(&smpboot_threads_lock);
 put_online_cpus();
 free_cpumask_var(plug_thread->cpumask);
}
EXPORT_SYMBOL_GPL(smpboot_unregister_percpu_thread);
int smpboot_update_cpumask_percpu_thread(struct smp_hotplug_thread *plug_thread,
      const struct cpumask *new)
{
 struct cpumask *old = plug_thread->cpumask;
 cpumask_var_t tmp;
 unsigned int cpu;

 if (!alloc_cpumask_var(&tmp, GFP_KERNEL))
  return -ENOMEM;

 get_online_cpus();
 mutex_lock(&smpboot_threads_lock);


 cpumask_andnot(tmp, old, new);
 for_each_cpu_and(cpu, tmp, cpu_online_mask)
  smpboot_park_thread(plug_thread, cpu);


 cpumask_andnot(tmp, new, old);
 for_each_cpu_and(cpu, tmp, cpu_online_mask)
  smpboot_unpark_thread(plug_thread, cpu);

 cpumask_copy(old, new);

 mutex_unlock(&smpboot_threads_lock);
 put_online_cpus();

 free_cpumask_var(tmp);

 return 0;
}
EXPORT_SYMBOL_GPL(smpboot_update_cpumask_percpu_thread);

static DEFINE_PER_CPU(atomic_t, cpu_hotplug_state) = ATOMIC_INIT(CPU_POST_DEAD);





int cpu_report_state(int cpu)
{
 return atomic_read(&per_cpu(cpu_hotplug_state, cpu));
}
int cpu_check_up_prepare(int cpu)
{
 if (!IS_ENABLED(CONFIG_HOTPLUG_CPU)) {
  atomic_set(&per_cpu(cpu_hotplug_state, cpu), CPU_UP_PREPARE);
  return 0;
 }

 switch (atomic_read(&per_cpu(cpu_hotplug_state, cpu))) {

 case CPU_POST_DEAD:


  atomic_set(&per_cpu(cpu_hotplug_state, cpu), CPU_UP_PREPARE);
  return 0;

 case CPU_DEAD_FROZEN:
  return -EBUSY;

 case CPU_BROKEN:
  return -EAGAIN;

 default:


  return -EIO;
 }
}







void cpu_set_state_online(int cpu)
{
 (void)atomic_xchg(&per_cpu(cpu_hotplug_state, cpu), CPU_ONLINE);
}





bool cpu_wait_death(unsigned int cpu, int seconds)
{
 int jf_left = seconds * HZ;
 int oldstate;
 bool ret = true;
 int sleep_jf = 1;

 might_sleep();


 if (atomic_read(&per_cpu(cpu_hotplug_state, cpu)) == CPU_DEAD)
  goto update_state;
 udelay(5);


 while (atomic_read(&per_cpu(cpu_hotplug_state, cpu)) != CPU_DEAD) {
  schedule_timeout_uninterruptible(sleep_jf);
  jf_left -= sleep_jf;
  if (jf_left <= 0)
   break;
  sleep_jf = DIV_ROUND_UP(sleep_jf * 11, 10);
 }
update_state:
 oldstate = atomic_read(&per_cpu(cpu_hotplug_state, cpu));
 if (oldstate == CPU_DEAD) {

  smp_mb();
  atomic_set(&per_cpu(cpu_hotplug_state, cpu), CPU_POST_DEAD);
 } else {

  if (atomic_cmpxchg(&per_cpu(cpu_hotplug_state, cpu),
       oldstate, CPU_BROKEN) != oldstate)
   goto update_state;
  ret = false;
 }
 return ret;
}
bool cpu_report_death(void)
{
 int oldstate;
 int newstate;
 int cpu = smp_processor_id();

 do {
  oldstate = atomic_read(&per_cpu(cpu_hotplug_state, cpu));
  if (oldstate != CPU_BROKEN)
   newstate = CPU_DEAD;
  else
   newstate = CPU_DEAD_FROZEN;
 } while (atomic_cmpxchg(&per_cpu(cpu_hotplug_state, cpu),
    oldstate, newstate) != oldstate);
 return newstate == CPU_DEAD;
}








enum {
 CSD_FLAG_LOCK = 0x01,
 CSD_FLAG_SYNCHRONOUS = 0x02,
};

struct call_function_data {
 struct call_single_data __percpu *csd;
 cpumask_var_t cpumask;
};

static DEFINE_PER_CPU_SHARED_ALIGNED(struct call_function_data, cfd_data);

static DEFINE_PER_CPU_SHARED_ALIGNED(struct llist_head, call_single_queue);

static void flush_smp_call_function_queue(bool warn_cpu_offline);

static int
hotplug_cfd(struct notifier_block *nfb, unsigned long action, void *hcpu)
{
 long cpu = (long)hcpu;
 struct call_function_data *cfd = &per_cpu(cfd_data, cpu);

 switch (action) {
 case CPU_UP_PREPARE:
 case CPU_UP_PREPARE_FROZEN:
  if (!zalloc_cpumask_var_node(&cfd->cpumask, GFP_KERNEL,
    cpu_to_node(cpu)))
   return notifier_from_errno(-ENOMEM);
  cfd->csd = alloc_percpu(struct call_single_data);
  if (!cfd->csd) {
   free_cpumask_var(cfd->cpumask);
   return notifier_from_errno(-ENOMEM);
  }
  break;

 case CPU_UP_CANCELED:
 case CPU_UP_CANCELED_FROZEN:


 case CPU_DEAD:
 case CPU_DEAD_FROZEN:
  free_cpumask_var(cfd->cpumask);
  free_percpu(cfd->csd);
  break;

 case CPU_DYING:
 case CPU_DYING_FROZEN:
  flush_smp_call_function_queue(false);
  break;
 };

 return NOTIFY_OK;
}

static struct notifier_block hotplug_cfd_notifier = {
 .notifier_call = hotplug_cfd,
};

void __init call_function_init(void)
{
 void *cpu = (void *)(long)smp_processor_id();
 int i;

 for_each_possible_cpu(i)
  init_llist_head(&per_cpu(call_single_queue, i));

 hotplug_cfd(&hotplug_cfd_notifier, CPU_UP_PREPARE, cpu);
 register_cpu_notifier(&hotplug_cfd_notifier);
}
static __always_inline void csd_lock_wait(struct call_single_data *csd)
{
 smp_cond_acquire(!(csd->flags & CSD_FLAG_LOCK));
}

static __always_inline void csd_lock(struct call_single_data *csd)
{
 csd_lock_wait(csd);
 csd->flags |= CSD_FLAG_LOCK;






 smp_wmb();
}

static __always_inline void csd_unlock(struct call_single_data *csd)
{
 WARN_ON(!(csd->flags & CSD_FLAG_LOCK));




 smp_store_release(&csd->flags, 0);
}

static DEFINE_PER_CPU_SHARED_ALIGNED(struct call_single_data, csd_data);






static int generic_exec_single(int cpu, struct call_single_data *csd,
          smp_call_func_t func, void *info)
{
 if (cpu == smp_processor_id()) {
  unsigned long flags;





  csd_unlock(csd);
  local_irq_save(flags);
  func(info);
  local_irq_restore(flags);
  return 0;
 }


 if ((unsigned)cpu >= nr_cpu_ids || !cpu_online(cpu)) {
  csd_unlock(csd);
  return -ENXIO;
 }

 csd->func = func;
 csd->info = info;
 if (llist_add(&csd->llist, &per_cpu(call_single_queue, cpu)))
  arch_send_call_function_single_ipi(cpu);

 return 0;
}







void generic_smp_call_function_single_interrupt(void)
{
 flush_smp_call_function_queue(true);
}
static void flush_smp_call_function_queue(bool warn_cpu_offline)
{
 struct llist_head *head;
 struct llist_node *entry;
 struct call_single_data *csd, *csd_next;
 static bool warned;

 WARN_ON(!irqs_disabled());

 head = this_cpu_ptr(&call_single_queue);
 entry = llist_del_all(head);
 entry = llist_reverse_order(entry);


 if (unlikely(warn_cpu_offline && !cpu_online(smp_processor_id()) &&
       !warned && !llist_empty(head))) {
  warned = true;
  WARN(1, "IPI on offline CPU %d\n", smp_processor_id());





  llist_for_each_entry(csd, entry, llist)
   pr_warn("IPI callback %pS sent to offline CPU\n",
    csd->func);
 }

 llist_for_each_entry_safe(csd, csd_next, entry, llist) {
  smp_call_func_t func = csd->func;
  void *info = csd->info;


  if (csd->flags & CSD_FLAG_SYNCHRONOUS) {
   func(info);
   csd_unlock(csd);
  } else {
   csd_unlock(csd);
   func(info);
  }
 }







 irq_work_run();
}
int smp_call_function_single(int cpu, smp_call_func_t func, void *info,
        int wait)
{
 struct call_single_data *csd;
 struct call_single_data csd_stack = { .flags = CSD_FLAG_LOCK | CSD_FLAG_SYNCHRONOUS };
 int this_cpu;
 int err;





 this_cpu = get_cpu();







 WARN_ON_ONCE(cpu_online(this_cpu) && irqs_disabled()
       && !oops_in_progress);

 csd = &csd_stack;
 if (!wait) {
  csd = this_cpu_ptr(&csd_data);
  csd_lock(csd);
 }

 err = generic_exec_single(cpu, csd, func, info);

 if (wait)
  csd_lock_wait(csd);

 put_cpu();

 return err;
}
EXPORT_SYMBOL(smp_call_function_single);
int smp_call_function_single_async(int cpu, struct call_single_data *csd)
{
 int err = 0;

 preempt_disable();


 if (WARN_ON_ONCE(csd->flags & CSD_FLAG_LOCK))
  csd_lock_wait(csd);

 csd->flags = CSD_FLAG_LOCK;
 smp_wmb();

 err = generic_exec_single(cpu, csd, csd->func, csd->info);
 preempt_enable();

 return err;
}
EXPORT_SYMBOL_GPL(smp_call_function_single_async);
int smp_call_function_any(const struct cpumask *mask,
     smp_call_func_t func, void *info, int wait)
{
 unsigned int cpu;
 const struct cpumask *nodemask;
 int ret;


 cpu = get_cpu();
 if (cpumask_test_cpu(cpu, mask))
  goto call;


 nodemask = cpumask_of_node(cpu_to_node(cpu));
 for (cpu = cpumask_first_and(nodemask, mask); cpu < nr_cpu_ids;
      cpu = cpumask_next_and(cpu, nodemask, mask)) {
  if (cpu_online(cpu))
   goto call;
 }


 cpu = cpumask_any_and(mask, cpu_online_mask);
call:
 ret = smp_call_function_single(cpu, func, info, wait);
 put_cpu();
 return ret;
}
EXPORT_SYMBOL_GPL(smp_call_function_any);
void smp_call_function_many(const struct cpumask *mask,
       smp_call_func_t func, void *info, bool wait)
{
 struct call_function_data *cfd;
 int cpu, next_cpu, this_cpu = smp_processor_id();







 WARN_ON_ONCE(cpu_online(this_cpu) && irqs_disabled()
       && !oops_in_progress && !early_boot_irqs_disabled);


 cpu = cpumask_first_and(mask, cpu_online_mask);
 if (cpu == this_cpu)
  cpu = cpumask_next_and(cpu, mask, cpu_online_mask);


 if (cpu >= nr_cpu_ids)
  return;


 next_cpu = cpumask_next_and(cpu, mask, cpu_online_mask);
 if (next_cpu == this_cpu)
  next_cpu = cpumask_next_and(next_cpu, mask, cpu_online_mask);


 if (next_cpu >= nr_cpu_ids) {
  smp_call_function_single(cpu, func, info, wait);
  return;
 }

 cfd = this_cpu_ptr(&cfd_data);

 cpumask_and(cfd->cpumask, mask, cpu_online_mask);
 cpumask_clear_cpu(this_cpu, cfd->cpumask);


 if (unlikely(!cpumask_weight(cfd->cpumask)))
  return;

 for_each_cpu(cpu, cfd->cpumask) {
  struct call_single_data *csd = per_cpu_ptr(cfd->csd, cpu);

  csd_lock(csd);
  if (wait)
   csd->flags |= CSD_FLAG_SYNCHRONOUS;
  csd->func = func;
  csd->info = info;
  llist_add(&csd->llist, &per_cpu(call_single_queue, cpu));
 }


 arch_send_call_function_ipi_mask(cfd->cpumask);

 if (wait) {
  for_each_cpu(cpu, cfd->cpumask) {
   struct call_single_data *csd;

   csd = per_cpu_ptr(cfd->csd, cpu);
   csd_lock_wait(csd);
  }
 }
}
EXPORT_SYMBOL(smp_call_function_many);
int smp_call_function(smp_call_func_t func, void *info, int wait)
{
 preempt_disable();
 smp_call_function_many(cpu_online_mask, func, info, wait);
 preempt_enable();

 return 0;
}
EXPORT_SYMBOL(smp_call_function);


unsigned int setup_max_cpus = NR_CPUS;
EXPORT_SYMBOL(setup_max_cpus);
void __weak arch_disable_smp_support(void) { }

static int __init nosmp(char *str)
{
 setup_max_cpus = 0;
 arch_disable_smp_support();

 return 0;
}

early_param("nosmp", nosmp);


static int __init nrcpus(char *str)
{
 int nr_cpus;

 get_option(&str, &nr_cpus);
 if (nr_cpus > 0 && nr_cpus < nr_cpu_ids)
  nr_cpu_ids = nr_cpus;

 return 0;
}

early_param("nr_cpus", nrcpus);

static int __init maxcpus(char *str)
{
 get_option(&str, &setup_max_cpus);
 if (setup_max_cpus == 0)
  arch_disable_smp_support();

 return 0;
}

early_param("maxcpus", maxcpus);


int nr_cpu_ids __read_mostly = NR_CPUS;
EXPORT_SYMBOL(nr_cpu_ids);


void __init setup_nr_cpu_ids(void)
{
 nr_cpu_ids = find_last_bit(cpumask_bits(cpu_possible_mask),NR_CPUS) + 1;
}

void __weak smp_announce(void)
{
 printk(KERN_INFO "Brought up %d CPUs\n", num_online_cpus());
}


void __init smp_init(void)
{
 unsigned int cpu;

 idle_threads_init();
 cpuhp_threads_init();


 for_each_present_cpu(cpu) {
  if (num_online_cpus() >= setup_max_cpus)
   break;
  if (!cpu_online(cpu))
   cpu_up(cpu);
 }


 smp_announce();
 smp_cpus_done(setup_max_cpus);
}






int on_each_cpu(void (*func) (void *info), void *info, int wait)
{
 unsigned long flags;
 int ret = 0;

 preempt_disable();
 ret = smp_call_function(func, info, wait);
 local_irq_save(flags);
 func(info);
 local_irq_restore(flags);
 preempt_enable();
 return ret;
}
EXPORT_SYMBOL(on_each_cpu);
void on_each_cpu_mask(const struct cpumask *mask, smp_call_func_t func,
   void *info, bool wait)
{
 int cpu = get_cpu();

 smp_call_function_many(mask, func, info, wait);
 if (cpumask_test_cpu(cpu, mask)) {
  unsigned long flags;
  local_irq_save(flags);
  func(info);
  local_irq_restore(flags);
 }
 put_cpu();
}
EXPORT_SYMBOL(on_each_cpu_mask);
void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
   smp_call_func_t func, void *info, bool wait,
   gfp_t gfp_flags)
{
 cpumask_var_t cpus;
 int cpu, ret;

 might_sleep_if(gfpflags_allow_blocking(gfp_flags));

 if (likely(zalloc_cpumask_var(&cpus, (gfp_flags|__GFP_NOWARN)))) {
  preempt_disable();
  for_each_online_cpu(cpu)
   if (cond_func(cpu, info))
    cpumask_set_cpu(cpu, cpus);
  on_each_cpu_mask(cpus, func, info, wait);
  preempt_enable();
  free_cpumask_var(cpus);
 } else {




  preempt_disable();
  for_each_online_cpu(cpu)
   if (cond_func(cpu, info)) {
    ret = smp_call_function_single(cpu, func,
        info, wait);
    WARN_ON_ONCE(ret);
   }
  preempt_enable();
 }
}
EXPORT_SYMBOL(on_each_cpu_cond);

static void do_nothing(void *unused)
{
}
void kick_all_cpus_sync(void)
{

 smp_mb();
 smp_call_function(do_nothing, NULL, 1);
}
EXPORT_SYMBOL_GPL(kick_all_cpus_sync);







void wake_up_all_idle_cpus(void)
{
 int cpu;

 preempt_disable();
 for_each_online_cpu(cpu) {
  if (cpu == smp_processor_id())
   continue;

  wake_up_if_idle(cpu);
 }
 preempt_enable();
}
EXPORT_SYMBOL_GPL(wake_up_all_idle_cpus);



static int swsusp_page_is_free(struct page *);
static void swsusp_set_page_forbidden(struct page *);
static void swsusp_unset_page_forbidden(struct page *);






unsigned long reserved_size;

void __init hibernate_reserved_size_init(void)
{
 reserved_size = SPARE_PAGES * PAGE_SIZE;
}







unsigned long image_size;

void __init hibernate_image_size_init(void)
{
 image_size = ((totalram_pages * 2) / 5) * PAGE_SIZE;
}






struct pbe *restore_pblist;


static void *buffer;

static unsigned int allocated_unsafe_pages;

static void *get_image_page(gfp_t gfp_mask, int safe_needed)
{
 void *res;

 res = (void *)get_zeroed_page(gfp_mask);
 if (safe_needed)
  while (res && swsusp_page_is_free(virt_to_page(res))) {

   swsusp_set_page_forbidden(virt_to_page(res));
   allocated_unsafe_pages++;
   res = (void *)get_zeroed_page(gfp_mask);
  }
 if (res) {
  swsusp_set_page_forbidden(virt_to_page(res));
  swsusp_set_page_free(virt_to_page(res));
 }
 return res;
}

unsigned long get_safe_page(gfp_t gfp_mask)
{
 return (unsigned long)get_image_page(gfp_mask, PG_SAFE);
}

static struct page *alloc_image_page(gfp_t gfp_mask)
{
 struct page *page;

 page = alloc_page(gfp_mask);
 if (page) {
  swsusp_set_page_forbidden(page);
  swsusp_set_page_free(page);
 }
 return page;
}






static inline void free_image_page(void *addr, int clear_nosave_free)
{
 struct page *page;

 BUG_ON(!virt_addr_valid(addr));

 page = virt_to_page(addr);

 swsusp_unset_page_forbidden(page);
 if (clear_nosave_free)
  swsusp_unset_page_free(page);

 __free_page(page);
}




struct linked_page {
 struct linked_page *next;
 char data[LINKED_PAGE_DATA_SIZE];
} __packed;

static inline void
free_list_of_pages(struct linked_page *list, int clear_page_nosave)
{
 while (list) {
  struct linked_page *lp = list->next;

  free_image_page(list, clear_page_nosave);
  list = lp;
 }
}
struct chain_allocator {
 struct linked_page *chain;
 unsigned int used_space;


 gfp_t gfp_mask;
 int safe_needed;
};

static void
chain_init(struct chain_allocator *ca, gfp_t gfp_mask, int safe_needed)
{
 ca->chain = NULL;
 ca->used_space = LINKED_PAGE_DATA_SIZE;
 ca->gfp_mask = gfp_mask;
 ca->safe_needed = safe_needed;
}

static void *chain_alloc(struct chain_allocator *ca, unsigned int size)
{
 void *ret;

 if (LINKED_PAGE_DATA_SIZE - ca->used_space < size) {
  struct linked_page *lp;

  lp = get_image_page(ca->gfp_mask, ca->safe_needed);
  if (!lp)
   return NULL;

  lp->next = ca->chain;
  ca->chain = lp;
  ca->used_space = 0;
 }
 ret = ca->chain->data + ca->used_space;
 ca->used_space += size;
 return ret;
}







struct rtree_node {
 struct list_head list;
 unsigned long *data;
};





struct mem_zone_bm_rtree {
 struct list_head list;
 struct list_head nodes;
 struct list_head leaves;
 unsigned long start_pfn;
 unsigned long end_pfn;
 struct rtree_node *rtree;
 int levels;
 unsigned int blocks;
};



struct bm_position {
 struct mem_zone_bm_rtree *zone;
 struct rtree_node *node;
 unsigned long node_pfn;
 int node_bit;
};

struct memory_bitmap {
 struct list_head zones;
 struct linked_page *p_list;



 struct bm_position cur;
};



static struct rtree_node *alloc_rtree_node(gfp_t gfp_mask, int safe_needed,
        struct chain_allocator *ca,
        struct list_head *list)
{
 struct rtree_node *node;

 node = chain_alloc(ca, sizeof(struct rtree_node));
 if (!node)
  return NULL;

 node->data = get_image_page(gfp_mask, safe_needed);
 if (!node->data)
  return NULL;

 list_add_tail(&node->list, list);

 return node;
}
static int add_rtree_block(struct mem_zone_bm_rtree *zone, gfp_t gfp_mask,
      int safe_needed, struct chain_allocator *ca)
{
 struct rtree_node *node, *block, **dst;
 unsigned int levels_needed, block_nr;
 int i;

 block_nr = zone->blocks;
 levels_needed = 0;


 while (block_nr) {
  levels_needed += 1;
  block_nr >>= BM_RTREE_LEVEL_SHIFT;
 }


 for (i = zone->levels; i < levels_needed; i++) {
  node = alloc_rtree_node(gfp_mask, safe_needed, ca,
     &zone->nodes);
  if (!node)
   return -ENOMEM;

  node->data[0] = (unsigned long)zone->rtree;
  zone->rtree = node;
  zone->levels += 1;
 }


 block = alloc_rtree_node(gfp_mask, safe_needed, ca, &zone->leaves);
 if (!block)
  return -ENOMEM;


 node = zone->rtree;
 dst = &zone->rtree;
 block_nr = zone->blocks;
 for (i = zone->levels; i > 0; i--) {
  int index;

  if (!node) {
   node = alloc_rtree_node(gfp_mask, safe_needed, ca,
      &zone->nodes);
   if (!node)
    return -ENOMEM;
   *dst = node;
  }

  index = block_nr >> ((i - 1) * BM_RTREE_LEVEL_SHIFT);
  index &= BM_RTREE_LEVEL_MASK;
  dst = (struct rtree_node **)&((*dst)->data[index]);
  node = *dst;
 }

 zone->blocks += 1;
 *dst = block;

 return 0;
}

static void free_zone_bm_rtree(struct mem_zone_bm_rtree *zone,
          int clear_nosave_free);
static struct mem_zone_bm_rtree *
create_zone_bm_rtree(gfp_t gfp_mask, int safe_needed,
       struct chain_allocator *ca,
       unsigned long start, unsigned long end)
{
 struct mem_zone_bm_rtree *zone;
 unsigned int i, nr_blocks;
 unsigned long pages;

 pages = end - start;
 zone = chain_alloc(ca, sizeof(struct mem_zone_bm_rtree));
 if (!zone)
  return NULL;

 INIT_LIST_HEAD(&zone->nodes);
 INIT_LIST_HEAD(&zone->leaves);
 zone->start_pfn = start;
 zone->end_pfn = end;
 nr_blocks = DIV_ROUND_UP(pages, BM_BITS_PER_BLOCK);

 for (i = 0; i < nr_blocks; i++) {
  if (add_rtree_block(zone, gfp_mask, safe_needed, ca)) {
   free_zone_bm_rtree(zone, PG_UNSAFE_CLEAR);
   return NULL;
  }
 }

 return zone;
}
static void free_zone_bm_rtree(struct mem_zone_bm_rtree *zone,
          int clear_nosave_free)
{
 struct rtree_node *node;

 list_for_each_entry(node, &zone->nodes, list)
  free_image_page(node->data, clear_nosave_free);

 list_for_each_entry(node, &zone->leaves, list)
  free_image_page(node->data, clear_nosave_free);
}

static void memory_bm_position_reset(struct memory_bitmap *bm)
{
 bm->cur.zone = list_entry(bm->zones.next, struct mem_zone_bm_rtree,
      list);
 bm->cur.node = list_entry(bm->cur.zone->leaves.next,
      struct rtree_node, list);
 bm->cur.node_pfn = 0;
 bm->cur.node_bit = 0;
}

static void memory_bm_free(struct memory_bitmap *bm, int clear_nosave_free);

struct mem_extent {
 struct list_head hook;
 unsigned long start;
 unsigned long end;
};





static void free_mem_extents(struct list_head *list)
{
 struct mem_extent *ext, *aux;

 list_for_each_entry_safe(ext, aux, list, hook) {
  list_del(&ext->hook);
  kfree(ext);
 }
}







static int create_mem_extents(struct list_head *list, gfp_t gfp_mask)
{
 struct zone *zone;

 INIT_LIST_HEAD(list);

 for_each_populated_zone(zone) {
  unsigned long zone_start, zone_end;
  struct mem_extent *ext, *cur, *aux;

  zone_start = zone->zone_start_pfn;
  zone_end = zone_end_pfn(zone);

  list_for_each_entry(ext, list, hook)
   if (zone_start <= ext->end)
    break;

  if (&ext->hook == list || zone_end < ext->start) {

   struct mem_extent *new_ext;

   new_ext = kzalloc(sizeof(struct mem_extent), gfp_mask);
   if (!new_ext) {
    free_mem_extents(list);
    return -ENOMEM;
   }
   new_ext->start = zone_start;
   new_ext->end = zone_end;
   list_add_tail(&new_ext->hook, &ext->hook);
   continue;
  }


  if (zone_start < ext->start)
   ext->start = zone_start;
  if (zone_end > ext->end)
   ext->end = zone_end;


  cur = ext;
  list_for_each_entry_safe_continue(cur, aux, list, hook) {
   if (zone_end < cur->start)
    break;
   if (zone_end < cur->end)
    ext->end = cur->end;
   list_del(&cur->hook);
   kfree(cur);
  }
 }

 return 0;
}




static int
memory_bm_create(struct memory_bitmap *bm, gfp_t gfp_mask, int safe_needed)
{
 struct chain_allocator ca;
 struct list_head mem_extents;
 struct mem_extent *ext;
 int error;

 chain_init(&ca, gfp_mask, safe_needed);
 INIT_LIST_HEAD(&bm->zones);

 error = create_mem_extents(&mem_extents, gfp_mask);
 if (error)
  return error;

 list_for_each_entry(ext, &mem_extents, hook) {
  struct mem_zone_bm_rtree *zone;

  zone = create_zone_bm_rtree(gfp_mask, safe_needed, &ca,
         ext->start, ext->end);
  if (!zone) {
   error = -ENOMEM;
   goto Error;
  }
  list_add_tail(&zone->list, &bm->zones);
 }

 bm->p_list = ca.chain;
 memory_bm_position_reset(bm);
 Exit:
 free_mem_extents(&mem_extents);
 return error;

 Error:
 bm->p_list = ca.chain;
 memory_bm_free(bm, PG_UNSAFE_CLEAR);
 goto Exit;
}




static void memory_bm_free(struct memory_bitmap *bm, int clear_nosave_free)
{
 struct mem_zone_bm_rtree *zone;

 list_for_each_entry(zone, &bm->zones, list)
  free_zone_bm_rtree(zone, clear_nosave_free);

 free_list_of_pages(bm->p_list, clear_nosave_free);

 INIT_LIST_HEAD(&bm->zones);
}
static int memory_bm_find_bit(struct memory_bitmap *bm, unsigned long pfn,
         void **addr, unsigned int *bit_nr)
{
 struct mem_zone_bm_rtree *curr, *zone;
 struct rtree_node *node;
 int i, block_nr;

 zone = bm->cur.zone;

 if (pfn >= zone->start_pfn && pfn < zone->end_pfn)
  goto zone_found;

 zone = NULL;


 list_for_each_entry(curr, &bm->zones, list) {
  if (pfn >= curr->start_pfn && pfn < curr->end_pfn) {
   zone = curr;
   break;
  }
 }

 if (!zone)
  return -EFAULT;

zone_found:





 node = bm->cur.node;
 if (((pfn - zone->start_pfn) & ~BM_BLOCK_MASK) == bm->cur.node_pfn)
  goto node_found;

 node = zone->rtree;
 block_nr = (pfn - zone->start_pfn) >> BM_BLOCK_SHIFT;

 for (i = zone->levels; i > 0; i--) {
  int index;

  index = block_nr >> ((i - 1) * BM_RTREE_LEVEL_SHIFT);
  index &= BM_RTREE_LEVEL_MASK;
  BUG_ON(node->data[index] == 0);
  node = (struct rtree_node *)node->data[index];
 }

node_found:

 bm->cur.zone = zone;
 bm->cur.node = node;
 bm->cur.node_pfn = (pfn - zone->start_pfn) & ~BM_BLOCK_MASK;


 *addr = node->data;
 *bit_nr = (pfn - zone->start_pfn) & BM_BLOCK_MASK;

 return 0;
}

static void memory_bm_set_bit(struct memory_bitmap *bm, unsigned long pfn)
{
 void *addr;
 unsigned int bit;
 int error;

 error = memory_bm_find_bit(bm, pfn, &addr, &bit);
 BUG_ON(error);
 set_bit(bit, addr);
}

static int mem_bm_set_bit_check(struct memory_bitmap *bm, unsigned long pfn)
{
 void *addr;
 unsigned int bit;
 int error;

 error = memory_bm_find_bit(bm, pfn, &addr, &bit);
 if (!error)
  set_bit(bit, addr);

 return error;
}

static void memory_bm_clear_bit(struct memory_bitmap *bm, unsigned long pfn)
{
 void *addr;
 unsigned int bit;
 int error;

 error = memory_bm_find_bit(bm, pfn, &addr, &bit);
 BUG_ON(error);
 clear_bit(bit, addr);
}

static void memory_bm_clear_current(struct memory_bitmap *bm)
{
 int bit;

 bit = max(bm->cur.node_bit - 1, 0);
 clear_bit(bit, bm->cur.node->data);
}

static int memory_bm_test_bit(struct memory_bitmap *bm, unsigned long pfn)
{
 void *addr;
 unsigned int bit;
 int error;

 error = memory_bm_find_bit(bm, pfn, &addr, &bit);
 BUG_ON(error);
 return test_bit(bit, addr);
}

static bool memory_bm_pfn_present(struct memory_bitmap *bm, unsigned long pfn)
{
 void *addr;
 unsigned int bit;

 return !memory_bm_find_bit(bm, pfn, &addr, &bit);
}
static bool rtree_next_node(struct memory_bitmap *bm)
{
 bm->cur.node = list_entry(bm->cur.node->list.next,
      struct rtree_node, list);
 if (&bm->cur.node->list != &bm->cur.zone->leaves) {
  bm->cur.node_pfn += BM_BITS_PER_BLOCK;
  bm->cur.node_bit = 0;
  touch_softlockup_watchdog();
  return true;
 }


 bm->cur.zone = list_entry(bm->cur.zone->list.next,
      struct mem_zone_bm_rtree, list);
 if (&bm->cur.zone->list != &bm->zones) {
  bm->cur.node = list_entry(bm->cur.zone->leaves.next,
       struct rtree_node, list);
  bm->cur.node_pfn = 0;
  bm->cur.node_bit = 0;
  return true;
 }


 return false;
}
static unsigned long memory_bm_next_pfn(struct memory_bitmap *bm)
{
 unsigned long bits, pfn, pages;
 int bit;

 do {
  pages = bm->cur.zone->end_pfn - bm->cur.zone->start_pfn;
  bits = min(pages - bm->cur.node_pfn, BM_BITS_PER_BLOCK);
  bit = find_next_bit(bm->cur.node->data, bits,
       bm->cur.node_bit);
  if (bit < bits) {
   pfn = bm->cur.zone->start_pfn + bm->cur.node_pfn + bit;
   bm->cur.node_bit = bit + 1;
   return pfn;
  }
 } while (rtree_next_node(bm));

 return BM_END_OF_MAP;
}






struct nosave_region {
 struct list_head list;
 unsigned long start_pfn;
 unsigned long end_pfn;
};

static LIST_HEAD(nosave_regions);







void __init
__register_nosave_region(unsigned long start_pfn, unsigned long end_pfn,
    int use_kmalloc)
{
 struct nosave_region *region;

 if (start_pfn >= end_pfn)
  return;

 if (!list_empty(&nosave_regions)) {

  region = list_entry(nosave_regions.prev,
     struct nosave_region, list);
  if (region->end_pfn == start_pfn) {
   region->end_pfn = end_pfn;
   goto Report;
  }
 }
 if (use_kmalloc) {

  region = kmalloc(sizeof(struct nosave_region), GFP_KERNEL);
  BUG_ON(!region);
 } else

  region = memblock_virt_alloc(sizeof(struct nosave_region), 0);
 region->start_pfn = start_pfn;
 region->end_pfn = end_pfn;
 list_add_tail(&region->list, &nosave_regions);
 Report:
 printk(KERN_INFO "PM: Registered nosave memory: [mem %#010llx-%#010llx]\n",
  (unsigned long long) start_pfn << PAGE_SHIFT,
  ((unsigned long long) end_pfn << PAGE_SHIFT) - 1);
}





static struct memory_bitmap *forbidden_pages_map;


static struct memory_bitmap *free_pages_map;






void swsusp_set_page_free(struct page *page)
{
 if (free_pages_map)
  memory_bm_set_bit(free_pages_map, page_to_pfn(page));
}

static int swsusp_page_is_free(struct page *page)
{
 return free_pages_map ?
  memory_bm_test_bit(free_pages_map, page_to_pfn(page)) : 0;
}

void swsusp_unset_page_free(struct page *page)
{
 if (free_pages_map)
  memory_bm_clear_bit(free_pages_map, page_to_pfn(page));
}

static void swsusp_set_page_forbidden(struct page *page)
{
 if (forbidden_pages_map)
  memory_bm_set_bit(forbidden_pages_map, page_to_pfn(page));
}

int swsusp_page_is_forbidden(struct page *page)
{
 return forbidden_pages_map ?
  memory_bm_test_bit(forbidden_pages_map, page_to_pfn(page)) : 0;
}

static void swsusp_unset_page_forbidden(struct page *page)
{
 if (forbidden_pages_map)
  memory_bm_clear_bit(forbidden_pages_map, page_to_pfn(page));
}






static void mark_nosave_pages(struct memory_bitmap *bm)
{
 struct nosave_region *region;

 if (list_empty(&nosave_regions))
  return;

 list_for_each_entry(region, &nosave_regions, list) {
  unsigned long pfn;

  pr_debug("PM: Marking nosave pages: [mem %#010llx-%#010llx]\n",
    (unsigned long long) region->start_pfn << PAGE_SHIFT,
    ((unsigned long long) region->end_pfn << PAGE_SHIFT)
    - 1);

  for (pfn = region->start_pfn; pfn < region->end_pfn; pfn++)
   if (pfn_valid(pfn)) {






    mem_bm_set_bit_check(bm, pfn);
   }
 }
}
int create_basic_memory_bitmaps(void)
{
 struct memory_bitmap *bm1, *bm2;
 int error = 0;

 if (forbidden_pages_map && free_pages_map)
  return 0;
 else
  BUG_ON(forbidden_pages_map || free_pages_map);

 bm1 = kzalloc(sizeof(struct memory_bitmap), GFP_KERNEL);
 if (!bm1)
  return -ENOMEM;

 error = memory_bm_create(bm1, GFP_KERNEL, PG_ANY);
 if (error)
  goto Free_first_object;

 bm2 = kzalloc(sizeof(struct memory_bitmap), GFP_KERNEL);
 if (!bm2)
  goto Free_first_bitmap;

 error = memory_bm_create(bm2, GFP_KERNEL, PG_ANY);
 if (error)
  goto Free_second_object;

 forbidden_pages_map = bm1;
 free_pages_map = bm2;
 mark_nosave_pages(forbidden_pages_map);

 pr_debug("PM: Basic memory bitmaps created\n");

 return 0;

 Free_second_object:
 kfree(bm2);
 Free_first_bitmap:
  memory_bm_free(bm1, PG_UNSAFE_CLEAR);
 Free_first_object:
 kfree(bm1);
 return -ENOMEM;
}
void free_basic_memory_bitmaps(void)
{
 struct memory_bitmap *bm1, *bm2;

 if (WARN_ON(!(forbidden_pages_map && free_pages_map)))
  return;

 bm1 = forbidden_pages_map;
 bm2 = free_pages_map;
 forbidden_pages_map = NULL;
 free_pages_map = NULL;
 memory_bm_free(bm1, PG_UNSAFE_CLEAR);
 kfree(bm1);
 memory_bm_free(bm2, PG_UNSAFE_CLEAR);
 kfree(bm2);

 pr_debug("PM: Basic memory bitmaps freed\n");
}







unsigned int snapshot_additional_pages(struct zone *zone)
{
 unsigned int rtree, nodes;

 rtree = nodes = DIV_ROUND_UP(zone->spanned_pages, BM_BITS_PER_BLOCK);
 rtree += DIV_ROUND_UP(rtree * sizeof(struct rtree_node),
         LINKED_PAGE_DATA_SIZE);
 while (nodes > 1) {
  nodes = DIV_ROUND_UP(nodes, BM_ENTRIES_PER_LEVEL);
  rtree += nodes;
 }

 return 2 * rtree;
}






static unsigned int count_free_highmem_pages(void)
{
 struct zone *zone;
 unsigned int cnt = 0;

 for_each_populated_zone(zone)
  if (is_highmem(zone))
   cnt += zone_page_state(zone, NR_FREE_PAGES);

 return cnt;
}
static struct page *saveable_highmem_page(struct zone *zone, unsigned long pfn)
{
 struct page *page;

 if (!pfn_valid(pfn))
  return NULL;

 page = pfn_to_page(pfn);
 if (page_zone(page) != zone)
  return NULL;

 BUG_ON(!PageHighMem(page));

 if (swsusp_page_is_forbidden(page) || swsusp_page_is_free(page) ||
     PageReserved(page))
  return NULL;

 if (page_is_guard(page))
  return NULL;

 return page;
}






static unsigned int count_highmem_pages(void)
{
 struct zone *zone;
 unsigned int n = 0;

 for_each_populated_zone(zone) {
  unsigned long pfn, max_zone_pfn;

  if (!is_highmem(zone))
   continue;

  mark_free_pages(zone);
  max_zone_pfn = zone_end_pfn(zone);
  for (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)
   if (saveable_highmem_page(zone, pfn))
    n++;
 }
 return n;
}
static inline void *saveable_highmem_page(struct zone *z, unsigned long p)
{
 return NULL;
}
static struct page *saveable_page(struct zone *zone, unsigned long pfn)
{
 struct page *page;

 if (!pfn_valid(pfn))
  return NULL;

 page = pfn_to_page(pfn);
 if (page_zone(page) != zone)
  return NULL;

 BUG_ON(PageHighMem(page));

 if (swsusp_page_is_forbidden(page) || swsusp_page_is_free(page))
  return NULL;

 if (PageReserved(page)
     && (!kernel_page_present(page) || pfn_is_nosave(pfn)))
  return NULL;

 if (page_is_guard(page))
  return NULL;

 return page;
}






static unsigned int count_data_pages(void)
{
 struct zone *zone;
 unsigned long pfn, max_zone_pfn;
 unsigned int n = 0;

 for_each_populated_zone(zone) {
  if (is_highmem(zone))
   continue;

  mark_free_pages(zone);
  max_zone_pfn = zone_end_pfn(zone);
  for (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)
   if (saveable_page(zone, pfn))
    n++;
 }
 return n;
}




static inline void do_copy_page(long *dst, long *src)
{
 int n;

 for (n = PAGE_SIZE / sizeof(long); n; n--)
  *dst++ = *src++;
}
static void safe_copy_page(void *dst, struct page *s_page)
{
 if (kernel_page_present(s_page)) {
  do_copy_page(dst, page_address(s_page));
 } else {
  kernel_map_pages(s_page, 1, 1);
  do_copy_page(dst, page_address(s_page));
  kernel_map_pages(s_page, 1, 0);
 }
}


static inline struct page *
page_is_saveable(struct zone *zone, unsigned long pfn)
{
 return is_highmem(zone) ?
  saveable_highmem_page(zone, pfn) : saveable_page(zone, pfn);
}

static void copy_data_page(unsigned long dst_pfn, unsigned long src_pfn)
{
 struct page *s_page, *d_page;
 void *src, *dst;

 s_page = pfn_to_page(src_pfn);
 d_page = pfn_to_page(dst_pfn);
 if (PageHighMem(s_page)) {
  src = kmap_atomic(s_page);
  dst = kmap_atomic(d_page);
  do_copy_page(dst, src);
  kunmap_atomic(dst);
  kunmap_atomic(src);
 } else {
  if (PageHighMem(d_page)) {



   safe_copy_page(buffer, s_page);
   dst = kmap_atomic(d_page);
   copy_page(dst, buffer);
   kunmap_atomic(dst);
  } else {
   safe_copy_page(page_address(d_page), s_page);
  }
 }
}

static inline void copy_data_page(unsigned long dst_pfn, unsigned long src_pfn)
{
 safe_copy_page(page_address(pfn_to_page(dst_pfn)),
    pfn_to_page(src_pfn));
}

static void
copy_data_pages(struct memory_bitmap *copy_bm, struct memory_bitmap *orig_bm)
{
 struct zone *zone;
 unsigned long pfn;

 for_each_populated_zone(zone) {
  unsigned long max_zone_pfn;

  mark_free_pages(zone);
  max_zone_pfn = zone_end_pfn(zone);
  for (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)
   if (page_is_saveable(zone, pfn))
    memory_bm_set_bit(orig_bm, pfn);
 }
 memory_bm_position_reset(orig_bm);
 memory_bm_position_reset(copy_bm);
 for(;;) {
  pfn = memory_bm_next_pfn(orig_bm);
  if (unlikely(pfn == BM_END_OF_MAP))
   break;
  copy_data_page(memory_bm_next_pfn(copy_bm), pfn);
 }
}


static unsigned int nr_copy_pages;

static unsigned int nr_meta_pages;




unsigned int alloc_normal, alloc_highmem;




static struct memory_bitmap orig_bm;
static struct memory_bitmap copy_bm;
void swsusp_free(void)
{
 unsigned long fb_pfn, fr_pfn;

 if (!forbidden_pages_map || !free_pages_map)
  goto out;

 memory_bm_position_reset(forbidden_pages_map);
 memory_bm_position_reset(free_pages_map);

loop:
 fr_pfn = memory_bm_next_pfn(free_pages_map);
 fb_pfn = memory_bm_next_pfn(forbidden_pages_map);





 do {
  if (fb_pfn < fr_pfn)
   fb_pfn = memory_bm_next_pfn(forbidden_pages_map);
  if (fr_pfn < fb_pfn)
   fr_pfn = memory_bm_next_pfn(free_pages_map);
 } while (fb_pfn != fr_pfn);

 if (fr_pfn != BM_END_OF_MAP && pfn_valid(fr_pfn)) {
  struct page *page = pfn_to_page(fr_pfn);

  memory_bm_clear_current(forbidden_pages_map);
  memory_bm_clear_current(free_pages_map);
  __free_page(page);
  goto loop;
 }

out:
 nr_copy_pages = 0;
 nr_meta_pages = 0;
 restore_pblist = NULL;
 buffer = NULL;
 alloc_normal = 0;
 alloc_highmem = 0;
}



static unsigned long preallocate_image_pages(unsigned long nr_pages, gfp_t mask)
{
 unsigned long nr_alloc = 0;

 while (nr_pages > 0) {
  struct page *page;

  page = alloc_image_page(mask);
  if (!page)
   break;
  memory_bm_set_bit(&copy_bm, page_to_pfn(page));
  if (PageHighMem(page))
   alloc_highmem++;
  else
   alloc_normal++;
  nr_pages--;
  nr_alloc++;
 }

 return nr_alloc;
}

static unsigned long preallocate_image_memory(unsigned long nr_pages,
           unsigned long avail_normal)
{
 unsigned long alloc;

 if (avail_normal <= alloc_normal)
  return 0;

 alloc = avail_normal - alloc_normal;
 if (nr_pages < alloc)
  alloc = nr_pages;

 return preallocate_image_pages(alloc, GFP_IMAGE);
}

static unsigned long preallocate_image_highmem(unsigned long nr_pages)
{
 return preallocate_image_pages(nr_pages, GFP_IMAGE | __GFP_HIGHMEM);
}




static unsigned long __fraction(u64 x, u64 multiplier, u64 base)
{
 x *= multiplier;
 do_div(x, base);
 return (unsigned long)x;
}

static unsigned long preallocate_highmem_fraction(unsigned long nr_pages,
      unsigned long highmem,
      unsigned long total)
{
 unsigned long alloc = __fraction(nr_pages, highmem, total);

 return preallocate_image_pages(alloc, GFP_IMAGE | __GFP_HIGHMEM);
}
static inline unsigned long preallocate_image_highmem(unsigned long nr_pages)
{
 return 0;
}

static inline unsigned long preallocate_highmem_fraction(unsigned long nr_pages,
      unsigned long highmem,
      unsigned long total)
{
 return 0;
}




static unsigned long free_unnecessary_pages(void)
{
 unsigned long save, to_free_normal, to_free_highmem, free;

 save = count_data_pages();
 if (alloc_normal >= save) {
  to_free_normal = alloc_normal - save;
  save = 0;
 } else {
  to_free_normal = 0;
  save -= alloc_normal;
 }
 save += count_highmem_pages();
 if (alloc_highmem >= save) {
  to_free_highmem = alloc_highmem - save;
 } else {
  to_free_highmem = 0;
  save -= alloc_highmem;
  if (to_free_normal > save)
   to_free_normal -= save;
  else
   to_free_normal = 0;
 }
 free = to_free_normal + to_free_highmem;

 memory_bm_position_reset(&copy_bm);

 while (to_free_normal > 0 || to_free_highmem > 0) {
  unsigned long pfn = memory_bm_next_pfn(&copy_bm);
  struct page *page = pfn_to_page(pfn);

  if (PageHighMem(page)) {
   if (!to_free_highmem)
    continue;
   to_free_highmem--;
   alloc_highmem--;
  } else {
   if (!to_free_normal)
    continue;
   to_free_normal--;
   alloc_normal--;
  }
  memory_bm_clear_bit(&copy_bm, pfn);
  swsusp_unset_page_forbidden(page);
  swsusp_unset_page_free(page);
  __free_page(page);
 }

 return free;
}
static unsigned long minimum_image_size(unsigned long saveable)
{
 unsigned long size;

 size = global_page_state(NR_SLAB_RECLAIMABLE)
  + global_page_state(NR_ACTIVE_ANON)
  + global_page_state(NR_INACTIVE_ANON)
  + global_page_state(NR_ACTIVE_FILE)
  + global_page_state(NR_INACTIVE_FILE)
  - global_page_state(NR_FILE_MAPPED);

 return saveable <= size ? 0 : saveable - size;
}
int hibernate_preallocate_memory(void)
{
 struct zone *zone;
 unsigned long saveable, size, max_size, count, highmem, pages = 0;
 unsigned long alloc, save_highmem, pages_highmem, avail_normal;
 ktime_t start, stop;
 int error;

 printk(KERN_INFO "PM: Preallocating image memory... ");
 start = ktime_get();

 error = memory_bm_create(&orig_bm, GFP_IMAGE, PG_ANY);
 if (error)
  goto err_out;

 error = memory_bm_create(&copy_bm, GFP_IMAGE, PG_ANY);
 if (error)
  goto err_out;

 alloc_normal = 0;
 alloc_highmem = 0;


 save_highmem = count_highmem_pages();
 saveable = count_data_pages();





 count = saveable;
 saveable += save_highmem;
 highmem = save_highmem;
 size = 0;
 for_each_populated_zone(zone) {
  size += snapshot_additional_pages(zone);
  if (is_highmem(zone))
   highmem += zone_page_state(zone, NR_FREE_PAGES);
  else
   count += zone_page_state(zone, NR_FREE_PAGES);
 }
 avail_normal = count;
 count += highmem;
 count -= totalreserve_pages;


 size += page_key_additional_pages(saveable);


 max_size = (count - (size + PAGES_FOR_IO)) / 2
   - 2 * DIV_ROUND_UP(reserved_size, PAGE_SIZE);

 size = DIV_ROUND_UP(image_size, PAGE_SIZE);
 if (size > max_size)
  size = max_size;





 if (size >= saveable) {
  pages = preallocate_image_highmem(save_highmem);
  pages += preallocate_image_memory(saveable - pages, avail_normal);
  goto out;
 }


 pages = minimum_image_size(saveable);





 if (avail_normal > pages)
  avail_normal -= pages;
 else
  avail_normal = 0;
 if (size < pages)
  size = min_t(unsigned long, pages, max_size);







 shrink_all_memory(saveable - size);
 pages_highmem = preallocate_image_highmem(highmem / 2);
 alloc = count - max_size;
 if (alloc > pages_highmem)
  alloc -= pages_highmem;
 else
  alloc = 0;
 pages = preallocate_image_memory(alloc, avail_normal);
 if (pages < alloc) {

  alloc -= pages;
  pages += pages_highmem;
  pages_highmem = preallocate_image_highmem(alloc);
  if (pages_highmem < alloc)
   goto err_out;
  pages += pages_highmem;




  alloc = (count - pages) - size;
  pages += preallocate_image_highmem(alloc);
 } else {




  alloc = max_size - size;
  size = preallocate_highmem_fraction(alloc, highmem, count);
  pages_highmem += size;
  alloc -= size;
  size = preallocate_image_memory(alloc, avail_normal);
  pages_highmem += preallocate_image_highmem(alloc - size);
  pages += pages_highmem + size;
 }






 pages -= free_unnecessary_pages();

 out:
 stop = ktime_get();
 printk(KERN_CONT "done (allocated %lu pages)\n", pages);
 swsusp_show_speed(start, stop, pages, "Allocated");

 return 0;

 err_out:
 printk(KERN_CONT "\n");
 swsusp_free();
 return -ENOMEM;
}






static unsigned int count_pages_for_highmem(unsigned int nr_highmem)
{
 unsigned int free_highmem = count_free_highmem_pages() + alloc_highmem;

 if (free_highmem >= nr_highmem)
  nr_highmem = 0;
 else
  nr_highmem -= free_highmem;

 return nr_highmem;
}
static unsigned int
count_pages_for_highmem(unsigned int nr_highmem) { return 0; }






static int enough_free_mem(unsigned int nr_pages, unsigned int nr_highmem)
{
 struct zone *zone;
 unsigned int free = alloc_normal;

 for_each_populated_zone(zone)
  if (!is_highmem(zone))
   free += zone_page_state(zone, NR_FREE_PAGES);

 nr_pages += count_pages_for_highmem(nr_highmem);
 pr_debug("PM: Normal pages needed: %u + %u, available pages: %u\n",
  nr_pages, PAGES_FOR_IO, free);

 return free > nr_pages + PAGES_FOR_IO;
}






static inline int get_highmem_buffer(int safe_needed)
{
 buffer = get_image_page(GFP_ATOMIC | __GFP_COLD, safe_needed);
 return buffer ? 0 : -ENOMEM;
}







static inline unsigned int
alloc_highmem_pages(struct memory_bitmap *bm, unsigned int nr_highmem)
{
 unsigned int to_alloc = count_free_highmem_pages();

 if (to_alloc > nr_highmem)
  to_alloc = nr_highmem;

 nr_highmem -= to_alloc;
 while (to_alloc-- > 0) {
  struct page *page;

  page = alloc_image_page(__GFP_HIGHMEM|__GFP_KSWAPD_RECLAIM);
  memory_bm_set_bit(bm, page_to_pfn(page));
 }
 return nr_highmem;
}
static inline int get_highmem_buffer(int safe_needed) { return 0; }

static inline unsigned int
alloc_highmem_pages(struct memory_bitmap *bm, unsigned int n) { return 0; }
static int
swsusp_alloc(struct memory_bitmap *orig_bm, struct memory_bitmap *copy_bm,
  unsigned int nr_pages, unsigned int nr_highmem)
{
 if (nr_highmem > 0) {
  if (get_highmem_buffer(PG_ANY))
   goto err_out;
  if (nr_highmem > alloc_highmem) {
   nr_highmem -= alloc_highmem;
   nr_pages += alloc_highmem_pages(copy_bm, nr_highmem);
  }
 }
 if (nr_pages > alloc_normal) {
  nr_pages -= alloc_normal;
  while (nr_pages-- > 0) {
   struct page *page;

   page = alloc_image_page(GFP_ATOMIC | __GFP_COLD);
   if (!page)
    goto err_out;
   memory_bm_set_bit(copy_bm, page_to_pfn(page));
  }
 }

 return 0;

 err_out:
 swsusp_free();
 return -ENOMEM;
}

asmlinkage __visible int swsusp_save(void)
{
 unsigned int nr_pages, nr_highmem;

 printk(KERN_INFO "PM: Creating hibernation image:\n");

 drain_local_pages(NULL);
 nr_pages = count_data_pages();
 nr_highmem = count_highmem_pages();
 printk(KERN_INFO "PM: Need to copy %u pages\n", nr_pages + nr_highmem);

 if (!enough_free_mem(nr_pages, nr_highmem)) {
  printk(KERN_ERR "PM: Not enough free memory\n");
  return -ENOMEM;
 }

 if (swsusp_alloc(&orig_bm, &copy_bm, nr_pages, nr_highmem)) {
  printk(KERN_ERR "PM: Memory allocation failed\n");
  return -ENOMEM;
 }




 drain_local_pages(NULL);
 copy_data_pages(&copy_bm, &orig_bm);







 nr_pages += nr_highmem;
 nr_copy_pages = nr_pages;
 nr_meta_pages = DIV_ROUND_UP(nr_pages * sizeof(long), PAGE_SIZE);

 printk(KERN_INFO "PM: Hibernation image created (%d pages copied)\n",
  nr_pages);

 return 0;
}

static int init_header_complete(struct swsusp_info *info)
{
 memcpy(&info->uts, init_utsname(), sizeof(struct new_utsname));
 info->version_code = LINUX_VERSION_CODE;
 return 0;
}

static char *check_image_kernel(struct swsusp_info *info)
{
 if (info->version_code != LINUX_VERSION_CODE)
  return "kernel version";
 if (strcmp(info->uts.sysname,init_utsname()->sysname))
  return "system type";
 if (strcmp(info->uts.release,init_utsname()->release))
  return "kernel release";
 if (strcmp(info->uts.version,init_utsname()->version))
  return "version";
 if (strcmp(info->uts.machine,init_utsname()->machine))
  return "machine";
 return NULL;
}

unsigned long snapshot_get_image_size(void)
{
 return nr_copy_pages + nr_meta_pages + 1;
}

static int init_header(struct swsusp_info *info)
{
 memset(info, 0, sizeof(struct swsusp_info));
 info->num_physpages = get_num_physpages();
 info->image_pages = nr_copy_pages;
 info->pages = snapshot_get_image_size();
 info->size = info->pages;
 info->size <<= PAGE_SHIFT;
 return init_header_complete(info);
}






static inline void
pack_pfns(unsigned long *buf, struct memory_bitmap *bm)
{
 int j;

 for (j = 0; j < PAGE_SIZE / sizeof(long); j++) {
  buf[j] = memory_bm_next_pfn(bm);
  if (unlikely(buf[j] == BM_END_OF_MAP))
   break;

  page_key_read(buf + j);
 }
}
int snapshot_read_next(struct snapshot_handle *handle)
{
 if (handle->cur > nr_meta_pages + nr_copy_pages)
  return 0;

 if (!buffer) {

  buffer = get_image_page(GFP_ATOMIC, PG_ANY);
  if (!buffer)
   return -ENOMEM;
 }
 if (!handle->cur) {
  int error;

  error = init_header((struct swsusp_info *)buffer);
  if (error)
   return error;
  handle->buffer = buffer;
  memory_bm_position_reset(&orig_bm);
  memory_bm_position_reset(&copy_bm);
 } else if (handle->cur <= nr_meta_pages) {
  clear_page(buffer);
  pack_pfns(buffer, &orig_bm);
 } else {
  struct page *page;

  page = pfn_to_page(memory_bm_next_pfn(&copy_bm));
  if (PageHighMem(page)) {




   void *kaddr;

   kaddr = kmap_atomic(page);
   copy_page(buffer, kaddr);
   kunmap_atomic(kaddr);
   handle->buffer = buffer;
  } else {
   handle->buffer = page_address(page);
  }
 }
 handle->cur++;
 return PAGE_SIZE;
}







static int mark_unsafe_pages(struct memory_bitmap *bm)
{
 struct zone *zone;
 unsigned long pfn, max_zone_pfn;


 for_each_populated_zone(zone) {
  max_zone_pfn = zone_end_pfn(zone);
  for (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)
   if (pfn_valid(pfn))
    swsusp_unset_page_free(pfn_to_page(pfn));
 }


 memory_bm_position_reset(bm);
 do {
  pfn = memory_bm_next_pfn(bm);
  if (likely(pfn != BM_END_OF_MAP)) {
   if (likely(pfn_valid(pfn)))
    swsusp_set_page_free(pfn_to_page(pfn));
   else
    return -EFAULT;
  }
 } while (pfn != BM_END_OF_MAP);

 allocated_unsafe_pages = 0;

 return 0;
}

static void
duplicate_memory_bitmap(struct memory_bitmap *dst, struct memory_bitmap *src)
{
 unsigned long pfn;

 memory_bm_position_reset(src);
 pfn = memory_bm_next_pfn(src);
 while (pfn != BM_END_OF_MAP) {
  memory_bm_set_bit(dst, pfn);
  pfn = memory_bm_next_pfn(src);
 }
}

static int check_header(struct swsusp_info *info)
{
 char *reason;

 reason = check_image_kernel(info);
 if (!reason && info->num_physpages != get_num_physpages())
  reason = "memory size";
 if (reason) {
  printk(KERN_ERR "PM: Image mismatch: %s\n", reason);
  return -EPERM;
 }
 return 0;
}





static int
load_header(struct swsusp_info *info)
{
 int error;

 restore_pblist = NULL;
 error = check_header(info);
 if (!error) {
  nr_copy_pages = info->image_pages;
  nr_meta_pages = info->pages - info->image_pages - 1;
 }
 return error;
}





static int unpack_orig_pfns(unsigned long *buf, struct memory_bitmap *bm)
{
 int j;

 for (j = 0; j < PAGE_SIZE / sizeof(long); j++) {
  if (unlikely(buf[j] == BM_END_OF_MAP))
   break;


  page_key_memorize(buf + j);

  if (memory_bm_pfn_present(bm, buf[j]))
   memory_bm_set_bit(bm, buf[j]);
  else
   return -EFAULT;
 }

 return 0;
}




static struct linked_page *safe_pages_list;





struct highmem_pbe {
 struct page *copy_page;
 struct page *orig_page;
 struct highmem_pbe *next;
};






static struct highmem_pbe *highmem_pblist;







static unsigned int count_highmem_image_pages(struct memory_bitmap *bm)
{
 unsigned long pfn;
 unsigned int cnt = 0;

 memory_bm_position_reset(bm);
 pfn = memory_bm_next_pfn(bm);
 while (pfn != BM_END_OF_MAP) {
  if (PageHighMem(pfn_to_page(pfn)))
   cnt++;

  pfn = memory_bm_next_pfn(bm);
 }
 return cnt;
}
static unsigned int safe_highmem_pages;

static struct memory_bitmap *safe_highmem_bm;

static int
prepare_highmem_image(struct memory_bitmap *bm, unsigned int *nr_highmem_p)
{
 unsigned int to_alloc;

 if (memory_bm_create(bm, GFP_ATOMIC, PG_SAFE))
  return -ENOMEM;

 if (get_highmem_buffer(PG_SAFE))
  return -ENOMEM;

 to_alloc = count_free_highmem_pages();
 if (to_alloc > *nr_highmem_p)
  to_alloc = *nr_highmem_p;
 else
  *nr_highmem_p = to_alloc;

 safe_highmem_pages = 0;
 while (to_alloc-- > 0) {
  struct page *page;

  page = alloc_page(__GFP_HIGHMEM);
  if (!swsusp_page_is_free(page)) {

   memory_bm_set_bit(bm, page_to_pfn(page));
   safe_highmem_pages++;
  }

  swsusp_set_page_forbidden(page);
  swsusp_set_page_free(page);
 }
 memory_bm_position_reset(bm);
 safe_highmem_bm = bm;
 return 0;
}
static struct page *last_highmem_page;

static void *
get_highmem_page_buffer(struct page *page, struct chain_allocator *ca)
{
 struct highmem_pbe *pbe;
 void *kaddr;

 if (swsusp_page_is_forbidden(page) && swsusp_page_is_free(page)) {



  last_highmem_page = page;
  return buffer;
 }



 pbe = chain_alloc(ca, sizeof(struct highmem_pbe));
 if (!pbe) {
  swsusp_free();
  return ERR_PTR(-ENOMEM);
 }
 pbe->orig_page = page;
 if (safe_highmem_pages > 0) {
  struct page *tmp;


  kaddr = buffer;
  tmp = pfn_to_page(memory_bm_next_pfn(safe_highmem_bm));
  safe_highmem_pages--;
  last_highmem_page = tmp;
  pbe->copy_page = tmp;
 } else {

  kaddr = safe_pages_list;
  safe_pages_list = safe_pages_list->next;
  pbe->copy_page = virt_to_page(kaddr);
 }
 pbe->next = highmem_pblist;
 highmem_pblist = pbe;
 return kaddr;
}







static void copy_last_highmem_page(void)
{
 if (last_highmem_page) {
  void *dst;

  dst = kmap_atomic(last_highmem_page);
  copy_page(dst, buffer);
  kunmap_atomic(dst);
  last_highmem_page = NULL;
 }
}

static inline int last_highmem_page_copied(void)
{
 return !last_highmem_page;
}

static inline void free_highmem_data(void)
{
 if (safe_highmem_bm)
  memory_bm_free(safe_highmem_bm, PG_UNSAFE_CLEAR);

 if (buffer)
  free_image_page(buffer, PG_UNSAFE_CLEAR);
}
static unsigned int
count_highmem_image_pages(struct memory_bitmap *bm) { return 0; }

static inline int
prepare_highmem_image(struct memory_bitmap *bm, unsigned int *nr_highmem_p)
{
 return 0;
}

static inline void *
get_highmem_page_buffer(struct page *page, struct chain_allocator *ca)
{
 return ERR_PTR(-EINVAL);
}

static inline void copy_last_highmem_page(void) {}
static inline int last_highmem_page_copied(void) { return 1; }
static inline void free_highmem_data(void) {}

static int
prepare_image(struct memory_bitmap *new_bm, struct memory_bitmap *bm)
{
 unsigned int nr_pages, nr_highmem;
 struct linked_page *sp_list, *lp;
 int error;


 free_image_page(buffer, PG_UNSAFE_CLEAR);
 buffer = NULL;

 nr_highmem = count_highmem_image_pages(bm);
 error = mark_unsafe_pages(bm);
 if (error)
  goto Free;

 error = memory_bm_create(new_bm, GFP_ATOMIC, PG_SAFE);
 if (error)
  goto Free;

 duplicate_memory_bitmap(new_bm, bm);
 memory_bm_free(bm, PG_UNSAFE_KEEP);
 if (nr_highmem > 0) {
  error = prepare_highmem_image(bm, &nr_highmem);
  if (error)
   goto Free;
 }






 sp_list = NULL;

 nr_pages = nr_copy_pages - nr_highmem - allocated_unsafe_pages;
 nr_pages = DIV_ROUND_UP(nr_pages, PBES_PER_LINKED_PAGE);
 while (nr_pages > 0) {
  lp = get_image_page(GFP_ATOMIC, PG_SAFE);
  if (!lp) {
   error = -ENOMEM;
   goto Free;
  }
  lp->next = sp_list;
  sp_list = lp;
  nr_pages--;
 }

 safe_pages_list = NULL;
 nr_pages = nr_copy_pages - nr_highmem - allocated_unsafe_pages;
 while (nr_pages > 0) {
  lp = (struct linked_page *)get_zeroed_page(GFP_ATOMIC);
  if (!lp) {
   error = -ENOMEM;
   goto Free;
  }
  if (!swsusp_page_is_free(virt_to_page(lp))) {

   lp->next = safe_pages_list;
   safe_pages_list = lp;
  }

  swsusp_set_page_forbidden(virt_to_page(lp));
  swsusp_set_page_free(virt_to_page(lp));
  nr_pages--;
 }

 while (sp_list) {
  lp = sp_list->next;
  free_image_page(sp_list, PG_UNSAFE_CLEAR);
  sp_list = lp;
 }
 return 0;

 Free:
 swsusp_free();
 return error;
}






static void *get_buffer(struct memory_bitmap *bm, struct chain_allocator *ca)
{
 struct pbe *pbe;
 struct page *page;
 unsigned long pfn = memory_bm_next_pfn(bm);

 if (pfn == BM_END_OF_MAP)
  return ERR_PTR(-EFAULT);

 page = pfn_to_page(pfn);
 if (PageHighMem(page))
  return get_highmem_page_buffer(page, ca);

 if (swsusp_page_is_forbidden(page) && swsusp_page_is_free(page))



  return page_address(page);




 pbe = chain_alloc(ca, sizeof(struct pbe));
 if (!pbe) {
  swsusp_free();
  return ERR_PTR(-ENOMEM);
 }
 pbe->orig_address = page_address(page);
 pbe->address = safe_pages_list;
 safe_pages_list = safe_pages_list->next;
 pbe->next = restore_pblist;
 restore_pblist = pbe;
 return pbe->address;
}
int snapshot_write_next(struct snapshot_handle *handle)
{
 static struct chain_allocator ca;
 int error = 0;


 if (handle->cur > 1 && handle->cur > nr_meta_pages + nr_copy_pages)
  return 0;

 handle->sync_read = 1;

 if (!handle->cur) {
  if (!buffer)

   buffer = get_image_page(GFP_ATOMIC, PG_ANY);

  if (!buffer)
   return -ENOMEM;

  handle->buffer = buffer;
 } else if (handle->cur == 1) {
  error = load_header(buffer);
  if (error)
   return error;

  error = memory_bm_create(&copy_bm, GFP_ATOMIC, PG_ANY);
  if (error)
   return error;


  error = page_key_alloc(nr_copy_pages);
  if (error)
   return error;

 } else if (handle->cur <= nr_meta_pages + 1) {
  error = unpack_orig_pfns(buffer, &copy_bm);
  if (error)
   return error;

  if (handle->cur == nr_meta_pages + 1) {
   error = prepare_image(&orig_bm, &copy_bm);
   if (error)
    return error;

   chain_init(&ca, GFP_ATOMIC, PG_SAFE);
   memory_bm_position_reset(&orig_bm);
   restore_pblist = NULL;
   handle->buffer = get_buffer(&orig_bm, &ca);
   handle->sync_read = 0;
   if (IS_ERR(handle->buffer))
    return PTR_ERR(handle->buffer);
  }
 } else {
  copy_last_highmem_page();

  page_key_write(handle->buffer);
  handle->buffer = get_buffer(&orig_bm, &ca);
  if (IS_ERR(handle->buffer))
   return PTR_ERR(handle->buffer);
  if (handle->buffer != buffer)
   handle->sync_read = 0;
 }
 handle->cur++;
 return PAGE_SIZE;
}
void snapshot_write_finalize(struct snapshot_handle *handle)
{
 copy_last_highmem_page();

 page_key_write(handle->buffer);
 page_key_free();

 if (handle->cur > 1 && handle->cur > nr_meta_pages + nr_copy_pages) {
  memory_bm_free(&orig_bm, PG_UNSAFE_CLEAR);
  free_highmem_data();
 }
}

int snapshot_image_loaded(struct snapshot_handle *handle)
{
 return !(!nr_copy_pages || !last_highmem_page_copied() ||
   handle->cur <= nr_meta_pages + nr_copy_pages);
}


static inline void
swap_two_pages_data(struct page *p1, struct page *p2, void *buf)
{
 void *kaddr1, *kaddr2;

 kaddr1 = kmap_atomic(p1);
 kaddr2 = kmap_atomic(p2);
 copy_page(buf, kaddr1);
 copy_page(kaddr1, kaddr2);
 copy_page(kaddr2, buf);
 kunmap_atomic(kaddr2);
 kunmap_atomic(kaddr1);
}
int restore_highmem(void)
{
 struct highmem_pbe *pbe = highmem_pblist;
 void *buf;

 if (!pbe)
  return 0;

 buf = get_image_page(GFP_ATOMIC, PG_SAFE);
 if (!buf)
  return -ENOMEM;

 while (pbe) {
  swap_two_pages_data(pbe->copy_page, pbe->orig_page, buf);
  pbe = pbe->next;
 }
 free_image_page(buf, PG_UNSAFE_CLEAR);
 return 0;
}


irq_cpustat_t irq_stat[NR_CPUS] ____cacheline_aligned;
EXPORT_SYMBOL(irq_stat);

static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;

DEFINE_PER_CPU(struct task_struct *, ksoftirqd);

const char * const softirq_to_name[NR_SOFTIRQS] = {
 "HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "BLOCK_IOPOLL",
 "TASKLET", "SCHED", "HRTIMER", "RCU"
};







static void wakeup_softirqd(void)
{

 struct task_struct *tsk = __this_cpu_read(ksoftirqd);

 if (tsk && tsk->state != TASK_RUNNING)
  wake_up_process(tsk);
}
void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
{
 unsigned long flags;

 WARN_ON_ONCE(in_irq());

 raw_local_irq_save(flags);







 __preempt_count_add(cnt);



 if (softirq_count() == (cnt & SOFTIRQ_MASK))
  trace_softirqs_off(ip);
 raw_local_irq_restore(flags);

 if (preempt_count() == cnt) {
  current->preempt_disable_ip = get_lock_parent_ip();
  trace_preempt_off(CALLER_ADDR0, get_lock_parent_ip());
 }
}
EXPORT_SYMBOL(__local_bh_disable_ip);

static void __local_bh_enable(unsigned int cnt)
{
 WARN_ON_ONCE(!irqs_disabled());

 if (softirq_count() == (cnt & SOFTIRQ_MASK))
  trace_softirqs_on(_RET_IP_);
 preempt_count_sub(cnt);
}






void _local_bh_enable(void)
{
 WARN_ON_ONCE(in_irq());
 __local_bh_enable(SOFTIRQ_DISABLE_OFFSET);
}
EXPORT_SYMBOL(_local_bh_enable);

void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
{
 WARN_ON_ONCE(in_irq() || irqs_disabled());
 local_irq_disable();



 if (softirq_count() == SOFTIRQ_DISABLE_OFFSET)
  trace_softirqs_on(ip);




 preempt_count_sub(cnt - 1);

 if (unlikely(!in_interrupt() && local_softirq_pending())) {




  do_softirq();
 }

 preempt_count_dec();
 local_irq_enable();
 preempt_check_resched();
}
EXPORT_SYMBOL(__local_bh_enable_ip);







static inline bool lockdep_softirq_start(void)
{
 bool in_hardirq = false;

 if (trace_hardirq_context(current)) {
  in_hardirq = true;
  trace_hardirq_exit();
 }

 lockdep_softirq_enter();

 return in_hardirq;
}

static inline void lockdep_softirq_end(bool in_hardirq)
{
 lockdep_softirq_exit();

 if (in_hardirq)
  trace_hardirq_enter();
}
static inline bool lockdep_softirq_start(void) { return false; }
static inline void lockdep_softirq_end(bool in_hardirq) { }

asmlinkage __visible void __softirq_entry __do_softirq(void)
{
 unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
 unsigned long old_flags = current->flags;
 int max_restart = MAX_SOFTIRQ_RESTART;
 struct softirq_action *h;
 bool in_hardirq;
 __u32 pending;
 int softirq_bit;






 current->flags &= ~PF_MEMALLOC;

 pending = local_softirq_pending();
 account_irq_enter_time(current);

 __local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
 in_hardirq = lockdep_softirq_start();

restart:

 set_softirq_pending(0);

 local_irq_enable();

 h = softirq_vec;

 while ((softirq_bit = ffs(pending))) {
  unsigned int vec_nr;
  int prev_count;

  h += softirq_bit - 1;

  vec_nr = h - softirq_vec;
  prev_count = preempt_count();

  kstat_incr_softirqs_this_cpu(vec_nr);

  trace_softirq_entry(vec_nr);
  h->action(h);
  trace_softirq_exit(vec_nr);
  if (unlikely(prev_count != preempt_count())) {
   pr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
          vec_nr, softirq_to_name[vec_nr], h->action,
          prev_count, preempt_count());
   preempt_count_set(prev_count);
  }
  h++;
  pending >>= softirq_bit;
 }

 rcu_bh_qs();
 local_irq_disable();

 pending = local_softirq_pending();
 if (pending) {
  if (time_before(jiffies, end) && !need_resched() &&
      --max_restart)
   goto restart;

  wakeup_softirqd();
 }

 lockdep_softirq_end(in_hardirq);
 account_irq_exit_time(current);
 __local_bh_enable(SOFTIRQ_OFFSET);
 WARN_ON_ONCE(in_interrupt());
 tsk_restore_flags(current, old_flags, PF_MEMALLOC);
}

asmlinkage __visible void do_softirq(void)
{
 __u32 pending;
 unsigned long flags;

 if (in_interrupt())
  return;

 local_irq_save(flags);

 pending = local_softirq_pending();

 if (pending)
  do_softirq_own_stack();

 local_irq_restore(flags);
}




void irq_enter(void)
{
 rcu_irq_enter();
 if (is_idle_task(current) && !in_interrupt()) {




  local_bh_disable();
  tick_irq_enter();
  _local_bh_enable();
 }

 __irq_enter();
}

static inline void invoke_softirq(void)
{
 if (!force_irqthreads) {





  __do_softirq();





  do_softirq_own_stack();
 } else {
  wakeup_softirqd();
 }
}

static inline void tick_irq_exit(void)
{
 int cpu = smp_processor_id();


 if ((idle_cpu(cpu) && !need_resched()) || tick_nohz_full_cpu(cpu)) {
  if (!in_interrupt())
   tick_nohz_irq_exit();
 }
}




void irq_exit(void)
{
 local_irq_disable();
 WARN_ON_ONCE(!irqs_disabled());

 account_irq_exit_time(current);
 preempt_count_sub(HARDIRQ_OFFSET);
 if (!in_interrupt() && local_softirq_pending())
  invoke_softirq();

 tick_irq_exit();
 rcu_irq_exit();
 trace_hardirq_exit();
}




inline void raise_softirq_irqoff(unsigned int nr)
{
 __raise_softirq_irqoff(nr);
 if (!in_interrupt())
  wakeup_softirqd();
}

void raise_softirq(unsigned int nr)
{
 unsigned long flags;

 local_irq_save(flags);
 raise_softirq_irqoff(nr);
 local_irq_restore(flags);
}

void __raise_softirq_irqoff(unsigned int nr)
{
 trace_softirq_raise(nr);
 or_softirq_pending(1UL << nr);
}

void open_softirq(int nr, void (*action)(struct softirq_action *))
{
 softirq_vec[nr].action = action;
}




struct tasklet_head {
 struct tasklet_struct *head;
 struct tasklet_struct **tail;
};

static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);
static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);

void __tasklet_schedule(struct tasklet_struct *t)
{
 unsigned long flags;

 local_irq_save(flags);
 t->next = NULL;
 *__this_cpu_read(tasklet_vec.tail) = t;
 __this_cpu_write(tasklet_vec.tail, &(t->next));
 raise_softirq_irqoff(TASKLET_SOFTIRQ);
 local_irq_restore(flags);
}
EXPORT_SYMBOL(__tasklet_schedule);

void __tasklet_hi_schedule(struct tasklet_struct *t)
{
 unsigned long flags;

 local_irq_save(flags);
 t->next = NULL;
 *__this_cpu_read(tasklet_hi_vec.tail) = t;
 __this_cpu_write(tasklet_hi_vec.tail, &(t->next));
 raise_softirq_irqoff(HI_SOFTIRQ);
 local_irq_restore(flags);
}
EXPORT_SYMBOL(__tasklet_hi_schedule);

void __tasklet_hi_schedule_first(struct tasklet_struct *t)
{
 BUG_ON(!irqs_disabled());

 t->next = __this_cpu_read(tasklet_hi_vec.head);
 __this_cpu_write(tasklet_hi_vec.head, t);
 __raise_softirq_irqoff(HI_SOFTIRQ);
}
EXPORT_SYMBOL(__tasklet_hi_schedule_first);

static void tasklet_action(struct softirq_action *a)
{
 struct tasklet_struct *list;

 local_irq_disable();
 list = __this_cpu_read(tasklet_vec.head);
 __this_cpu_write(tasklet_vec.head, NULL);
 __this_cpu_write(tasklet_vec.tail, this_cpu_ptr(&tasklet_vec.head));
 local_irq_enable();

 while (list) {
  struct tasklet_struct *t = list;

  list = list->next;

  if (tasklet_trylock(t)) {
   if (!atomic_read(&t->count)) {
    if (!test_and_clear_bit(TASKLET_STATE_SCHED,
       &t->state))
     BUG();
    t->func(t->data);
    tasklet_unlock(t);
    continue;
   }
   tasklet_unlock(t);
  }

  local_irq_disable();
  t->next = NULL;
  *__this_cpu_read(tasklet_vec.tail) = t;
  __this_cpu_write(tasklet_vec.tail, &(t->next));
  __raise_softirq_irqoff(TASKLET_SOFTIRQ);
  local_irq_enable();
 }
}

static void tasklet_hi_action(struct softirq_action *a)
{
 struct tasklet_struct *list;

 local_irq_disable();
 list = __this_cpu_read(tasklet_hi_vec.head);
 __this_cpu_write(tasklet_hi_vec.head, NULL);
 __this_cpu_write(tasklet_hi_vec.tail, this_cpu_ptr(&tasklet_hi_vec.head));
 local_irq_enable();

 while (list) {
  struct tasklet_struct *t = list;

  list = list->next;

  if (tasklet_trylock(t)) {
   if (!atomic_read(&t->count)) {
    if (!test_and_clear_bit(TASKLET_STATE_SCHED,
       &t->state))
     BUG();
    t->func(t->data);
    tasklet_unlock(t);
    continue;
   }
   tasklet_unlock(t);
  }

  local_irq_disable();
  t->next = NULL;
  *__this_cpu_read(tasklet_hi_vec.tail) = t;
  __this_cpu_write(tasklet_hi_vec.tail, &(t->next));
  __raise_softirq_irqoff(HI_SOFTIRQ);
  local_irq_enable();
 }
}

void tasklet_init(struct tasklet_struct *t,
    void (*func)(unsigned long), unsigned long data)
{
 t->next = NULL;
 t->state = 0;
 atomic_set(&t->count, 0);
 t->func = func;
 t->data = data;
}
EXPORT_SYMBOL(tasklet_init);

void tasklet_kill(struct tasklet_struct *t)
{
 if (in_interrupt())
  pr_notice("Attempt to kill tasklet from interrupt\n");

 while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
  do {
   yield();
  } while (test_bit(TASKLET_STATE_SCHED, &t->state));
 }
 tasklet_unlock_wait(t);
 clear_bit(TASKLET_STATE_SCHED, &t->state);
}
EXPORT_SYMBOL(tasklet_kill);
static enum hrtimer_restart __hrtimer_tasklet_trampoline(struct hrtimer *timer)
{
 struct tasklet_hrtimer *ttimer =
  container_of(timer, struct tasklet_hrtimer, timer);

 tasklet_hi_schedule(&ttimer->tasklet);
 return HRTIMER_NORESTART;
}





static void __tasklet_hrtimer_trampoline(unsigned long data)
{
 struct tasklet_hrtimer *ttimer = (void *)data;
 enum hrtimer_restart restart;

 restart = ttimer->function(&ttimer->timer);
 if (restart != HRTIMER_NORESTART)
  hrtimer_restart(&ttimer->timer);
}
void tasklet_hrtimer_init(struct tasklet_hrtimer *ttimer,
     enum hrtimer_restart (*function)(struct hrtimer *),
     clockid_t which_clock, enum hrtimer_mode mode)
{
 hrtimer_init(&ttimer->timer, which_clock, mode);
 ttimer->timer.function = __hrtimer_tasklet_trampoline;
 tasklet_init(&ttimer->tasklet, __tasklet_hrtimer_trampoline,
       (unsigned long)ttimer);
 ttimer->function = function;
}
EXPORT_SYMBOL_GPL(tasklet_hrtimer_init);

void __init softirq_init(void)
{
 int cpu;

 for_each_possible_cpu(cpu) {
  per_cpu(tasklet_vec, cpu).tail =
   &per_cpu(tasklet_vec, cpu).head;
  per_cpu(tasklet_hi_vec, cpu).tail =
   &per_cpu(tasklet_hi_vec, cpu).head;
 }

 open_softirq(TASKLET_SOFTIRQ, tasklet_action);
 open_softirq(HI_SOFTIRQ, tasklet_hi_action);
}

static int ksoftirqd_should_run(unsigned int cpu)
{
 return local_softirq_pending();
}

static void run_ksoftirqd(unsigned int cpu)
{
 local_irq_disable();
 if (local_softirq_pending()) {




  __do_softirq();
  local_irq_enable();
  cond_resched_rcu_qs();
  return;
 }
 local_irq_enable();
}

void tasklet_kill_immediate(struct tasklet_struct *t, unsigned int cpu)
{
 struct tasklet_struct **i;

 BUG_ON(cpu_online(cpu));
 BUG_ON(test_bit(TASKLET_STATE_RUN, &t->state));

 if (!test_bit(TASKLET_STATE_SCHED, &t->state))
  return;


 for (i = &per_cpu(tasklet_vec, cpu).head; *i; i = &(*i)->next) {
  if (*i == t) {
   *i = t->next;

   if (*i == NULL)
    per_cpu(tasklet_vec, cpu).tail = i;
   return;
  }
 }
 BUG();
}

static void takeover_tasklets(unsigned int cpu)
{

 local_irq_disable();


 if (&per_cpu(tasklet_vec, cpu).head != per_cpu(tasklet_vec, cpu).tail) {
  *__this_cpu_read(tasklet_vec.tail) = per_cpu(tasklet_vec, cpu).head;
  this_cpu_write(tasklet_vec.tail, per_cpu(tasklet_vec, cpu).tail);
  per_cpu(tasklet_vec, cpu).head = NULL;
  per_cpu(tasklet_vec, cpu).tail = &per_cpu(tasklet_vec, cpu).head;
 }
 raise_softirq_irqoff(TASKLET_SOFTIRQ);

 if (&per_cpu(tasklet_hi_vec, cpu).head != per_cpu(tasklet_hi_vec, cpu).tail) {
  *__this_cpu_read(tasklet_hi_vec.tail) = per_cpu(tasklet_hi_vec, cpu).head;
  __this_cpu_write(tasklet_hi_vec.tail, per_cpu(tasklet_hi_vec, cpu).tail);
  per_cpu(tasklet_hi_vec, cpu).head = NULL;
  per_cpu(tasklet_hi_vec, cpu).tail = &per_cpu(tasklet_hi_vec, cpu).head;
 }
 raise_softirq_irqoff(HI_SOFTIRQ);

 local_irq_enable();
}

static int cpu_callback(struct notifier_block *nfb, unsigned long action,
   void *hcpu)
{
 switch (action) {
 case CPU_DEAD:
 case CPU_DEAD_FROZEN:
  takeover_tasklets((unsigned long)hcpu);
  break;
 }
 return NOTIFY_OK;
}

static struct notifier_block cpu_nfb = {
 .notifier_call = cpu_callback
};

static struct smp_hotplug_thread softirq_threads = {
 .store = &ksoftirqd,
 .thread_should_run = ksoftirqd_should_run,
 .thread_fn = run_ksoftirqd,
 .thread_comm = "ksoftirqd/%u",
};

static __init int spawn_ksoftirqd(void)
{
 register_cpu_notifier(&cpu_nfb);

 BUG_ON(smpboot_register_percpu_thread(&softirq_threads));

 return 0;
}
early_initcall(spawn_ksoftirqd);






int __init __weak early_irq_init(void)
{
 return 0;
}

int __init __weak arch_probe_nr_irqs(void)
{
 return NR_IRQS_LEGACY;
}

int __init __weak arch_early_irq_init(void)
{
 return 0;
}

unsigned int __weak arch_dynirq_lower_bound(unsigned int from)
{
 return from;
}


static int irqfixup __read_mostly;

static void poll_spurious_irqs(unsigned long dummy);
static DEFINE_TIMER(poll_spurious_irq_timer, poll_spurious_irqs, 0, 0);
static int irq_poll_cpu;
static atomic_t irq_poll_active;
bool irq_wait_for_poll(struct irq_desc *desc)
{
 if (WARN_ONCE(irq_poll_cpu == smp_processor_id(),
        "irq poll in progress on cpu %d for irq %d\n",
        smp_processor_id(), desc->irq_data.irq))
  return false;

 do {
  raw_spin_unlock(&desc->lock);
  while (irqd_irq_inprogress(&desc->irq_data))
   cpu_relax();
  raw_spin_lock(&desc->lock);
 } while (irqd_irq_inprogress(&desc->irq_data));

 return !irqd_irq_disabled(&desc->irq_data) && desc->action;
 return false;
}





static int try_one_irq(struct irq_desc *desc, bool force)
{
 irqreturn_t ret = IRQ_NONE;
 struct irqaction *action;

 raw_spin_lock(&desc->lock);





 if (irq_settings_is_per_cpu(desc) ||
     irq_settings_is_nested_thread(desc) ||
     irq_settings_is_polled(desc))
  goto out;





 if (irqd_irq_disabled(&desc->irq_data) && !force)
  goto out;





 action = desc->action;
 if (!action || !(action->flags & IRQF_SHARED) ||
     (action->flags & __IRQF_TIMER))
  goto out;


 if (irqd_irq_inprogress(&desc->irq_data)) {




  desc->istate |= IRQS_PENDING;
  goto out;
 }


 desc->istate |= IRQS_POLL_INPROGRESS;
 do {
  if (handle_irq_event(desc) == IRQ_HANDLED)
   ret = IRQ_HANDLED;

  action = desc->action;
 } while ((desc->istate & IRQS_PENDING) && action);
 desc->istate &= ~IRQS_POLL_INPROGRESS;
out:
 raw_spin_unlock(&desc->lock);
 return ret == IRQ_HANDLED;
}

static int misrouted_irq(int irq)
{
 struct irq_desc *desc;
 int i, ok = 0;

 if (atomic_inc_return(&irq_poll_active) != 1)
  goto out;

 irq_poll_cpu = smp_processor_id();

 for_each_irq_desc(i, desc) {
  if (!i)
    continue;

  if (i == irq)
   continue;

  if (try_one_irq(desc, false))
   ok = 1;
 }
out:
 atomic_dec(&irq_poll_active);

 return ok;
}

static void poll_spurious_irqs(unsigned long dummy)
{
 struct irq_desc *desc;
 int i;

 if (atomic_inc_return(&irq_poll_active) != 1)
  goto out;
 irq_poll_cpu = smp_processor_id();

 for_each_irq_desc(i, desc) {
  unsigned int state;

  if (!i)
    continue;


  state = desc->istate;
  barrier();
  if (!(state & IRQS_SPURIOUS_DISABLED))
   continue;

  local_irq_disable();
  try_one_irq(desc, true);
  local_irq_enable();
 }
out:
 atomic_dec(&irq_poll_active);
 mod_timer(&poll_spurious_irq_timer,
    jiffies + POLL_SPURIOUS_IRQ_INTERVAL);
}

static inline int bad_action_ret(irqreturn_t action_ret)
{
 if (likely(action_ret <= (IRQ_HANDLED | IRQ_WAKE_THREAD)))
  return 0;
 return 1;
}
static void __report_bad_irq(struct irq_desc *desc, irqreturn_t action_ret)
{
 unsigned int irq = irq_desc_get_irq(desc);
 struct irqaction *action;
 unsigned long flags;

 if (bad_action_ret(action_ret)) {
  printk(KERN_ERR "irq event %d: bogus return value %x\n",
    irq, action_ret);
 } else {
  printk(KERN_ERR "irq %d: nobody cared (try booting with "
    "the \"irqpoll\" option)\n", irq);
 }
 dump_stack();
 printk(KERN_ERR "handlers:\n");







 raw_spin_lock_irqsave(&desc->lock, flags);
 for_each_action_of_desc(desc, action) {
  printk(KERN_ERR "[<%p>] %pf", action->handler, action->handler);
  if (action->thread_fn)
   printk(KERN_CONT " threaded [<%p>] %pf",
     action->thread_fn, action->thread_fn);
  printk(KERN_CONT "\n");
 }
 raw_spin_unlock_irqrestore(&desc->lock, flags);
}

static void report_bad_irq(struct irq_desc *desc, irqreturn_t action_ret)
{
 static int count = 100;

 if (count > 0) {
  count--;
  __report_bad_irq(desc, action_ret);
 }
}

static inline int
try_misrouted_irq(unsigned int irq, struct irq_desc *desc,
    irqreturn_t action_ret)
{
 struct irqaction *action;

 if (!irqfixup)
  return 0;


 if (action_ret == IRQ_NONE)
  return 1;






 if (irqfixup < 2)
  return 0;

 if (!irq)
  return 1;







 action = desc->action;
 barrier();
 return action && (action->flags & IRQF_IRQPOLL);
}


void note_interrupt(struct irq_desc *desc, irqreturn_t action_ret)
{
 unsigned int irq;

 if (desc->istate & IRQS_POLL_INPROGRESS ||
     irq_settings_is_polled(desc))
  return;

 if (bad_action_ret(action_ret)) {
  report_bad_irq(desc, action_ret);
  return;
 }
 if (action_ret & IRQ_WAKE_THREAD) {






  if (action_ret == IRQ_WAKE_THREAD) {
   int handled;
   if (!(desc->threads_handled_last & SPURIOUS_DEFERRED)) {
    desc->threads_handled_last |= SPURIOUS_DEFERRED;
    return;
   }
   handled = atomic_read(&desc->threads_handled);
   handled |= SPURIOUS_DEFERRED;
   if (handled != desc->threads_handled_last) {
    action_ret = IRQ_HANDLED;
    desc->threads_handled_last = handled;
   } else {
    action_ret = IRQ_NONE;
   }
  } else {
   desc->threads_handled_last &= ~SPURIOUS_DEFERRED;
  }
 }

 if (unlikely(action_ret == IRQ_NONE)) {






  if (time_after(jiffies, desc->last_unhandled + HZ/10))
   desc->irqs_unhandled = 1;
  else
   desc->irqs_unhandled++;
  desc->last_unhandled = jiffies;
 }

 irq = irq_desc_get_irq(desc);
 if (unlikely(try_misrouted_irq(irq, desc, action_ret))) {
  int ok = misrouted_irq(irq);
  if (action_ret == IRQ_NONE)
   desc->irqs_unhandled -= ok;
 }

 desc->irq_count++;
 if (likely(desc->irq_count < 100000))
  return;

 desc->irq_count = 0;
 if (unlikely(desc->irqs_unhandled > 99900)) {



  __report_bad_irq(desc, action_ret);



  printk(KERN_EMERG "Disabling IRQ #%d\n", irq);
  desc->istate |= IRQS_SPURIOUS_DISABLED;
  desc->depth++;
  irq_disable(desc);

  mod_timer(&poll_spurious_irq_timer,
     jiffies + POLL_SPURIOUS_IRQ_INTERVAL);
 }
 desc->irqs_unhandled = 0;
}

bool noirqdebug __read_mostly;

int noirqdebug_setup(char *str)
{
 noirqdebug = 1;
 printk(KERN_INFO "IRQ lockup detection disabled\n");

 return 1;
}

__setup("noirqdebug", noirqdebug_setup);
module_param(noirqdebug, bool, 0644);
MODULE_PARM_DESC(noirqdebug, "Disable irq lockup detection when true");

static int __init irqfixup_setup(char *str)
{
 irqfixup = 1;
 printk(KERN_WARNING "Misrouted IRQ fixup support enabled.\n");
 printk(KERN_WARNING "This may impact system performance.\n");

 return 1;
}

__setup("irqfixup", irqfixup_setup);
module_param(irqfixup, int, 0644);

static int __init irqpoll_setup(char *str)
{
 irqfixup = 2;
 printk(KERN_WARNING "Misrouted IRQ fixup and polling support "
    "enabled\n");
 printk(KERN_WARNING "This may significantly impact system "
    "performance\n");
 return 1;
}

__setup("irqpoll", irqpoll_setup);







struct stack_map_bucket {
 struct pcpu_freelist_node fnode;
 u32 hash;
 u32 nr;
 u64 ip[];
};

struct bpf_stack_map {
 struct bpf_map map;
 void *elems;
 struct pcpu_freelist freelist;
 u32 n_buckets;
 struct stack_map_bucket *buckets[];
};

static int prealloc_elems_and_freelist(struct bpf_stack_map *smap)
{
 u32 elem_size = sizeof(struct stack_map_bucket) + smap->map.value_size;
 int err;

 smap->elems = vzalloc(elem_size * smap->map.max_entries);
 if (!smap->elems)
  return -ENOMEM;

 err = pcpu_freelist_init(&smap->freelist);
 if (err)
  goto free_elems;

 pcpu_freelist_populate(&smap->freelist, smap->elems, elem_size,
          smap->map.max_entries);
 return 0;

free_elems:
 vfree(smap->elems);
 return err;
}


static struct bpf_map *stack_map_alloc(union bpf_attr *attr)
{
 u32 value_size = attr->value_size;
 struct bpf_stack_map *smap;
 u64 cost, n_buckets;
 int err;

 if (!capable(CAP_SYS_ADMIN))
  return ERR_PTR(-EPERM);

 if (attr->map_flags)
  return ERR_PTR(-EINVAL);


 if (attr->max_entries == 0 || attr->key_size != 4 ||
     value_size < 8 || value_size % 8 ||
     value_size / 8 > sysctl_perf_event_max_stack)
  return ERR_PTR(-EINVAL);


 n_buckets = roundup_pow_of_two(attr->max_entries);

 cost = n_buckets * sizeof(struct stack_map_bucket *) + sizeof(*smap);
 if (cost >= U32_MAX - PAGE_SIZE)
  return ERR_PTR(-E2BIG);

 smap = kzalloc(cost, GFP_USER | __GFP_NOWARN);
 if (!smap) {
  smap = vzalloc(cost);
  if (!smap)
   return ERR_PTR(-ENOMEM);
 }

 err = -E2BIG;
 cost += n_buckets * (value_size + sizeof(struct stack_map_bucket));
 if (cost >= U32_MAX - PAGE_SIZE)
  goto free_smap;

 smap->map.map_type = attr->map_type;
 smap->map.key_size = attr->key_size;
 smap->map.value_size = value_size;
 smap->map.max_entries = attr->max_entries;
 smap->n_buckets = n_buckets;
 smap->map.pages = round_up(cost, PAGE_SIZE) >> PAGE_SHIFT;

 err = bpf_map_precharge_memlock(smap->map.pages);
 if (err)
  goto free_smap;

 err = get_callchain_buffers();
 if (err)
  goto free_smap;

 err = prealloc_elems_and_freelist(smap);
 if (err)
  goto put_buffers;

 return &smap->map;

put_buffers:
 put_callchain_buffers();
free_smap:
 kvfree(smap);
 return ERR_PTR(err);
}

u64 bpf_get_stackid(u64 r1, u64 r2, u64 flags, u64 r4, u64 r5)
{
 struct pt_regs *regs = (struct pt_regs *) (long) r1;
 struct bpf_map *map = (struct bpf_map *) (long) r2;
 struct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);
 struct perf_callchain_entry *trace;
 struct stack_map_bucket *bucket, *new_bucket, *old_bucket;
 u32 max_depth = map->value_size / 8;

 u32 init_nr = sysctl_perf_event_max_stack - max_depth;
 u32 skip = flags & BPF_F_SKIP_FIELD_MASK;
 u32 hash, id, trace_nr, trace_len;
 bool user = flags & BPF_F_USER_STACK;
 bool kernel = !user;
 u64 *ips;

 if (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |
          BPF_F_FAST_STACK_CMP | BPF_F_REUSE_STACKID)))
  return -EINVAL;

 trace = get_perf_callchain(regs, init_nr, kernel, user,
       sysctl_perf_event_max_stack, false, false);

 if (unlikely(!trace))

  return -EFAULT;




 trace_nr = trace->nr - init_nr;

 if (trace_nr <= skip)

  return -EFAULT;

 trace_nr -= skip;
 trace_len = trace_nr * sizeof(u64);
 ips = trace->ip + skip + init_nr;
 hash = jhash2((u32 *)ips, trace_len / sizeof(u32), 0);
 id = hash & (smap->n_buckets - 1);
 bucket = READ_ONCE(smap->buckets[id]);

 if (bucket && bucket->hash == hash) {
  if (flags & BPF_F_FAST_STACK_CMP)
   return id;
  if (bucket->nr == trace_nr &&
      memcmp(bucket->ip, ips, trace_len) == 0)
   return id;
 }


 if (bucket && !(flags & BPF_F_REUSE_STACKID))
  return -EEXIST;

 new_bucket = (struct stack_map_bucket *)
  pcpu_freelist_pop(&smap->freelist);
 if (unlikely(!new_bucket))
  return -ENOMEM;

 memcpy(new_bucket->ip, ips, trace_len);
 new_bucket->hash = hash;
 new_bucket->nr = trace_nr;

 old_bucket = xchg(&smap->buckets[id], new_bucket);
 if (old_bucket)
  pcpu_freelist_push(&smap->freelist, &old_bucket->fnode);
 return id;
}

const struct bpf_func_proto bpf_get_stackid_proto = {
 .func = bpf_get_stackid,
 .gpl_only = true,
 .ret_type = RET_INTEGER,
 .arg1_type = ARG_PTR_TO_CTX,
 .arg2_type = ARG_CONST_MAP_PTR,
 .arg3_type = ARG_ANYTHING,
};


static void *stack_map_lookup_elem(struct bpf_map *map, void *key)
{
 return NULL;
}


int bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)
{
 struct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);
 struct stack_map_bucket *bucket, *old_bucket;
 u32 id = *(u32 *)key, trace_len;

 if (unlikely(id >= smap->n_buckets))
  return -ENOENT;

 bucket = xchg(&smap->buckets[id], NULL);
 if (!bucket)
  return -ENOENT;

 trace_len = bucket->nr * sizeof(u64);
 memcpy(value, bucket->ip, trace_len);
 memset(value + trace_len, 0, map->value_size - trace_len);

 old_bucket = xchg(&smap->buckets[id], bucket);
 if (old_bucket)
  pcpu_freelist_push(&smap->freelist, &old_bucket->fnode);
 return 0;
}

static int stack_map_get_next_key(struct bpf_map *map, void *key, void *next_key)
{
 return -EINVAL;
}

static int stack_map_update_elem(struct bpf_map *map, void *key, void *value,
     u64 map_flags)
{
 return -EINVAL;
}


static int stack_map_delete_elem(struct bpf_map *map, void *key)
{
 struct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);
 struct stack_map_bucket *old_bucket;
 u32 id = *(u32 *)key;

 if (unlikely(id >= smap->n_buckets))
  return -E2BIG;

 old_bucket = xchg(&smap->buckets[id], NULL);
 if (old_bucket) {
  pcpu_freelist_push(&smap->freelist, &old_bucket->fnode);
  return 0;
 } else {
  return -ENOENT;
 }
}


static void stack_map_free(struct bpf_map *map)
{
 struct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);


 synchronize_rcu();

 vfree(smap->elems);
 pcpu_freelist_destroy(&smap->freelist);
 kvfree(smap);
 put_callchain_buffers();
}

static const struct bpf_map_ops stack_map_ops = {
 .map_alloc = stack_map_alloc,
 .map_free = stack_map_free,
 .map_get_next_key = stack_map_get_next_key,
 .map_lookup_elem = stack_map_lookup_elem,
 .map_update_elem = stack_map_update_elem,
 .map_delete_elem = stack_map_delete_elem,
};

static struct bpf_map_type_list stack_map_type __read_mostly = {
 .ops = &stack_map_ops,
 .type = BPF_MAP_TYPE_STACK_TRACE,
};

static int __init register_stack_map(void)
{
 bpf_register_map_type(&stack_map_type);
 return 0;
}
late_initcall(register_stack_map);








void print_stack_trace(struct stack_trace *trace, int spaces)
{
 int i;

 if (WARN_ON(!trace->entries))
  return;

 for (i = 0; i < trace->nr_entries; i++) {
  printk("%*c", 1 + spaces, ' ');
  print_ip_sym(trace->entries[i]);
 }
}
EXPORT_SYMBOL_GPL(print_stack_trace);

int snprint_stack_trace(char *buf, size_t size,
   struct stack_trace *trace, int spaces)
{
 int i;
 unsigned long ip;
 int generated;
 int total = 0;

 if (WARN_ON(!trace->entries))
  return 0;

 for (i = 0; i < trace->nr_entries; i++) {
  ip = trace->entries[i];
  generated = snprintf(buf, size, "%*c[<%p>] %pS\n",
    1 + spaces, ' ', (void *) ip, (void *) ip);

  total += generated;


  if (generated >= size) {
   buf += size;
   size = 0;
  } else {
   buf += generated;
   size -= generated;
  }
 }

 return total;
}
EXPORT_SYMBOL_GPL(snprint_stack_trace);






__weak void
save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
{
 WARN_ONCE(1, KERN_INFO "save_stack_trace_tsk() not implemented yet.\n");
}

__weak void
save_stack_trace_regs(struct pt_regs *regs, struct stack_trace *trace)
{
 WARN_ONCE(1, KERN_INFO "save_stack_trace_regs() not implemented yet.\n");
}





struct cpu_stop_done {
 atomic_t nr_todo;
 int ret;
 struct completion completion;
};


struct cpu_stopper {
 struct task_struct *thread;

 spinlock_t lock;
 bool enabled;
 struct list_head works;

 struct cpu_stop_work stop_work;
};

static DEFINE_PER_CPU(struct cpu_stopper, cpu_stopper);
static bool stop_machine_initialized = false;







DEFINE_STATIC_LGLOCK(stop_cpus_lock);

static void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)
{
 memset(done, 0, sizeof(*done));
 atomic_set(&done->nr_todo, nr_todo);
 init_completion(&done->completion);
}


static void cpu_stop_signal_done(struct cpu_stop_done *done)
{
 if (atomic_dec_and_test(&done->nr_todo))
  complete(&done->completion);
}

static void __cpu_stop_queue_work(struct cpu_stopper *stopper,
     struct cpu_stop_work *work)
{
 list_add_tail(&work->list, &stopper->works);
 wake_up_process(stopper->thread);
}


static bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)
{
 struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
 unsigned long flags;
 bool enabled;

 spin_lock_irqsave(&stopper->lock, flags);
 enabled = stopper->enabled;
 if (enabled)
  __cpu_stop_queue_work(stopper, work);
 else if (work->done)
  cpu_stop_signal_done(work->done);
 spin_unlock_irqrestore(&stopper->lock, flags);

 return enabled;
}
int stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)
{
 struct cpu_stop_done done;
 struct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done };

 cpu_stop_init_done(&done, 1);
 if (!cpu_stop_queue_work(cpu, &work))
  return -ENOENT;
 wait_for_completion(&done.completion);
 return done.ret;
}


enum multi_stop_state {

 MULTI_STOP_NONE,

 MULTI_STOP_PREPARE,

 MULTI_STOP_DISABLE_IRQ,

 MULTI_STOP_RUN,

 MULTI_STOP_EXIT,
};

struct multi_stop_data {
 cpu_stop_fn_t fn;
 void *data;

 unsigned int num_threads;
 const struct cpumask *active_cpus;

 enum multi_stop_state state;
 atomic_t thread_ack;
};

static void set_state(struct multi_stop_data *msdata,
        enum multi_stop_state newstate)
{

 atomic_set(&msdata->thread_ack, msdata->num_threads);
 smp_wmb();
 msdata->state = newstate;
}


static void ack_state(struct multi_stop_data *msdata)
{
 if (atomic_dec_and_test(&msdata->thread_ack))
  set_state(msdata, msdata->state + 1);
}


static int multi_cpu_stop(void *data)
{
 struct multi_stop_data *msdata = data;
 enum multi_stop_state curstate = MULTI_STOP_NONE;
 int cpu = smp_processor_id(), err = 0;
 unsigned long flags;
 bool is_active;





 local_save_flags(flags);

 if (!msdata->active_cpus)
  is_active = cpu == cpumask_first(cpu_online_mask);
 else
  is_active = cpumask_test_cpu(cpu, msdata->active_cpus);


 do {

  cpu_relax();
  if (msdata->state != curstate) {
   curstate = msdata->state;
   switch (curstate) {
   case MULTI_STOP_DISABLE_IRQ:
    local_irq_disable();
    hard_irq_disable();
    break;
   case MULTI_STOP_RUN:
    if (is_active)
     err = msdata->fn(msdata->data);
    break;
   default:
    break;
   }
   ack_state(msdata);
  }
 } while (curstate != MULTI_STOP_EXIT);

 local_irq_restore(flags);
 return err;
}

static int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,
        int cpu2, struct cpu_stop_work *work2)
{
 struct cpu_stopper *stopper1 = per_cpu_ptr(&cpu_stopper, cpu1);
 struct cpu_stopper *stopper2 = per_cpu_ptr(&cpu_stopper, cpu2);
 int err;

 lg_double_lock(&stop_cpus_lock, cpu1, cpu2);
 spin_lock_irq(&stopper1->lock);
 spin_lock_nested(&stopper2->lock, SINGLE_DEPTH_NESTING);

 err = -ENOENT;
 if (!stopper1->enabled || !stopper2->enabled)
  goto unlock;

 err = 0;
 __cpu_stop_queue_work(stopper1, work1);
 __cpu_stop_queue_work(stopper2, work2);
unlock:
 spin_unlock(&stopper2->lock);
 spin_unlock_irq(&stopper1->lock);
 lg_double_unlock(&stop_cpus_lock, cpu1, cpu2);

 return err;
}
int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *arg)
{
 struct cpu_stop_done done;
 struct cpu_stop_work work1, work2;
 struct multi_stop_data msdata;

 msdata = (struct multi_stop_data){
  .fn = fn,
  .data = arg,
  .num_threads = 2,
  .active_cpus = cpumask_of(cpu1),
 };

 work1 = work2 = (struct cpu_stop_work){
  .fn = multi_cpu_stop,
  .arg = &msdata,
  .done = &done
 };

 cpu_stop_init_done(&done, 2);
 set_state(&msdata, MULTI_STOP_PREPARE);

 if (cpu1 > cpu2)
  swap(cpu1, cpu2);
 if (cpu_stop_queue_two_works(cpu1, &work1, cpu2, &work2))
  return -ENOENT;

 wait_for_completion(&done.completion);
 return done.ret;
}
bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,
   struct cpu_stop_work *work_buf)
{
 *work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, };
 return cpu_stop_queue_work(cpu, work_buf);
}


static DEFINE_MUTEX(stop_cpus_mutex);

static bool queue_stop_cpus_work(const struct cpumask *cpumask,
     cpu_stop_fn_t fn, void *arg,
     struct cpu_stop_done *done)
{
 struct cpu_stop_work *work;
 unsigned int cpu;
 bool queued = false;






 lg_global_lock(&stop_cpus_lock);
 for_each_cpu(cpu, cpumask) {
  work = &per_cpu(cpu_stopper.stop_work, cpu);
  work->fn = fn;
  work->arg = arg;
  work->done = done;
  if (cpu_stop_queue_work(cpu, work))
   queued = true;
 }
 lg_global_unlock(&stop_cpus_lock);

 return queued;
}

static int __stop_cpus(const struct cpumask *cpumask,
         cpu_stop_fn_t fn, void *arg)
{
 struct cpu_stop_done done;

 cpu_stop_init_done(&done, cpumask_weight(cpumask));
 if (!queue_stop_cpus_work(cpumask, fn, arg, &done))
  return -ENOENT;
 wait_for_completion(&done.completion);
 return done.ret;
}
int stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)
{
 int ret;


 mutex_lock(&stop_cpus_mutex);
 ret = __stop_cpus(cpumask, fn, arg);
 mutex_unlock(&stop_cpus_mutex);
 return ret;
}
int try_stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)
{
 int ret;


 if (!mutex_trylock(&stop_cpus_mutex))
  return -EAGAIN;
 ret = __stop_cpus(cpumask, fn, arg);
 mutex_unlock(&stop_cpus_mutex);
 return ret;
}

static int cpu_stop_should_run(unsigned int cpu)
{
 struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
 unsigned long flags;
 int run;

 spin_lock_irqsave(&stopper->lock, flags);
 run = !list_empty(&stopper->works);
 spin_unlock_irqrestore(&stopper->lock, flags);
 return run;
}

static void cpu_stopper_thread(unsigned int cpu)
{
 struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
 struct cpu_stop_work *work;

repeat:
 work = NULL;
 spin_lock_irq(&stopper->lock);
 if (!list_empty(&stopper->works)) {
  work = list_first_entry(&stopper->works,
     struct cpu_stop_work, list);
  list_del_init(&work->list);
 }
 spin_unlock_irq(&stopper->lock);

 if (work) {
  cpu_stop_fn_t fn = work->fn;
  void *arg = work->arg;
  struct cpu_stop_done *done = work->done;
  int ret;


  preempt_count_inc();
  ret = fn(arg);
  if (done) {
   if (ret)
    done->ret = ret;
   cpu_stop_signal_done(done);
  }
  preempt_count_dec();
  WARN_ONCE(preempt_count(),
     "cpu_stop: %pf(%p) leaked preempt count\n", fn, arg);
  goto repeat;
 }
}

void stop_machine_park(int cpu)
{
 struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);





 stopper->enabled = false;
 kthread_park(stopper->thread);
}

extern void sched_set_stop_task(int cpu, struct task_struct *stop);

static void cpu_stop_create(unsigned int cpu)
{
 sched_set_stop_task(cpu, per_cpu(cpu_stopper.thread, cpu));
}

static void cpu_stop_park(unsigned int cpu)
{
 struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);

 WARN_ON(!list_empty(&stopper->works));
}

void stop_machine_unpark(int cpu)
{
 struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);

 stopper->enabled = true;
 kthread_unpark(stopper->thread);
}

static struct smp_hotplug_thread cpu_stop_threads = {
 .store = &cpu_stopper.thread,
 .thread_should_run = cpu_stop_should_run,
 .thread_fn = cpu_stopper_thread,
 .thread_comm = "migration/%u",
 .create = cpu_stop_create,
 .park = cpu_stop_park,
 .selfparking = true,
};

static int __init cpu_stop_init(void)
{
 unsigned int cpu;

 for_each_possible_cpu(cpu) {
  struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);

  spin_lock_init(&stopper->lock);
  INIT_LIST_HEAD(&stopper->works);
 }

 BUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));
 stop_machine_unpark(raw_smp_processor_id());
 stop_machine_initialized = true;
 return 0;
}
early_initcall(cpu_stop_init);

static int __stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)
{
 struct multi_stop_data msdata = {
  .fn = fn,
  .data = data,
  .num_threads = num_online_cpus(),
  .active_cpus = cpus,
 };

 if (!stop_machine_initialized) {





  unsigned long flags;
  int ret;

  WARN_ON_ONCE(msdata.num_threads != 1);

  local_irq_save(flags);
  hard_irq_disable();
  ret = (*fn)(data);
  local_irq_restore(flags);

  return ret;
 }


 set_state(&msdata, MULTI_STOP_PREPARE);
 return stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);
}

int stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)
{
 int ret;


 get_online_cpus();
 ret = __stop_machine(fn, data, cpus);
 put_online_cpus();
 return ret;
}
EXPORT_SYMBOL_GPL(stop_machine);
int stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,
      const struct cpumask *cpus)
{
 struct multi_stop_data msdata = { .fn = fn, .data = data,
         .active_cpus = cpus };
 struct cpu_stop_done done;
 int ret;


 BUG_ON(cpu_active(raw_smp_processor_id()));
 msdata.num_threads = num_active_cpus() + 1;


 while (!mutex_trylock(&stop_cpus_mutex))
  cpu_relax();


 set_state(&msdata, MULTI_STOP_PREPARE);
 cpu_stop_init_done(&done, num_active_cpus());
 queue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,
        &done);
 ret = multi_cpu_stop(&msdata);


 while (!completion_done(&done.completion))
  cpu_relax();

 mutex_unlock(&stop_cpus_mutex);
 return ret ?: done.ret;
}


const char *pm_labels[] = { "mem", "standby", "freeze", NULL };
const char *pm_states[PM_SUSPEND_MAX];

unsigned int pm_suspend_global_flags;
EXPORT_SYMBOL_GPL(pm_suspend_global_flags);

static const struct platform_suspend_ops *suspend_ops;
static const struct platform_freeze_ops *freeze_ops;
static DECLARE_WAIT_QUEUE_HEAD(suspend_freeze_wait_head);

enum freeze_state __read_mostly suspend_freeze_state;
static DEFINE_SPINLOCK(suspend_freeze_lock);

void freeze_set_ops(const struct platform_freeze_ops *ops)
{
 lock_system_sleep();
 freeze_ops = ops;
 unlock_system_sleep();
}

static void freeze_begin(void)
{
 suspend_freeze_state = FREEZE_STATE_NONE;
}

static void freeze_enter(void)
{
 spin_lock_irq(&suspend_freeze_lock);
 if (pm_wakeup_pending())
  goto out;

 suspend_freeze_state = FREEZE_STATE_ENTER;
 spin_unlock_irq(&suspend_freeze_lock);

 get_online_cpus();
 cpuidle_resume();


 wake_up_all_idle_cpus();
 pr_debug("PM: suspend-to-idle\n");

 wait_event(suspend_freeze_wait_head,
     suspend_freeze_state == FREEZE_STATE_WAKE);
 pr_debug("PM: resume from suspend-to-idle\n");

 cpuidle_pause();
 put_online_cpus();

 spin_lock_irq(&suspend_freeze_lock);

 out:
 suspend_freeze_state = FREEZE_STATE_NONE;
 spin_unlock_irq(&suspend_freeze_lock);
}

void freeze_wake(void)
{
 unsigned long flags;

 spin_lock_irqsave(&suspend_freeze_lock, flags);
 if (suspend_freeze_state > FREEZE_STATE_NONE) {
  suspend_freeze_state = FREEZE_STATE_WAKE;
  wake_up(&suspend_freeze_wait_head);
 }
 spin_unlock_irqrestore(&suspend_freeze_lock, flags);
}
EXPORT_SYMBOL_GPL(freeze_wake);

static bool valid_state(suspend_state_t state)
{





 return suspend_ops && suspend_ops->valid && suspend_ops->valid(state);
}







static bool relative_states;

static int __init sleep_states_setup(char *str)
{
 relative_states = !strncmp(str, "1", 1);
 pm_states[PM_SUSPEND_FREEZE] = pm_labels[relative_states ? 0 : 2];
 return 1;
}

__setup("relative_sleep_states=", sleep_states_setup);





void suspend_set_ops(const struct platform_suspend_ops *ops)
{
 suspend_state_t i;
 int j = 0;

 lock_system_sleep();

 suspend_ops = ops;
 for (i = PM_SUSPEND_MEM; i >= PM_SUSPEND_STANDBY; i--)
  if (valid_state(i)) {
   pm_states[i] = pm_labels[j++];
  } else if (!relative_states) {
   pm_states[i] = NULL;
   j++;
  }

 pm_states[PM_SUSPEND_FREEZE] = pm_labels[j];

 unlock_system_sleep();
}
EXPORT_SYMBOL_GPL(suspend_set_ops);
int suspend_valid_only_mem(suspend_state_t state)
{
 return state == PM_SUSPEND_MEM;
}
EXPORT_SYMBOL_GPL(suspend_valid_only_mem);

static bool sleep_state_supported(suspend_state_t state)
{
 return state == PM_SUSPEND_FREEZE || (suspend_ops && suspend_ops->enter);
}

static int platform_suspend_prepare(suspend_state_t state)
{
 return state != PM_SUSPEND_FREEZE && suspend_ops->prepare ?
  suspend_ops->prepare() : 0;
}

static int platform_suspend_prepare_late(suspend_state_t state)
{
 return state == PM_SUSPEND_FREEZE && freeze_ops && freeze_ops->prepare ?
  freeze_ops->prepare() : 0;
}

static int platform_suspend_prepare_noirq(suspend_state_t state)
{
 return state != PM_SUSPEND_FREEZE && suspend_ops->prepare_late ?
  suspend_ops->prepare_late() : 0;
}

static void platform_resume_noirq(suspend_state_t state)
{
 if (state != PM_SUSPEND_FREEZE && suspend_ops->wake)
  suspend_ops->wake();
}

static void platform_resume_early(suspend_state_t state)
{
 if (state == PM_SUSPEND_FREEZE && freeze_ops && freeze_ops->restore)
  freeze_ops->restore();
}

static void platform_resume_finish(suspend_state_t state)
{
 if (state != PM_SUSPEND_FREEZE && suspend_ops->finish)
  suspend_ops->finish();
}

static int platform_suspend_begin(suspend_state_t state)
{
 if (state == PM_SUSPEND_FREEZE && freeze_ops && freeze_ops->begin)
  return freeze_ops->begin();
 else if (suspend_ops->begin)
  return suspend_ops->begin(state);
 else
  return 0;
}

static void platform_resume_end(suspend_state_t state)
{
 if (state == PM_SUSPEND_FREEZE && freeze_ops && freeze_ops->end)
  freeze_ops->end();
 else if (suspend_ops->end)
  suspend_ops->end();
}

static void platform_recover(suspend_state_t state)
{
 if (state != PM_SUSPEND_FREEZE && suspend_ops->recover)
  suspend_ops->recover();
}

static bool platform_suspend_again(suspend_state_t state)
{
 return state != PM_SUSPEND_FREEZE && suspend_ops->suspend_again ?
  suspend_ops->suspend_again() : false;
}

static unsigned int pm_test_delay = 5;
module_param(pm_test_delay, uint, 0644);
MODULE_PARM_DESC(pm_test_delay,
   "Number of seconds to wait before resuming from suspend test");

static int suspend_test(int level)
{
 if (pm_test_level == level) {
  pr_info("suspend debug: Waiting for %d second(s).\n",
    pm_test_delay);
  mdelay(pm_test_delay * 1000);
  return 1;
 }
 return 0;
}
static int suspend_prepare(suspend_state_t state)
{
 int error;

 if (!sleep_state_supported(state))
  return -EPERM;

 pm_prepare_console();

 error = pm_notifier_call_chain(PM_SUSPEND_PREPARE);
 if (error)
  goto Finish;

 trace_suspend_resume(TPS("freeze_processes"), 0, true);
 error = suspend_freeze_processes();
 trace_suspend_resume(TPS("freeze_processes"), 0, false);
 if (!error)
  return 0;

 suspend_stats.failed_freeze++;
 dpm_save_failed_step(SUSPEND_FREEZE);
 Finish:
 pm_notifier_call_chain(PM_POST_SUSPEND);
 pm_restore_console();
 return error;
}


void __weak arch_suspend_disable_irqs(void)
{
 local_irq_disable();
}


void __weak arch_suspend_enable_irqs(void)
{
 local_irq_enable();
}
static int suspend_enter(suspend_state_t state, bool *wakeup)
{
 int error;

 error = platform_suspend_prepare(state);
 if (error)
  goto Platform_finish;

 error = dpm_suspend_late(PMSG_SUSPEND);
 if (error) {
  pr_err("PM: late suspend of devices failed\n");
  goto Platform_finish;
 }
 error = platform_suspend_prepare_late(state);
 if (error)
  goto Devices_early_resume;

 error = dpm_suspend_noirq(PMSG_SUSPEND);
 if (error) {
  pr_err("PM: noirq suspend of devices failed\n");
  goto Platform_early_resume;
 }
 error = platform_suspend_prepare_noirq(state);
 if (error)
  goto Platform_wake;

 if (suspend_test(TEST_PLATFORM))
  goto Platform_wake;







 if (state == PM_SUSPEND_FREEZE) {
  trace_suspend_resume(TPS("machine_suspend"), state, true);
  freeze_enter();
  trace_suspend_resume(TPS("machine_suspend"), state, false);
  goto Platform_wake;
 }

 error = disable_nonboot_cpus();
 if (error || suspend_test(TEST_CPUS))
  goto Enable_cpus;

 arch_suspend_disable_irqs();
 BUG_ON(!irqs_disabled());

 error = syscore_suspend();
 if (!error) {
  *wakeup = pm_wakeup_pending();
  if (!(suspend_test(TEST_CORE) || *wakeup)) {
   trace_suspend_resume(TPS("machine_suspend"),
    state, true);
   error = suspend_ops->enter(state);
   trace_suspend_resume(TPS("machine_suspend"),
    state, false);
   events_check_enabled = false;
  } else if (*wakeup) {
   error = -EBUSY;
  }
  syscore_resume();
 }

 arch_suspend_enable_irqs();
 BUG_ON(irqs_disabled());

 Enable_cpus:
 enable_nonboot_cpus();

 Platform_wake:
 platform_resume_noirq(state);
 dpm_resume_noirq(PMSG_RESUME);

 Platform_early_resume:
 platform_resume_early(state);

 Devices_early_resume:
 dpm_resume_early(PMSG_RESUME);

 Platform_finish:
 platform_resume_finish(state);
 return error;
}





int suspend_devices_and_enter(suspend_state_t state)
{
 int error;
 bool wakeup = false;

 if (!sleep_state_supported(state))
  return -ENOSYS;

 error = platform_suspend_begin(state);
 if (error)
  goto Close;

 suspend_console();
 suspend_test_start();
 error = dpm_suspend_start(PMSG_SUSPEND);
 if (error) {
  pr_err("PM: Some devices failed to suspend, or early wake event detected\n");
  goto Recover_platform;
 }
 suspend_test_finish("suspend devices");
 if (suspend_test(TEST_DEVICES))
  goto Recover_platform;

 do {
  error = suspend_enter(state, &wakeup);
 } while (!error && !wakeup && platform_suspend_again(state));

 Resume_devices:
 suspend_test_start();
 dpm_resume_end(PMSG_RESUME);
 suspend_test_finish("resume devices");
 trace_suspend_resume(TPS("resume_console"), state, true);
 resume_console();
 trace_suspend_resume(TPS("resume_console"), state, false);

 Close:
 platform_resume_end(state);
 return error;

 Recover_platform:
 platform_recover(state);
 goto Resume_devices;
}







static void suspend_finish(void)
{
 suspend_thaw_processes();
 pm_notifier_call_chain(PM_POST_SUSPEND);
 pm_restore_console();
}
static int enter_state(suspend_state_t state)
{
 int error;

 trace_suspend_resume(TPS("suspend_enter"), state, true);
 if (state == PM_SUSPEND_FREEZE) {
  if (pm_test_level != TEST_NONE && pm_test_level <= TEST_CPUS) {
   pr_warn("PM: Unsupported test mode for suspend to idle, please choose none/freezer/devices/platform.\n");
   return -EAGAIN;
  }
 } else if (!valid_state(state)) {
  return -EINVAL;
 }
 if (!mutex_trylock(&pm_mutex))
  return -EBUSY;

 if (state == PM_SUSPEND_FREEZE)
  freeze_begin();

 trace_suspend_resume(TPS("sync_filesystems"), 0, true);
 printk(KERN_INFO "PM: Syncing filesystems ... ");
 sys_sync();
 printk("done.\n");
 trace_suspend_resume(TPS("sync_filesystems"), 0, false);

 pr_debug("PM: Preparing system for sleep (%s)\n", pm_states[state]);
 pm_suspend_clear_flags();
 error = suspend_prepare(state);
 if (error)
  goto Unlock;

 if (suspend_test(TEST_FREEZER))
  goto Finish;

 trace_suspend_resume(TPS("suspend_enter"), state, false);
 pr_debug("PM: Suspending system (%s)\n", pm_states[state]);
 pm_restrict_gfp_mask();
 error = suspend_devices_and_enter(state);
 pm_restore_gfp_mask();

 Finish:
 pr_debug("PM: Finishing wakeup.\n");
 suspend_finish();
 Unlock:
 mutex_unlock(&pm_mutex);
 return error;
}
int pm_suspend(suspend_state_t state)
{
 int error;

 if (state <= PM_SUSPEND_ON || state >= PM_SUSPEND_MAX)
  return -EINVAL;

 error = enter_state(state);
 if (error) {
  suspend_stats.fail++;
  dpm_save_failed_errno(error);
 } else {
  suspend_stats.success++;
 }
 return error;
}
EXPORT_SYMBOL(pm_suspend);


static unsigned long suspend_test_start_time;
static u32 test_repeat_count_max = 1;
static u32 test_repeat_count_current;

void suspend_test_start(void)
{




 suspend_test_start_time = jiffies;
}

void suspend_test_finish(const char *label)
{
 long nj = jiffies - suspend_test_start_time;
 unsigned msec;

 msec = jiffies_to_msecs(abs(nj));
 pr_info("PM: %s took %d.%03d seconds\n", label,
   msec / 1000, msec % 1000);
 WARN(msec > (TEST_SUSPEND_SECONDS * 1000),
      "Component: %s, time: %u\n", label, msec);
}






static void __init test_wakealarm(struct rtc_device *rtc, suspend_state_t state)
{
 static char err_readtime[] __initdata =
  KERN_ERR "PM: can't read %s time, err %d\n";
 static char err_wakealarm [] __initdata =
  KERN_ERR "PM: can't set %s wakealarm, err %d\n";
 static char err_suspend[] __initdata =
  KERN_ERR "PM: suspend test failed, error %d\n";
 static char info_test[] __initdata =
  KERN_INFO "PM: test RTC wakeup from '%s' suspend\n";

 unsigned long now;
 struct rtc_wkalrm alm;
 int status;


repeat:
 status = rtc_read_time(rtc, &alm.time);
 if (status < 0) {
  printk(err_readtime, dev_name(&rtc->dev), status);
  return;
 }
 rtc_tm_to_time(&alm.time, &now);

 memset(&alm, 0, sizeof alm);
 rtc_time_to_tm(now + TEST_SUSPEND_SECONDS, &alm.time);
 alm.enabled = true;

 status = rtc_set_alarm(rtc, &alm);
 if (status < 0) {
  printk(err_wakealarm, dev_name(&rtc->dev), status);
  return;
 }

 if (state == PM_SUSPEND_MEM) {
  printk(info_test, pm_states[state]);
  status = pm_suspend(state);
  if (status == -ENODEV)
   state = PM_SUSPEND_STANDBY;
 }
 if (state == PM_SUSPEND_STANDBY) {
  printk(info_test, pm_states[state]);
  status = pm_suspend(state);
  if (status < 0)
   state = PM_SUSPEND_FREEZE;
 }
 if (state == PM_SUSPEND_FREEZE) {
  printk(info_test, pm_states[state]);
  status = pm_suspend(state);
 }

 if (status < 0)
  printk(err_suspend, status);

 test_repeat_count_current++;
 if (test_repeat_count_current < test_repeat_count_max)
  goto repeat;





 alm.enabled = false;
 rtc_set_alarm(rtc, &alm);
}

static int __init has_wakealarm(struct device *dev, const void *data)
{
 struct rtc_device *candidate = to_rtc_device(dev);

 if (!candidate->ops->set_alarm)
  return 0;
 if (!device_may_wakeup(candidate->dev.parent))
  return 0;

 return 1;
}






static const char *test_state_label __initdata;

static char warn_bad_state[] __initdata =
 KERN_WARNING "PM: can't test '%s' suspend state\n";

static int __init setup_test_suspend(char *value)
{
 int i;
 char *repeat;
 char *suspend_type;


 value++;
 suspend_type = strsep(&value, ",");
 if (!suspend_type)
  return 0;

 repeat = strsep(&value, ",");
 if (repeat) {
  if (kstrtou32(repeat, 0, &test_repeat_count_max))
   return 0;
 }

 for (i = 0; pm_labels[i]; i++)
  if (!strcmp(pm_labels[i], suspend_type)) {
   test_state_label = pm_labels[i];
   return 0;
  }

 printk(warn_bad_state, suspend_type);
 return 0;
}
__setup("test_suspend", setup_test_suspend);

static int __init test_suspend(void)
{
 static char warn_no_rtc[] __initdata =
  KERN_WARNING "PM: no wakealarm-capable RTC driver is ready\n";

 struct rtc_device *rtc = NULL;
 struct device *dev;
 suspend_state_t test_state;


 if (!test_state_label)
  return 0;

 for (test_state = PM_SUSPEND_MIN; test_state < PM_SUSPEND_MAX; test_state++) {
  const char *state_label = pm_states[test_state];

  if (state_label && !strcmp(test_state_label, state_label))
   break;
 }
 if (test_state == PM_SUSPEND_MAX) {
  printk(warn_bad_state, test_state_label);
  return 0;
 }


 dev = class_find_device(rtc_class, NULL, NULL, has_wakealarm);
 if (dev)
  rtc = rtc_class_open(dev_name(dev));
 if (!rtc) {
  printk(warn_no_rtc);
  return 0;
 }


 test_wakealarm(rtc, test_state);
 rtc_class_close(rtc);
 return 0;
}
late_initcall(test_suspend);








static bool clean_pages_on_read;
static bool clean_pages_on_decompress;




static inline unsigned long low_free_pages(void)
{
 return nr_free_pages() - nr_free_highpages();
}





static inline unsigned long reqd_free_pages(void)
{
 return low_free_pages() / 2;
}

struct swap_map_page {
 sector_t entries[MAP_PAGE_ENTRIES];
 sector_t next_swap;
};

struct swap_map_page_list {
 struct swap_map_page *map;
 struct swap_map_page_list *next;
};






struct swap_map_handle {
 struct swap_map_page *cur;
 struct swap_map_page_list *maps;
 sector_t cur_swap;
 sector_t first_sector;
 unsigned int k;
 unsigned long reqd_free_pages;
 u32 crc32;
};

struct swsusp_header {
 char reserved[PAGE_SIZE - 20 - sizeof(sector_t) - sizeof(int) -
               sizeof(u32)];
 u32 crc32;
 sector_t image;
 unsigned int flags;
 char orig_sig[10];
 char sig[10];
} __packed;

static struct swsusp_header *swsusp_header;






struct swsusp_extent {
 struct rb_node node;
 unsigned long start;
 unsigned long end;
};

static struct rb_root swsusp_extents = RB_ROOT;

static int swsusp_extents_insert(unsigned long swap_offset)
{
 struct rb_node **new = &(swsusp_extents.rb_node);
 struct rb_node *parent = NULL;
 struct swsusp_extent *ext;


 while (*new) {
  ext = rb_entry(*new, struct swsusp_extent, node);
  parent = *new;
  if (swap_offset < ext->start) {

   if (swap_offset == ext->start - 1) {
    ext->start--;
    return 0;
   }
   new = &((*new)->rb_left);
  } else if (swap_offset > ext->end) {

   if (swap_offset == ext->end + 1) {
    ext->end++;
    return 0;
   }
   new = &((*new)->rb_right);
  } else {

   return -EINVAL;
  }
 }

 ext = kzalloc(sizeof(struct swsusp_extent), GFP_KERNEL);
 if (!ext)
  return -ENOMEM;

 ext->start = swap_offset;
 ext->end = swap_offset;
 rb_link_node(&ext->node, parent, new);
 rb_insert_color(&ext->node, &swsusp_extents);
 return 0;
}






sector_t alloc_swapdev_block(int swap)
{
 unsigned long offset;

 offset = swp_offset(get_swap_page_of_type(swap));
 if (offset) {
  if (swsusp_extents_insert(offset))
   swap_free(swp_entry(swap, offset));
  else
   return swapdev_block(swap, offset);
 }
 return 0;
}







void free_all_swap_pages(int swap)
{
 struct rb_node *node;

 while ((node = swsusp_extents.rb_node)) {
  struct swsusp_extent *ext;
  unsigned long offset;

  ext = container_of(node, struct swsusp_extent, node);
  rb_erase(node, &swsusp_extents);
  for (offset = ext->start; offset <= ext->end; offset++)
   swap_free(swp_entry(swap, offset));

  kfree(ext);
 }
}

int swsusp_swap_in_use(void)
{
 return (swsusp_extents.rb_node != NULL);
}





static unsigned short root_swap = 0xffff;
static struct block_device *hib_resume_bdev;

struct hib_bio_batch {
 atomic_t count;
 wait_queue_head_t wait;
 int error;
};

static void hib_init_batch(struct hib_bio_batch *hb)
{
 atomic_set(&hb->count, 0);
 init_waitqueue_head(&hb->wait);
 hb->error = 0;
}

static void hib_end_io(struct bio *bio)
{
 struct hib_bio_batch *hb = bio->bi_private;
 struct page *page = bio->bi_io_vec[0].bv_page;

 if (bio->bi_error) {
  printk(KERN_ALERT "Read-error on swap-device (%u:%u:%Lu)\n",
    imajor(bio->bi_bdev->bd_inode),
    iminor(bio->bi_bdev->bd_inode),
    (unsigned long long)bio->bi_iter.bi_sector);
 }

 if (bio_data_dir(bio) == WRITE)
  put_page(page);
 else if (clean_pages_on_read)
  flush_icache_range((unsigned long)page_address(page),
       (unsigned long)page_address(page) + PAGE_SIZE);

 if (bio->bi_error && !hb->error)
  hb->error = bio->bi_error;
 if (atomic_dec_and_test(&hb->count))
  wake_up(&hb->wait);

 bio_put(bio);
}

static int hib_submit_io(int rw, pgoff_t page_off, void *addr,
  struct hib_bio_batch *hb)
{
 struct page *page = virt_to_page(addr);
 struct bio *bio;
 int error = 0;

 bio = bio_alloc(__GFP_RECLAIM | __GFP_HIGH, 1);
 bio->bi_iter.bi_sector = page_off * (PAGE_SIZE >> 9);
 bio->bi_bdev = hib_resume_bdev;

 if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
  printk(KERN_ERR "PM: Adding page to bio failed at %llu\n",
   (unsigned long long)bio->bi_iter.bi_sector);
  bio_put(bio);
  return -EFAULT;
 }

 if (hb) {
  bio->bi_end_io = hib_end_io;
  bio->bi_private = hb;
  atomic_inc(&hb->count);
  submit_bio(rw, bio);
 } else {
  error = submit_bio_wait(rw, bio);
  bio_put(bio);
 }

 return error;
}

static int hib_wait_io(struct hib_bio_batch *hb)
{
 wait_event(hb->wait, atomic_read(&hb->count) == 0);
 return hb->error;
}





static int mark_swapfiles(struct swap_map_handle *handle, unsigned int flags)
{
 int error;

 hib_submit_io(READ_SYNC, swsusp_resume_block, swsusp_header, NULL);
 if (!memcmp("SWAP-SPACE",swsusp_header->sig, 10) ||
     !memcmp("SWAPSPACE2",swsusp_header->sig, 10)) {
  memcpy(swsusp_header->orig_sig,swsusp_header->sig, 10);
  memcpy(swsusp_header->sig, HIBERNATE_SIG, 10);
  swsusp_header->image = handle->first_sector;
  swsusp_header->flags = flags;
  if (flags & SF_CRC32_MODE)
   swsusp_header->crc32 = handle->crc32;
  error = hib_submit_io(WRITE_SYNC, swsusp_resume_block,
     swsusp_header, NULL);
 } else {
  printk(KERN_ERR "PM: Swap header not found!\n");
  error = -ENODEV;
 }
 return error;
}







static int swsusp_swap_check(void)
{
 int res;

 res = swap_type_of(swsusp_resume_device, swsusp_resume_block,
   &hib_resume_bdev);
 if (res < 0)
  return res;

 root_swap = res;
 res = blkdev_get(hib_resume_bdev, FMODE_WRITE, NULL);
 if (res)
  return res;

 res = set_blocksize(hib_resume_bdev, PAGE_SIZE);
 if (res < 0)
  blkdev_put(hib_resume_bdev, FMODE_WRITE);

 return res;
}
static int write_page(void *buf, sector_t offset, struct hib_bio_batch *hb)
{
 void *src;
 int ret;

 if (!offset)
  return -ENOSPC;

 if (hb) {
  src = (void *)__get_free_page(__GFP_RECLAIM | __GFP_NOWARN |
                                __GFP_NORETRY);
  if (src) {
   copy_page(src, buf);
  } else {
   ret = hib_wait_io(hb);
   if (ret)
    return ret;
   src = (void *)__get_free_page(__GFP_RECLAIM |
                                 __GFP_NOWARN |
                                 __GFP_NORETRY);
   if (src) {
    copy_page(src, buf);
   } else {
    WARN_ON_ONCE(1);
    hb = NULL;
    src = buf;
   }
  }
 } else {
  src = buf;
 }
 return hib_submit_io(WRITE_SYNC, offset, src, hb);
}

static void release_swap_writer(struct swap_map_handle *handle)
{
 if (handle->cur)
  free_page((unsigned long)handle->cur);
 handle->cur = NULL;
}

static int get_swap_writer(struct swap_map_handle *handle)
{
 int ret;

 ret = swsusp_swap_check();
 if (ret) {
  if (ret != -ENOSPC)
   printk(KERN_ERR "PM: Cannot find swap device, try "
     "swapon -a.\n");
  return ret;
 }
 handle->cur = (struct swap_map_page *)get_zeroed_page(GFP_KERNEL);
 if (!handle->cur) {
  ret = -ENOMEM;
  goto err_close;
 }
 handle->cur_swap = alloc_swapdev_block(root_swap);
 if (!handle->cur_swap) {
  ret = -ENOSPC;
  goto err_rel;
 }
 handle->k = 0;
 handle->reqd_free_pages = reqd_free_pages();
 handle->first_sector = handle->cur_swap;
 return 0;
err_rel:
 release_swap_writer(handle);
err_close:
 swsusp_close(FMODE_WRITE);
 return ret;
}

static int swap_write_page(struct swap_map_handle *handle, void *buf,
  struct hib_bio_batch *hb)
{
 int error = 0;
 sector_t offset;

 if (!handle->cur)
  return -EINVAL;
 offset = alloc_swapdev_block(root_swap);
 error = write_page(buf, offset, hb);
 if (error)
  return error;
 handle->cur->entries[handle->k++] = offset;
 if (handle->k >= MAP_PAGE_ENTRIES) {
  offset = alloc_swapdev_block(root_swap);
  if (!offset)
   return -ENOSPC;
  handle->cur->next_swap = offset;
  error = write_page(handle->cur, handle->cur_swap, hb);
  if (error)
   goto out;
  clear_page(handle->cur);
  handle->cur_swap = offset;
  handle->k = 0;

  if (hb && low_free_pages() <= handle->reqd_free_pages) {
   error = hib_wait_io(hb);
   if (error)
    goto out;




   handle->reqd_free_pages = reqd_free_pages();
  }
 }
 out:
 return error;
}

static int flush_swap_writer(struct swap_map_handle *handle)
{
 if (handle->cur && handle->cur_swap)
  return write_page(handle->cur, handle->cur_swap, NULL);
 else
  return -EINVAL;
}

static int swap_writer_finish(struct swap_map_handle *handle,
  unsigned int flags, int error)
{
 if (!error) {
  flush_swap_writer(handle);
  printk(KERN_INFO "PM: S");
  error = mark_swapfiles(handle, flags);
  printk("|\n");
 }

 if (error)
  free_all_swap_pages(root_swap);
 release_swap_writer(handle);
 swsusp_close(FMODE_WRITE);

 return error;
}






                LZO_HEADER, PAGE_SIZE)










static int save_image(struct swap_map_handle *handle,
                      struct snapshot_handle *snapshot,
                      unsigned int nr_to_write)
{
 unsigned int m;
 int ret;
 int nr_pages;
 int err2;
 struct hib_bio_batch hb;
 ktime_t start;
 ktime_t stop;

 hib_init_batch(&hb);

 printk(KERN_INFO "PM: Saving image data pages (%u pages)...\n",
  nr_to_write);
 m = nr_to_write / 10;
 if (!m)
  m = 1;
 nr_pages = 0;
 start = ktime_get();
 while (1) {
  ret = snapshot_read_next(snapshot);
  if (ret <= 0)
   break;
  ret = swap_write_page(handle, data_of(*snapshot), &hb);
  if (ret)
   break;
  if (!(nr_pages % m))
   printk(KERN_INFO "PM: Image saving progress: %3d%%\n",
          nr_pages / m * 10);
  nr_pages++;
 }
 err2 = hib_wait_io(&hb);
 stop = ktime_get();
 if (!ret)
  ret = err2;
 if (!ret)
  printk(KERN_INFO "PM: Image saving done.\n");
 swsusp_show_speed(start, stop, nr_to_write, "Wrote");
 return ret;
}




struct crc_data {
 struct task_struct *thr;
 atomic_t ready;
 atomic_t stop;
 unsigned run_threads;
 wait_queue_head_t go;
 wait_queue_head_t done;
 u32 *crc32;
 size_t *unc_len[LZO_THREADS];
 unsigned char *unc[LZO_THREADS];
};




static int crc32_threadfn(void *data)
{
 struct crc_data *d = data;
 unsigned i;

 while (1) {
  wait_event(d->go, atomic_read(&d->ready) ||
                    kthread_should_stop());
  if (kthread_should_stop()) {
   d->thr = NULL;
   atomic_set(&d->stop, 1);
   wake_up(&d->done);
   break;
  }
  atomic_set(&d->ready, 0);

  for (i = 0; i < d->run_threads; i++)
   *d->crc32 = crc32_le(*d->crc32,
                        d->unc[i], *d->unc_len[i]);
  atomic_set(&d->stop, 1);
  wake_up(&d->done);
 }
 return 0;
}



struct cmp_data {
 struct task_struct *thr;
 atomic_t ready;
 atomic_t stop;
 int ret;
 wait_queue_head_t go;
 wait_queue_head_t done;
 size_t unc_len;
 size_t cmp_len;
 unsigned char unc[LZO_UNC_SIZE];
 unsigned char cmp[LZO_CMP_SIZE];
 unsigned char wrk[LZO1X_1_MEM_COMPRESS];
};




static int lzo_compress_threadfn(void *data)
{
 struct cmp_data *d = data;

 while (1) {
  wait_event(d->go, atomic_read(&d->ready) ||
                    kthread_should_stop());
  if (kthread_should_stop()) {
   d->thr = NULL;
   d->ret = -1;
   atomic_set(&d->stop, 1);
   wake_up(&d->done);
   break;
  }
  atomic_set(&d->ready, 0);

  d->ret = lzo1x_1_compress(d->unc, d->unc_len,
                            d->cmp + LZO_HEADER, &d->cmp_len,
                            d->wrk);
  atomic_set(&d->stop, 1);
  wake_up(&d->done);
 }
 return 0;
}







static int save_image_lzo(struct swap_map_handle *handle,
                          struct snapshot_handle *snapshot,
                          unsigned int nr_to_write)
{
 unsigned int m;
 int ret = 0;
 int nr_pages;
 int err2;
 struct hib_bio_batch hb;
 ktime_t start;
 ktime_t stop;
 size_t off;
 unsigned thr, run_threads, nr_threads;
 unsigned char *page = NULL;
 struct cmp_data *data = NULL;
 struct crc_data *crc = NULL;

 hib_init_batch(&hb);





 nr_threads = num_online_cpus() - 1;
 nr_threads = clamp_val(nr_threads, 1, LZO_THREADS);

 page = (void *)__get_free_page(__GFP_RECLAIM | __GFP_HIGH);
 if (!page) {
  printk(KERN_ERR "PM: Failed to allocate LZO page\n");
  ret = -ENOMEM;
  goto out_clean;
 }

 data = vmalloc(sizeof(*data) * nr_threads);
 if (!data) {
  printk(KERN_ERR "PM: Failed to allocate LZO data\n");
  ret = -ENOMEM;
  goto out_clean;
 }
 for (thr = 0; thr < nr_threads; thr++)
  memset(&data[thr], 0, offsetof(struct cmp_data, go));

 crc = kmalloc(sizeof(*crc), GFP_KERNEL);
 if (!crc) {
  printk(KERN_ERR "PM: Failed to allocate crc\n");
  ret = -ENOMEM;
  goto out_clean;
 }
 memset(crc, 0, offsetof(struct crc_data, go));




 for (thr = 0; thr < nr_threads; thr++) {
  init_waitqueue_head(&data[thr].go);
  init_waitqueue_head(&data[thr].done);

  data[thr].thr = kthread_run(lzo_compress_threadfn,
                              &data[thr],
                              "image_compress/%u", thr);
  if (IS_ERR(data[thr].thr)) {
   data[thr].thr = NULL;
   printk(KERN_ERR
          "PM: Cannot start compression threads\n");
   ret = -ENOMEM;
   goto out_clean;
  }
 }




 init_waitqueue_head(&crc->go);
 init_waitqueue_head(&crc->done);

 handle->crc32 = 0;
 crc->crc32 = &handle->crc32;
 for (thr = 0; thr < nr_threads; thr++) {
  crc->unc[thr] = data[thr].unc;
  crc->unc_len[thr] = &data[thr].unc_len;
 }

 crc->thr = kthread_run(crc32_threadfn, crc, "image_crc32");
 if (IS_ERR(crc->thr)) {
  crc->thr = NULL;
  printk(KERN_ERR "PM: Cannot start CRC32 thread\n");
  ret = -ENOMEM;
  goto out_clean;
 }





 handle->reqd_free_pages = reqd_free_pages();

 printk(KERN_INFO
  "PM: Using %u thread(s) for compression.\n"
  "PM: Compressing and saving image data (%u pages)...\n",
  nr_threads, nr_to_write);
 m = nr_to_write / 10;
 if (!m)
  m = 1;
 nr_pages = 0;
 start = ktime_get();
 for (;;) {
  for (thr = 0; thr < nr_threads; thr++) {
   for (off = 0; off < LZO_UNC_SIZE; off += PAGE_SIZE) {
    ret = snapshot_read_next(snapshot);
    if (ret < 0)
     goto out_finish;

    if (!ret)
     break;

    memcpy(data[thr].unc + off,
           data_of(*snapshot), PAGE_SIZE);

    if (!(nr_pages % m))
     printk(KERN_INFO
            "PM: Image saving progress: "
            "%3d%%\n",
                   nr_pages / m * 10);
    nr_pages++;
   }
   if (!off)
    break;

   data[thr].unc_len = off;

   atomic_set(&data[thr].ready, 1);
   wake_up(&data[thr].go);
  }

  if (!thr)
   break;

  crc->run_threads = thr;
  atomic_set(&crc->ready, 1);
  wake_up(&crc->go);

  for (run_threads = thr, thr = 0; thr < run_threads; thr++) {
   wait_event(data[thr].done,
              atomic_read(&data[thr].stop));
   atomic_set(&data[thr].stop, 0);

   ret = data[thr].ret;

   if (ret < 0) {
    printk(KERN_ERR "PM: LZO compression failed\n");
    goto out_finish;
   }

   if (unlikely(!data[thr].cmp_len ||
                data[thr].cmp_len >
                lzo1x_worst_compress(data[thr].unc_len))) {
    printk(KERN_ERR
           "PM: Invalid LZO compressed length\n");
    ret = -1;
    goto out_finish;
   }

   *(size_t *)data[thr].cmp = data[thr].cmp_len;
   for (off = 0;
        off < LZO_HEADER + data[thr].cmp_len;
        off += PAGE_SIZE) {
    memcpy(page, data[thr].cmp + off, PAGE_SIZE);

    ret = swap_write_page(handle, page, &hb);
    if (ret)
     goto out_finish;
   }
  }

  wait_event(crc->done, atomic_read(&crc->stop));
  atomic_set(&crc->stop, 0);
 }

out_finish:
 err2 = hib_wait_io(&hb);
 stop = ktime_get();
 if (!ret)
  ret = err2;
 if (!ret)
  printk(KERN_INFO "PM: Image saving done.\n");
 swsusp_show_speed(start, stop, nr_to_write, "Wrote");
out_clean:
 if (crc) {
  if (crc->thr)
   kthread_stop(crc->thr);
  kfree(crc);
 }
 if (data) {
  for (thr = 0; thr < nr_threads; thr++)
   if (data[thr].thr)
    kthread_stop(data[thr].thr);
  vfree(data);
 }
 if (page) free_page((unsigned long)page);

 return ret;
}
static int enough_swap(unsigned int nr_pages, unsigned int flags)
{
 unsigned int free_swap = count_swap_pages(root_swap, 1);
 unsigned int required;

 pr_debug("PM: Free swap pages: %u\n", free_swap);

 required = PAGES_FOR_IO + nr_pages;
 return free_swap > required;
}
int swsusp_write(unsigned int flags)
{
 struct swap_map_handle handle;
 struct snapshot_handle snapshot;
 struct swsusp_info *header;
 unsigned long pages;
 int error;

 pages = snapshot_get_image_size();
 error = get_swap_writer(&handle);
 if (error) {
  printk(KERN_ERR "PM: Cannot get swap writer\n");
  return error;
 }
 if (flags & SF_NOCOMPRESS_MODE) {
  if (!enough_swap(pages, flags)) {
   printk(KERN_ERR "PM: Not enough free swap\n");
   error = -ENOSPC;
   goto out_finish;
  }
 }
 memset(&snapshot, 0, sizeof(struct snapshot_handle));
 error = snapshot_read_next(&snapshot);
 if (error < PAGE_SIZE) {
  if (error >= 0)
   error = -EFAULT;

  goto out_finish;
 }
 header = (struct swsusp_info *)data_of(snapshot);
 error = swap_write_page(&handle, header, NULL);
 if (!error) {
  error = (flags & SF_NOCOMPRESS_MODE) ?
   save_image(&handle, &snapshot, pages - 1) :
   save_image_lzo(&handle, &snapshot, pages - 1);
 }
out_finish:
 error = swap_writer_finish(&handle, flags, error);
 return error;
}






static void release_swap_reader(struct swap_map_handle *handle)
{
 struct swap_map_page_list *tmp;

 while (handle->maps) {
  if (handle->maps->map)
   free_page((unsigned long)handle->maps->map);
  tmp = handle->maps;
  handle->maps = handle->maps->next;
  kfree(tmp);
 }
 handle->cur = NULL;
}

static int get_swap_reader(struct swap_map_handle *handle,
  unsigned int *flags_p)
{
 int error;
 struct swap_map_page_list *tmp, *last;
 sector_t offset;

 *flags_p = swsusp_header->flags;

 if (!swsusp_header->image)
  return -EINVAL;

 handle->cur = NULL;
 last = handle->maps = NULL;
 offset = swsusp_header->image;
 while (offset) {
  tmp = kmalloc(sizeof(*handle->maps), GFP_KERNEL);
  if (!tmp) {
   release_swap_reader(handle);
   return -ENOMEM;
  }
  memset(tmp, 0, sizeof(*tmp));
  if (!handle->maps)
   handle->maps = tmp;
  if (last)
   last->next = tmp;
  last = tmp;

  tmp->map = (struct swap_map_page *)
      __get_free_page(__GFP_RECLAIM | __GFP_HIGH);
  if (!tmp->map) {
   release_swap_reader(handle);
   return -ENOMEM;
  }

  error = hib_submit_io(READ_SYNC, offset, tmp->map, NULL);
  if (error) {
   release_swap_reader(handle);
   return error;
  }
  offset = tmp->map->next_swap;
 }
 handle->k = 0;
 handle->cur = handle->maps->map;
 return 0;
}

static int swap_read_page(struct swap_map_handle *handle, void *buf,
  struct hib_bio_batch *hb)
{
 sector_t offset;
 int error;
 struct swap_map_page_list *tmp;

 if (!handle->cur)
  return -EINVAL;
 offset = handle->cur->entries[handle->k];
 if (!offset)
  return -EFAULT;
 error = hib_submit_io(READ_SYNC, offset, buf, hb);
 if (error)
  return error;
 if (++handle->k >= MAP_PAGE_ENTRIES) {
  handle->k = 0;
  free_page((unsigned long)handle->maps->map);
  tmp = handle->maps;
  handle->maps = handle->maps->next;
  kfree(tmp);
  if (!handle->maps)
   release_swap_reader(handle);
  else
   handle->cur = handle->maps->map;
 }
 return error;
}

static int swap_reader_finish(struct swap_map_handle *handle)
{
 release_swap_reader(handle);

 return 0;
}







static int load_image(struct swap_map_handle *handle,
                      struct snapshot_handle *snapshot,
                      unsigned int nr_to_read)
{
 unsigned int m;
 int ret = 0;
 ktime_t start;
 ktime_t stop;
 struct hib_bio_batch hb;
 int err2;
 unsigned nr_pages;

 hib_init_batch(&hb);

 clean_pages_on_read = true;
 printk(KERN_INFO "PM: Loading image data pages (%u pages)...\n",
  nr_to_read);
 m = nr_to_read / 10;
 if (!m)
  m = 1;
 nr_pages = 0;
 start = ktime_get();
 for ( ; ; ) {
  ret = snapshot_write_next(snapshot);
  if (ret <= 0)
   break;
  ret = swap_read_page(handle, data_of(*snapshot), &hb);
  if (ret)
   break;
  if (snapshot->sync_read)
   ret = hib_wait_io(&hb);
  if (ret)
   break;
  if (!(nr_pages % m))
   printk(KERN_INFO "PM: Image loading progress: %3d%%\n",
          nr_pages / m * 10);
  nr_pages++;
 }
 err2 = hib_wait_io(&hb);
 stop = ktime_get();
 if (!ret)
  ret = err2;
 if (!ret) {
  printk(KERN_INFO "PM: Image loading done.\n");
  snapshot_write_finalize(snapshot);
  if (!snapshot_image_loaded(snapshot))
   ret = -ENODATA;
 }
 swsusp_show_speed(start, stop, nr_to_read, "Read");
 return ret;
}




struct dec_data {
 struct task_struct *thr;
 atomic_t ready;
 atomic_t stop;
 int ret;
 wait_queue_head_t go;
 wait_queue_head_t done;
 size_t unc_len;
 size_t cmp_len;
 unsigned char unc[LZO_UNC_SIZE];
 unsigned char cmp[LZO_CMP_SIZE];
};




static int lzo_decompress_threadfn(void *data)
{
 struct dec_data *d = data;

 while (1) {
  wait_event(d->go, atomic_read(&d->ready) ||
                    kthread_should_stop());
  if (kthread_should_stop()) {
   d->thr = NULL;
   d->ret = -1;
   atomic_set(&d->stop, 1);
   wake_up(&d->done);
   break;
  }
  atomic_set(&d->ready, 0);

  d->unc_len = LZO_UNC_SIZE;
  d->ret = lzo1x_decompress_safe(d->cmp + LZO_HEADER, d->cmp_len,
                                 d->unc, &d->unc_len);
  if (clean_pages_on_decompress)
   flush_icache_range((unsigned long)d->unc,
        (unsigned long)d->unc + d->unc_len);

  atomic_set(&d->stop, 1);
  wake_up(&d->done);
 }
 return 0;
}







static int load_image_lzo(struct swap_map_handle *handle,
                          struct snapshot_handle *snapshot,
                          unsigned int nr_to_read)
{
 unsigned int m;
 int ret = 0;
 int eof = 0;
 struct hib_bio_batch hb;
 ktime_t start;
 ktime_t stop;
 unsigned nr_pages;
 size_t off;
 unsigned i, thr, run_threads, nr_threads;
 unsigned ring = 0, pg = 0, ring_size = 0,
          have = 0, want, need, asked = 0;
 unsigned long read_pages = 0;
 unsigned char **page = NULL;
 struct dec_data *data = NULL;
 struct crc_data *crc = NULL;

 hib_init_batch(&hb);





 nr_threads = num_online_cpus() - 1;
 nr_threads = clamp_val(nr_threads, 1, LZO_THREADS);

 page = vmalloc(sizeof(*page) * LZO_MAX_RD_PAGES);
 if (!page) {
  printk(KERN_ERR "PM: Failed to allocate LZO page\n");
  ret = -ENOMEM;
  goto out_clean;
 }

 data = vmalloc(sizeof(*data) * nr_threads);
 if (!data) {
  printk(KERN_ERR "PM: Failed to allocate LZO data\n");
  ret = -ENOMEM;
  goto out_clean;
 }
 for (thr = 0; thr < nr_threads; thr++)
  memset(&data[thr], 0, offsetof(struct dec_data, go));

 crc = kmalloc(sizeof(*crc), GFP_KERNEL);
 if (!crc) {
  printk(KERN_ERR "PM: Failed to allocate crc\n");
  ret = -ENOMEM;
  goto out_clean;
 }
 memset(crc, 0, offsetof(struct crc_data, go));

 clean_pages_on_decompress = true;




 for (thr = 0; thr < nr_threads; thr++) {
  init_waitqueue_head(&data[thr].go);
  init_waitqueue_head(&data[thr].done);

  data[thr].thr = kthread_run(lzo_decompress_threadfn,
                              &data[thr],
                              "image_decompress/%u", thr);
  if (IS_ERR(data[thr].thr)) {
   data[thr].thr = NULL;
   printk(KERN_ERR
          "PM: Cannot start decompression threads\n");
   ret = -ENOMEM;
   goto out_clean;
  }
 }




 init_waitqueue_head(&crc->go);
 init_waitqueue_head(&crc->done);

 handle->crc32 = 0;
 crc->crc32 = &handle->crc32;
 for (thr = 0; thr < nr_threads; thr++) {
  crc->unc[thr] = data[thr].unc;
  crc->unc_len[thr] = &data[thr].unc_len;
 }

 crc->thr = kthread_run(crc32_threadfn, crc, "image_crc32");
 if (IS_ERR(crc->thr)) {
  crc->thr = NULL;
  printk(KERN_ERR "PM: Cannot start CRC32 thread\n");
  ret = -ENOMEM;
  goto out_clean;
 }
 if (low_free_pages() > snapshot_get_image_size())
  read_pages = (low_free_pages() - snapshot_get_image_size()) / 2;
 read_pages = clamp_val(read_pages, LZO_MIN_RD_PAGES, LZO_MAX_RD_PAGES);

 for (i = 0; i < read_pages; i++) {
  page[i] = (void *)__get_free_page(i < LZO_CMP_PAGES ?
        __GFP_RECLAIM | __GFP_HIGH :
        __GFP_RECLAIM | __GFP_NOWARN |
        __GFP_NORETRY);

  if (!page[i]) {
   if (i < LZO_CMP_PAGES) {
    ring_size = i;
    printk(KERN_ERR
           "PM: Failed to allocate LZO pages\n");
    ret = -ENOMEM;
    goto out_clean;
   } else {
    break;
   }
  }
 }
 want = ring_size = i;

 printk(KERN_INFO
  "PM: Using %u thread(s) for decompression.\n"
  "PM: Loading and decompressing image data (%u pages)...\n",
  nr_threads, nr_to_read);
 m = nr_to_read / 10;
 if (!m)
  m = 1;
 nr_pages = 0;
 start = ktime_get();

 ret = snapshot_write_next(snapshot);
 if (ret <= 0)
  goto out_finish;

 for(;;) {
  for (i = 0; !eof && i < want; i++) {
   ret = swap_read_page(handle, page[ring], &hb);
   if (ret) {




    if (handle->cur &&
        handle->cur->entries[handle->k]) {
     goto out_finish;
    } else {
     eof = 1;
     break;
    }
   }
   if (++ring >= ring_size)
    ring = 0;
  }
  asked += i;
  want -= i;




  if (!have) {
   if (!asked)
    break;

   ret = hib_wait_io(&hb);
   if (ret)
    goto out_finish;
   have += asked;
   asked = 0;
   if (eof)
    eof = 2;
  }

  if (crc->run_threads) {
   wait_event(crc->done, atomic_read(&crc->stop));
   atomic_set(&crc->stop, 0);
   crc->run_threads = 0;
  }

  for (thr = 0; have && thr < nr_threads; thr++) {
   data[thr].cmp_len = *(size_t *)page[pg];
   if (unlikely(!data[thr].cmp_len ||
                data[thr].cmp_len >
                lzo1x_worst_compress(LZO_UNC_SIZE))) {
    printk(KERN_ERR
           "PM: Invalid LZO compressed length\n");
    ret = -1;
    goto out_finish;
   }

   need = DIV_ROUND_UP(data[thr].cmp_len + LZO_HEADER,
                       PAGE_SIZE);
   if (need > have) {
    if (eof > 1) {
     ret = -1;
     goto out_finish;
    }
    break;
   }

   for (off = 0;
        off < LZO_HEADER + data[thr].cmp_len;
        off += PAGE_SIZE) {
    memcpy(data[thr].cmp + off,
           page[pg], PAGE_SIZE);
    have--;
    want++;
    if (++pg >= ring_size)
     pg = 0;
   }

   atomic_set(&data[thr].ready, 1);
   wake_up(&data[thr].go);
  }




  if (have < LZO_CMP_PAGES && asked) {
   ret = hib_wait_io(&hb);
   if (ret)
    goto out_finish;
   have += asked;
   asked = 0;
   if (eof)
    eof = 2;
  }

  for (run_threads = thr, thr = 0; thr < run_threads; thr++) {
   wait_event(data[thr].done,
              atomic_read(&data[thr].stop));
   atomic_set(&data[thr].stop, 0);

   ret = data[thr].ret;

   if (ret < 0) {
    printk(KERN_ERR
           "PM: LZO decompression failed\n");
    goto out_finish;
   }

   if (unlikely(!data[thr].unc_len ||
                data[thr].unc_len > LZO_UNC_SIZE ||
                data[thr].unc_len & (PAGE_SIZE - 1))) {
    printk(KERN_ERR
           "PM: Invalid LZO uncompressed length\n");
    ret = -1;
    goto out_finish;
   }

   for (off = 0;
        off < data[thr].unc_len; off += PAGE_SIZE) {
    memcpy(data_of(*snapshot),
           data[thr].unc + off, PAGE_SIZE);

    if (!(nr_pages % m))
     printk(KERN_INFO
            "PM: Image loading progress: "
            "%3d%%\n",
            nr_pages / m * 10);
    nr_pages++;

    ret = snapshot_write_next(snapshot);
    if (ret <= 0) {
     crc->run_threads = thr + 1;
     atomic_set(&crc->ready, 1);
     wake_up(&crc->go);
     goto out_finish;
    }
   }
  }

  crc->run_threads = thr;
  atomic_set(&crc->ready, 1);
  wake_up(&crc->go);
 }

out_finish:
 if (crc->run_threads) {
  wait_event(crc->done, atomic_read(&crc->stop));
  atomic_set(&crc->stop, 0);
 }
 stop = ktime_get();
 if (!ret) {
  printk(KERN_INFO "PM: Image loading done.\n");
  snapshot_write_finalize(snapshot);
  if (!snapshot_image_loaded(snapshot))
   ret = -ENODATA;
  if (!ret) {
   if (swsusp_header->flags & SF_CRC32_MODE) {
    if(handle->crc32 != swsusp_header->crc32) {
     printk(KERN_ERR
            "PM: Invalid image CRC32!\n");
     ret = -ENODATA;
    }
   }
  }
 }
 swsusp_show_speed(start, stop, nr_to_read, "Read");
out_clean:
 for (i = 0; i < ring_size; i++)
  free_page((unsigned long)page[i]);
 if (crc) {
  if (crc->thr)
   kthread_stop(crc->thr);
  kfree(crc);
 }
 if (data) {
  for (thr = 0; thr < nr_threads; thr++)
   if (data[thr].thr)
    kthread_stop(data[thr].thr);
  vfree(data);
 }
 vfree(page);

 return ret;
}







int swsusp_read(unsigned int *flags_p)
{
 int error;
 struct swap_map_handle handle;
 struct snapshot_handle snapshot;
 struct swsusp_info *header;

 memset(&snapshot, 0, sizeof(struct snapshot_handle));
 error = snapshot_write_next(&snapshot);
 if (error < PAGE_SIZE)
  return error < 0 ? error : -EFAULT;
 header = (struct swsusp_info *)data_of(snapshot);
 error = get_swap_reader(&handle, flags_p);
 if (error)
  goto end;
 if (!error)
  error = swap_read_page(&handle, header, NULL);
 if (!error) {
  error = (*flags_p & SF_NOCOMPRESS_MODE) ?
   load_image(&handle, &snapshot, header->pages - 1) :
   load_image_lzo(&handle, &snapshot, header->pages - 1);
 }
 swap_reader_finish(&handle);
end:
 if (!error)
  pr_debug("PM: Image successfully loaded\n");
 else
  pr_debug("PM: Error %d resuming\n", error);
 return error;
}





int swsusp_check(void)
{
 int error;

 hib_resume_bdev = blkdev_get_by_dev(swsusp_resume_device,
         FMODE_READ, NULL);
 if (!IS_ERR(hib_resume_bdev)) {
  set_blocksize(hib_resume_bdev, PAGE_SIZE);
  clear_page(swsusp_header);
  error = hib_submit_io(READ_SYNC, swsusp_resume_block,
     swsusp_header, NULL);
  if (error)
   goto put;

  if (!memcmp(HIBERNATE_SIG, swsusp_header->sig, 10)) {
   memcpy(swsusp_header->sig, swsusp_header->orig_sig, 10);

   error = hib_submit_io(WRITE_SYNC, swsusp_resume_block,
      swsusp_header, NULL);
  } else {
   error = -EINVAL;
  }

put:
  if (error)
   blkdev_put(hib_resume_bdev, FMODE_READ);
  else
   pr_debug("PM: Image signature found, resuming\n");
 } else {
  error = PTR_ERR(hib_resume_bdev);
 }

 if (error)
  pr_debug("PM: Image not found (code %d)\n", error);

 return error;
}





void swsusp_close(fmode_t mode)
{
 if (IS_ERR(hib_resume_bdev)) {
  pr_debug("PM: Image device not initialised\n");
  return;
 }

 blkdev_put(hib_resume_bdev, mode);
}





int swsusp_unmark(void)
{
 int error;

 hib_submit_io(READ_SYNC, swsusp_resume_block, swsusp_header, NULL);
 if (!memcmp(HIBERNATE_SIG,swsusp_header->sig, 10)) {
  memcpy(swsusp_header->sig,swsusp_header->orig_sig, 10);
  error = hib_submit_io(WRITE_SYNC, swsusp_resume_block,
     swsusp_header, NULL);
 } else {
  printk(KERN_ERR "PM: Cannot find swsusp signature!\n");
  error = -ENODEV;
 }




 free_all_swap_pages(root_swap);

 return error;
}

static int swsusp_header_init(void)
{
 swsusp_header = (struct swsusp_header*) __get_free_page(GFP_KERNEL);
 if (!swsusp_header)
  panic("Could not allocate memory for swsusp_header\n");
 return 0;
}

core_initcall(swsusp_header_init);


















int overflowuid = DEFAULT_OVERFLOWUID;
int overflowgid = DEFAULT_OVERFLOWGID;

EXPORT_SYMBOL(overflowuid);
EXPORT_SYMBOL(overflowgid);






int fs_overflowuid = DEFAULT_FS_OVERFLOWUID;
int fs_overflowgid = DEFAULT_FS_OVERFLOWUID;

EXPORT_SYMBOL(fs_overflowuid);
EXPORT_SYMBOL(fs_overflowgid);







static bool set_one_prio_perm(struct task_struct *p)
{
 const struct cred *cred = current_cred(), *pcred = __task_cred(p);

 if (uid_eq(pcred->uid, cred->euid) ||
     uid_eq(pcred->euid, cred->euid))
  return true;
 if (ns_capable(pcred->user_ns, CAP_SYS_NICE))
  return true;
 return false;
}





static int set_one_prio(struct task_struct *p, int niceval, int error)
{
 int no_nice;

 if (!set_one_prio_perm(p)) {
  error = -EPERM;
  goto out;
 }
 if (niceval < task_nice(p) && !can_nice(p, niceval)) {
  error = -EACCES;
  goto out;
 }
 no_nice = security_task_setnice(p, niceval);
 if (no_nice) {
  error = no_nice;
  goto out;
 }
 if (error == -ESRCH)
  error = 0;
 set_user_nice(p, niceval);
out:
 return error;
}

SYSCALL_DEFINE3(setpriority, int, which, int, who, int, niceval)
{
 struct task_struct *g, *p;
 struct user_struct *user;
 const struct cred *cred = current_cred();
 int error = -EINVAL;
 struct pid *pgrp;
 kuid_t uid;

 if (which > PRIO_USER || which < PRIO_PROCESS)
  goto out;


 error = -ESRCH;
 if (niceval < MIN_NICE)
  niceval = MIN_NICE;
 if (niceval > MAX_NICE)
  niceval = MAX_NICE;

 rcu_read_lock();
 read_lock(&tasklist_lock);
 switch (which) {
 case PRIO_PROCESS:
  if (who)
   p = find_task_by_vpid(who);
  else
   p = current;
  if (p)
   error = set_one_prio(p, niceval, error);
  break;
 case PRIO_PGRP:
  if (who)
   pgrp = find_vpid(who);
  else
   pgrp = task_pgrp(current);
  do_each_pid_thread(pgrp, PIDTYPE_PGID, p) {
   error = set_one_prio(p, niceval, error);
  } while_each_pid_thread(pgrp, PIDTYPE_PGID, p);
  break;
 case PRIO_USER:
  uid = make_kuid(cred->user_ns, who);
  user = cred->user;
  if (!who)
   uid = cred->uid;
  else if (!uid_eq(uid, cred->uid)) {
   user = find_user(uid);
   if (!user)
    goto out_unlock;
  }
  do_each_thread(g, p) {
   if (uid_eq(task_uid(p), uid) && task_pid_vnr(p))
    error = set_one_prio(p, niceval, error);
  } while_each_thread(g, p);
  if (!uid_eq(uid, cred->uid))
   free_uid(user);
  break;
 }
out_unlock:
 read_unlock(&tasklist_lock);
 rcu_read_unlock();
out:
 return error;
}







SYSCALL_DEFINE2(getpriority, int, which, int, who)
{
 struct task_struct *g, *p;
 struct user_struct *user;
 const struct cred *cred = current_cred();
 long niceval, retval = -ESRCH;
 struct pid *pgrp;
 kuid_t uid;

 if (which > PRIO_USER || which < PRIO_PROCESS)
  return -EINVAL;

 rcu_read_lock();
 read_lock(&tasklist_lock);
 switch (which) {
 case PRIO_PROCESS:
  if (who)
   p = find_task_by_vpid(who);
  else
   p = current;
  if (p) {
   niceval = nice_to_rlimit(task_nice(p));
   if (niceval > retval)
    retval = niceval;
  }
  break;
 case PRIO_PGRP:
  if (who)
   pgrp = find_vpid(who);
  else
   pgrp = task_pgrp(current);
  do_each_pid_thread(pgrp, PIDTYPE_PGID, p) {
   niceval = nice_to_rlimit(task_nice(p));
   if (niceval > retval)
    retval = niceval;
  } while_each_pid_thread(pgrp, PIDTYPE_PGID, p);
  break;
 case PRIO_USER:
  uid = make_kuid(cred->user_ns, who);
  user = cred->user;
  if (!who)
   uid = cred->uid;
  else if (!uid_eq(uid, cred->uid)) {
   user = find_user(uid);
   if (!user)
    goto out_unlock;
  }
  do_each_thread(g, p) {
   if (uid_eq(task_uid(p), uid) && task_pid_vnr(p)) {
    niceval = nice_to_rlimit(task_nice(p));
    if (niceval > retval)
     retval = niceval;
   }
  } while_each_thread(g, p);
  if (!uid_eq(uid, cred->uid))
   free_uid(user);
  break;
 }
out_unlock:
 read_unlock(&tasklist_lock);
 rcu_read_unlock();

 return retval;
}
SYSCALL_DEFINE2(setregid, gid_t, rgid, gid_t, egid)
{
 struct user_namespace *ns = current_user_ns();
 const struct cred *old;
 struct cred *new;
 int retval;
 kgid_t krgid, kegid;

 krgid = make_kgid(ns, rgid);
 kegid = make_kgid(ns, egid);

 if ((rgid != (gid_t) -1) && !gid_valid(krgid))
  return -EINVAL;
 if ((egid != (gid_t) -1) && !gid_valid(kegid))
  return -EINVAL;

 new = prepare_creds();
 if (!new)
  return -ENOMEM;
 old = current_cred();

 retval = -EPERM;
 if (rgid != (gid_t) -1) {
  if (gid_eq(old->gid, krgid) ||
      gid_eq(old->egid, krgid) ||
      ns_capable(old->user_ns, CAP_SETGID))
   new->gid = krgid;
  else
   goto error;
 }
 if (egid != (gid_t) -1) {
  if (gid_eq(old->gid, kegid) ||
      gid_eq(old->egid, kegid) ||
      gid_eq(old->sgid, kegid) ||
      ns_capable(old->user_ns, CAP_SETGID))
   new->egid = kegid;
  else
   goto error;
 }

 if (rgid != (gid_t) -1 ||
     (egid != (gid_t) -1 && !gid_eq(kegid, old->gid)))
  new->sgid = new->egid;
 new->fsgid = new->egid;

 return commit_creds(new);

error:
 abort_creds(new);
 return retval;
}






SYSCALL_DEFINE1(setgid, gid_t, gid)
{
 struct user_namespace *ns = current_user_ns();
 const struct cred *old;
 struct cred *new;
 int retval;
 kgid_t kgid;

 kgid = make_kgid(ns, gid);
 if (!gid_valid(kgid))
  return -EINVAL;

 new = prepare_creds();
 if (!new)
  return -ENOMEM;
 old = current_cred();

 retval = -EPERM;
 if (ns_capable(old->user_ns, CAP_SETGID))
  new->gid = new->egid = new->sgid = new->fsgid = kgid;
 else if (gid_eq(kgid, old->gid) || gid_eq(kgid, old->sgid))
  new->egid = new->fsgid = kgid;
 else
  goto error;

 return commit_creds(new);

error:
 abort_creds(new);
 return retval;
}




static int set_user(struct cred *new)
{
 struct user_struct *new_user;

 new_user = alloc_uid(new->uid);
 if (!new_user)
  return -EAGAIN;
 if (atomic_read(&new_user->processes) >= rlimit(RLIMIT_NPROC) &&
   new_user != INIT_USER)
  current->flags |= PF_NPROC_EXCEEDED;
 else
  current->flags &= ~PF_NPROC_EXCEEDED;

 free_uid(new->user);
 new->user = new_user;
 return 0;
}
SYSCALL_DEFINE2(setreuid, uid_t, ruid, uid_t, euid)
{
 struct user_namespace *ns = current_user_ns();
 const struct cred *old;
 struct cred *new;
 int retval;
 kuid_t kruid, keuid;

 kruid = make_kuid(ns, ruid);
 keuid = make_kuid(ns, euid);

 if ((ruid != (uid_t) -1) && !uid_valid(kruid))
  return -EINVAL;
 if ((euid != (uid_t) -1) && !uid_valid(keuid))
  return -EINVAL;

 new = prepare_creds();
 if (!new)
  return -ENOMEM;
 old = current_cred();

 retval = -EPERM;
 if (ruid != (uid_t) -1) {
  new->uid = kruid;
  if (!uid_eq(old->uid, kruid) &&
      !uid_eq(old->euid, kruid) &&
      !ns_capable(old->user_ns, CAP_SETUID))
   goto error;
 }

 if (euid != (uid_t) -1) {
  new->euid = keuid;
  if (!uid_eq(old->uid, keuid) &&
      !uid_eq(old->euid, keuid) &&
      !uid_eq(old->suid, keuid) &&
      !ns_capable(old->user_ns, CAP_SETUID))
   goto error;
 }

 if (!uid_eq(new->uid, old->uid)) {
  retval = set_user(new);
  if (retval < 0)
   goto error;
 }
 if (ruid != (uid_t) -1 ||
     (euid != (uid_t) -1 && !uid_eq(keuid, old->uid)))
  new->suid = new->euid;
 new->fsuid = new->euid;

 retval = security_task_fix_setuid(new, old, LSM_SETID_RE);
 if (retval < 0)
  goto error;

 return commit_creds(new);

error:
 abort_creds(new);
 return retval;
}
SYSCALL_DEFINE1(setuid, uid_t, uid)
{
 struct user_namespace *ns = current_user_ns();
 const struct cred *old;
 struct cred *new;
 int retval;
 kuid_t kuid;

 kuid = make_kuid(ns, uid);
 if (!uid_valid(kuid))
  return -EINVAL;

 new = prepare_creds();
 if (!new)
  return -ENOMEM;
 old = current_cred();

 retval = -EPERM;
 if (ns_capable(old->user_ns, CAP_SETUID)) {
  new->suid = new->uid = kuid;
  if (!uid_eq(kuid, old->uid)) {
   retval = set_user(new);
   if (retval < 0)
    goto error;
  }
 } else if (!uid_eq(kuid, old->uid) && !uid_eq(kuid, new->suid)) {
  goto error;
 }

 new->fsuid = new->euid = kuid;

 retval = security_task_fix_setuid(new, old, LSM_SETID_ID);
 if (retval < 0)
  goto error;

 return commit_creds(new);

error:
 abort_creds(new);
 return retval;
}






SYSCALL_DEFINE3(setresuid, uid_t, ruid, uid_t, euid, uid_t, suid)
{
 struct user_namespace *ns = current_user_ns();
 const struct cred *old;
 struct cred *new;
 int retval;
 kuid_t kruid, keuid, ksuid;

 kruid = make_kuid(ns, ruid);
 keuid = make_kuid(ns, euid);
 ksuid = make_kuid(ns, suid);

 if ((ruid != (uid_t) -1) && !uid_valid(kruid))
  return -EINVAL;

 if ((euid != (uid_t) -1) && !uid_valid(keuid))
  return -EINVAL;

 if ((suid != (uid_t) -1) && !uid_valid(ksuid))
  return -EINVAL;

 new = prepare_creds();
 if (!new)
  return -ENOMEM;

 old = current_cred();

 retval = -EPERM;
 if (!ns_capable(old->user_ns, CAP_SETUID)) {
  if (ruid != (uid_t) -1 && !uid_eq(kruid, old->uid) &&
      !uid_eq(kruid, old->euid) && !uid_eq(kruid, old->suid))
   goto error;
  if (euid != (uid_t) -1 && !uid_eq(keuid, old->uid) &&
      !uid_eq(keuid, old->euid) && !uid_eq(keuid, old->suid))
   goto error;
  if (suid != (uid_t) -1 && !uid_eq(ksuid, old->uid) &&
      !uid_eq(ksuid, old->euid) && !uid_eq(ksuid, old->suid))
   goto error;
 }

 if (ruid != (uid_t) -1) {
  new->uid = kruid;
  if (!uid_eq(kruid, old->uid)) {
   retval = set_user(new);
   if (retval < 0)
    goto error;
  }
 }
 if (euid != (uid_t) -1)
  new->euid = keuid;
 if (suid != (uid_t) -1)
  new->suid = ksuid;
 new->fsuid = new->euid;

 retval = security_task_fix_setuid(new, old, LSM_SETID_RES);
 if (retval < 0)
  goto error;

 return commit_creds(new);

error:
 abort_creds(new);
 return retval;
}

SYSCALL_DEFINE3(getresuid, uid_t __user *, ruidp, uid_t __user *, euidp, uid_t __user *, suidp)
{
 const struct cred *cred = current_cred();
 int retval;
 uid_t ruid, euid, suid;

 ruid = from_kuid_munged(cred->user_ns, cred->uid);
 euid = from_kuid_munged(cred->user_ns, cred->euid);
 suid = from_kuid_munged(cred->user_ns, cred->suid);

 retval = put_user(ruid, ruidp);
 if (!retval) {
  retval = put_user(euid, euidp);
  if (!retval)
   return put_user(suid, suidp);
 }
 return retval;
}




SYSCALL_DEFINE3(setresgid, gid_t, rgid, gid_t, egid, gid_t, sgid)
{
 struct user_namespace *ns = current_user_ns();
 const struct cred *old;
 struct cred *new;
 int retval;
 kgid_t krgid, kegid, ksgid;

 krgid = make_kgid(ns, rgid);
 kegid = make_kgid(ns, egid);
 ksgid = make_kgid(ns, sgid);

 if ((rgid != (gid_t) -1) && !gid_valid(krgid))
  return -EINVAL;
 if ((egid != (gid_t) -1) && !gid_valid(kegid))
  return -EINVAL;
 if ((sgid != (gid_t) -1) && !gid_valid(ksgid))
  return -EINVAL;

 new = prepare_creds();
 if (!new)
  return -ENOMEM;
 old = current_cred();

 retval = -EPERM;
 if (!ns_capable(old->user_ns, CAP_SETGID)) {
  if (rgid != (gid_t) -1 && !gid_eq(krgid, old->gid) &&
      !gid_eq(krgid, old->egid) && !gid_eq(krgid, old->sgid))
   goto error;
  if (egid != (gid_t) -1 && !gid_eq(kegid, old->gid) &&
      !gid_eq(kegid, old->egid) && !gid_eq(kegid, old->sgid))
   goto error;
  if (sgid != (gid_t) -1 && !gid_eq(ksgid, old->gid) &&
      !gid_eq(ksgid, old->egid) && !gid_eq(ksgid, old->sgid))
   goto error;
 }

 if (rgid != (gid_t) -1)
  new->gid = krgid;
 if (egid != (gid_t) -1)
  new->egid = kegid;
 if (sgid != (gid_t) -1)
  new->sgid = ksgid;
 new->fsgid = new->egid;

 return commit_creds(new);

error:
 abort_creds(new);
 return retval;
}

SYSCALL_DEFINE3(getresgid, gid_t __user *, rgidp, gid_t __user *, egidp, gid_t __user *, sgidp)
{
 const struct cred *cred = current_cred();
 int retval;
 gid_t rgid, egid, sgid;

 rgid = from_kgid_munged(cred->user_ns, cred->gid);
 egid = from_kgid_munged(cred->user_ns, cred->egid);
 sgid = from_kgid_munged(cred->user_ns, cred->sgid);

 retval = put_user(rgid, rgidp);
 if (!retval) {
  retval = put_user(egid, egidp);
  if (!retval)
   retval = put_user(sgid, sgidp);
 }

 return retval;
}
SYSCALL_DEFINE1(setfsuid, uid_t, uid)
{
 const struct cred *old;
 struct cred *new;
 uid_t old_fsuid;
 kuid_t kuid;

 old = current_cred();
 old_fsuid = from_kuid_munged(old->user_ns, old->fsuid);

 kuid = make_kuid(old->user_ns, uid);
 if (!uid_valid(kuid))
  return old_fsuid;

 new = prepare_creds();
 if (!new)
  return old_fsuid;

 if (uid_eq(kuid, old->uid) || uid_eq(kuid, old->euid) ||
     uid_eq(kuid, old->suid) || uid_eq(kuid, old->fsuid) ||
     ns_capable(old->user_ns, CAP_SETUID)) {
  if (!uid_eq(kuid, old->fsuid)) {
   new->fsuid = kuid;
   if (security_task_fix_setuid(new, old, LSM_SETID_FS) == 0)
    goto change_okay;
  }
 }

 abort_creds(new);
 return old_fsuid;

change_okay:
 commit_creds(new);
 return old_fsuid;
}




SYSCALL_DEFINE1(setfsgid, gid_t, gid)
{
 const struct cred *old;
 struct cred *new;
 gid_t old_fsgid;
 kgid_t kgid;

 old = current_cred();
 old_fsgid = from_kgid_munged(old->user_ns, old->fsgid);

 kgid = make_kgid(old->user_ns, gid);
 if (!gid_valid(kgid))
  return old_fsgid;

 new = prepare_creds();
 if (!new)
  return old_fsgid;

 if (gid_eq(kgid, old->gid) || gid_eq(kgid, old->egid) ||
     gid_eq(kgid, old->sgid) || gid_eq(kgid, old->fsgid) ||
     ns_capable(old->user_ns, CAP_SETGID)) {
  if (!gid_eq(kgid, old->fsgid)) {
   new->fsgid = kgid;
   goto change_okay;
  }
 }

 abort_creds(new);
 return old_fsgid;

change_okay:
 commit_creds(new);
 return old_fsgid;
}
SYSCALL_DEFINE0(getpid)
{
 return task_tgid_vnr(current);
}


SYSCALL_DEFINE0(gettid)
{
 return task_pid_vnr(current);
}







SYSCALL_DEFINE0(getppid)
{
 int pid;

 rcu_read_lock();
 pid = task_tgid_vnr(rcu_dereference(current->real_parent));
 rcu_read_unlock();

 return pid;
}

SYSCALL_DEFINE0(getuid)
{

 return from_kuid_munged(current_user_ns(), current_uid());
}

SYSCALL_DEFINE0(geteuid)
{

 return from_kuid_munged(current_user_ns(), current_euid());
}

SYSCALL_DEFINE0(getgid)
{

 return from_kgid_munged(current_user_ns(), current_gid());
}

SYSCALL_DEFINE0(getegid)
{

 return from_kgid_munged(current_user_ns(), current_egid());
}

void do_sys_times(struct tms *tms)
{
 cputime_t tgutime, tgstime, cutime, cstime;

 thread_group_cputime_adjusted(current, &tgutime, &tgstime);
 cutime = current->signal->cutime;
 cstime = current->signal->cstime;
 tms->tms_utime = cputime_to_clock_t(tgutime);
 tms->tms_stime = cputime_to_clock_t(tgstime);
 tms->tms_cutime = cputime_to_clock_t(cutime);
 tms->tms_cstime = cputime_to_clock_t(cstime);
}

SYSCALL_DEFINE1(times, struct tms __user *, tbuf)
{
 if (tbuf) {
  struct tms tmp;

  do_sys_times(&tmp);
  if (copy_to_user(tbuf, &tmp, sizeof(struct tms)))
   return -EFAULT;
 }
 force_successful_syscall_return();
 return (long) jiffies_64_to_clock_t(get_jiffies_64());
}
SYSCALL_DEFINE2(setpgid, pid_t, pid, pid_t, pgid)
{
 struct task_struct *p;
 struct task_struct *group_leader = current->group_leader;
 struct pid *pgrp;
 int err;

 if (!pid)
  pid = task_pid_vnr(group_leader);
 if (!pgid)
  pgid = pid;
 if (pgid < 0)
  return -EINVAL;
 rcu_read_lock();




 write_lock_irq(&tasklist_lock);

 err = -ESRCH;
 p = find_task_by_vpid(pid);
 if (!p)
  goto out;

 err = -EINVAL;
 if (!thread_group_leader(p))
  goto out;

 if (same_thread_group(p->real_parent, group_leader)) {
  err = -EPERM;
  if (task_session(p) != task_session(group_leader))
   goto out;
  err = -EACCES;
  if (!(p->flags & PF_FORKNOEXEC))
   goto out;
 } else {
  err = -ESRCH;
  if (p != group_leader)
   goto out;
 }

 err = -EPERM;
 if (p->signal->leader)
  goto out;

 pgrp = task_pid(p);
 if (pgid != pid) {
  struct task_struct *g;

  pgrp = find_vpid(pgid);
  g = pid_task(pgrp, PIDTYPE_PGID);
  if (!g || task_session(g) != task_session(group_leader))
   goto out;
 }

 err = security_task_setpgid(p, pgid);
 if (err)
  goto out;

 if (task_pgrp(p) != pgrp)
  change_pid(p, PIDTYPE_PGID, pgrp);

 err = 0;
out:

 write_unlock_irq(&tasklist_lock);
 rcu_read_unlock();
 return err;
}

SYSCALL_DEFINE1(getpgid, pid_t, pid)
{
 struct task_struct *p;
 struct pid *grp;
 int retval;

 rcu_read_lock();
 if (!pid)
  grp = task_pgrp(current);
 else {
  retval = -ESRCH;
  p = find_task_by_vpid(pid);
  if (!p)
   goto out;
  grp = task_pgrp(p);
  if (!grp)
   goto out;

  retval = security_task_getpgid(p);
  if (retval)
   goto out;
 }
 retval = pid_vnr(grp);
out:
 rcu_read_unlock();
 return retval;
}


SYSCALL_DEFINE0(getpgrp)
{
 return sys_getpgid(0);
}


SYSCALL_DEFINE1(getsid, pid_t, pid)
{
 struct task_struct *p;
 struct pid *sid;
 int retval;

 rcu_read_lock();
 if (!pid)
  sid = task_session(current);
 else {
  retval = -ESRCH;
  p = find_task_by_vpid(pid);
  if (!p)
   goto out;
  sid = task_session(p);
  if (!sid)
   goto out;

  retval = security_task_getsid(p);
  if (retval)
   goto out;
 }
 retval = pid_vnr(sid);
out:
 rcu_read_unlock();
 return retval;
}

static void set_special_pids(struct pid *pid)
{
 struct task_struct *curr = current->group_leader;

 if (task_session(curr) != pid)
  change_pid(curr, PIDTYPE_SID, pid);

 if (task_pgrp(curr) != pid)
  change_pid(curr, PIDTYPE_PGID, pid);
}

SYSCALL_DEFINE0(setsid)
{
 struct task_struct *group_leader = current->group_leader;
 struct pid *sid = task_pid(group_leader);
 pid_t session = pid_vnr(sid);
 int err = -EPERM;

 write_lock_irq(&tasklist_lock);

 if (group_leader->signal->leader)
  goto out;




 if (pid_task(sid, PIDTYPE_PGID))
  goto out;

 group_leader->signal->leader = 1;
 set_special_pids(sid);

 proc_clear_tty(group_leader);

 err = session;
out:
 write_unlock_irq(&tasklist_lock);
 if (err > 0) {
  proc_sid_connector(group_leader);
  sched_autogroup_create_attach(group_leader);
 }
 return err;
}

DECLARE_RWSEM(uts_sem);

 (personality(current->personality) == PER_LINUX32 && \
  copy_to_user(name->machine, COMPAT_UTS_MACHINE, \
        sizeof(COMPAT_UTS_MACHINE)))






static int override_release(char __user *release, size_t len)
{
 int ret = 0;

 if (current->personality & UNAME26) {
  const char *rest = UTS_RELEASE;
  char buf[65] = { 0 };
  int ndots = 0;
  unsigned v;
  size_t copy;

  while (*rest) {
   if (*rest == '.' && ++ndots >= 3)
    break;
   if (!isdigit(*rest) && *rest != '.')
    break;
   rest++;
  }
  v = ((LINUX_VERSION_CODE >> 8) & 0xff) + 60;
  copy = clamp_t(size_t, len, 1, sizeof(buf));
  copy = scnprintf(buf, copy, "2.6.%u%s", v, rest);
  ret = copy_to_user(release, buf, copy + 1);
 }
 return ret;
}

SYSCALL_DEFINE1(newuname, struct new_utsname __user *, name)
{
 int errno = 0;

 down_read(&uts_sem);
 if (copy_to_user(name, utsname(), sizeof *name))
  errno = -EFAULT;
 up_read(&uts_sem);

 if (!errno && override_release(name->release, sizeof(name->release)))
  errno = -EFAULT;
 if (!errno && override_architecture(name))
  errno = -EFAULT;
 return errno;
}




SYSCALL_DEFINE1(uname, struct old_utsname __user *, name)
{
 int error = 0;

 if (!name)
  return -EFAULT;

 down_read(&uts_sem);
 if (copy_to_user(name, utsname(), sizeof(*name)))
  error = -EFAULT;
 up_read(&uts_sem);

 if (!error && override_release(name->release, sizeof(name->release)))
  error = -EFAULT;
 if (!error && override_architecture(name))
  error = -EFAULT;
 return error;
}

SYSCALL_DEFINE1(olduname, struct oldold_utsname __user *, name)
{
 int error;

 if (!name)
  return -EFAULT;
 if (!access_ok(VERIFY_WRITE, name, sizeof(struct oldold_utsname)))
  return -EFAULT;

 down_read(&uts_sem);
 error = __copy_to_user(&name->sysname, &utsname()->sysname,
          __OLD_UTS_LEN);
 error |= __put_user(0, name->sysname + __OLD_UTS_LEN);
 error |= __copy_to_user(&name->nodename, &utsname()->nodename,
    __OLD_UTS_LEN);
 error |= __put_user(0, name->nodename + __OLD_UTS_LEN);
 error |= __copy_to_user(&name->release, &utsname()->release,
    __OLD_UTS_LEN);
 error |= __put_user(0, name->release + __OLD_UTS_LEN);
 error |= __copy_to_user(&name->version, &utsname()->version,
    __OLD_UTS_LEN);
 error |= __put_user(0, name->version + __OLD_UTS_LEN);
 error |= __copy_to_user(&name->machine, &utsname()->machine,
    __OLD_UTS_LEN);
 error |= __put_user(0, name->machine + __OLD_UTS_LEN);
 up_read(&uts_sem);

 if (!error && override_architecture(name))
  error = -EFAULT;
 if (!error && override_release(name->release, sizeof(name->release)))
  error = -EFAULT;
 return error ? -EFAULT : 0;
}

SYSCALL_DEFINE2(sethostname, char __user *, name, int, len)
{
 int errno;
 char tmp[__NEW_UTS_LEN];

 if (!ns_capable(current->nsproxy->uts_ns->user_ns, CAP_SYS_ADMIN))
  return -EPERM;

 if (len < 0 || len > __NEW_UTS_LEN)
  return -EINVAL;
 down_write(&uts_sem);
 errno = -EFAULT;
 if (!copy_from_user(tmp, name, len)) {
  struct new_utsname *u = utsname();

  memcpy(u->nodename, tmp, len);
  memset(u->nodename + len, 0, sizeof(u->nodename) - len);
  errno = 0;
  uts_proc_notify(UTS_PROC_HOSTNAME);
 }
 up_write(&uts_sem);
 return errno;
}


SYSCALL_DEFINE2(gethostname, char __user *, name, int, len)
{
 int i, errno;
 struct new_utsname *u;

 if (len < 0)
  return -EINVAL;
 down_read(&uts_sem);
 u = utsname();
 i = 1 + strlen(u->nodename);
 if (i > len)
  i = len;
 errno = 0;
 if (copy_to_user(name, u->nodename, i))
  errno = -EFAULT;
 up_read(&uts_sem);
 return errno;
}






SYSCALL_DEFINE2(setdomainname, char __user *, name, int, len)
{
 int errno;
 char tmp[__NEW_UTS_LEN];

 if (!ns_capable(current->nsproxy->uts_ns->user_ns, CAP_SYS_ADMIN))
  return -EPERM;
 if (len < 0 || len > __NEW_UTS_LEN)
  return -EINVAL;

 down_write(&uts_sem);
 errno = -EFAULT;
 if (!copy_from_user(tmp, name, len)) {
  struct new_utsname *u = utsname();

  memcpy(u->domainname, tmp, len);
  memset(u->domainname + len, 0, sizeof(u->domainname) - len);
  errno = 0;
  uts_proc_notify(UTS_PROC_DOMAINNAME);
 }
 up_write(&uts_sem);
 return errno;
}

SYSCALL_DEFINE2(getrlimit, unsigned int, resource, struct rlimit __user *, rlim)
{
 struct rlimit value;
 int ret;

 ret = do_prlimit(current, resource, NULL, &value);
 if (!ret)
  ret = copy_to_user(rlim, &value, sizeof(*rlim)) ? -EFAULT : 0;

 return ret;
}





SYSCALL_DEFINE2(old_getrlimit, unsigned int, resource,
  struct rlimit __user *, rlim)
{
 struct rlimit x;
 if (resource >= RLIM_NLIMITS)
  return -EINVAL;

 task_lock(current->group_leader);
 x = current->signal->rlim[resource];
 task_unlock(current->group_leader);
 if (x.rlim_cur > 0x7FFFFFFF)
  x.rlim_cur = 0x7FFFFFFF;
 if (x.rlim_max > 0x7FFFFFFF)
  x.rlim_max = 0x7FFFFFFF;
 return copy_to_user(rlim, &x, sizeof(x)) ? -EFAULT : 0;
}


static inline bool rlim64_is_infinity(__u64 rlim64)
{
 return rlim64 >= ULONG_MAX;
 return rlim64 == RLIM64_INFINITY;
}

static void rlim_to_rlim64(const struct rlimit *rlim, struct rlimit64 *rlim64)
{
 if (rlim->rlim_cur == RLIM_INFINITY)
  rlim64->rlim_cur = RLIM64_INFINITY;
 else
  rlim64->rlim_cur = rlim->rlim_cur;
 if (rlim->rlim_max == RLIM_INFINITY)
  rlim64->rlim_max = RLIM64_INFINITY;
 else
  rlim64->rlim_max = rlim->rlim_max;
}

static void rlim64_to_rlim(const struct rlimit64 *rlim64, struct rlimit *rlim)
{
 if (rlim64_is_infinity(rlim64->rlim_cur))
  rlim->rlim_cur = RLIM_INFINITY;
 else
  rlim->rlim_cur = (unsigned long)rlim64->rlim_cur;
 if (rlim64_is_infinity(rlim64->rlim_max))
  rlim->rlim_max = RLIM_INFINITY;
 else
  rlim->rlim_max = (unsigned long)rlim64->rlim_max;
}


int do_prlimit(struct task_struct *tsk, unsigned int resource,
  struct rlimit *new_rlim, struct rlimit *old_rlim)
{
 struct rlimit *rlim;
 int retval = 0;

 if (resource >= RLIM_NLIMITS)
  return -EINVAL;
 if (new_rlim) {
  if (new_rlim->rlim_cur > new_rlim->rlim_max)
   return -EINVAL;
  if (resource == RLIMIT_NOFILE &&
    new_rlim->rlim_max > sysctl_nr_open)
   return -EPERM;
 }


 read_lock(&tasklist_lock);
 if (!tsk->sighand) {
  retval = -ESRCH;
  goto out;
 }

 rlim = tsk->signal->rlim + resource;
 task_lock(tsk->group_leader);
 if (new_rlim) {


  if (new_rlim->rlim_max > rlim->rlim_max &&
    !capable(CAP_SYS_RESOURCE))
   retval = -EPERM;
  if (!retval)
   retval = security_task_setrlimit(tsk->group_leader,
     resource, new_rlim);
  if (resource == RLIMIT_CPU && new_rlim->rlim_cur == 0) {






   new_rlim->rlim_cur = 1;
  }
 }
 if (!retval) {
  if (old_rlim)
   *old_rlim = *rlim;
  if (new_rlim)
   *rlim = *new_rlim;
 }
 task_unlock(tsk->group_leader);







  if (!retval && new_rlim && resource == RLIMIT_CPU &&
    new_rlim->rlim_cur != RLIM_INFINITY)
  update_rlimit_cpu(tsk, new_rlim->rlim_cur);
out:
 read_unlock(&tasklist_lock);
 return retval;
}


static int check_prlimit_permission(struct task_struct *task)
{
 const struct cred *cred = current_cred(), *tcred;

 if (current == task)
  return 0;

 tcred = __task_cred(task);
 if (uid_eq(cred->uid, tcred->euid) &&
     uid_eq(cred->uid, tcred->suid) &&
     uid_eq(cred->uid, tcred->uid) &&
     gid_eq(cred->gid, tcred->egid) &&
     gid_eq(cred->gid, tcred->sgid) &&
     gid_eq(cred->gid, tcred->gid))
  return 0;
 if (ns_capable(tcred->user_ns, CAP_SYS_RESOURCE))
  return 0;

 return -EPERM;
}

SYSCALL_DEFINE4(prlimit64, pid_t, pid, unsigned int, resource,
  const struct rlimit64 __user *, new_rlim,
  struct rlimit64 __user *, old_rlim)
{
 struct rlimit64 old64, new64;
 struct rlimit old, new;
 struct task_struct *tsk;
 int ret;

 if (new_rlim) {
  if (copy_from_user(&new64, new_rlim, sizeof(new64)))
   return -EFAULT;
  rlim64_to_rlim(&new64, &new);
 }

 rcu_read_lock();
 tsk = pid ? find_task_by_vpid(pid) : current;
 if (!tsk) {
  rcu_read_unlock();
  return -ESRCH;
 }
 ret = check_prlimit_permission(tsk);
 if (ret) {
  rcu_read_unlock();
  return ret;
 }
 get_task_struct(tsk);
 rcu_read_unlock();

 ret = do_prlimit(tsk, resource, new_rlim ? &new : NULL,
   old_rlim ? &old : NULL);

 if (!ret && old_rlim) {
  rlim_to_rlim64(&old, &old64);
  if (copy_to_user(old_rlim, &old64, sizeof(old64)))
   ret = -EFAULT;
 }

 put_task_struct(tsk);
 return ret;
}

SYSCALL_DEFINE2(setrlimit, unsigned int, resource, struct rlimit __user *, rlim)
{
 struct rlimit new_rlim;

 if (copy_from_user(&new_rlim, rlim, sizeof(*rlim)))
  return -EFAULT;
 return do_prlimit(current, resource, &new_rlim, NULL);
}
static void accumulate_thread_rusage(struct task_struct *t, struct rusage *r)
{
 r->ru_nvcsw += t->nvcsw;
 r->ru_nivcsw += t->nivcsw;
 r->ru_minflt += t->min_flt;
 r->ru_majflt += t->maj_flt;
 r->ru_inblock += task_io_get_inblock(t);
 r->ru_oublock += task_io_get_oublock(t);
}

static void k_getrusage(struct task_struct *p, int who, struct rusage *r)
{
 struct task_struct *t;
 unsigned long flags;
 cputime_t tgutime, tgstime, utime, stime;
 unsigned long maxrss = 0;

 memset((char *)r, 0, sizeof (*r));
 utime = stime = 0;

 if (who == RUSAGE_THREAD) {
  task_cputime_adjusted(current, &utime, &stime);
  accumulate_thread_rusage(p, r);
  maxrss = p->signal->maxrss;
  goto out;
 }

 if (!lock_task_sighand(p, &flags))
  return;

 switch (who) {
 case RUSAGE_BOTH:
 case RUSAGE_CHILDREN:
  utime = p->signal->cutime;
  stime = p->signal->cstime;
  r->ru_nvcsw = p->signal->cnvcsw;
  r->ru_nivcsw = p->signal->cnivcsw;
  r->ru_minflt = p->signal->cmin_flt;
  r->ru_majflt = p->signal->cmaj_flt;
  r->ru_inblock = p->signal->cinblock;
  r->ru_oublock = p->signal->coublock;
  maxrss = p->signal->cmaxrss;

  if (who == RUSAGE_CHILDREN)
   break;

 case RUSAGE_SELF:
  thread_group_cputime_adjusted(p, &tgutime, &tgstime);
  utime += tgutime;
  stime += tgstime;
  r->ru_nvcsw += p->signal->nvcsw;
  r->ru_nivcsw += p->signal->nivcsw;
  r->ru_minflt += p->signal->min_flt;
  r->ru_majflt += p->signal->maj_flt;
  r->ru_inblock += p->signal->inblock;
  r->ru_oublock += p->signal->oublock;
  if (maxrss < p->signal->maxrss)
   maxrss = p->signal->maxrss;
  t = p;
  do {
   accumulate_thread_rusage(t, r);
  } while_each_thread(p, t);
  break;

 default:
  BUG();
 }
 unlock_task_sighand(p, &flags);

out:
 cputime_to_timeval(utime, &r->ru_utime);
 cputime_to_timeval(stime, &r->ru_stime);

 if (who != RUSAGE_CHILDREN) {
  struct mm_struct *mm = get_task_mm(p);

  if (mm) {
   setmax_mm_hiwater_rss(&maxrss, mm);
   mmput(mm);
  }
 }
 r->ru_maxrss = maxrss * (PAGE_SIZE / 1024);
}

int getrusage(struct task_struct *p, int who, struct rusage __user *ru)
{
 struct rusage r;

 k_getrusage(p, who, &r);
 return copy_to_user(ru, &r, sizeof(r)) ? -EFAULT : 0;
}

SYSCALL_DEFINE2(getrusage, int, who, struct rusage __user *, ru)
{
 if (who != RUSAGE_SELF && who != RUSAGE_CHILDREN &&
     who != RUSAGE_THREAD)
  return -EINVAL;
 return getrusage(current, who, ru);
}

COMPAT_SYSCALL_DEFINE2(getrusage, int, who, struct compat_rusage __user *, ru)
{
 struct rusage r;

 if (who != RUSAGE_SELF && who != RUSAGE_CHILDREN &&
     who != RUSAGE_THREAD)
  return -EINVAL;

 k_getrusage(current, who, &r);
 return put_compat_rusage(&r, ru);
}

SYSCALL_DEFINE1(umask, int, mask)
{
 mask = xchg(&current->fs->umask, mask & S_IRWXUGO);
 return mask;
}

static int prctl_set_mm_exe_file(struct mm_struct *mm, unsigned int fd)
{
 struct fd exe;
 struct file *old_exe, *exe_file;
 struct inode *inode;
 int err;

 exe = fdget(fd);
 if (!exe.file)
  return -EBADF;

 inode = file_inode(exe.file);






 err = -EACCES;
 if (!S_ISREG(inode->i_mode) || path_noexec(&exe.file->f_path))
  goto exit;

 err = inode_permission(inode, MAY_EXEC);
 if (err)
  goto exit;




 exe_file = get_mm_exe_file(mm);
 err = -EBUSY;
 if (exe_file) {
  struct vm_area_struct *vma;

  down_read(&mm->mmap_sem);
  for (vma = mm->mmap; vma; vma = vma->vm_next) {
   if (!vma->vm_file)
    continue;
   if (path_equal(&vma->vm_file->f_path,
           &exe_file->f_path))
    goto exit_err;
  }

  up_read(&mm->mmap_sem);
  fput(exe_file);
 }







 err = -EPERM;
 if (test_and_set_bit(MMF_EXE_FILE_CHANGED, &mm->flags))
  goto exit;

 err = 0;

 get_file(exe.file);
 old_exe = xchg(&mm->exe_file, exe.file);
 if (old_exe)
  fput(old_exe);
exit:
 fdput(exe);
 return err;
exit_err:
 up_read(&mm->mmap_sem);
 fput(exe_file);
 goto exit;
}





static int validate_prctl_map(struct prctl_mm_map *prctl_map)
{
 unsigned long mmap_max_addr = TASK_SIZE;
 struct mm_struct *mm = current->mm;
 int error = -EINVAL, i;

 static const unsigned char offsets[] = {
  offsetof(struct prctl_mm_map, start_code),
  offsetof(struct prctl_mm_map, end_code),
  offsetof(struct prctl_mm_map, start_data),
  offsetof(struct prctl_mm_map, end_data),
  offsetof(struct prctl_mm_map, start_brk),
  offsetof(struct prctl_mm_map, brk),
  offsetof(struct prctl_mm_map, start_stack),
  offsetof(struct prctl_mm_map, arg_start),
  offsetof(struct prctl_mm_map, arg_end),
  offsetof(struct prctl_mm_map, env_start),
  offsetof(struct prctl_mm_map, env_end),
 };





 for (i = 0; i < ARRAY_SIZE(offsets); i++) {
  u64 val = *(u64 *)((char *)prctl_map + offsets[i]);

  if ((unsigned long)val >= mmap_max_addr ||
      (unsigned long)val < mmap_min_addr)
   goto out;
 }




 ((unsigned long)prctl_map->__m1 __op \
  (unsigned long)prctl_map->__m2) ? 0 : -EINVAL
 error = __prctl_check_order(start_code, <, end_code);
 error |= __prctl_check_order(start_data, <, end_data);
 error |= __prctl_check_order(start_brk, <=, brk);
 error |= __prctl_check_order(arg_start, <=, arg_end);
 error |= __prctl_check_order(env_start, <=, env_end);
 if (error)
  goto out;

 error = -EINVAL;




 if (prctl_map->start_brk <= prctl_map->end_data ||
     prctl_map->brk <= prctl_map->end_data)
  goto out;




 if (check_data_rlimit(rlimit(RLIMIT_DATA), prctl_map->brk,
         prctl_map->start_brk, prctl_map->end_data,
         prctl_map->start_data))
   goto out;




 if (prctl_map->auxv_size) {
  if (!prctl_map->auxv || prctl_map->auxv_size > sizeof(mm->saved_auxv))
   goto out;
 }






 if (prctl_map->exe_fd != (u32)-1) {
  struct user_namespace *ns = current_user_ns();
  const struct cred *cred = current_cred();

  if (!uid_eq(cred->uid, make_kuid(ns, 0)) ||
      !gid_eq(cred->gid, make_kgid(ns, 0)))
   goto out;
 }

 error = 0;
out:
 return error;
}

static int prctl_set_mm_map(int opt, const void __user *addr, unsigned long data_size)
{
 struct prctl_mm_map prctl_map = { .exe_fd = (u32)-1, };
 unsigned long user_auxv[AT_VECTOR_SIZE];
 struct mm_struct *mm = current->mm;
 int error;

 BUILD_BUG_ON(sizeof(user_auxv) != sizeof(mm->saved_auxv));
 BUILD_BUG_ON(sizeof(struct prctl_mm_map) > 256);

 if (opt == PR_SET_MM_MAP_SIZE)
  return put_user((unsigned int)sizeof(prctl_map),
    (unsigned int __user *)addr);

 if (data_size != sizeof(prctl_map))
  return -EINVAL;

 if (copy_from_user(&prctl_map, addr, sizeof(prctl_map)))
  return -EFAULT;

 error = validate_prctl_map(&prctl_map);
 if (error)
  return error;

 if (prctl_map.auxv_size) {
  memset(user_auxv, 0, sizeof(user_auxv));
  if (copy_from_user(user_auxv,
       (const void __user *)prctl_map.auxv,
       prctl_map.auxv_size))
   return -EFAULT;


  user_auxv[AT_VECTOR_SIZE - 2] = AT_NULL;
  user_auxv[AT_VECTOR_SIZE - 1] = AT_NULL;
 }

 if (prctl_map.exe_fd != (u32)-1) {
  error = prctl_set_mm_exe_file(mm, prctl_map.exe_fd);
  if (error)
   return error;
 }

 down_write(&mm->mmap_sem);
 mm->start_code = prctl_map.start_code;
 mm->end_code = prctl_map.end_code;
 mm->start_data = prctl_map.start_data;
 mm->end_data = prctl_map.end_data;
 mm->start_brk = prctl_map.start_brk;
 mm->brk = prctl_map.brk;
 mm->start_stack = prctl_map.start_stack;
 mm->arg_start = prctl_map.arg_start;
 mm->arg_end = prctl_map.arg_end;
 mm->env_start = prctl_map.env_start;
 mm->env_end = prctl_map.env_end;
 if (prctl_map.auxv_size)
  memcpy(mm->saved_auxv, user_auxv, sizeof(user_auxv));

 up_write(&mm->mmap_sem);
 return 0;
}

static int prctl_set_auxv(struct mm_struct *mm, unsigned long addr,
     unsigned long len)
{






 unsigned long user_auxv[AT_VECTOR_SIZE];

 if (len > sizeof(user_auxv))
  return -EINVAL;

 if (copy_from_user(user_auxv, (const void __user *)addr, len))
  return -EFAULT;


 user_auxv[AT_VECTOR_SIZE - 2] = 0;
 user_auxv[AT_VECTOR_SIZE - 1] = 0;

 BUILD_BUG_ON(sizeof(user_auxv) != sizeof(mm->saved_auxv));

 task_lock(current);
 memcpy(mm->saved_auxv, user_auxv, len);
 task_unlock(current);

 return 0;
}

static int prctl_set_mm(int opt, unsigned long addr,
   unsigned long arg4, unsigned long arg5)
{
 struct mm_struct *mm = current->mm;
 struct prctl_mm_map prctl_map;
 struct vm_area_struct *vma;
 int error;

 if (arg5 || (arg4 && (opt != PR_SET_MM_AUXV &&
         opt != PR_SET_MM_MAP &&
         opt != PR_SET_MM_MAP_SIZE)))
  return -EINVAL;

 if (opt == PR_SET_MM_MAP || opt == PR_SET_MM_MAP_SIZE)
  return prctl_set_mm_map(opt, (const void __user *)addr, arg4);

 if (!capable(CAP_SYS_RESOURCE))
  return -EPERM;

 if (opt == PR_SET_MM_EXE_FILE)
  return prctl_set_mm_exe_file(mm, (unsigned int)addr);

 if (opt == PR_SET_MM_AUXV)
  return prctl_set_auxv(mm, addr, arg4);

 if (addr >= TASK_SIZE || addr < mmap_min_addr)
  return -EINVAL;

 error = -EINVAL;

 down_write(&mm->mmap_sem);
 vma = find_vma(mm, addr);

 prctl_map.start_code = mm->start_code;
 prctl_map.end_code = mm->end_code;
 prctl_map.start_data = mm->start_data;
 prctl_map.end_data = mm->end_data;
 prctl_map.start_brk = mm->start_brk;
 prctl_map.brk = mm->brk;
 prctl_map.start_stack = mm->start_stack;
 prctl_map.arg_start = mm->arg_start;
 prctl_map.arg_end = mm->arg_end;
 prctl_map.env_start = mm->env_start;
 prctl_map.env_end = mm->env_end;
 prctl_map.auxv = NULL;
 prctl_map.auxv_size = 0;
 prctl_map.exe_fd = -1;

 switch (opt) {
 case PR_SET_MM_START_CODE:
  prctl_map.start_code = addr;
  break;
 case PR_SET_MM_END_CODE:
  prctl_map.end_code = addr;
  break;
 case PR_SET_MM_START_DATA:
  prctl_map.start_data = addr;
  break;
 case PR_SET_MM_END_DATA:
  prctl_map.end_data = addr;
  break;
 case PR_SET_MM_START_STACK:
  prctl_map.start_stack = addr;
  break;
 case PR_SET_MM_START_BRK:
  prctl_map.start_brk = addr;
  break;
 case PR_SET_MM_BRK:
  prctl_map.brk = addr;
  break;
 case PR_SET_MM_ARG_START:
  prctl_map.arg_start = addr;
  break;
 case PR_SET_MM_ARG_END:
  prctl_map.arg_end = addr;
  break;
 case PR_SET_MM_ENV_START:
  prctl_map.env_start = addr;
  break;
 case PR_SET_MM_ENV_END:
  prctl_map.env_end = addr;
  break;
 default:
  goto out;
 }

 error = validate_prctl_map(&prctl_map);
 if (error)
  goto out;

 switch (opt) {







 case PR_SET_MM_START_STACK:
 case PR_SET_MM_ARG_START:
 case PR_SET_MM_ARG_END:
 case PR_SET_MM_ENV_START:
 case PR_SET_MM_ENV_END:
  if (!vma) {
   error = -EFAULT;
   goto out;
  }
 }

 mm->start_code = prctl_map.start_code;
 mm->end_code = prctl_map.end_code;
 mm->start_data = prctl_map.start_data;
 mm->end_data = prctl_map.end_data;
 mm->start_brk = prctl_map.start_brk;
 mm->brk = prctl_map.brk;
 mm->start_stack = prctl_map.start_stack;
 mm->arg_start = prctl_map.arg_start;
 mm->arg_end = prctl_map.arg_end;
 mm->env_start = prctl_map.env_start;
 mm->env_end = prctl_map.env_end;

 error = 0;
out:
 up_write(&mm->mmap_sem);
 return error;
}

static int prctl_get_tid_address(struct task_struct *me, int __user **tid_addr)
{
 return put_user(me->clear_child_tid, tid_addr);
}
static int prctl_get_tid_address(struct task_struct *me, int __user **tid_addr)
{
 return -EINVAL;
}

SYSCALL_DEFINE5(prctl, int, option, unsigned long, arg2, unsigned long, arg3,
  unsigned long, arg4, unsigned long, arg5)
{
 struct task_struct *me = current;
 unsigned char comm[sizeof(me->comm)];
 long error;

 error = security_task_prctl(option, arg2, arg3, arg4, arg5);
 if (error != -ENOSYS)
  return error;

 error = 0;
 switch (option) {
 case PR_SET_PDEATHSIG:
  if (!valid_signal(arg2)) {
   error = -EINVAL;
   break;
  }
  me->pdeath_signal = arg2;
  break;
 case PR_GET_PDEATHSIG:
  error = put_user(me->pdeath_signal, (int __user *)arg2);
  break;
 case PR_GET_DUMPABLE:
  error = get_dumpable(me->mm);
  break;
 case PR_SET_DUMPABLE:
  if (arg2 != SUID_DUMP_DISABLE && arg2 != SUID_DUMP_USER) {
   error = -EINVAL;
   break;
  }
  set_dumpable(me->mm, arg2);
  break;

 case PR_SET_UNALIGN:
  error = SET_UNALIGN_CTL(me, arg2);
  break;
 case PR_GET_UNALIGN:
  error = GET_UNALIGN_CTL(me, arg2);
  break;
 case PR_SET_FPEMU:
  error = SET_FPEMU_CTL(me, arg2);
  break;
 case PR_GET_FPEMU:
  error = GET_FPEMU_CTL(me, arg2);
  break;
 case PR_SET_FPEXC:
  error = SET_FPEXC_CTL(me, arg2);
  break;
 case PR_GET_FPEXC:
  error = GET_FPEXC_CTL(me, arg2);
  break;
 case PR_GET_TIMING:
  error = PR_TIMING_STATISTICAL;
  break;
 case PR_SET_TIMING:
  if (arg2 != PR_TIMING_STATISTICAL)
   error = -EINVAL;
  break;
 case PR_SET_NAME:
  comm[sizeof(me->comm) - 1] = 0;
  if (strncpy_from_user(comm, (char __user *)arg2,
          sizeof(me->comm) - 1) < 0)
   return -EFAULT;
  set_task_comm(me, comm);
  proc_comm_connector(me);
  break;
 case PR_GET_NAME:
  get_task_comm(comm, me);
  if (copy_to_user((char __user *)arg2, comm, sizeof(comm)))
   return -EFAULT;
  break;
 case PR_GET_ENDIAN:
  error = GET_ENDIAN(me, arg2);
  break;
 case PR_SET_ENDIAN:
  error = SET_ENDIAN(me, arg2);
  break;
 case PR_GET_SECCOMP:
  error = prctl_get_seccomp();
  break;
 case PR_SET_SECCOMP:
  error = prctl_set_seccomp(arg2, (char __user *)arg3);
  break;
 case PR_GET_TSC:
  error = GET_TSC_CTL(arg2);
  break;
 case PR_SET_TSC:
  error = SET_TSC_CTL(arg2);
  break;
 case PR_TASK_PERF_EVENTS_DISABLE:
  error = perf_event_task_disable();
  break;
 case PR_TASK_PERF_EVENTS_ENABLE:
  error = perf_event_task_enable();
  break;
 case PR_GET_TIMERSLACK:
  if (current->timer_slack_ns > ULONG_MAX)
   error = ULONG_MAX;
  else
   error = current->timer_slack_ns;
  break;
 case PR_SET_TIMERSLACK:
  if (arg2 <= 0)
   current->timer_slack_ns =
     current->default_timer_slack_ns;
  else
   current->timer_slack_ns = arg2;
  break;
 case PR_MCE_KILL:
  if (arg4 | arg5)
   return -EINVAL;
  switch (arg2) {
  case PR_MCE_KILL_CLEAR:
   if (arg3 != 0)
    return -EINVAL;
   current->flags &= ~PF_MCE_PROCESS;
   break;
  case PR_MCE_KILL_SET:
   current->flags |= PF_MCE_PROCESS;
   if (arg3 == PR_MCE_KILL_EARLY)
    current->flags |= PF_MCE_EARLY;
   else if (arg3 == PR_MCE_KILL_LATE)
    current->flags &= ~PF_MCE_EARLY;
   else if (arg3 == PR_MCE_KILL_DEFAULT)
    current->flags &=
      ~(PF_MCE_EARLY|PF_MCE_PROCESS);
   else
    return -EINVAL;
   break;
  default:
   return -EINVAL;
  }
  break;
 case PR_MCE_KILL_GET:
  if (arg2 | arg3 | arg4 | arg5)
   return -EINVAL;
  if (current->flags & PF_MCE_PROCESS)
   error = (current->flags & PF_MCE_EARLY) ?
    PR_MCE_KILL_EARLY : PR_MCE_KILL_LATE;
  else
   error = PR_MCE_KILL_DEFAULT;
  break;
 case PR_SET_MM:
  error = prctl_set_mm(arg2, arg3, arg4, arg5);
  break;
 case PR_GET_TID_ADDRESS:
  error = prctl_get_tid_address(me, (int __user **)arg2);
  break;
 case PR_SET_CHILD_SUBREAPER:
  me->signal->is_child_subreaper = !!arg2;
  break;
 case PR_GET_CHILD_SUBREAPER:
  error = put_user(me->signal->is_child_subreaper,
     (int __user *)arg2);
  break;
 case PR_SET_NO_NEW_PRIVS:
  if (arg2 != 1 || arg3 || arg4 || arg5)
   return -EINVAL;

  task_set_no_new_privs(current);
  break;
 case PR_GET_NO_NEW_PRIVS:
  if (arg2 || arg3 || arg4 || arg5)
   return -EINVAL;
  return task_no_new_privs(current) ? 1 : 0;
 case PR_GET_THP_DISABLE:
  if (arg2 || arg3 || arg4 || arg5)
   return -EINVAL;
  error = !!(me->mm->def_flags & VM_NOHUGEPAGE);
  break;
 case PR_SET_THP_DISABLE:
  if (arg3 || arg4 || arg5)
   return -EINVAL;
  if (down_write_killable(&me->mm->mmap_sem))
   return -EINTR;
  if (arg2)
   me->mm->def_flags |= VM_NOHUGEPAGE;
  else
   me->mm->def_flags &= ~VM_NOHUGEPAGE;
  up_write(&me->mm->mmap_sem);
  break;
 case PR_MPX_ENABLE_MANAGEMENT:
  if (arg2 || arg3 || arg4 || arg5)
   return -EINVAL;
  error = MPX_ENABLE_MANAGEMENT();
  break;
 case PR_MPX_DISABLE_MANAGEMENT:
  if (arg2 || arg3 || arg4 || arg5)
   return -EINVAL;
  error = MPX_DISABLE_MANAGEMENT();
  break;
 case PR_SET_FP_MODE:
  error = SET_FP_MODE(me, arg2);
  break;
 case PR_GET_FP_MODE:
  error = GET_FP_MODE(me);
  break;
 default:
  error = -EINVAL;
  break;
 }
 return error;
}

SYSCALL_DEFINE3(getcpu, unsigned __user *, cpup, unsigned __user *, nodep,
  struct getcpu_cache __user *, unused)
{
 int err = 0;
 int cpu = raw_smp_processor_id();

 if (cpup)
  err |= put_user(cpu, cpup);
 if (nodep)
  err |= put_user(cpu_to_node(cpu), nodep);
 return err ? -EFAULT : 0;
}





static int do_sysinfo(struct sysinfo *info)
{
 unsigned long mem_total, sav_total;
 unsigned int mem_unit, bitcount;
 struct timespec tp;

 memset(info, 0, sizeof(struct sysinfo));

 get_monotonic_boottime(&tp);
 info->uptime = tp.tv_sec + (tp.tv_nsec ? 1 : 0);

 get_avenrun(info->loads, 0, SI_LOAD_SHIFT - FSHIFT);

 info->procs = nr_threads;

 si_meminfo(info);
 si_swapinfo(info);
 mem_total = info->totalram + info->totalswap;
 if (mem_total < info->totalram || mem_total < info->totalswap)
  goto out;
 bitcount = 0;
 mem_unit = info->mem_unit;
 while (mem_unit > 1) {
  bitcount++;
  mem_unit >>= 1;
  sav_total = mem_total;
  mem_total <<= 1;
  if (mem_total < sav_total)
   goto out;
 }
 info->mem_unit = 1;
 info->totalram <<= bitcount;
 info->freeram <<= bitcount;
 info->sharedram <<= bitcount;
 info->bufferram <<= bitcount;
 info->totalswap <<= bitcount;
 info->freeswap <<= bitcount;
 info->totalhigh <<= bitcount;
 info->freehigh <<= bitcount;

out:
 return 0;
}

SYSCALL_DEFINE1(sysinfo, struct sysinfo __user *, info)
{
 struct sysinfo val;

 do_sysinfo(&val);

 if (copy_to_user(info, &val, sizeof(struct sysinfo)))
  return -EFAULT;

 return 0;
}

struct compat_sysinfo {
 s32 uptime;
 u32 loads[3];
 u32 totalram;
 u32 freeram;
 u32 sharedram;
 u32 bufferram;
 u32 totalswap;
 u32 freeswap;
 u16 procs;
 u16 pad;
 u32 totalhigh;
 u32 freehigh;
 u32 mem_unit;
 char _f[20-2*sizeof(u32)-sizeof(int)];
};

COMPAT_SYSCALL_DEFINE1(sysinfo, struct compat_sysinfo __user *, info)
{
 struct sysinfo s;

 do_sysinfo(&s);




 if (upper_32_bits(s.totalram) || upper_32_bits(s.totalswap)) {
  int bitcount = 0;

  while (s.mem_unit < PAGE_SIZE) {
   s.mem_unit <<= 1;
   bitcount++;
  }

  s.totalram >>= bitcount;
  s.freeram >>= bitcount;
  s.sharedram >>= bitcount;
  s.bufferram >>= bitcount;
  s.totalswap >>= bitcount;
  s.freeswap >>= bitcount;
  s.totalhigh >>= bitcount;
  s.freehigh >>= bitcount;
 }

 if (!access_ok(VERIFY_WRITE, info, sizeof(struct compat_sysinfo)) ||
     __put_user(s.uptime, &info->uptime) ||
     __put_user(s.loads[0], &info->loads[0]) ||
     __put_user(s.loads[1], &info->loads[1]) ||
     __put_user(s.loads[2], &info->loads[2]) ||
     __put_user(s.totalram, &info->totalram) ||
     __put_user(s.freeram, &info->freeram) ||
     __put_user(s.sharedram, &info->sharedram) ||
     __put_user(s.bufferram, &info->bufferram) ||
     __put_user(s.totalswap, &info->totalswap) ||
     __put_user(s.freeswap, &info->freeswap) ||
     __put_user(s.procs, &info->procs) ||
     __put_user(s.totalhigh, &info->totalhigh) ||
     __put_user(s.freehigh, &info->freehigh) ||
     __put_user(s.mem_unit, &info->mem_unit))
  return -EFAULT;

 return 0;
}

DEFINE_PER_CPU(int, bpf_prog_active);

int sysctl_unprivileged_bpf_disabled __read_mostly;

static LIST_HEAD(bpf_map_types);

static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
{
 struct bpf_map_type_list *tl;
 struct bpf_map *map;

 list_for_each_entry(tl, &bpf_map_types, list_node) {
  if (tl->type == attr->map_type) {
   map = tl->ops->map_alloc(attr);
   if (IS_ERR(map))
    return map;
   map->ops = tl->ops;
   map->map_type = attr->map_type;
   return map;
  }
 }
 return ERR_PTR(-EINVAL);
}


void bpf_register_map_type(struct bpf_map_type_list *tl)
{
 list_add(&tl->list_node, &bpf_map_types);
}

int bpf_map_precharge_memlock(u32 pages)
{
 struct user_struct *user = get_current_user();
 unsigned long memlock_limit, cur;

 memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 cur = atomic_long_read(&user->locked_vm);
 free_uid(user);
 if (cur + pages > memlock_limit)
  return -EPERM;
 return 0;
}

static int bpf_map_charge_memlock(struct bpf_map *map)
{
 struct user_struct *user = get_current_user();
 unsigned long memlock_limit;

 memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;

 atomic_long_add(map->pages, &user->locked_vm);

 if (atomic_long_read(&user->locked_vm) > memlock_limit) {
  atomic_long_sub(map->pages, &user->locked_vm);
  free_uid(user);
  return -EPERM;
 }
 map->user = user;
 return 0;
}

static void bpf_map_uncharge_memlock(struct bpf_map *map)
{
 struct user_struct *user = map->user;

 atomic_long_sub(map->pages, &user->locked_vm);
 free_uid(user);
}


static void bpf_map_free_deferred(struct work_struct *work)
{
 struct bpf_map *map = container_of(work, struct bpf_map, work);

 bpf_map_uncharge_memlock(map);

 map->ops->map_free(map);
}

static void bpf_map_put_uref(struct bpf_map *map)
{
 if (atomic_dec_and_test(&map->usercnt)) {
  if (map->map_type == BPF_MAP_TYPE_PROG_ARRAY)
   bpf_fd_array_map_clear(map);
 }
}




void bpf_map_put(struct bpf_map *map)
{
 if (atomic_dec_and_test(&map->refcnt)) {
  INIT_WORK(&map->work, bpf_map_free_deferred);
  schedule_work(&map->work);
 }
}

void bpf_map_put_with_uref(struct bpf_map *map)
{
 bpf_map_put_uref(map);
 bpf_map_put(map);
}

static int bpf_map_release(struct inode *inode, struct file *filp)
{
 bpf_map_put_with_uref(filp->private_data);
 return 0;
}

static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
{
 const struct bpf_map *map = filp->private_data;

 seq_printf(m,
     "map_type:\t%u\n"
     "key_size:\t%u\n"
     "value_size:\t%u\n"
     "max_entries:\t%u\n"
     "map_flags:\t%#x\n",
     map->map_type,
     map->key_size,
     map->value_size,
     map->max_entries,
     map->map_flags);
}

static const struct file_operations bpf_map_fops = {
 .show_fdinfo = bpf_map_show_fdinfo,
 .release = bpf_map_release,
};

int bpf_map_new_fd(struct bpf_map *map)
{
 return anon_inode_getfd("bpf-map", &bpf_map_fops, map,
    O_RDWR | O_CLOEXEC);
}


 memchr_inv((void *) &attr->CMD##_LAST_FIELD + \
     sizeof(attr->CMD##_LAST_FIELD), 0, \
     sizeof(*attr) - \
     offsetof(union bpf_attr, CMD##_LAST_FIELD) - \
     sizeof(attr->CMD##_LAST_FIELD)) != NULL


static int map_create(union bpf_attr *attr)
{
 struct bpf_map *map;
 int err;

 err = CHECK_ATTR(BPF_MAP_CREATE);
 if (err)
  return -EINVAL;


 map = find_and_alloc_map(attr);
 if (IS_ERR(map))
  return PTR_ERR(map);

 atomic_set(&map->refcnt, 1);
 atomic_set(&map->usercnt, 1);

 err = bpf_map_charge_memlock(map);
 if (err)
  goto free_map;

 err = bpf_map_new_fd(map);
 if (err < 0)

  goto free_map;

 return err;

free_map:
 map->ops->map_free(map);
 return err;
}




struct bpf_map *__bpf_map_get(struct fd f)
{
 if (!f.file)
  return ERR_PTR(-EBADF);
 if (f.file->f_op != &bpf_map_fops) {
  fdput(f);
  return ERR_PTR(-EINVAL);
 }

 return f.file->private_data;
}



struct bpf_map *bpf_map_inc(struct bpf_map *map, bool uref)
{
 if (atomic_inc_return(&map->refcnt) > BPF_MAX_REFCNT) {
  atomic_dec(&map->refcnt);
  return ERR_PTR(-EBUSY);
 }
 if (uref)
  atomic_inc(&map->usercnt);
 return map;
}

struct bpf_map *bpf_map_get_with_uref(u32 ufd)
{
 struct fd f = fdget(ufd);
 struct bpf_map *map;

 map = __bpf_map_get(f);
 if (IS_ERR(map))
  return map;

 map = bpf_map_inc(map, true);
 fdput(f);

 return map;
}


static void __user *u64_to_ptr(__u64 val)
{
 return (void __user *) (unsigned long) val;
}

int __weak bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)
{
 return -ENOTSUPP;
}



static int map_lookup_elem(union bpf_attr *attr)
{
 void __user *ukey = u64_to_ptr(attr->key);
 void __user *uvalue = u64_to_ptr(attr->value);
 int ufd = attr->map_fd;
 struct bpf_map *map;
 void *key, *value, *ptr;
 u32 value_size;
 struct fd f;
 int err;

 if (CHECK_ATTR(BPF_MAP_LOOKUP_ELEM))
  return -EINVAL;

 f = fdget(ufd);
 map = __bpf_map_get(f);
 if (IS_ERR(map))
  return PTR_ERR(map);

 err = -ENOMEM;
 key = kmalloc(map->key_size, GFP_USER);
 if (!key)
  goto err_put;

 err = -EFAULT;
 if (copy_from_user(key, ukey, map->key_size) != 0)
  goto free_key;

 if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
     map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
  value_size = round_up(map->value_size, 8) * num_possible_cpus();
 else
  value_size = map->value_size;

 err = -ENOMEM;
 value = kmalloc(value_size, GFP_USER | __GFP_NOWARN);
 if (!value)
  goto free_key;

 if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH) {
  err = bpf_percpu_hash_copy(map, key, value);
 } else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
  err = bpf_percpu_array_copy(map, key, value);
 } else if (map->map_type == BPF_MAP_TYPE_STACK_TRACE) {
  err = bpf_stackmap_copy(map, key, value);
 } else {
  rcu_read_lock();
  ptr = map->ops->map_lookup_elem(map, key);
  if (ptr)
   memcpy(value, ptr, value_size);
  rcu_read_unlock();
  err = ptr ? 0 : -ENOENT;
 }

 if (err)
  goto free_value;

 err = -EFAULT;
 if (copy_to_user(uvalue, value, value_size) != 0)
  goto free_value;

 err = 0;

free_value:
 kfree(value);
free_key:
 kfree(key);
err_put:
 fdput(f);
 return err;
}


static int map_update_elem(union bpf_attr *attr)
{
 void __user *ukey = u64_to_ptr(attr->key);
 void __user *uvalue = u64_to_ptr(attr->value);
 int ufd = attr->map_fd;
 struct bpf_map *map;
 void *key, *value;
 u32 value_size;
 struct fd f;
 int err;

 if (CHECK_ATTR(BPF_MAP_UPDATE_ELEM))
  return -EINVAL;

 f = fdget(ufd);
 map = __bpf_map_get(f);
 if (IS_ERR(map))
  return PTR_ERR(map);

 err = -ENOMEM;
 key = kmalloc(map->key_size, GFP_USER);
 if (!key)
  goto err_put;

 err = -EFAULT;
 if (copy_from_user(key, ukey, map->key_size) != 0)
  goto free_key;

 if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
     map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
  value_size = round_up(map->value_size, 8) * num_possible_cpus();
 else
  value_size = map->value_size;

 err = -ENOMEM;
 value = kmalloc(value_size, GFP_USER | __GFP_NOWARN);
 if (!value)
  goto free_key;

 err = -EFAULT;
 if (copy_from_user(value, uvalue, value_size) != 0)
  goto free_value;




 preempt_disable();
 __this_cpu_inc(bpf_prog_active);
 if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH) {
  err = bpf_percpu_hash_update(map, key, value, attr->flags);
 } else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
  err = bpf_percpu_array_update(map, key, value, attr->flags);
 } else {
  rcu_read_lock();
  err = map->ops->map_update_elem(map, key, value, attr->flags);
  rcu_read_unlock();
 }
 __this_cpu_dec(bpf_prog_active);
 preempt_enable();

free_value:
 kfree(value);
free_key:
 kfree(key);
err_put:
 fdput(f);
 return err;
}


static int map_delete_elem(union bpf_attr *attr)
{
 void __user *ukey = u64_to_ptr(attr->key);
 int ufd = attr->map_fd;
 struct bpf_map *map;
 struct fd f;
 void *key;
 int err;

 if (CHECK_ATTR(BPF_MAP_DELETE_ELEM))
  return -EINVAL;

 f = fdget(ufd);
 map = __bpf_map_get(f);
 if (IS_ERR(map))
  return PTR_ERR(map);

 err = -ENOMEM;
 key = kmalloc(map->key_size, GFP_USER);
 if (!key)
  goto err_put;

 err = -EFAULT;
 if (copy_from_user(key, ukey, map->key_size) != 0)
  goto free_key;

 preempt_disable();
 __this_cpu_inc(bpf_prog_active);
 rcu_read_lock();
 err = map->ops->map_delete_elem(map, key);
 rcu_read_unlock();
 __this_cpu_dec(bpf_prog_active);
 preempt_enable();

free_key:
 kfree(key);
err_put:
 fdput(f);
 return err;
}



static int map_get_next_key(union bpf_attr *attr)
{
 void __user *ukey = u64_to_ptr(attr->key);
 void __user *unext_key = u64_to_ptr(attr->next_key);
 int ufd = attr->map_fd;
 struct bpf_map *map;
 void *key, *next_key;
 struct fd f;
 int err;

 if (CHECK_ATTR(BPF_MAP_GET_NEXT_KEY))
  return -EINVAL;

 f = fdget(ufd);
 map = __bpf_map_get(f);
 if (IS_ERR(map))
  return PTR_ERR(map);

 err = -ENOMEM;
 key = kmalloc(map->key_size, GFP_USER);
 if (!key)
  goto err_put;

 err = -EFAULT;
 if (copy_from_user(key, ukey, map->key_size) != 0)
  goto free_key;

 err = -ENOMEM;
 next_key = kmalloc(map->key_size, GFP_USER);
 if (!next_key)
  goto free_key;

 rcu_read_lock();
 err = map->ops->map_get_next_key(map, key, next_key);
 rcu_read_unlock();
 if (err)
  goto free_next_key;

 err = -EFAULT;
 if (copy_to_user(unext_key, next_key, map->key_size) != 0)
  goto free_next_key;

 err = 0;

free_next_key:
 kfree(next_key);
free_key:
 kfree(key);
err_put:
 fdput(f);
 return err;
}

static LIST_HEAD(bpf_prog_types);

static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)
{
 struct bpf_prog_type_list *tl;

 list_for_each_entry(tl, &bpf_prog_types, list_node) {
  if (tl->type == type) {
   prog->aux->ops = tl->ops;
   prog->type = type;
   return 0;
  }
 }

 return -EINVAL;
}

void bpf_register_prog_type(struct bpf_prog_type_list *tl)
{
 list_add(&tl->list_node, &bpf_prog_types);
}
static void fixup_bpf_calls(struct bpf_prog *prog)
{
 const struct bpf_func_proto *fn;
 int i;

 for (i = 0; i < prog->len; i++) {
  struct bpf_insn *insn = &prog->insnsi[i];

  if (insn->code == (BPF_JMP | BPF_CALL)) {




   BUG_ON(!prog->aux->ops->get_func_proto);

   if (insn->imm == BPF_FUNC_get_route_realm)
    prog->dst_needed = 1;
   if (insn->imm == BPF_FUNC_get_prandom_u32)
    bpf_user_rnd_init_once();
   if (insn->imm == BPF_FUNC_tail_call) {







    insn->imm = 0;
    insn->code |= BPF_X;
    continue;
   }

   fn = prog->aux->ops->get_func_proto(insn->imm);



   BUG_ON(!fn->func);
   insn->imm = fn->func - __bpf_call_base;
  }
 }
}


static void free_used_maps(struct bpf_prog_aux *aux)
{
 int i;

 for (i = 0; i < aux->used_map_cnt; i++)
  bpf_map_put(aux->used_maps[i]);

 kfree(aux->used_maps);
}

static int bpf_prog_charge_memlock(struct bpf_prog *prog)
{
 struct user_struct *user = get_current_user();
 unsigned long memlock_limit;

 memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;

 atomic_long_add(prog->pages, &user->locked_vm);
 if (atomic_long_read(&user->locked_vm) > memlock_limit) {
  atomic_long_sub(prog->pages, &user->locked_vm);
  free_uid(user);
  return -EPERM;
 }
 prog->aux->user = user;
 return 0;
}

static void bpf_prog_uncharge_memlock(struct bpf_prog *prog)
{
 struct user_struct *user = prog->aux->user;

 atomic_long_sub(prog->pages, &user->locked_vm);
 free_uid(user);
}

static void __prog_put_common(struct rcu_head *rcu)
{
 struct bpf_prog_aux *aux = container_of(rcu, struct bpf_prog_aux, rcu);

 free_used_maps(aux);
 bpf_prog_uncharge_memlock(aux->prog);
 bpf_prog_free(aux->prog);
}


void bpf_prog_put_rcu(struct bpf_prog *prog)
{
 if (atomic_dec_and_test(&prog->aux->refcnt))
  call_rcu(&prog->aux->rcu, __prog_put_common);
}

void bpf_prog_put(struct bpf_prog *prog)
{
 if (atomic_dec_and_test(&prog->aux->refcnt))
  __prog_put_common(&prog->aux->rcu);
}
EXPORT_SYMBOL_GPL(bpf_prog_put);

static int bpf_prog_release(struct inode *inode, struct file *filp)
{
 struct bpf_prog *prog = filp->private_data;

 bpf_prog_put_rcu(prog);
 return 0;
}

static const struct file_operations bpf_prog_fops = {
        .release = bpf_prog_release,
};

int bpf_prog_new_fd(struct bpf_prog *prog)
{
 return anon_inode_getfd("bpf-prog", &bpf_prog_fops, prog,
    O_RDWR | O_CLOEXEC);
}

static struct bpf_prog *__bpf_prog_get(struct fd f)
{
 if (!f.file)
  return ERR_PTR(-EBADF);
 if (f.file->f_op != &bpf_prog_fops) {
  fdput(f);
  return ERR_PTR(-EINVAL);
 }

 return f.file->private_data;
}

struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
{
 if (atomic_inc_return(&prog->aux->refcnt) > BPF_MAX_REFCNT) {
  atomic_dec(&prog->aux->refcnt);
  return ERR_PTR(-EBUSY);
 }
 return prog;
}




struct bpf_prog *bpf_prog_get(u32 ufd)
{
 struct fd f = fdget(ufd);
 struct bpf_prog *prog;

 prog = __bpf_prog_get(f);
 if (IS_ERR(prog))
  return prog;

 prog = bpf_prog_inc(prog);
 fdput(f);

 return prog;
}
EXPORT_SYMBOL_GPL(bpf_prog_get);



static int bpf_prog_load(union bpf_attr *attr)
{
 enum bpf_prog_type type = attr->prog_type;
 struct bpf_prog *prog;
 int err;
 char license[128];
 bool is_gpl;

 if (CHECK_ATTR(BPF_PROG_LOAD))
  return -EINVAL;


 if (strncpy_from_user(license, u64_to_ptr(attr->license),
         sizeof(license) - 1) < 0)
  return -EFAULT;
 license[sizeof(license) - 1] = 0;


 is_gpl = license_is_gpl_compatible(license);

 if (attr->insn_cnt >= BPF_MAXINSNS)
  return -EINVAL;

 if (type == BPF_PROG_TYPE_KPROBE &&
     attr->kern_version != LINUX_VERSION_CODE)
  return -EINVAL;

 if (type != BPF_PROG_TYPE_SOCKET_FILTER && !capable(CAP_SYS_ADMIN))
  return -EPERM;


 prog = bpf_prog_alloc(bpf_prog_size(attr->insn_cnt), GFP_USER);
 if (!prog)
  return -ENOMEM;

 err = bpf_prog_charge_memlock(prog);
 if (err)
  goto free_prog_nouncharge;

 prog->len = attr->insn_cnt;

 err = -EFAULT;
 if (copy_from_user(prog->insns, u64_to_ptr(attr->insns),
      prog->len * sizeof(struct bpf_insn)) != 0)
  goto free_prog;

 prog->orig_prog = NULL;
 prog->jited = 0;

 atomic_set(&prog->aux->refcnt, 1);
 prog->gpl_compatible = is_gpl ? 1 : 0;


 err = find_prog_type(type, prog);
 if (err < 0)
  goto free_prog;


 err = bpf_check(&prog, attr);
 if (err < 0)
  goto free_used_maps;


 fixup_bpf_calls(prog);


 prog = bpf_prog_select_runtime(prog, &err);
 if (err < 0)
  goto free_used_maps;

 err = bpf_prog_new_fd(prog);
 if (err < 0)

  goto free_used_maps;

 return err;

free_used_maps:
 free_used_maps(prog->aux);
free_prog:
 bpf_prog_uncharge_memlock(prog);
free_prog_nouncharge:
 bpf_prog_free(prog);
 return err;
}


static int bpf_obj_pin(const union bpf_attr *attr)
{
 if (CHECK_ATTR(BPF_OBJ))
  return -EINVAL;

 return bpf_obj_pin_user(attr->bpf_fd, u64_to_ptr(attr->pathname));
}

static int bpf_obj_get(const union bpf_attr *attr)
{
 if (CHECK_ATTR(BPF_OBJ) || attr->bpf_fd != 0)
  return -EINVAL;

 return bpf_obj_get_user(u64_to_ptr(attr->pathname));
}

SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
{
 union bpf_attr attr = {};
 int err;

 if (!capable(CAP_SYS_ADMIN) && sysctl_unprivileged_bpf_disabled)
  return -EPERM;

 if (!access_ok(VERIFY_READ, uattr, 1))
  return -EFAULT;

 if (size > PAGE_SIZE)
  return -E2BIG;






 if (size > sizeof(attr)) {
  unsigned char __user *addr;
  unsigned char __user *end;
  unsigned char val;

  addr = (void __user *)uattr + sizeof(attr);
  end = (void __user *)uattr + size;

  for (; addr < end; addr++) {
   err = get_user(val, addr);
   if (err)
    return err;
   if (val)
    return -E2BIG;
  }
  size = sizeof(attr);
 }


 if (copy_from_user(&attr, uattr, size) != 0)
  return -EFAULT;

 switch (cmd) {
 case BPF_MAP_CREATE:
  err = map_create(&attr);
  break;
 case BPF_MAP_LOOKUP_ELEM:
  err = map_lookup_elem(&attr);
  break;
 case BPF_MAP_UPDATE_ELEM:
  err = map_update_elem(&attr);
  break;
 case BPF_MAP_DELETE_ELEM:
  err = map_delete_elem(&attr);
  break;
 case BPF_MAP_GET_NEXT_KEY:
  err = map_get_next_key(&attr);
  break;
 case BPF_PROG_LOAD:
  err = bpf_prog_load(&attr);
  break;
 case BPF_OBJ_PIN:
  err = bpf_obj_pin(&attr);
  break;
 case BPF_OBJ_GET:
  err = bpf_obj_get(&attr);
  break;
 default:
  err = -EINVAL;
  break;
 }

 return err;
}


struct bin_table;
typedef ssize_t bin_convert_t(struct file *file,
 void __user *oldval, size_t oldlen, void __user *newval, size_t newlen);

static bin_convert_t bin_dir;
static bin_convert_t bin_string;
static bin_convert_t bin_intvec;
static bin_convert_t bin_ulongvec;
static bin_convert_t bin_uuid;
static bin_convert_t bin_dn_node_address;



struct bin_table {
 bin_convert_t *convert;
 int ctl_name;
 const char *procname;
 const struct bin_table *child;
};

static const struct bin_table bin_random_table[] = {
 { CTL_INT, RANDOM_POOLSIZE, "poolsize" },
 { CTL_INT, RANDOM_ENTROPY_COUNT, "entropy_avail" },
 { CTL_INT, RANDOM_READ_THRESH, "read_wakeup_threshold" },
 { CTL_INT, RANDOM_WRITE_THRESH, "write_wakeup_threshold" },
 { CTL_UUID, RANDOM_BOOT_ID, "boot_id" },
 { CTL_UUID, RANDOM_UUID, "uuid" },
 {}
};

static const struct bin_table bin_pty_table[] = {
 { CTL_INT, PTY_MAX, "max" },
 { CTL_INT, PTY_NR, "nr" },
 {}
};

static const struct bin_table bin_kern_table[] = {
 { CTL_STR, KERN_OSTYPE, "ostype" },
 { CTL_STR, KERN_OSRELEASE, "osrelease" },

 { CTL_STR, KERN_VERSION, "version" },


 { CTL_STR, KERN_NODENAME, "hostname" },
 { CTL_STR, KERN_DOMAINNAME, "domainname" },

 { CTL_INT, KERN_PANIC, "panic" },
 { CTL_INT, KERN_REALROOTDEV, "real-root-dev" },

 { CTL_STR, KERN_SPARC_REBOOT, "reboot-cmd" },
 { CTL_INT, KERN_CTLALTDEL, "ctrl-alt-del" },
 { CTL_INT, KERN_PRINTK, "printk" },




 { CTL_INT, KERN_PPC_POWERSAVE_NAP, "powersave-nap" },

 { CTL_STR, KERN_MODPROBE, "modprobe" },
 { CTL_INT, KERN_SG_BIG_BUFF, "sg-big-buff" },
 { CTL_INT, KERN_ACCT, "acct" },





 { CTL_ULONG, KERN_SHMMAX, "shmmax" },
 { CTL_INT, KERN_MSGMAX, "msgmax" },
 { CTL_INT, KERN_MSGMNB, "msgmnb" },

 { CTL_INT, KERN_SYSRQ, "sysrq" },
 { CTL_INT, KERN_MAX_THREADS, "threads-max" },
 { CTL_DIR, KERN_RANDOM, "random", bin_random_table },
 { CTL_ULONG, KERN_SHMALL, "shmall" },
 { CTL_INT, KERN_MSGMNI, "msgmni" },
 { CTL_INT, KERN_SEM, "sem" },
 { CTL_INT, KERN_SPARC_STOP_A, "stop-a" },
 { CTL_INT, KERN_SHMMNI, "shmmni" },

 { CTL_INT, KERN_OVERFLOWUID, "overflowuid" },
 { CTL_INT, KERN_OVERFLOWGID, "overflowgid" },

 { CTL_STR, KERN_HOTPLUG, "hotplug", },
 { CTL_INT, KERN_IEEE_EMULATION_WARNINGS, "ieee_emulation_warnings" },

 { CTL_INT, KERN_S390_USER_DEBUG_LOGGING, "userprocess_debug" },
 { CTL_INT, KERN_CORE_USES_PID, "core_uses_pid" },

 { CTL_INT, KERN_CADPID, "cad_pid" },
 { CTL_INT, KERN_PIDMAX, "pid_max" },
 { CTL_STR, KERN_CORE_PATTERN, "core_pattern" },
 { CTL_INT, KERN_PANIC_ON_OOPS, "panic_on_oops" },
 { CTL_INT, KERN_HPPA_PWRSW, "soft-power" },
 { CTL_INT, KERN_HPPA_UNALIGNED, "unaligned-trap" },

 { CTL_INT, KERN_PRINTK_RATELIMIT, "printk_ratelimit" },
 { CTL_INT, KERN_PRINTK_RATELIMIT_BURST, "printk_ratelimit_burst" },

 { CTL_DIR, KERN_PTY, "pty", bin_pty_table },
 { CTL_INT, KERN_NGROUPS_MAX, "ngroups_max" },
 { CTL_INT, KERN_SPARC_SCONS_PWROFF, "scons-poweroff" },

 { CTL_INT, KERN_UNKNOWN_NMI_PANIC, "unknown_nmi_panic" },
 { CTL_INT, KERN_BOOTLOADER_TYPE, "bootloader_type" },
 { CTL_INT, KERN_RANDOMIZE, "randomize_va_space" },

 { CTL_INT, KERN_SPIN_RETRY, "spin_retry" },

 { CTL_INT, KERN_IA64_UNALIGNED, "ignore-unaligned-usertrap" },
 { CTL_INT, KERN_COMPAT_LOG, "compat-log" },
 { CTL_INT, KERN_MAX_LOCK_DEPTH, "max_lock_depth" },
 { CTL_INT, KERN_PANIC_ON_NMI, "panic_on_unrecovered_nmi" },
 { CTL_INT, KERN_PANIC_ON_WARN, "panic_on_warn" },
 {}
};

static const struct bin_table bin_vm_table[] = {
 { CTL_INT, VM_OVERCOMMIT_MEMORY, "overcommit_memory" },
 { CTL_INT, VM_PAGE_CLUSTER, "page-cluster" },
 { CTL_INT, VM_DIRTY_BACKGROUND, "dirty_background_ratio" },
 { CTL_INT, VM_DIRTY_RATIO, "dirty_ratio" },



 { CTL_INT, VM_OVERCOMMIT_RATIO, "overcommit_ratio" },


 { CTL_INT, VM_SWAPPINESS, "swappiness" },
 { CTL_INT, VM_LOWMEM_RESERVE_RATIO, "lowmem_reserve_ratio" },
 { CTL_INT, VM_MIN_FREE_KBYTES, "min_free_kbytes" },
 { CTL_INT, VM_MAX_MAP_COUNT, "max_map_count" },
 { CTL_INT, VM_LAPTOP_MODE, "laptop_mode" },
 { CTL_INT, VM_BLOCK_DUMP, "block_dump" },
 { CTL_INT, VM_HUGETLB_GROUP, "hugetlb_shm_group" },
 { CTL_INT, VM_VFS_CACHE_PRESSURE, "vfs_cache_pressure" },
 { CTL_INT, VM_LEGACY_VA_LAYOUT, "legacy_va_layout" },

 { CTL_INT, VM_DROP_PAGECACHE, "drop_caches" },
 { CTL_INT, VM_PERCPU_PAGELIST_FRACTION, "percpu_pagelist_fraction" },
 { CTL_INT, VM_ZONE_RECLAIM_MODE, "zone_reclaim_mode" },
 { CTL_INT, VM_MIN_UNMAPPED, "min_unmapped_ratio" },
 { CTL_INT, VM_PANIC_ON_OOM, "panic_on_oom" },
 { CTL_INT, VM_VDSO_ENABLED, "vdso_enabled" },
 { CTL_INT, VM_MIN_SLAB, "min_slab_ratio" },

 {}
};

static const struct bin_table bin_net_core_table[] = {
 { CTL_INT, NET_CORE_WMEM_MAX, "wmem_max" },
 { CTL_INT, NET_CORE_RMEM_MAX, "rmem_max" },
 { CTL_INT, NET_CORE_WMEM_DEFAULT, "wmem_default" },
 { CTL_INT, NET_CORE_RMEM_DEFAULT, "rmem_default" },

 { CTL_INT, NET_CORE_MAX_BACKLOG, "netdev_max_backlog" },

 { CTL_INT, NET_CORE_MSG_COST, "message_cost" },
 { CTL_INT, NET_CORE_MSG_BURST, "message_burst" },
 { CTL_INT, NET_CORE_OPTMEM_MAX, "optmem_max" },






 { CTL_INT, NET_CORE_DEV_WEIGHT, "dev_weight" },
 { CTL_INT, NET_CORE_SOMAXCONN, "somaxconn" },
 { CTL_INT, NET_CORE_BUDGET, "netdev_budget" },
 { CTL_INT, NET_CORE_AEVENT_ETIME, "xfrm_aevent_etime" },
 { CTL_INT, NET_CORE_AEVENT_RSEQTH, "xfrm_aevent_rseqth" },
 { CTL_INT, NET_CORE_WARNINGS, "warnings" },
 {},
};

static const struct bin_table bin_net_unix_table[] = {


 { CTL_INT, NET_UNIX_MAX_DGRAM_QLEN, "max_dgram_qlen" },
 {}
};

static const struct bin_table bin_net_ipv4_route_table[] = {
 { CTL_INT, NET_IPV4_ROUTE_FLUSH, "flush" },


 { CTL_INT, NET_IPV4_ROUTE_GC_THRESH, "gc_thresh" },
 { CTL_INT, NET_IPV4_ROUTE_MAX_SIZE, "max_size" },
 { CTL_INT, NET_IPV4_ROUTE_GC_MIN_INTERVAL, "gc_min_interval" },
 { CTL_INT, NET_IPV4_ROUTE_GC_MIN_INTERVAL_MS, "gc_min_interval_ms" },
 { CTL_INT, NET_IPV4_ROUTE_GC_TIMEOUT, "gc_timeout" },

 { CTL_INT, NET_IPV4_ROUTE_REDIRECT_LOAD, "redirect_load" },
 { CTL_INT, NET_IPV4_ROUTE_REDIRECT_NUMBER, "redirect_number" },
 { CTL_INT, NET_IPV4_ROUTE_REDIRECT_SILENCE, "redirect_silence" },
 { CTL_INT, NET_IPV4_ROUTE_ERROR_COST, "error_cost" },
 { CTL_INT, NET_IPV4_ROUTE_ERROR_BURST, "error_burst" },
 { CTL_INT, NET_IPV4_ROUTE_GC_ELASTICITY, "gc_elasticity" },
 { CTL_INT, NET_IPV4_ROUTE_MTU_EXPIRES, "mtu_expires" },
 { CTL_INT, NET_IPV4_ROUTE_MIN_PMTU, "min_pmtu" },
 { CTL_INT, NET_IPV4_ROUTE_MIN_ADVMSS, "min_adv_mss" },
 {}
};

static const struct bin_table bin_net_ipv4_conf_vars_table[] = {
 { CTL_INT, NET_IPV4_CONF_FORWARDING, "forwarding" },
 { CTL_INT, NET_IPV4_CONF_MC_FORWARDING, "mc_forwarding" },

 { CTL_INT, NET_IPV4_CONF_ACCEPT_REDIRECTS, "accept_redirects" },
 { CTL_INT, NET_IPV4_CONF_SECURE_REDIRECTS, "secure_redirects" },
 { CTL_INT, NET_IPV4_CONF_SEND_REDIRECTS, "send_redirects" },
 { CTL_INT, NET_IPV4_CONF_SHARED_MEDIA, "shared_media" },
 { CTL_INT, NET_IPV4_CONF_RP_FILTER, "rp_filter" },
 { CTL_INT, NET_IPV4_CONF_ACCEPT_SOURCE_ROUTE, "accept_source_route" },
 { CTL_INT, NET_IPV4_CONF_PROXY_ARP, "proxy_arp" },
 { CTL_INT, NET_IPV4_CONF_MEDIUM_ID, "medium_id" },
 { CTL_INT, NET_IPV4_CONF_BOOTP_RELAY, "bootp_relay" },
 { CTL_INT, NET_IPV4_CONF_LOG_MARTIANS, "log_martians" },
 { CTL_INT, NET_IPV4_CONF_TAG, "tag" },
 { CTL_INT, NET_IPV4_CONF_ARPFILTER, "arp_filter" },
 { CTL_INT, NET_IPV4_CONF_ARP_ANNOUNCE, "arp_announce" },
 { CTL_INT, NET_IPV4_CONF_ARP_IGNORE, "arp_ignore" },
 { CTL_INT, NET_IPV4_CONF_ARP_ACCEPT, "arp_accept" },
 { CTL_INT, NET_IPV4_CONF_ARP_NOTIFY, "arp_notify" },

 { CTL_INT, NET_IPV4_CONF_NOXFRM, "disable_xfrm" },
 { CTL_INT, NET_IPV4_CONF_NOPOLICY, "disable_policy" },
 { CTL_INT, NET_IPV4_CONF_FORCE_IGMP_VERSION, "force_igmp_version" },
 { CTL_INT, NET_IPV4_CONF_PROMOTE_SECONDARIES, "promote_secondaries" },
 {}
};

static const struct bin_table bin_net_ipv4_conf_table[] = {
 { CTL_DIR, NET_PROTO_CONF_ALL, "all", bin_net_ipv4_conf_vars_table },
 { CTL_DIR, NET_PROTO_CONF_DEFAULT, "default", bin_net_ipv4_conf_vars_table },
 { CTL_DIR, 0, NULL, bin_net_ipv4_conf_vars_table },
 {}
};

static const struct bin_table bin_net_neigh_vars_table[] = {
 { CTL_INT, NET_NEIGH_MCAST_SOLICIT, "mcast_solicit" },
 { CTL_INT, NET_NEIGH_UCAST_SOLICIT, "ucast_solicit" },
 { CTL_INT, NET_NEIGH_APP_SOLICIT, "app_solicit" },

 { CTL_INT, NET_NEIGH_REACHABLE_TIME, "base_reachable_time" },
 { CTL_INT, NET_NEIGH_DELAY_PROBE_TIME, "delay_first_probe_time" },
 { CTL_INT, NET_NEIGH_GC_STALE_TIME, "gc_stale_time" },
 { CTL_INT, NET_NEIGH_UNRES_QLEN, "unres_qlen" },
 { CTL_INT, NET_NEIGH_PROXY_QLEN, "proxy_qlen" },



 { CTL_INT, NET_NEIGH_GC_INTERVAL, "gc_interval" },
 { CTL_INT, NET_NEIGH_GC_THRESH1, "gc_thresh1" },
 { CTL_INT, NET_NEIGH_GC_THRESH2, "gc_thresh2" },
 { CTL_INT, NET_NEIGH_GC_THRESH3, "gc_thresh3" },
 { CTL_INT, NET_NEIGH_RETRANS_TIME_MS, "retrans_time_ms" },
 { CTL_INT, NET_NEIGH_REACHABLE_TIME_MS, "base_reachable_time_ms" },
 {}
};

static const struct bin_table bin_net_neigh_table[] = {
 { CTL_DIR, NET_PROTO_CONF_DEFAULT, "default", bin_net_neigh_vars_table },
 { CTL_DIR, 0, NULL, bin_net_neigh_vars_table },
 {}
};

static const struct bin_table bin_net_ipv4_netfilter_table[] = {
 { CTL_INT, NET_IPV4_NF_CONNTRACK_MAX, "ip_conntrack_max" },
 { CTL_INT, NET_IPV4_NF_CONNTRACK_BUCKETS, "ip_conntrack_buckets" },
 { CTL_INT, NET_IPV4_NF_CONNTRACK_LOG_INVALID, "ip_conntrack_log_invalid" },

 { CTL_INT, NET_IPV4_NF_CONNTRACK_TCP_LOOSE, "ip_conntrack_tcp_loose" },
 { CTL_INT, NET_IPV4_NF_CONNTRACK_TCP_BE_LIBERAL, "ip_conntrack_tcp_be_liberal" },
 { CTL_INT, NET_IPV4_NF_CONNTRACK_TCP_MAX_RETRANS, "ip_conntrack_tcp_max_retrans" },
 { CTL_INT, NET_IPV4_NF_CONNTRACK_COUNT, "ip_conntrack_count" },
 { CTL_INT, NET_IPV4_NF_CONNTRACK_CHECKSUM, "ip_conntrack_checksum" },
 {}
};

static const struct bin_table bin_net_ipv4_table[] = {
 {CTL_INT, NET_IPV4_FORWARD, "ip_forward" },

 { CTL_DIR, NET_IPV4_CONF, "conf", bin_net_ipv4_conf_table },
 { CTL_DIR, NET_IPV4_NEIGH, "neigh", bin_net_neigh_table },
 { CTL_DIR, NET_IPV4_ROUTE, "route", bin_net_ipv4_route_table },

 { CTL_DIR, NET_IPV4_NETFILTER, "netfilter", bin_net_ipv4_netfilter_table },

 { CTL_INT, NET_IPV4_TCP_TIMESTAMPS, "tcp_timestamps" },
 { CTL_INT, NET_IPV4_TCP_WINDOW_SCALING, "tcp_window_scaling" },
 { CTL_INT, NET_IPV4_TCP_SACK, "tcp_sack" },
 { CTL_INT, NET_IPV4_TCP_RETRANS_COLLAPSE, "tcp_retrans_collapse" },
 { CTL_INT, NET_IPV4_DEFAULT_TTL, "ip_default_ttl" },

 { CTL_INT, NET_IPV4_NO_PMTU_DISC, "ip_no_pmtu_disc" },
 { CTL_INT, NET_IPV4_NONLOCAL_BIND, "ip_nonlocal_bind" },
 { CTL_INT, NET_IPV4_TCP_SYN_RETRIES, "tcp_syn_retries" },
 { CTL_INT, NET_TCP_SYNACK_RETRIES, "tcp_synack_retries" },
 { CTL_INT, NET_TCP_MAX_ORPHANS, "tcp_max_orphans" },
 { CTL_INT, NET_TCP_MAX_TW_BUCKETS, "tcp_max_tw_buckets" },
 { CTL_INT, NET_IPV4_DYNADDR, "ip_dynaddr" },
 { CTL_INT, NET_IPV4_TCP_KEEPALIVE_TIME, "tcp_keepalive_time" },
 { CTL_INT, NET_IPV4_TCP_KEEPALIVE_PROBES, "tcp_keepalive_probes" },
 { CTL_INT, NET_IPV4_TCP_KEEPALIVE_INTVL, "tcp_keepalive_intvl" },
 { CTL_INT, NET_IPV4_TCP_RETRIES1, "tcp_retries1" },
 { CTL_INT, NET_IPV4_TCP_RETRIES2, "tcp_retries2" },
 { CTL_INT, NET_IPV4_TCP_FIN_TIMEOUT, "tcp_fin_timeout" },
 { CTL_INT, NET_TCP_SYNCOOKIES, "tcp_syncookies" },
 { CTL_INT, NET_TCP_TW_RECYCLE, "tcp_tw_recycle" },
 { CTL_INT, NET_TCP_ABORT_ON_OVERFLOW, "tcp_abort_on_overflow" },
 { CTL_INT, NET_TCP_STDURG, "tcp_stdurg" },
 { CTL_INT, NET_TCP_RFC1337, "tcp_rfc1337" },
 { CTL_INT, NET_TCP_MAX_SYN_BACKLOG, "tcp_max_syn_backlog" },
 { CTL_INT, NET_IPV4_LOCAL_PORT_RANGE, "ip_local_port_range" },
 { CTL_INT, NET_IPV4_IGMP_MAX_MEMBERSHIPS, "igmp_max_memberships" },
 { CTL_INT, NET_IPV4_IGMP_MAX_MSF, "igmp_max_msf" },
 { CTL_INT, NET_IPV4_INET_PEER_THRESHOLD, "inet_peer_threshold" },
 { CTL_INT, NET_IPV4_INET_PEER_MINTTL, "inet_peer_minttl" },
 { CTL_INT, NET_IPV4_INET_PEER_MAXTTL, "inet_peer_maxttl" },
 { CTL_INT, NET_IPV4_INET_PEER_GC_MINTIME, "inet_peer_gc_mintime" },
 { CTL_INT, NET_IPV4_INET_PEER_GC_MAXTIME, "inet_peer_gc_maxtime" },
 { CTL_INT, NET_TCP_ORPHAN_RETRIES, "tcp_orphan_retries" },
 { CTL_INT, NET_TCP_FACK, "tcp_fack" },
 { CTL_INT, NET_TCP_REORDERING, "tcp_reordering" },
 { CTL_INT, NET_TCP_ECN, "tcp_ecn" },
 { CTL_INT, NET_TCP_DSACK, "tcp_dsack" },
 { CTL_INT, NET_TCP_MEM, "tcp_mem" },
 { CTL_INT, NET_TCP_WMEM, "tcp_wmem" },
 { CTL_INT, NET_TCP_RMEM, "tcp_rmem" },
 { CTL_INT, NET_TCP_APP_WIN, "tcp_app_win" },
 { CTL_INT, NET_TCP_ADV_WIN_SCALE, "tcp_adv_win_scale" },
 { CTL_INT, NET_TCP_TW_REUSE, "tcp_tw_reuse" },
 { CTL_INT, NET_TCP_FRTO, "tcp_frto" },
 { CTL_INT, NET_TCP_FRTO_RESPONSE, "tcp_frto_response" },
 { CTL_INT, NET_TCP_LOW_LATENCY, "tcp_low_latency" },
 { CTL_INT, NET_TCP_NO_METRICS_SAVE, "tcp_no_metrics_save" },
 { CTL_INT, NET_TCP_MODERATE_RCVBUF, "tcp_moderate_rcvbuf" },
 { CTL_INT, NET_TCP_TSO_WIN_DIVISOR, "tcp_tso_win_divisor" },
 { CTL_STR, NET_TCP_CONG_CONTROL, "tcp_congestion_control" },
 { CTL_INT, NET_TCP_MTU_PROBING, "tcp_mtu_probing" },
 { CTL_INT, NET_TCP_BASE_MSS, "tcp_base_mss" },
 { CTL_INT, NET_IPV4_TCP_WORKAROUND_SIGNED_WINDOWS, "tcp_workaround_signed_windows" },
 { CTL_INT, NET_TCP_SLOW_START_AFTER_IDLE, "tcp_slow_start_after_idle" },
 { CTL_INT, NET_CIPSOV4_CACHE_ENABLE, "cipso_cache_enable" },
 { CTL_INT, NET_CIPSOV4_CACHE_BUCKET_SIZE, "cipso_cache_bucket_size" },
 { CTL_INT, NET_CIPSOV4_RBM_OPTFMT, "cipso_rbm_optfmt" },
 { CTL_INT, NET_CIPSOV4_RBM_STRICTVALID, "cipso_rbm_strictvalid" },

 { CTL_STR, NET_TCP_ALLOWED_CONG_CONTROL, "tcp_allowed_congestion_control" },
 { CTL_INT, NET_TCP_MAX_SSTHRESH, "tcp_max_ssthresh" },

 { CTL_INT, NET_IPV4_ICMP_ECHO_IGNORE_ALL, "icmp_echo_ignore_all" },
 { CTL_INT, NET_IPV4_ICMP_ECHO_IGNORE_BROADCASTS, "icmp_echo_ignore_broadcasts" },
 { CTL_INT, NET_IPV4_ICMP_IGNORE_BOGUS_ERROR_RESPONSES, "icmp_ignore_bogus_error_responses" },
 { CTL_INT, NET_IPV4_ICMP_ERRORS_USE_INBOUND_IFADDR, "icmp_errors_use_inbound_ifaddr" },
 { CTL_INT, NET_IPV4_ICMP_RATELIMIT, "icmp_ratelimit" },
 { CTL_INT, NET_IPV4_ICMP_RATEMASK, "icmp_ratemask" },

 { CTL_INT, NET_IPV4_IPFRAG_HIGH_THRESH, "ipfrag_high_thresh" },
 { CTL_INT, NET_IPV4_IPFRAG_LOW_THRESH, "ipfrag_low_thresh" },
 { CTL_INT, NET_IPV4_IPFRAG_TIME, "ipfrag_time" },

 { CTL_INT, NET_IPV4_IPFRAG_SECRET_INTERVAL, "ipfrag_secret_interval" },


 { CTL_INT, 2088 , "ip_queue_maxlen" },
 {}
};

static const struct bin_table bin_net_ipx_table[] = {
 { CTL_INT, NET_IPX_PPROP_BROADCASTING, "ipx_pprop_broadcasting" },

 {}
};

static const struct bin_table bin_net_atalk_table[] = {
 { CTL_INT, NET_ATALK_AARP_EXPIRY_TIME, "aarp-expiry-time" },
 { CTL_INT, NET_ATALK_AARP_TICK_TIME, "aarp-tick-time" },
 { CTL_INT, NET_ATALK_AARP_RETRANSMIT_LIMIT, "aarp-retransmit-limit" },
 { CTL_INT, NET_ATALK_AARP_RESOLVE_TIME, "aarp-resolve-time" },
 {},
};

static const struct bin_table bin_net_netrom_table[] = {
 { CTL_INT, NET_NETROM_DEFAULT_PATH_QUALITY, "default_path_quality" },
 { CTL_INT, NET_NETROM_OBSOLESCENCE_COUNT_INITIALISER, "obsolescence_count_initialiser" },
 { CTL_INT, NET_NETROM_NETWORK_TTL_INITIALISER, "network_ttl_initialiser" },
 { CTL_INT, NET_NETROM_TRANSPORT_TIMEOUT, "transport_timeout" },
 { CTL_INT, NET_NETROM_TRANSPORT_MAXIMUM_TRIES, "transport_maximum_tries" },
 { CTL_INT, NET_NETROM_TRANSPORT_ACKNOWLEDGE_DELAY, "transport_acknowledge_delay" },
 { CTL_INT, NET_NETROM_TRANSPORT_BUSY_DELAY, "transport_busy_delay" },
 { CTL_INT, NET_NETROM_TRANSPORT_REQUESTED_WINDOW_SIZE, "transport_requested_window_size" },
 { CTL_INT, NET_NETROM_TRANSPORT_NO_ACTIVITY_TIMEOUT, "transport_no_activity_timeout" },
 { CTL_INT, NET_NETROM_ROUTING_CONTROL, "routing_control" },
 { CTL_INT, NET_NETROM_LINK_FAILS_COUNT, "link_fails_count" },
 { CTL_INT, NET_NETROM_RESET, "reset" },
 {}
};

static const struct bin_table bin_net_ax25_param_table[] = {
 { CTL_INT, NET_AX25_IP_DEFAULT_MODE, "ip_default_mode" },
 { CTL_INT, NET_AX25_DEFAULT_MODE, "ax25_default_mode" },
 { CTL_INT, NET_AX25_BACKOFF_TYPE, "backoff_type" },
 { CTL_INT, NET_AX25_CONNECT_MODE, "connect_mode" },
 { CTL_INT, NET_AX25_STANDARD_WINDOW, "standard_window_size" },
 { CTL_INT, NET_AX25_EXTENDED_WINDOW, "extended_window_size" },
 { CTL_INT, NET_AX25_T1_TIMEOUT, "t1_timeout" },
 { CTL_INT, NET_AX25_T2_TIMEOUT, "t2_timeout" },
 { CTL_INT, NET_AX25_T3_TIMEOUT, "t3_timeout" },
 { CTL_INT, NET_AX25_IDLE_TIMEOUT, "idle_timeout" },
 { CTL_INT, NET_AX25_N2, "maximum_retry_count" },
 { CTL_INT, NET_AX25_PACLEN, "maximum_packet_length" },
 { CTL_INT, NET_AX25_PROTOCOL, "protocol" },
 { CTL_INT, NET_AX25_DAMA_SLAVE_TIMEOUT, "dama_slave_timeout" },
 {}
};

static const struct bin_table bin_net_ax25_table[] = {
 { CTL_DIR, 0, NULL, bin_net_ax25_param_table },
 {}
};

static const struct bin_table bin_net_rose_table[] = {
 { CTL_INT, NET_ROSE_RESTART_REQUEST_TIMEOUT, "restart_request_timeout" },
 { CTL_INT, NET_ROSE_CALL_REQUEST_TIMEOUT, "call_request_timeout" },
 { CTL_INT, NET_ROSE_RESET_REQUEST_TIMEOUT, "reset_request_timeout" },
 { CTL_INT, NET_ROSE_CLEAR_REQUEST_TIMEOUT, "clear_request_timeout" },
 { CTL_INT, NET_ROSE_ACK_HOLD_BACK_TIMEOUT, "acknowledge_hold_back_timeout" },
 { CTL_INT, NET_ROSE_ROUTING_CONTROL, "routing_control" },
 { CTL_INT, NET_ROSE_LINK_FAIL_TIMEOUT, "link_fail_timeout" },
 { CTL_INT, NET_ROSE_MAX_VCS, "maximum_virtual_circuits" },
 { CTL_INT, NET_ROSE_WINDOW_SIZE, "window_size" },
 { CTL_INT, NET_ROSE_NO_ACTIVITY_TIMEOUT, "no_activity_timeout" },
 {}
};

static const struct bin_table bin_net_ipv6_conf_var_table[] = {
 { CTL_INT, NET_IPV6_FORWARDING, "forwarding" },
 { CTL_INT, NET_IPV6_HOP_LIMIT, "hop_limit" },
 { CTL_INT, NET_IPV6_MTU, "mtu" },
 { CTL_INT, NET_IPV6_ACCEPT_RA, "accept_ra" },
 { CTL_INT, NET_IPV6_ACCEPT_REDIRECTS, "accept_redirects" },
 { CTL_INT, NET_IPV6_AUTOCONF, "autoconf" },
 { CTL_INT, NET_IPV6_DAD_TRANSMITS, "dad_transmits" },
 { CTL_INT, NET_IPV6_RTR_SOLICITS, "router_solicitations" },
 { CTL_INT, NET_IPV6_RTR_SOLICIT_INTERVAL, "router_solicitation_interval" },
 { CTL_INT, NET_IPV6_RTR_SOLICIT_DELAY, "router_solicitation_delay" },
 { CTL_INT, NET_IPV6_USE_TEMPADDR, "use_tempaddr" },
 { CTL_INT, NET_IPV6_TEMP_VALID_LFT, "temp_valid_lft" },
 { CTL_INT, NET_IPV6_TEMP_PREFERED_LFT, "temp_prefered_lft" },
 { CTL_INT, NET_IPV6_REGEN_MAX_RETRY, "regen_max_retry" },
 { CTL_INT, NET_IPV6_MAX_DESYNC_FACTOR, "max_desync_factor" },
 { CTL_INT, NET_IPV6_MAX_ADDRESSES, "max_addresses" },
 { CTL_INT, NET_IPV6_FORCE_MLD_VERSION, "force_mld_version" },
 { CTL_INT, NET_IPV6_ACCEPT_RA_DEFRTR, "accept_ra_defrtr" },
 { CTL_INT, NET_IPV6_ACCEPT_RA_PINFO, "accept_ra_pinfo" },
 { CTL_INT, NET_IPV6_ACCEPT_RA_RTR_PREF, "accept_ra_rtr_pref" },
 { CTL_INT, NET_IPV6_RTR_PROBE_INTERVAL, "router_probe_interval" },
 { CTL_INT, NET_IPV6_ACCEPT_RA_RT_INFO_MAX_PLEN, "accept_ra_rt_info_max_plen" },
 { CTL_INT, NET_IPV6_PROXY_NDP, "proxy_ndp" },
 { CTL_INT, NET_IPV6_ACCEPT_SOURCE_ROUTE, "accept_source_route" },
 { CTL_INT, NET_IPV6_ACCEPT_RA_FROM_LOCAL, "accept_ra_from_local" },
 {}
};

static const struct bin_table bin_net_ipv6_conf_table[] = {
 { CTL_DIR, NET_PROTO_CONF_ALL, "all", bin_net_ipv6_conf_var_table },
 { CTL_DIR, NET_PROTO_CONF_DEFAULT, "default", bin_net_ipv6_conf_var_table },
 { CTL_DIR, 0, NULL, bin_net_ipv6_conf_var_table },
 {}
};

static const struct bin_table bin_net_ipv6_route_table[] = {

 { CTL_INT, NET_IPV6_ROUTE_GC_THRESH, "gc_thresh" },
 { CTL_INT, NET_IPV6_ROUTE_MAX_SIZE, "max_size" },
 { CTL_INT, NET_IPV6_ROUTE_GC_MIN_INTERVAL, "gc_min_interval" },
 { CTL_INT, NET_IPV6_ROUTE_GC_TIMEOUT, "gc_timeout" },
 { CTL_INT, NET_IPV6_ROUTE_GC_INTERVAL, "gc_interval" },
 { CTL_INT, NET_IPV6_ROUTE_GC_ELASTICITY, "gc_elasticity" },
 { CTL_INT, NET_IPV6_ROUTE_MTU_EXPIRES, "mtu_expires" },
 { CTL_INT, NET_IPV6_ROUTE_MIN_ADVMSS, "min_adv_mss" },
 { CTL_INT, NET_IPV6_ROUTE_GC_MIN_INTERVAL_MS, "gc_min_interval_ms" },
 {}
};

static const struct bin_table bin_net_ipv6_icmp_table[] = {
 { CTL_INT, NET_IPV6_ICMP_RATELIMIT, "ratelimit" },
 {}
};

static const struct bin_table bin_net_ipv6_table[] = {
 { CTL_DIR, NET_IPV6_CONF, "conf", bin_net_ipv6_conf_table },
 { CTL_DIR, NET_IPV6_NEIGH, "neigh", bin_net_neigh_table },
 { CTL_DIR, NET_IPV6_ROUTE, "route", bin_net_ipv6_route_table },
 { CTL_DIR, NET_IPV6_ICMP, "icmp", bin_net_ipv6_icmp_table },
 { CTL_INT, NET_IPV6_BINDV6ONLY, "bindv6only" },
 { CTL_INT, NET_IPV6_IP6FRAG_HIGH_THRESH, "ip6frag_high_thresh" },
 { CTL_INT, NET_IPV6_IP6FRAG_LOW_THRESH, "ip6frag_low_thresh" },
 { CTL_INT, NET_IPV6_IP6FRAG_TIME, "ip6frag_time" },
 { CTL_INT, NET_IPV6_IP6FRAG_SECRET_INTERVAL, "ip6frag_secret_interval" },
 { CTL_INT, NET_IPV6_MLD_MAX_MSF, "mld_max_msf" },
 { CTL_INT, 2088 , "ip6_queue_maxlen" },
 {}
};

static const struct bin_table bin_net_x25_table[] = {
 { CTL_INT, NET_X25_RESTART_REQUEST_TIMEOUT, "restart_request_timeout" },
 { CTL_INT, NET_X25_CALL_REQUEST_TIMEOUT, "call_request_timeout" },
 { CTL_INT, NET_X25_RESET_REQUEST_TIMEOUT, "reset_request_timeout" },
 { CTL_INT, NET_X25_CLEAR_REQUEST_TIMEOUT, "clear_request_timeout" },
 { CTL_INT, NET_X25_ACK_HOLD_BACK_TIMEOUT, "acknowledgement_hold_back_timeout" },
 { CTL_INT, NET_X25_FORWARD, "x25_forward" },
 {}
};

static const struct bin_table bin_net_tr_table[] = {
 { CTL_INT, NET_TR_RIF_TIMEOUT, "rif_timeout" },
 {}
};


static const struct bin_table bin_net_decnet_conf_vars[] = {
 { CTL_INT, NET_DECNET_CONF_DEV_FORWARDING, "forwarding" },
 { CTL_INT, NET_DECNET_CONF_DEV_PRIORITY, "priority" },
 { CTL_INT, NET_DECNET_CONF_DEV_T2, "t2" },
 { CTL_INT, NET_DECNET_CONF_DEV_T3, "t3" },
 {}
};

static const struct bin_table bin_net_decnet_conf[] = {
 { CTL_DIR, NET_DECNET_CONF_ETHER, "ethernet", bin_net_decnet_conf_vars },
 { CTL_DIR, NET_DECNET_CONF_GRE, "ipgre", bin_net_decnet_conf_vars },
 { CTL_DIR, NET_DECNET_CONF_X25, "x25", bin_net_decnet_conf_vars },
 { CTL_DIR, NET_DECNET_CONF_PPP, "ppp", bin_net_decnet_conf_vars },
 { CTL_DIR, NET_DECNET_CONF_DDCMP, "ddcmp", bin_net_decnet_conf_vars },
 { CTL_DIR, NET_DECNET_CONF_LOOPBACK, "loopback", bin_net_decnet_conf_vars },
 { CTL_DIR, 0, NULL, bin_net_decnet_conf_vars },
 {}
};

static const struct bin_table bin_net_decnet_table[] = {
 { CTL_DIR, NET_DECNET_CONF, "conf", bin_net_decnet_conf },
 { CTL_DNADR, NET_DECNET_NODE_ADDRESS, "node_address" },
 { CTL_STR, NET_DECNET_NODE_NAME, "node_name" },
 { CTL_STR, NET_DECNET_DEFAULT_DEVICE, "default_device" },
 { CTL_INT, NET_DECNET_TIME_WAIT, "time_wait" },
 { CTL_INT, NET_DECNET_DN_COUNT, "dn_count" },
 { CTL_INT, NET_DECNET_DI_COUNT, "di_count" },
 { CTL_INT, NET_DECNET_DR_COUNT, "dr_count" },
 { CTL_INT, NET_DECNET_DST_GC_INTERVAL, "dst_gc_interval" },
 { CTL_INT, NET_DECNET_NO_FC_MAX_CWND, "no_fc_max_cwnd" },
 { CTL_INT, NET_DECNET_MEM, "decnet_mem" },
 { CTL_INT, NET_DECNET_RMEM, "decnet_rmem" },
 { CTL_INT, NET_DECNET_WMEM, "decnet_wmem" },
 { CTL_INT, NET_DECNET_DEBUG_LEVEL, "debug" },
 {}
};

static const struct bin_table bin_net_sctp_table[] = {
 { CTL_INT, NET_SCTP_RTO_INITIAL, "rto_initial" },
 { CTL_INT, NET_SCTP_RTO_MIN, "rto_min" },
 { CTL_INT, NET_SCTP_RTO_MAX, "rto_max" },
 { CTL_INT, NET_SCTP_RTO_ALPHA, "rto_alpha_exp_divisor" },
 { CTL_INT, NET_SCTP_RTO_BETA, "rto_beta_exp_divisor" },
 { CTL_INT, NET_SCTP_VALID_COOKIE_LIFE, "valid_cookie_life" },
 { CTL_INT, NET_SCTP_ASSOCIATION_MAX_RETRANS, "association_max_retrans" },
 { CTL_INT, NET_SCTP_PATH_MAX_RETRANS, "path_max_retrans" },
 { CTL_INT, NET_SCTP_MAX_INIT_RETRANSMITS, "max_init_retransmits" },
 { CTL_INT, NET_SCTP_HB_INTERVAL, "hb_interval" },
 { CTL_INT, NET_SCTP_PRESERVE_ENABLE, "cookie_preserve_enable" },
 { CTL_INT, NET_SCTP_MAX_BURST, "max_burst" },
 { CTL_INT, NET_SCTP_ADDIP_ENABLE, "addip_enable" },
 { CTL_INT, NET_SCTP_PRSCTP_ENABLE, "prsctp_enable" },
 { CTL_INT, NET_SCTP_SNDBUF_POLICY, "sndbuf_policy" },
 { CTL_INT, NET_SCTP_SACK_TIMEOUT, "sack_timeout" },
 { CTL_INT, NET_SCTP_RCVBUF_POLICY, "rcvbuf_policy" },
 {}
};

static const struct bin_table bin_net_llc_llc2_timeout_table[] = {
 { CTL_INT, NET_LLC2_ACK_TIMEOUT, "ack" },
 { CTL_INT, NET_LLC2_P_TIMEOUT, "p" },
 { CTL_INT, NET_LLC2_REJ_TIMEOUT, "rej" },
 { CTL_INT, NET_LLC2_BUSY_TIMEOUT, "busy" },
 {}
};

static const struct bin_table bin_net_llc_station_table[] = {
 { CTL_INT, NET_LLC_STATION_ACK_TIMEOUT, "ack_timeout" },
 {}
};

static const struct bin_table bin_net_llc_llc2_table[] = {
 { CTL_DIR, NET_LLC2, "timeout", bin_net_llc_llc2_timeout_table },
 {}
};

static const struct bin_table bin_net_llc_table[] = {
 { CTL_DIR, NET_LLC2, "llc2", bin_net_llc_llc2_table },
 { CTL_DIR, NET_LLC_STATION, "station", bin_net_llc_station_table },
 {}
};

static const struct bin_table bin_net_netfilter_table[] = {
 { CTL_INT, NET_NF_CONNTRACK_MAX, "nf_conntrack_max" },
 { CTL_INT, NET_NF_CONNTRACK_BUCKETS, "nf_conntrack_buckets" },
 { CTL_INT, NET_NF_CONNTRACK_LOG_INVALID, "nf_conntrack_log_invalid" },

 { CTL_INT, NET_NF_CONNTRACK_TCP_LOOSE, "nf_conntrack_tcp_loose" },
 { CTL_INT, NET_NF_CONNTRACK_TCP_BE_LIBERAL, "nf_conntrack_tcp_be_liberal" },
 { CTL_INT, NET_NF_CONNTRACK_TCP_MAX_RETRANS, "nf_conntrack_tcp_max_retrans" },







 { CTL_INT, NET_NF_CONNTRACK_COUNT, "nf_conntrack_count" },


 { CTL_INT, NET_NF_CONNTRACK_FRAG6_LOW_THRESH, "nf_conntrack_frag6_low_thresh" },
 { CTL_INT, NET_NF_CONNTRACK_FRAG6_HIGH_THRESH, "nf_conntrack_frag6_high_thresh" },
 { CTL_INT, NET_NF_CONNTRACK_CHECKSUM, "nf_conntrack_checksum" },

 {}
};

static const struct bin_table bin_net_irda_table[] = {
 { CTL_INT, NET_IRDA_DISCOVERY, "discovery" },
 { CTL_STR, NET_IRDA_DEVNAME, "devname" },
 { CTL_INT, NET_IRDA_DEBUG, "debug" },
 { CTL_INT, NET_IRDA_FAST_POLL, "fast_poll_increase" },
 { CTL_INT, NET_IRDA_DISCOVERY_SLOTS, "discovery_slots" },
 { CTL_INT, NET_IRDA_DISCOVERY_TIMEOUT, "discovery_timeout" },
 { CTL_INT, NET_IRDA_SLOT_TIMEOUT, "slot_timeout" },
 { CTL_INT, NET_IRDA_MAX_BAUD_RATE, "max_baud_rate" },
 { CTL_INT, NET_IRDA_MIN_TX_TURN_TIME, "min_tx_turn_time" },
 { CTL_INT, NET_IRDA_MAX_TX_DATA_SIZE, "max_tx_data_size" },
 { CTL_INT, NET_IRDA_MAX_TX_WINDOW, "max_tx_window" },
 { CTL_INT, NET_IRDA_MAX_NOREPLY_TIME, "max_noreply_time" },
 { CTL_INT, NET_IRDA_WARN_NOREPLY_TIME, "warn_noreply_time" },
 { CTL_INT, NET_IRDA_LAP_KEEPALIVE_TIME, "lap_keepalive_time" },
 {}
};

static const struct bin_table bin_net_table[] = {
 { CTL_DIR, NET_CORE, "core", bin_net_core_table },


 { CTL_DIR, NET_UNIX, "unix", bin_net_unix_table },
 { CTL_DIR, NET_IPV4, "ipv4", bin_net_ipv4_table },
 { CTL_DIR, NET_IPX, "ipx", bin_net_ipx_table },
 { CTL_DIR, NET_ATALK, "appletalk", bin_net_atalk_table },
 { CTL_DIR, NET_NETROM, "netrom", bin_net_netrom_table },
 { CTL_DIR, NET_AX25, "ax25", bin_net_ax25_table },

 { CTL_DIR, NET_ROSE, "rose", bin_net_rose_table },
 { CTL_DIR, NET_IPV6, "ipv6", bin_net_ipv6_table },
 { CTL_DIR, NET_X25, "x25", bin_net_x25_table },
 { CTL_DIR, NET_TR, "token-ring", bin_net_tr_table },
 { CTL_DIR, NET_DECNET, "decnet", bin_net_decnet_table },

 { CTL_DIR, NET_SCTP, "sctp", bin_net_sctp_table },
 { CTL_DIR, NET_LLC, "llc", bin_net_llc_table },
 { CTL_DIR, NET_NETFILTER, "netfilter", bin_net_netfilter_table },

 { CTL_DIR, NET_IRDA, "irda", bin_net_irda_table },
 { CTL_INT, 2089, "nf_conntrack_max" },
 {}
};

static const struct bin_table bin_fs_quota_table[] = {
 { CTL_INT, FS_DQ_LOOKUPS, "lookups" },
 { CTL_INT, FS_DQ_DROPS, "drops" },
 { CTL_INT, FS_DQ_READS, "reads" },
 { CTL_INT, FS_DQ_WRITES, "writes" },
 { CTL_INT, FS_DQ_CACHE_HITS, "cache_hits" },
 { CTL_INT, FS_DQ_ALLOCATED, "allocated_dquots" },
 { CTL_INT, FS_DQ_FREE, "free_dquots" },
 { CTL_INT, FS_DQ_SYNCS, "syncs" },
 { CTL_INT, FS_DQ_WARNINGS, "warnings" },
 {}
};

static const struct bin_table bin_fs_xfs_table[] = {
 { CTL_INT, XFS_SGID_INHERIT, "irix_sgid_inherit" },
 { CTL_INT, XFS_SYMLINK_MODE, "irix_symlink_mode" },
 { CTL_INT, XFS_PANIC_MASK, "panic_mask" },

 { CTL_INT, XFS_ERRLEVEL, "error_level" },
 { CTL_INT, XFS_SYNCD_TIMER, "xfssyncd_centisecs" },
 { CTL_INT, XFS_INHERIT_SYNC, "inherit_sync" },
 { CTL_INT, XFS_INHERIT_NODUMP, "inherit_nodump" },
 { CTL_INT, XFS_INHERIT_NOATIME, "inherit_noatime" },
 { CTL_INT, XFS_BUF_TIMER, "xfsbufd_centisecs" },
 { CTL_INT, XFS_BUF_AGE, "age_buffer_centisecs" },
 { CTL_INT, XFS_INHERIT_NOSYM, "inherit_nosymlinks" },
 { CTL_INT, XFS_ROTORSTEP, "rotorstep" },
 { CTL_INT, XFS_INHERIT_NODFRG, "inherit_nodefrag" },
 { CTL_INT, XFS_FILESTREAM_TIMER, "filestream_centisecs" },
 { CTL_INT, XFS_STATS_CLEAR, "stats_clear" },
 {}
};

static const struct bin_table bin_fs_ocfs2_nm_table[] = {
 { CTL_STR, 1, "hb_ctl_path" },
 {}
};

static const struct bin_table bin_fs_ocfs2_table[] = {
 { CTL_DIR, 1, "nm", bin_fs_ocfs2_nm_table },
 {}
};

static const struct bin_table bin_inotify_table[] = {
 { CTL_INT, INOTIFY_MAX_USER_INSTANCES, "max_user_instances" },
 { CTL_INT, INOTIFY_MAX_USER_WATCHES, "max_user_watches" },
 { CTL_INT, INOTIFY_MAX_QUEUED_EVENTS, "max_queued_events" },
 {}
};

static const struct bin_table bin_fs_table[] = {
 { CTL_INT, FS_NRINODE, "inode-nr" },
 { CTL_INT, FS_STATINODE, "inode-state" },




 { CTL_INT, FS_MAXFILE, "file-max" },
 { CTL_INT, FS_DENTRY, "dentry-state" },


 { CTL_INT, FS_OVERFLOWUID, "overflowuid" },
 { CTL_INT, FS_OVERFLOWGID, "overflowgid" },
 { CTL_INT, FS_LEASES, "leases-enable" },
 { CTL_INT, FS_DIR_NOTIFY, "dir-notify-enable" },
 { CTL_INT, FS_LEASE_TIME, "lease-break-time" },
 { CTL_DIR, FS_DQSTATS, "quota", bin_fs_quota_table },
 { CTL_DIR, FS_XFS, "xfs", bin_fs_xfs_table },
 { CTL_ULONG, FS_AIO_NR, "aio-nr" },
 { CTL_ULONG, FS_AIO_MAX_NR, "aio-max-nr" },
 { CTL_DIR, FS_INOTIFY, "inotify", bin_inotify_table },
 { CTL_DIR, FS_OCFS2, "ocfs2", bin_fs_ocfs2_table },
 { CTL_INT, KERN_SETUID_DUMPABLE, "suid_dumpable" },
 {}
};

static const struct bin_table bin_ipmi_table[] = {
 { CTL_INT, DEV_IPMI_POWEROFF_POWERCYCLE, "poweroff_powercycle" },
 {}
};

static const struct bin_table bin_mac_hid_files[] = {


 { CTL_INT, DEV_MAC_HID_MOUSE_BUTTON_EMULATION, "mouse_button_emulation" },
 { CTL_INT, DEV_MAC_HID_MOUSE_BUTTON2_KEYCODE, "mouse_button2_keycode" },
 { CTL_INT, DEV_MAC_HID_MOUSE_BUTTON3_KEYCODE, "mouse_button3_keycode" },

 {}
};

static const struct bin_table bin_raid_table[] = {
 { CTL_INT, DEV_RAID_SPEED_LIMIT_MIN, "speed_limit_min" },
 { CTL_INT, DEV_RAID_SPEED_LIMIT_MAX, "speed_limit_max" },
 {}
};

static const struct bin_table bin_scsi_table[] = {
 { CTL_INT, DEV_SCSI_LOGGING_LEVEL, "logging_level" },
 {}
};

static const struct bin_table bin_dev_table[] = {



 { CTL_DIR, DEV_RAID, "raid", bin_raid_table },
 { CTL_DIR, DEV_MAC_HID, "mac_hid", bin_mac_hid_files },
 { CTL_DIR, DEV_SCSI, "scsi", bin_scsi_table },
 { CTL_DIR, DEV_IPMI, "ipmi", bin_ipmi_table },
 {}
};

static const struct bin_table bin_bus_isa_table[] = {
 { CTL_INT, BUS_ISA_MEM_BASE, "membase" },
 { CTL_INT, BUS_ISA_PORT_BASE, "portbase" },
 { CTL_INT, BUS_ISA_PORT_SHIFT, "portshift" },
 {}
};

static const struct bin_table bin_bus_table[] = {
 { CTL_DIR, CTL_BUS_ISA, "isa", bin_bus_isa_table },
 {}
};


static const struct bin_table bin_s390dbf_table[] = {
 { CTL_INT, 5678 , "debug_stoppable" },
 { CTL_INT, 5679 , "debug_active" },
 {}
};

static const struct bin_table bin_sunrpc_table[] = {





 { CTL_INT, CTL_SLOTTABLE_UDP, "udp_slot_table_entries" },
 { CTL_INT, CTL_SLOTTABLE_TCP, "tcp_slot_table_entries" },
 { CTL_INT, CTL_MIN_RESVPORT, "min_resvport" },
 { CTL_INT, CTL_MAX_RESVPORT, "max_resvport" },
 {}
};

static const struct bin_table bin_pm_table[] = {


 { CTL_INT, 2 , "cmode" },
 { CTL_INT, 3 , "p0" },
 { CTL_INT, 4 , "cm" },
 {}
};

static const struct bin_table bin_root_table[] = {
 { CTL_DIR, CTL_KERN, "kernel", bin_kern_table },
 { CTL_DIR, CTL_VM, "vm", bin_vm_table },
 { CTL_DIR, CTL_NET, "net", bin_net_table },

 { CTL_DIR, CTL_FS, "fs", bin_fs_table },

 { CTL_DIR, CTL_DEV, "dev", bin_dev_table },
 { CTL_DIR, CTL_BUS, "bus", bin_bus_table },
 { CTL_DIR, CTL_ABI, "abi" },


 { CTL_DIR, CTL_S390DBF, "s390dbf", bin_s390dbf_table },
 { CTL_DIR, CTL_SUNRPC, "sunrpc", bin_sunrpc_table },
 { CTL_DIR, CTL_PM, "pm", bin_pm_table },
 {}
};

static ssize_t bin_dir(struct file *file,
 void __user *oldval, size_t oldlen, void __user *newval, size_t newlen)
{
 return -ENOTDIR;
}


static ssize_t bin_string(struct file *file,
 void __user *oldval, size_t oldlen, void __user *newval, size_t newlen)
{
 ssize_t result, copied = 0;

 if (oldval && oldlen) {
  char __user *lastp;
  loff_t pos = 0;
  int ch;

  result = vfs_read(file, oldval, oldlen, &pos);
  if (result < 0)
   goto out;

  copied = result;
  lastp = oldval + copied - 1;

  result = -EFAULT;
  if (get_user(ch, lastp))
   goto out;


  if (ch == '\n') {
   result = -EFAULT;
   if (put_user('\0', lastp))
    goto out;
   copied -= 1;
  }
 }

 if (newval && newlen) {
  loff_t pos = 0;

  result = vfs_write(file, newval, newlen, &pos);
  if (result < 0)
   goto out;
 }

 result = copied;
out:
 return result;
}

static ssize_t bin_intvec(struct file *file,
 void __user *oldval, size_t oldlen, void __user *newval, size_t newlen)
{
 ssize_t copied = 0;
 char *buffer;
 ssize_t result;

 result = -ENOMEM;
 buffer = kmalloc(BUFSZ, GFP_KERNEL);
 if (!buffer)
  goto out;

 if (oldval && oldlen) {
  unsigned __user *vec = oldval;
  size_t length = oldlen / sizeof(*vec);
  char *str, *end;
  int i;

  result = kernel_read(file, 0, buffer, BUFSZ - 1);
  if (result < 0)
   goto out_kfree;

  str = buffer;
  end = str + result;
  *end++ = '\0';
  for (i = 0; i < length; i++) {
   unsigned long value;

   value = simple_strtoul(str, &str, 10);
   while (isspace(*str))
    str++;

   result = -EFAULT;
   if (put_user(value, vec + i))
    goto out_kfree;

   copied += sizeof(*vec);
   if (!isdigit(*str))
    break;
  }
 }

 if (newval && newlen) {
  unsigned __user *vec = newval;
  size_t length = newlen / sizeof(*vec);
  char *str, *end;
  int i;

  str = buffer;
  end = str + BUFSZ;
  for (i = 0; i < length; i++) {
   unsigned long value;

   result = -EFAULT;
   if (get_user(value, vec + i))
    goto out_kfree;

   str += scnprintf(str, end - str, "%lu\t", value);
  }

  result = kernel_write(file, buffer, str - buffer, 0);
  if (result < 0)
   goto out_kfree;
 }
 result = copied;
out_kfree:
 kfree(buffer);
out:
 return result;
}

static ssize_t bin_ulongvec(struct file *file,
 void __user *oldval, size_t oldlen, void __user *newval, size_t newlen)
{
 ssize_t copied = 0;
 char *buffer;
 ssize_t result;

 result = -ENOMEM;
 buffer = kmalloc(BUFSZ, GFP_KERNEL);
 if (!buffer)
  goto out;

 if (oldval && oldlen) {
  unsigned long __user *vec = oldval;
  size_t length = oldlen / sizeof(*vec);
  char *str, *end;
  int i;

  result = kernel_read(file, 0, buffer, BUFSZ - 1);
  if (result < 0)
   goto out_kfree;

  str = buffer;
  end = str + result;
  *end++ = '\0';
  for (i = 0; i < length; i++) {
   unsigned long value;

   value = simple_strtoul(str, &str, 10);
   while (isspace(*str))
    str++;

   result = -EFAULT;
   if (put_user(value, vec + i))
    goto out_kfree;

   copied += sizeof(*vec);
   if (!isdigit(*str))
    break;
  }
 }

 if (newval && newlen) {
  unsigned long __user *vec = newval;
  size_t length = newlen / sizeof(*vec);
  char *str, *end;
  int i;

  str = buffer;
  end = str + BUFSZ;
  for (i = 0; i < length; i++) {
   unsigned long value;

   result = -EFAULT;
   if (get_user(value, vec + i))
    goto out_kfree;

   str += scnprintf(str, end - str, "%lu\t", value);
  }

  result = kernel_write(file, buffer, str - buffer, 0);
  if (result < 0)
   goto out_kfree;
 }
 result = copied;
out_kfree:
 kfree(buffer);
out:
 return result;
}

static ssize_t bin_uuid(struct file *file,
 void __user *oldval, size_t oldlen, void __user *newval, size_t newlen)
{
 ssize_t result, copied = 0;


 if (oldval && oldlen) {
  char buf[UUID_STRING_LEN + 1];
  uuid_be uuid;

  result = kernel_read(file, 0, buf, sizeof(buf) - 1);
  if (result < 0)
   goto out;

  buf[result] = '\0';

  result = -EIO;
  if (uuid_be_to_bin(buf, &uuid))
   goto out;

  if (oldlen > 16)
   oldlen = 16;

  result = -EFAULT;
  if (copy_to_user(oldval, &uuid, oldlen))
   goto out;

  copied = oldlen;
 }
 result = copied;
out:
 return result;
}

static ssize_t bin_dn_node_address(struct file *file,
 void __user *oldval, size_t oldlen, void __user *newval, size_t newlen)
{
 ssize_t result, copied = 0;

 if (oldval && oldlen) {
  char buf[15], *nodep;
  unsigned long area, node;
  __le16 dnaddr;

  result = kernel_read(file, 0, buf, sizeof(buf) - 1);
  if (result < 0)
   goto out;

  buf[result] = '\0';


  result = -EIO;
  nodep = strchr(buf, '.');
  if (!nodep)
   goto out;
  ++nodep;

  area = simple_strtoul(buf, NULL, 10);
  node = simple_strtoul(nodep, NULL, 10);

  result = -EIO;
  if ((area > 63)||(node > 1023))
   goto out;

  dnaddr = cpu_to_le16((area << 10) | node);

  result = -EFAULT;
  if (put_user(dnaddr, (__le16 __user *)oldval))
   goto out;

  copied = sizeof(dnaddr);
 }

 if (newval && newlen) {
  __le16 dnaddr;
  char buf[15];
  int len;

  result = -EINVAL;
  if (newlen != sizeof(dnaddr))
   goto out;

  result = -EFAULT;
  if (get_user(dnaddr, (__le16 __user *)newval))
   goto out;

  len = scnprintf(buf, sizeof(buf), "%hu.%hu",
    le16_to_cpu(dnaddr) >> 10,
    le16_to_cpu(dnaddr) & 0x3ff);

  result = kernel_write(file, buf, len, 0);
  if (result < 0)
   goto out;
 }

 result = copied;
out:
 return result;
}

static const struct bin_table *get_sysctl(const int *name, int nlen, char *path)
{
 const struct bin_table *table = &bin_root_table[0];
 int ctl_name;





 memcpy(path, "sys/", 4);
 path += 4;

repeat:
 if (!nlen)
  return ERR_PTR(-ENOTDIR);
 ctl_name = *name;
 name++;
 nlen--;
 for ( ; table->convert; table++) {
  int len = 0;





  if (!table->ctl_name) {
   struct net *net = current->nsproxy->net_ns;
   struct net_device *dev;
   dev = dev_get_by_index(net, ctl_name);
   if (dev) {
    len = strlen(dev->name);
    memcpy(path, dev->name, len);
    dev_put(dev);
   }

  } else if (ctl_name == table->ctl_name) {
   len = strlen(table->procname);
   memcpy(path, table->procname, len);
  }
  if (len) {
   path += len;
   if (table->child) {
    *path++ = '/';
    table = table->child;
    goto repeat;
   }
   *path = '\0';
   return table;
  }
 }
 return ERR_PTR(-ENOTDIR);
}

static char *sysctl_getname(const int *name, int nlen, const struct bin_table **tablep)
{
 char *tmp, *result;

 result = ERR_PTR(-ENOMEM);
 tmp = __getname();
 if (tmp) {
  const struct bin_table *table = get_sysctl(name, nlen, tmp);
  result = tmp;
  *tablep = table;
  if (IS_ERR(table)) {
   __putname(tmp);
   result = ERR_CAST(table);
  }
 }
 return result;
}

static ssize_t binary_sysctl(const int *name, int nlen,
 void __user *oldval, size_t oldlen, void __user *newval, size_t newlen)
{
 const struct bin_table *table = NULL;
 struct vfsmount *mnt;
 struct file *file;
 ssize_t result;
 char *pathname;
 int flags;

 pathname = sysctl_getname(name, nlen, &table);
 result = PTR_ERR(pathname);
 if (IS_ERR(pathname))
  goto out;


 if (oldval && oldlen && newval && newlen) {
  flags = O_RDWR;
 } else if (newval && newlen) {
  flags = O_WRONLY;
 } else if (oldval && oldlen) {
  flags = O_RDONLY;
 } else {
  result = 0;
  goto out_putname;
 }

 mnt = task_active_pid_ns(current)->proc_mnt;
 file = file_open_root(mnt->mnt_root, mnt, pathname, flags, 0);
 result = PTR_ERR(file);
 if (IS_ERR(file))
  goto out_putname;

 result = table->convert(file, oldval, oldlen, newval, newlen);

 fput(file);
out_putname:
 __putname(pathname);
out:
 return result;
}



static ssize_t binary_sysctl(const int *name, int nlen,
 void __user *oldval, size_t oldlen, void __user *newval, size_t newlen)
{
 return -ENOSYS;
}



static void deprecated_sysctl_warning(const int *name, int nlen)
{
 int i;





 if (name[0] == CTL_KERN && name[1] == KERN_VERSION)
  return;

 if (printk_ratelimit()) {
  printk(KERN_INFO
   "warning: process `%s' used the deprecated sysctl "
   "system call with ", current->comm);
  for (i = 0; i < nlen; i++)
   printk("%d.", name[i]);
  printk("\n");
 }
 return;
}


static DECLARE_BITMAP(warn_once_bitmap, WARN_ONCE_HASH_SIZE);

static void warn_on_bintable(const int *name, int nlen)
{
 int i;
 u32 hash = FNV32_OFFSET;

 for (i = 0; i < nlen; i++)
  hash = (hash ^ name[i]) * FNV32_PRIME;
 hash %= WARN_ONCE_HASH_SIZE;
 if (__test_and_set_bit(hash, warn_once_bitmap))
  return;
 deprecated_sysctl_warning(name, nlen);
}

static ssize_t do_sysctl(int __user *args_name, int nlen,
 void __user *oldval, size_t oldlen, void __user *newval, size_t newlen)
{
 int name[CTL_MAXNAME];
 int i;


 if (nlen < 0 || nlen > CTL_MAXNAME)
  return -ENOTDIR;

 for (i = 0; i < nlen; i++)
  if (get_user(name[i], args_name + i))
   return -EFAULT;

 warn_on_bintable(name, nlen);

 return binary_sysctl(name, nlen, oldval, oldlen, newval, newlen);
}

SYSCALL_DEFINE1(sysctl, struct __sysctl_args __user *, args)
{
 struct __sysctl_args tmp;
 size_t oldlen = 0;
 ssize_t result;

 if (copy_from_user(&tmp, args, sizeof(tmp)))
  return -EFAULT;

 if (tmp.oldval && !tmp.oldlenp)
  return -EFAULT;

 if (tmp.oldlenp && get_user(oldlen, tmp.oldlenp))
  return -EFAULT;

 result = do_sysctl(tmp.name, tmp.nlen, tmp.oldval, oldlen,
      tmp.newval, tmp.newlen);

 if (result >= 0) {
  oldlen = result;
  result = 0;
 }

 if (tmp.oldlenp && put_user(oldlen, tmp.oldlenp))
  return -EFAULT;

 return result;
}



struct compat_sysctl_args {
 compat_uptr_t name;
 int nlen;
 compat_uptr_t oldval;
 compat_uptr_t oldlenp;
 compat_uptr_t newval;
 compat_size_t newlen;
 compat_ulong_t __unused[4];
};

COMPAT_SYSCALL_DEFINE1(sysctl, struct compat_sysctl_args __user *, args)
{
 struct compat_sysctl_args tmp;
 compat_size_t __user *compat_oldlenp;
 size_t oldlen = 0;
 ssize_t result;

 if (copy_from_user(&tmp, args, sizeof(tmp)))
  return -EFAULT;

 if (tmp.oldval && !tmp.oldlenp)
  return -EFAULT;

 compat_oldlenp = compat_ptr(tmp.oldlenp);
 if (compat_oldlenp && get_user(oldlen, compat_oldlenp))
  return -EFAULT;

 result = do_sysctl(compat_ptr(tmp.name), tmp.nlen,
      compat_ptr(tmp.oldval), oldlen,
      compat_ptr(tmp.newval), tmp.newlen);

 if (result >= 0) {
  oldlen = result;
  result = 0;
 }

 if (compat_oldlenp && put_user(oldlen, compat_oldlenp))
  return -EFAULT;

 return result;
}







extern int suid_dumpable;
extern int core_uses_pid;
extern char core_pattern[];
extern unsigned int core_pipe_limit;
extern int pid_max;
extern int pid_max_min, pid_max_max;
extern int percpu_pagelist_fraction;
extern int compat_log;
extern int latencytop_enabled;
extern int sysctl_nr_open_min, sysctl_nr_open_max;
extern int sysctl_nr_trim_pages;


static int sixty = 60;

static int __maybe_unused neg_one = -1;

static int zero;
static int __maybe_unused one = 1;
static int __maybe_unused two = 2;
static int __maybe_unused four = 4;
static unsigned long one_ul = 1;
static int one_hundred = 100;
static int one_thousand = 1000;
static int ten_thousand = 10000;
static int six_hundred_forty_kb = 640 * 1024;


static unsigned long dirty_bytes_min = 2 * PAGE_SIZE;


static int maxolduid = 65535;
static int minolduid;

static int ngroups_max = NGROUPS_MAX;
static const int cap_last_cap = CAP_LAST_CAP;


static unsigned long hung_task_timeout_max = (LONG_MAX/HZ);


extern int pwrsw_enabled;

extern int unaligned_enabled;

extern int unaligned_dump_stack;

extern int no_unaligned_warning;



static int sysctl_writes_strict = SYSCTL_WRITES_STRICT;

static int proc_do_cad_pid(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos);
static int proc_taint(struct ctl_table *table, int write,
          void __user *buffer, size_t *lenp, loff_t *ppos);

static int proc_dointvec_minmax_sysadmin(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos);

static int proc_dointvec_minmax_coredump(struct ctl_table *table, int write,
  void __user *buffer, size_t *lenp, loff_t *ppos);
static int proc_dostring_coredump(struct ctl_table *table, int write,
  void __user *buffer, size_t *lenp, loff_t *ppos);


static int __sysrq_enabled = CONFIG_MAGIC_SYSRQ_DEFAULT_ENABLE;

static int sysrq_sysctl_handler(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp,
    loff_t *ppos)
{
 int error;

 error = proc_dointvec(table, write, buffer, lenp, ppos);
 if (error)
  return error;

 if (write)
  sysrq_toggle_support(__sysrq_enabled);

 return 0;
}


static struct ctl_table kern_table[];
static struct ctl_table vm_table[];
static struct ctl_table fs_table[];
static struct ctl_table debug_table[];
static struct ctl_table dev_table[];
extern struct ctl_table random_table[];
extern struct ctl_table epoll_table[];

int sysctl_legacy_va_layout;



static struct ctl_table sysctl_base_table[] = {
 {
  .procname = "kernel",
  .mode = 0555,
  .child = kern_table,
 },
 {
  .procname = "vm",
  .mode = 0555,
  .child = vm_table,
 },
 {
  .procname = "fs",
  .mode = 0555,
  .child = fs_table,
 },
 {
  .procname = "debug",
  .mode = 0555,
  .child = debug_table,
 },
 {
  .procname = "dev",
  .mode = 0555,
  .child = dev_table,
 },
 { }
};

static int min_sched_granularity_ns = 100000;
static int max_sched_granularity_ns = NSEC_PER_SEC;
static int min_wakeup_granularity_ns;
static int max_wakeup_granularity_ns = NSEC_PER_SEC;
static int min_sched_tunable_scaling = SCHED_TUNABLESCALING_NONE;
static int max_sched_tunable_scaling = SCHED_TUNABLESCALING_END-1;

static int min_extfrag_threshold;
static int max_extfrag_threshold = 1000;

static struct ctl_table kern_table[] = {
 {
  .procname = "sched_child_runs_first",
  .data = &sysctl_sched_child_runs_first,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "sched_min_granularity_ns",
  .data = &sysctl_sched_min_granularity,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = sched_proc_update_handler,
  .extra1 = &min_sched_granularity_ns,
  .extra2 = &max_sched_granularity_ns,
 },
 {
  .procname = "sched_latency_ns",
  .data = &sysctl_sched_latency,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = sched_proc_update_handler,
  .extra1 = &min_sched_granularity_ns,
  .extra2 = &max_sched_granularity_ns,
 },
 {
  .procname = "sched_wakeup_granularity_ns",
  .data = &sysctl_sched_wakeup_granularity,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = sched_proc_update_handler,
  .extra1 = &min_wakeup_granularity_ns,
  .extra2 = &max_wakeup_granularity_ns,
 },
 {
  .procname = "sched_tunable_scaling",
  .data = &sysctl_sched_tunable_scaling,
  .maxlen = sizeof(enum sched_tunable_scaling),
  .mode = 0644,
  .proc_handler = sched_proc_update_handler,
  .extra1 = &min_sched_tunable_scaling,
  .extra2 = &max_sched_tunable_scaling,
 },
 {
  .procname = "sched_migration_cost_ns",
  .data = &sysctl_sched_migration_cost,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "sched_nr_migrate",
  .data = &sysctl_sched_nr_migrate,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "sched_time_avg_ms",
  .data = &sysctl_sched_time_avg,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "sched_shares_window_ns",
  .data = &sysctl_sched_shares_window,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "sched_schedstats",
  .data = NULL,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = sysctl_schedstats,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "numa_balancing_scan_delay_ms",
  .data = &sysctl_numa_balancing_scan_delay,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "numa_balancing_scan_period_min_ms",
  .data = &sysctl_numa_balancing_scan_period_min,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "numa_balancing_scan_period_max_ms",
  .data = &sysctl_numa_balancing_scan_period_max,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "numa_balancing_scan_size_mb",
  .data = &sysctl_numa_balancing_scan_size,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &one,
 },
 {
  .procname = "numa_balancing",
  .data = NULL,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = sysctl_numa_balancing,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "sched_rt_period_us",
  .data = &sysctl_sched_rt_period,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = sched_rt_handler,
 },
 {
  .procname = "sched_rt_runtime_us",
  .data = &sysctl_sched_rt_runtime,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = sched_rt_handler,
 },
 {
  .procname = "sched_rr_timeslice_ms",
  .data = &sched_rr_timeslice,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = sched_rr_handler,
 },
 {
  .procname = "sched_autogroup_enabled",
  .data = &sysctl_sched_autogroup_enabled,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "sched_cfs_bandwidth_slice_us",
  .data = &sysctl_sched_cfs_bandwidth_slice,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &one,
 },
 {
  .procname = "prove_locking",
  .data = &prove_locking,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "lock_stat",
  .data = &lock_stat,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "panic",
  .data = &panic_timeout,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "core_uses_pid",
  .data = &core_uses_pid,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "core_pattern",
  .data = core_pattern,
  .maxlen = CORENAME_MAX_SIZE,
  .mode = 0644,
  .proc_handler = proc_dostring_coredump,
 },
 {
  .procname = "core_pipe_limit",
  .data = &core_pipe_limit,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "tainted",
  .maxlen = sizeof(long),
  .mode = 0644,
  .proc_handler = proc_taint,
 },
 {
  .procname = "sysctl_writes_strict",
  .data = &sysctl_writes_strict,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &neg_one,
  .extra2 = &one,
 },
 {
  .procname = "latencytop",
  .data = &latencytop_enabled,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = sysctl_latencytop,
 },
 {
  .procname = "real-root-dev",
  .data = &real_root_dev,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "print-fatal-signals",
  .data = &print_fatal_signals,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "reboot-cmd",
  .data = reboot_command,
  .maxlen = 256,
  .mode = 0644,
  .proc_handler = proc_dostring,
 },
 {
  .procname = "stop-a",
  .data = &stop_a_enabled,
  .maxlen = sizeof (int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "scons-poweroff",
  .data = &scons_pwroff,
  .maxlen = sizeof (int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "tsb-ratio",
  .data = &sysctl_tsb_ratio,
  .maxlen = sizeof (int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "soft-power",
  .data = &pwrsw_enabled,
  .maxlen = sizeof (int),
   .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "unaligned-trap",
  .data = &unaligned_enabled,
  .maxlen = sizeof (int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "ctrl-alt-del",
  .data = &C_A_D,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "ftrace_enabled",
  .data = &ftrace_enabled,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = ftrace_enable_sysctl,
 },
 {
  .procname = "stack_tracer_enabled",
  .data = &stack_tracer_enabled,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = stack_trace_sysctl,
 },
 {
  .procname = "ftrace_dump_on_oops",
  .data = &ftrace_dump_on_oops,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "traceoff_on_warning",
  .data = &__disable_trace_on_warning,
  .maxlen = sizeof(__disable_trace_on_warning),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "tracepoint_printk",
  .data = &tracepoint_printk,
  .maxlen = sizeof(tracepoint_printk),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "kexec_load_disabled",
  .data = &kexec_load_disabled,
  .maxlen = sizeof(int),
  .mode = 0644,

  .proc_handler = proc_dointvec_minmax,
  .extra1 = &one,
  .extra2 = &one,
 },
 {
  .procname = "modprobe",
  .data = &modprobe_path,
  .maxlen = KMOD_PATH_LEN,
  .mode = 0644,
  .proc_handler = proc_dostring,
 },
 {
  .procname = "modules_disabled",
  .data = &modules_disabled,
  .maxlen = sizeof(int),
  .mode = 0644,

  .proc_handler = proc_dointvec_minmax,
  .extra1 = &one,
  .extra2 = &one,
 },
 {
  .procname = "hotplug",
  .data = &uevent_helper,
  .maxlen = UEVENT_HELPER_PATH_LEN,
  .mode = 0644,
  .proc_handler = proc_dostring,
 },
 {
  .procname = "sg-big-buff",
  .data = &sg_big_buff,
  .maxlen = sizeof (int),
  .mode = 0444,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "acct",
  .data = &acct_parm,
  .maxlen = 3*sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "sysrq",
  .data = &__sysrq_enabled,
  .maxlen = sizeof (int),
  .mode = 0644,
  .proc_handler = sysrq_sysctl_handler,
 },
 {
  .procname = "cad_pid",
  .data = NULL,
  .maxlen = sizeof (int),
  .mode = 0600,
  .proc_handler = proc_do_cad_pid,
 },
 {
  .procname = "threads-max",
  .data = NULL,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = sysctl_max_threads,
 },
 {
  .procname = "random",
  .mode = 0555,
  .child = random_table,
 },
 {
  .procname = "usermodehelper",
  .mode = 0555,
  .child = usermodehelper_table,
 },
 {
  .procname = "overflowuid",
  .data = &overflowuid,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &minolduid,
  .extra2 = &maxolduid,
 },
 {
  .procname = "overflowgid",
  .data = &overflowgid,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &minolduid,
  .extra2 = &maxolduid,
 },
 {
  .procname = "ieee_emulation_warnings",
  .data = &sysctl_ieee_emulation_warnings,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "userprocess_debug",
  .data = &show_unhandled_signals,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "pid_max",
  .data = &pid_max,
  .maxlen = sizeof (int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &pid_max_min,
  .extra2 = &pid_max_max,
 },
 {
  .procname = "panic_on_oops",
  .data = &panic_on_oops,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "printk",
  .data = &console_loglevel,
  .maxlen = 4*sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "printk_ratelimit",
  .data = &printk_ratelimit_state.interval,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_jiffies,
 },
 {
  .procname = "printk_ratelimit_burst",
  .data = &printk_ratelimit_state.burst,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "printk_delay",
  .data = &printk_delay_msec,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &ten_thousand,
 },
 {
  .procname = "dmesg_restrict",
  .data = &dmesg_restrict,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax_sysadmin,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "kptr_restrict",
  .data = &kptr_restrict,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax_sysadmin,
  .extra1 = &zero,
  .extra2 = &two,
 },
 {
  .procname = "ngroups_max",
  .data = &ngroups_max,
  .maxlen = sizeof (int),
  .mode = 0444,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "cap_last_cap",
  .data = (void *)&cap_last_cap,
  .maxlen = sizeof(int),
  .mode = 0444,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "watchdog",
  .data = &watchdog_user_enabled,
  .maxlen = sizeof (int),
  .mode = 0644,
  .proc_handler = proc_watchdog,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "watchdog_thresh",
  .data = &watchdog_thresh,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_watchdog_thresh,
  .extra1 = &zero,
  .extra2 = &sixty,
 },
 {
  .procname = "nmi_watchdog",
  .data = &nmi_watchdog_enabled,
  .maxlen = sizeof (int),
  .mode = 0644,
  .proc_handler = proc_nmi_watchdog,
  .extra1 = &zero,
  .extra2 = &one,
  .extra2 = &zero,
 },
 {
  .procname = "soft_watchdog",
  .data = &soft_watchdog_enabled,
  .maxlen = sizeof (int),
  .mode = 0644,
  .proc_handler = proc_soft_watchdog,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "watchdog_cpumask",
  .data = &watchdog_cpumask_bits,
  .maxlen = NR_CPUS,
  .mode = 0644,
  .proc_handler = proc_watchdog_cpumask,
 },
 {
  .procname = "softlockup_panic",
  .data = &softlockup_panic,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "hardlockup_panic",
  .data = &hardlockup_panic,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "softlockup_all_cpu_backtrace",
  .data = &sysctl_softlockup_all_cpu_backtrace,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "hardlockup_all_cpu_backtrace",
  .data = &sysctl_hardlockup_all_cpu_backtrace,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "unknown_nmi_panic",
  .data = &unknown_nmi_panic,
  .maxlen = sizeof (int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "panic_on_unrecovered_nmi",
  .data = &panic_on_unrecovered_nmi,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "panic_on_io_nmi",
  .data = &panic_on_io_nmi,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "panic_on_stackoverflow",
  .data = &sysctl_panic_on_stackoverflow,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "bootloader_type",
  .data = &bootloader_type,
  .maxlen = sizeof (int),
  .mode = 0444,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "bootloader_version",
  .data = &bootloader_version,
  .maxlen = sizeof (int),
  .mode = 0444,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "kstack_depth_to_print",
  .data = &kstack_depth_to_print,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "io_delay_type",
  .data = &io_delay_type,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "randomize_va_space",
  .data = &randomize_va_space,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "spin_retry",
  .data = &spin_retry,
  .maxlen = sizeof (int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "acpi_video_flags",
  .data = &acpi_realmode_flags,
  .maxlen = sizeof (unsigned long),
  .mode = 0644,
  .proc_handler = proc_doulongvec_minmax,
 },
 {
  .procname = "ignore-unaligned-usertrap",
  .data = &no_unaligned_warning,
  .maxlen = sizeof (int),
   .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "unaligned-dump-stack",
  .data = &unaligned_dump_stack,
  .maxlen = sizeof (int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "hung_task_panic",
  .data = &sysctl_hung_task_panic,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "hung_task_check_count",
  .data = &sysctl_hung_task_check_count,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
 },
 {
  .procname = "hung_task_timeout_secs",
  .data = &sysctl_hung_task_timeout_secs,
  .maxlen = sizeof(unsigned long),
  .mode = 0644,
  .proc_handler = proc_dohung_task_timeout_secs,
  .extra2 = &hung_task_timeout_max,
 },
 {
  .procname = "hung_task_warnings",
  .data = &sysctl_hung_task_warnings,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &neg_one,
 },
 {
  .procname = "compat-log",
  .data = &compat_log,
  .maxlen = sizeof (int),
   .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "max_lock_depth",
  .data = &max_lock_depth,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "poweroff_cmd",
  .data = &poweroff_cmd,
  .maxlen = POWEROFF_CMD_PATH_LEN,
  .mode = 0644,
  .proc_handler = proc_dostring,
 },
 {
  .procname = "keys",
  .mode = 0555,
  .child = key_sysctls,
 },






 {
  .procname = "perf_event_paranoid",
  .data = &sysctl_perf_event_paranoid,
  .maxlen = sizeof(sysctl_perf_event_paranoid),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "perf_event_mlock_kb",
  .data = &sysctl_perf_event_mlock,
  .maxlen = sizeof(sysctl_perf_event_mlock),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "perf_event_max_sample_rate",
  .data = &sysctl_perf_event_sample_rate,
  .maxlen = sizeof(sysctl_perf_event_sample_rate),
  .mode = 0644,
  .proc_handler = perf_proc_update_handler,
  .extra1 = &one,
 },
 {
  .procname = "perf_cpu_time_max_percent",
  .data = &sysctl_perf_cpu_time_max_percent,
  .maxlen = sizeof(sysctl_perf_cpu_time_max_percent),
  .mode = 0644,
  .proc_handler = perf_cpu_time_max_percent_handler,
  .extra1 = &zero,
  .extra2 = &one_hundred,
 },
 {
  .procname = "perf_event_max_stack",
  .data = &sysctl_perf_event_max_stack,
  .maxlen = sizeof(sysctl_perf_event_max_stack),
  .mode = 0644,
  .proc_handler = perf_event_max_stack_handler,
  .extra1 = &zero,
  .extra2 = &six_hundred_forty_kb,
 },
 {
  .procname = "perf_event_max_contexts_per_stack",
  .data = &sysctl_perf_event_max_contexts_per_stack,
  .maxlen = sizeof(sysctl_perf_event_max_contexts_per_stack),
  .mode = 0644,
  .proc_handler = perf_event_max_stack_handler,
  .extra1 = &zero,
  .extra2 = &one_thousand,
 },
 {
  .procname = "kmemcheck",
  .data = &kmemcheck_enabled,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "panic_on_warn",
  .data = &panic_on_warn,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "timer_migration",
  .data = &sysctl_timer_migration,
  .maxlen = sizeof(unsigned int),
  .mode = 0644,
  .proc_handler = timer_migration_handler,
 },
 {
  .procname = "unprivileged_bpf_disabled",
  .data = &sysctl_unprivileged_bpf_disabled,
  .maxlen = sizeof(sysctl_unprivileged_bpf_disabled),
  .mode = 0644,

  .proc_handler = proc_dointvec_minmax,
  .extra1 = &one,
  .extra2 = &one,
 },
 { }
};

static struct ctl_table vm_table[] = {
 {
  .procname = "overcommit_memory",
  .data = &sysctl_overcommit_memory,
  .maxlen = sizeof(sysctl_overcommit_memory),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &two,
 },
 {
  .procname = "panic_on_oom",
  .data = &sysctl_panic_on_oom,
  .maxlen = sizeof(sysctl_panic_on_oom),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &two,
 },
 {
  .procname = "oom_kill_allocating_task",
  .data = &sysctl_oom_kill_allocating_task,
  .maxlen = sizeof(sysctl_oom_kill_allocating_task),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "oom_dump_tasks",
  .data = &sysctl_oom_dump_tasks,
  .maxlen = sizeof(sysctl_oom_dump_tasks),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "overcommit_ratio",
  .data = &sysctl_overcommit_ratio,
  .maxlen = sizeof(sysctl_overcommit_ratio),
  .mode = 0644,
  .proc_handler = overcommit_ratio_handler,
 },
 {
  .procname = "overcommit_kbytes",
  .data = &sysctl_overcommit_kbytes,
  .maxlen = sizeof(sysctl_overcommit_kbytes),
  .mode = 0644,
  .proc_handler = overcommit_kbytes_handler,
 },
 {
  .procname = "page-cluster",
  .data = &page_cluster,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
 },
 {
  .procname = "dirty_background_ratio",
  .data = &dirty_background_ratio,
  .maxlen = sizeof(dirty_background_ratio),
  .mode = 0644,
  .proc_handler = dirty_background_ratio_handler,
  .extra1 = &zero,
  .extra2 = &one_hundred,
 },
 {
  .procname = "dirty_background_bytes",
  .data = &dirty_background_bytes,
  .maxlen = sizeof(dirty_background_bytes),
  .mode = 0644,
  .proc_handler = dirty_background_bytes_handler,
  .extra1 = &one_ul,
 },
 {
  .procname = "dirty_ratio",
  .data = &vm_dirty_ratio,
  .maxlen = sizeof(vm_dirty_ratio),
  .mode = 0644,
  .proc_handler = dirty_ratio_handler,
  .extra1 = &zero,
  .extra2 = &one_hundred,
 },
 {
  .procname = "dirty_bytes",
  .data = &vm_dirty_bytes,
  .maxlen = sizeof(vm_dirty_bytes),
  .mode = 0644,
  .proc_handler = dirty_bytes_handler,
  .extra1 = &dirty_bytes_min,
 },
 {
  .procname = "dirty_writeback_centisecs",
  .data = &dirty_writeback_interval,
  .maxlen = sizeof(dirty_writeback_interval),
  .mode = 0644,
  .proc_handler = dirty_writeback_centisecs_handler,
 },
 {
  .procname = "dirty_expire_centisecs",
  .data = &dirty_expire_interval,
  .maxlen = sizeof(dirty_expire_interval),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
 },
 {
  .procname = "dirtytime_expire_seconds",
  .data = &dirtytime_expire_interval,
  .maxlen = sizeof(dirty_expire_interval),
  .mode = 0644,
  .proc_handler = dirtytime_interval_handler,
  .extra1 = &zero,
 },
 {
  .procname = "nr_pdflush_threads",
  .mode = 0444 ,
  .proc_handler = pdflush_proc_obsolete,
 },
 {
  .procname = "swappiness",
  .data = &vm_swappiness,
  .maxlen = sizeof(vm_swappiness),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one_hundred,
 },
 {
  .procname = "nr_hugepages",
  .data = NULL,
  .maxlen = sizeof(unsigned long),
  .mode = 0644,
  .proc_handler = hugetlb_sysctl_handler,
 },
 {
  .procname = "nr_hugepages_mempolicy",
  .data = NULL,
  .maxlen = sizeof(unsigned long),
  .mode = 0644,
  .proc_handler = &hugetlb_mempolicy_sysctl_handler,
 },
  {
  .procname = "hugetlb_shm_group",
  .data = &sysctl_hugetlb_shm_group,
  .maxlen = sizeof(gid_t),
  .mode = 0644,
  .proc_handler = proc_dointvec,
  },
  {
  .procname = "hugepages_treat_as_movable",
  .data = &hugepages_treat_as_movable,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "nr_overcommit_hugepages",
  .data = NULL,
  .maxlen = sizeof(unsigned long),
  .mode = 0644,
  .proc_handler = hugetlb_overcommit_handler,
 },
 {
  .procname = "lowmem_reserve_ratio",
  .data = &sysctl_lowmem_reserve_ratio,
  .maxlen = sizeof(sysctl_lowmem_reserve_ratio),
  .mode = 0644,
  .proc_handler = lowmem_reserve_ratio_sysctl_handler,
 },
 {
  .procname = "drop_caches",
  .data = &sysctl_drop_caches,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = drop_caches_sysctl_handler,
  .extra1 = &one,
  .extra2 = &four,
 },
 {
  .procname = "compact_memory",
  .data = &sysctl_compact_memory,
  .maxlen = sizeof(int),
  .mode = 0200,
  .proc_handler = sysctl_compaction_handler,
 },
 {
  .procname = "extfrag_threshold",
  .data = &sysctl_extfrag_threshold,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = sysctl_extfrag_handler,
  .extra1 = &min_extfrag_threshold,
  .extra2 = &max_extfrag_threshold,
 },
 {
  .procname = "compact_unevictable_allowed",
  .data = &sysctl_compact_unevictable_allowed,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
  .extra1 = &zero,
  .extra2 = &one,
 },

 {
  .procname = "min_free_kbytes",
  .data = &min_free_kbytes,
  .maxlen = sizeof(min_free_kbytes),
  .mode = 0644,
  .proc_handler = min_free_kbytes_sysctl_handler,
  .extra1 = &zero,
 },
 {
  .procname = "watermark_scale_factor",
  .data = &watermark_scale_factor,
  .maxlen = sizeof(watermark_scale_factor),
  .mode = 0644,
  .proc_handler = watermark_scale_factor_sysctl_handler,
  .extra1 = &one,
  .extra2 = &one_thousand,
 },
 {
  .procname = "percpu_pagelist_fraction",
  .data = &percpu_pagelist_fraction,
  .maxlen = sizeof(percpu_pagelist_fraction),
  .mode = 0644,
  .proc_handler = percpu_pagelist_fraction_sysctl_handler,
  .extra1 = &zero,
 },
 {
  .procname = "max_map_count",
  .data = &sysctl_max_map_count,
  .maxlen = sizeof(sysctl_max_map_count),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
 },
 {
  .procname = "nr_trim_pages",
  .data = &sysctl_nr_trim_pages,
  .maxlen = sizeof(sysctl_nr_trim_pages),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
 },
 {
  .procname = "laptop_mode",
  .data = &laptop_mode,
  .maxlen = sizeof(laptop_mode),
  .mode = 0644,
  .proc_handler = proc_dointvec_jiffies,
 },
 {
  .procname = "block_dump",
  .data = &block_dump,
  .maxlen = sizeof(block_dump),
  .mode = 0644,
  .proc_handler = proc_dointvec,
  .extra1 = &zero,
 },
 {
  .procname = "vfs_cache_pressure",
  .data = &sysctl_vfs_cache_pressure,
  .maxlen = sizeof(sysctl_vfs_cache_pressure),
  .mode = 0644,
  .proc_handler = proc_dointvec,
  .extra1 = &zero,
 },
 {
  .procname = "legacy_va_layout",
  .data = &sysctl_legacy_va_layout,
  .maxlen = sizeof(sysctl_legacy_va_layout),
  .mode = 0644,
  .proc_handler = proc_dointvec,
  .extra1 = &zero,
 },
 {
  .procname = "zone_reclaim_mode",
  .data = &zone_reclaim_mode,
  .maxlen = sizeof(zone_reclaim_mode),
  .mode = 0644,
  .proc_handler = proc_dointvec,
  .extra1 = &zero,
 },
 {
  .procname = "min_unmapped_ratio",
  .data = &sysctl_min_unmapped_ratio,
  .maxlen = sizeof(sysctl_min_unmapped_ratio),
  .mode = 0644,
  .proc_handler = sysctl_min_unmapped_ratio_sysctl_handler,
  .extra1 = &zero,
  .extra2 = &one_hundred,
 },
 {
  .procname = "min_slab_ratio",
  .data = &sysctl_min_slab_ratio,
  .maxlen = sizeof(sysctl_min_slab_ratio),
  .mode = 0644,
  .proc_handler = sysctl_min_slab_ratio_sysctl_handler,
  .extra1 = &zero,
  .extra2 = &one_hundred,
 },
 {
  .procname = "stat_interval",
  .data = &sysctl_stat_interval,
  .maxlen = sizeof(sysctl_stat_interval),
  .mode = 0644,
  .proc_handler = proc_dointvec_jiffies,
 },
 {
  .procname = "stat_refresh",
  .data = NULL,
  .maxlen = 0,
  .mode = 0600,
  .proc_handler = vmstat_refresh,
 },
 {
  .procname = "mmap_min_addr",
  .data = &dac_mmap_min_addr,
  .maxlen = sizeof(unsigned long),
  .mode = 0644,
  .proc_handler = mmap_min_addr_handler,
 },
 {
  .procname = "numa_zonelist_order",
  .data = &numa_zonelist_order,
  .maxlen = NUMA_ZONELIST_ORDER_LEN,
  .mode = 0644,
  .proc_handler = numa_zonelist_order_handler,
 },
   (defined(CONFIG_SUPERH) && defined(CONFIG_VSYSCALL))
 {
  .procname = "vdso_enabled",
  .data = &vdso32_enabled,
  .maxlen = sizeof(vdso32_enabled),
  .data = &vdso_enabled,
  .maxlen = sizeof(vdso_enabled),
  .mode = 0644,
  .proc_handler = proc_dointvec,
  .extra1 = &zero,
 },
 {
  .procname = "highmem_is_dirtyable",
  .data = &vm_highmem_is_dirtyable,
  .maxlen = sizeof(vm_highmem_is_dirtyable),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "memory_failure_early_kill",
  .data = &sysctl_memory_failure_early_kill,
  .maxlen = sizeof(sysctl_memory_failure_early_kill),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "memory_failure_recovery",
  .data = &sysctl_memory_failure_recovery,
  .maxlen = sizeof(sysctl_memory_failure_recovery),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "user_reserve_kbytes",
  .data = &sysctl_user_reserve_kbytes,
  .maxlen = sizeof(sysctl_user_reserve_kbytes),
  .mode = 0644,
  .proc_handler = proc_doulongvec_minmax,
 },
 {
  .procname = "admin_reserve_kbytes",
  .data = &sysctl_admin_reserve_kbytes,
  .maxlen = sizeof(sysctl_admin_reserve_kbytes),
  .mode = 0644,
  .proc_handler = proc_doulongvec_minmax,
 },
 {
  .procname = "mmap_rnd_bits",
  .data = &mmap_rnd_bits,
  .maxlen = sizeof(mmap_rnd_bits),
  .mode = 0600,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = (void *)&mmap_rnd_bits_min,
  .extra2 = (void *)&mmap_rnd_bits_max,
 },
 {
  .procname = "mmap_rnd_compat_bits",
  .data = &mmap_rnd_compat_bits,
  .maxlen = sizeof(mmap_rnd_compat_bits),
  .mode = 0600,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = (void *)&mmap_rnd_compat_bits_min,
  .extra2 = (void *)&mmap_rnd_compat_bits_max,
 },
 { }
};

static struct ctl_table fs_table[] = {
 {
  .procname = "inode-nr",
  .data = &inodes_stat,
  .maxlen = 2*sizeof(long),
  .mode = 0444,
  .proc_handler = proc_nr_inodes,
 },
 {
  .procname = "inode-state",
  .data = &inodes_stat,
  .maxlen = 7*sizeof(long),
  .mode = 0444,
  .proc_handler = proc_nr_inodes,
 },
 {
  .procname = "file-nr",
  .data = &files_stat,
  .maxlen = sizeof(files_stat),
  .mode = 0444,
  .proc_handler = proc_nr_files,
 },
 {
  .procname = "file-max",
  .data = &files_stat.max_files,
  .maxlen = sizeof(files_stat.max_files),
  .mode = 0644,
  .proc_handler = proc_doulongvec_minmax,
 },
 {
  .procname = "nr_open",
  .data = &sysctl_nr_open,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &sysctl_nr_open_min,
  .extra2 = &sysctl_nr_open_max,
 },
 {
  .procname = "dentry-state",
  .data = &dentry_stat,
  .maxlen = 6*sizeof(long),
  .mode = 0444,
  .proc_handler = proc_nr_dentry,
 },
 {
  .procname = "overflowuid",
  .data = &fs_overflowuid,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &minolduid,
  .extra2 = &maxolduid,
 },
 {
  .procname = "overflowgid",
  .data = &fs_overflowgid,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &minolduid,
  .extra2 = &maxolduid,
 },
 {
  .procname = "leases-enable",
  .data = &leases_enable,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "dir-notify-enable",
  .data = &dir_notify_enable,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "lease-break-time",
  .data = &lease_break_time,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec,
 },
 {
  .procname = "aio-nr",
  .data = &aio_nr,
  .maxlen = sizeof(aio_nr),
  .mode = 0444,
  .proc_handler = proc_doulongvec_minmax,
 },
 {
  .procname = "aio-max-nr",
  .data = &aio_max_nr,
  .maxlen = sizeof(aio_max_nr),
  .mode = 0644,
  .proc_handler = proc_doulongvec_minmax,
 },
 {
  .procname = "inotify",
  .mode = 0555,
  .child = inotify_table,
 },
 {
  .procname = "epoll",
  .mode = 0555,
  .child = epoll_table,
 },
 {
  .procname = "protected_symlinks",
  .data = &sysctl_protected_symlinks,
  .maxlen = sizeof(int),
  .mode = 0600,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "protected_hardlinks",
  .data = &sysctl_protected_hardlinks,
  .maxlen = sizeof(int),
  .mode = 0600,
  .proc_handler = proc_dointvec_minmax,
  .extra1 = &zero,
  .extra2 = &one,
 },
 {
  .procname = "suid_dumpable",
  .data = &suid_dumpable,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec_minmax_coredump,
  .extra1 = &zero,
  .extra2 = &two,
 },
 {
  .procname = "binfmt_misc",
  .mode = 0555,
  .child = sysctl_mount_point,
 },
 {
  .procname = "pipe-max-size",
  .data = &pipe_max_size,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = &pipe_proc_fn,
  .extra1 = &pipe_min_size,
 },
 {
  .procname = "pipe-user-pages-hard",
  .data = &pipe_user_pages_hard,
  .maxlen = sizeof(pipe_user_pages_hard),
  .mode = 0644,
  .proc_handler = proc_doulongvec_minmax,
 },
 {
  .procname = "pipe-user-pages-soft",
  .data = &pipe_user_pages_soft,
  .maxlen = sizeof(pipe_user_pages_soft),
  .mode = 0644,
  .proc_handler = proc_doulongvec_minmax,
 },
 { }
};

static struct ctl_table debug_table[] = {
 {
  .procname = "exception-trace",
  .data = &show_unhandled_signals,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_dointvec
 },
 {
  .procname = "kprobes-optimization",
  .data = &sysctl_kprobes_optimization,
  .maxlen = sizeof(int),
  .mode = 0644,
  .proc_handler = proc_kprobes_optimization_handler,
  .extra1 = &zero,
  .extra2 = &one,
 },
 { }
};

static struct ctl_table dev_table[] = {
 { }
};

int __init sysctl_init(void)
{
 struct ctl_table_header *hdr;

 hdr = register_sysctl_table(sysctl_base_table);
 kmemleak_not_leak(hdr);
 return 0;
}







static int _proc_do_string(char *data, int maxlen, int write,
      char __user *buffer,
      size_t *lenp, loff_t *ppos)
{
 size_t len;
 char __user *p;
 char c;

 if (!data || !maxlen || !*lenp) {
  *lenp = 0;
  return 0;
 }

 if (write) {
  if (sysctl_writes_strict == SYSCTL_WRITES_STRICT) {

   len = strlen(data);
   if (len > maxlen - 1)
    len = maxlen - 1;

   if (*ppos > len)
    return 0;
   len = *ppos;
  } else {

   len = 0;
  }

  *ppos += *lenp;
  p = buffer;
  while ((p - buffer) < *lenp && len < maxlen - 1) {
   if (get_user(c, p++))
    return -EFAULT;
   if (c == 0 || c == '\n')
    break;
   data[len++] = c;
  }
  data[len] = 0;
 } else {
  len = strlen(data);
  if (len > maxlen)
   len = maxlen;

  if (*ppos > len) {
   *lenp = 0;
   return 0;
  }

  data += *ppos;
  len -= *ppos;

  if (len > *lenp)
   len = *lenp;
  if (len)
   if (copy_to_user(buffer, data, len))
    return -EFAULT;
  if (len < *lenp) {
   if (put_user('\n', buffer + len))
    return -EFAULT;
   len++;
  }
  *lenp = len;
  *ppos += len;
 }
 return 0;
}

static void warn_sysctl_write(struct ctl_table *table)
{
 pr_warn_once("%s wrote to %s when file position was not 0!\n"
  "This will not be supported in the future. To silence this\n"
  "warning, set kernel.sysctl_writes_strict = -1\n",
  current->comm, table->procname);
}
int proc_dostring(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos)
{
 if (write && *ppos && sysctl_writes_strict == SYSCTL_WRITES_WARN)
  warn_sysctl_write(table);

 return _proc_do_string((char *)(table->data), table->maxlen, write,
          (char __user *)buffer, lenp, ppos);
}

static size_t proc_skip_spaces(char **buf)
{
 size_t ret;
 char *tmp = skip_spaces(*buf);
 ret = tmp - *buf;
 *buf = tmp;
 return ret;
}

static void proc_skip_char(char **buf, size_t *size, const char v)
{
 while (*size) {
  if (**buf != v)
   break;
  (*size)--;
  (*buf)++;
 }
}

static int proc_get_long(char **buf, size_t *size,
     unsigned long *val, bool *neg,
     const char *perm_tr, unsigned perm_tr_len, char *tr)
{
 int len;
 char *p, tmp[TMPBUFLEN];

 if (!*size)
  return -EINVAL;

 len = *size;
 if (len > TMPBUFLEN - 1)
  len = TMPBUFLEN - 1;

 memcpy(tmp, *buf, len);

 tmp[len] = 0;
 p = tmp;
 if (*p == '-' && *size > 1) {
  *neg = true;
  p++;
 } else
  *neg = false;
 if (!isdigit(*p))
  return -EINVAL;

 *val = simple_strtoul(p, &p, 0);

 len = p - tmp;




 if (len == TMPBUFLEN - 1)
  return -EINVAL;

 if (len < *size && perm_tr_len && !memchr(perm_tr, *p, perm_tr_len))
  return -EINVAL;

 if (tr && (len < *size))
  *tr = *p;

 *buf += len;
 *size -= len;

 return 0;
}
static int proc_put_long(void __user **buf, size_t *size, unsigned long val,
     bool neg)
{
 int len;
 char tmp[TMPBUFLEN], *p = tmp;

 sprintf(p, "%s%lu", neg ? "-" : "", val);
 len = strlen(tmp);
 if (len > *size)
  len = *size;
 if (copy_to_user(*buf, tmp, len))
  return -EFAULT;
 *size -= len;
 *buf += len;
 return 0;
}

static int proc_put_char(void __user **buf, size_t *size, char c)
{
 if (*size) {
  char __user **buffer = (char __user **)buf;
  if (put_user(c, *buffer))
   return -EFAULT;
  (*size)--, (*buffer)++;
  *buf = *buffer;
 }
 return 0;
}

static int do_proc_dointvec_conv(bool *negp, unsigned long *lvalp,
     int *valp,
     int write, void *data)
{
 if (write) {
  if (*negp) {
   if (*lvalp > (unsigned long) INT_MAX + 1)
    return -EINVAL;
   *valp = -*lvalp;
  } else {
   if (*lvalp > (unsigned long) INT_MAX)
    return -EINVAL;
   *valp = *lvalp;
  }
 } else {
  int val = *valp;
  if (val < 0) {
   *negp = true;
   *lvalp = -(unsigned long)val;
  } else {
   *negp = false;
   *lvalp = (unsigned long)val;
  }
 }
 return 0;
}

static const char proc_wspace_sep[] = { ' ', '\t', '\n' };

static int __do_proc_dointvec(void *tbl_data, struct ctl_table *table,
    int write, void __user *buffer,
    size_t *lenp, loff_t *ppos,
    int (*conv)(bool *negp, unsigned long *lvalp, int *valp,
         int write, void *data),
    void *data)
{
 int *i, vleft, first = 1, err = 0;
 size_t left;
 char *kbuf = NULL, *p;

 if (!tbl_data || !table->maxlen || !*lenp || (*ppos && !write)) {
  *lenp = 0;
  return 0;
 }

 i = (int *) tbl_data;
 vleft = table->maxlen / sizeof(*i);
 left = *lenp;

 if (!conv)
  conv = do_proc_dointvec_conv;

 if (write) {
  if (*ppos) {
   switch (sysctl_writes_strict) {
   case SYSCTL_WRITES_STRICT:
    goto out;
   case SYSCTL_WRITES_WARN:
    warn_sysctl_write(table);
    break;
   default:
    break;
   }
  }

  if (left > PAGE_SIZE - 1)
   left = PAGE_SIZE - 1;
  p = kbuf = memdup_user_nul(buffer, left);
  if (IS_ERR(kbuf))
   return PTR_ERR(kbuf);
 }

 for (; left && vleft--; i++, first=0) {
  unsigned long lval;
  bool neg;

  if (write) {
   left -= proc_skip_spaces(&p);

   if (!left)
    break;
   err = proc_get_long(&p, &left, &lval, &neg,
          proc_wspace_sep,
          sizeof(proc_wspace_sep), NULL);
   if (err)
    break;
   if (conv(&neg, &lval, i, 1, data)) {
    err = -EINVAL;
    break;
   }
  } else {
   if (conv(&neg, &lval, i, 0, data)) {
    err = -EINVAL;
    break;
   }
   if (!first)
    err = proc_put_char(&buffer, &left, '\t');
   if (err)
    break;
   err = proc_put_long(&buffer, &left, lval, neg);
   if (err)
    break;
  }
 }

 if (!write && !first && left && !err)
  err = proc_put_char(&buffer, &left, '\n');
 if (write && !err && left)
  left -= proc_skip_spaces(&p);
 if (write) {
  kfree(kbuf);
  if (first)
   return err ? : -EINVAL;
 }
 *lenp -= left;
out:
 *ppos += *lenp;
 return err;
}

static int do_proc_dointvec(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos,
    int (*conv)(bool *negp, unsigned long *lvalp, int *valp,
         int write, void *data),
    void *data)
{
 return __do_proc_dointvec(table->data, table, write,
   buffer, lenp, ppos, conv, data);
}
int proc_dointvec(struct ctl_table *table, int write,
       void __user *buffer, size_t *lenp, loff_t *ppos)
{
    return do_proc_dointvec(table,write,buffer,lenp,ppos,
           NULL,NULL);
}





static int proc_taint(struct ctl_table *table, int write,
          void __user *buffer, size_t *lenp, loff_t *ppos)
{
 struct ctl_table t;
 unsigned long tmptaint = get_taint();
 int err;

 if (write && !capable(CAP_SYS_ADMIN))
  return -EPERM;

 t = *table;
 t.data = &tmptaint;
 err = proc_doulongvec_minmax(&t, write, buffer, lenp, ppos);
 if (err < 0)
  return err;

 if (write) {




  int i;
  for (i = 0; i < BITS_PER_LONG && tmptaint >> i; i++) {
   if ((tmptaint >> i) & 1)
    add_taint(i, LOCKDEP_STILL_OK);
  }
 }

 return err;
}

static int proc_dointvec_minmax_sysadmin(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos)
{
 if (write && !capable(CAP_SYS_ADMIN))
  return -EPERM;

 return proc_dointvec_minmax(table, write, buffer, lenp, ppos);
}

struct do_proc_dointvec_minmax_conv_param {
 int *min;
 int *max;
};

static int do_proc_dointvec_minmax_conv(bool *negp, unsigned long *lvalp,
     int *valp,
     int write, void *data)
{
 struct do_proc_dointvec_minmax_conv_param *param = data;
 if (write) {
  int val = *negp ? -*lvalp : *lvalp;
  if ((param->min && *param->min > val) ||
      (param->max && *param->max < val))
   return -EINVAL;
  *valp = val;
 } else {
  int val = *valp;
  if (val < 0) {
   *negp = true;
   *lvalp = -(unsigned long)val;
  } else {
   *negp = false;
   *lvalp = (unsigned long)val;
  }
 }
 return 0;
}
int proc_dointvec_minmax(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos)
{
 struct do_proc_dointvec_minmax_conv_param param = {
  .min = (int *) table->extra1,
  .max = (int *) table->extra2,
 };
 return do_proc_dointvec(table, write, buffer, lenp, ppos,
    do_proc_dointvec_minmax_conv, &param);
}

static void validate_coredump_safety(void)
{
 if (suid_dumpable == SUID_DUMP_ROOT &&
     core_pattern[0] != '/' && core_pattern[0] != '|') {
  printk(KERN_WARNING "Unsafe core_pattern used with "\
   "suid_dumpable=2. Pipe handler or fully qualified "\
   "core dump path required.\n");
 }
}

static int proc_dointvec_minmax_coredump(struct ctl_table *table, int write,
  void __user *buffer, size_t *lenp, loff_t *ppos)
{
 int error = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
 if (!error)
  validate_coredump_safety();
 return error;
}

static int proc_dostring_coredump(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos)
{
 int error = proc_dostring(table, write, buffer, lenp, ppos);
 if (!error)
  validate_coredump_safety();
 return error;
}

static int __do_proc_doulongvec_minmax(void *data, struct ctl_table *table, int write,
         void __user *buffer,
         size_t *lenp, loff_t *ppos,
         unsigned long convmul,
         unsigned long convdiv)
{
 unsigned long *i, *min, *max;
 int vleft, first = 1, err = 0;
 size_t left;
 char *kbuf = NULL, *p;

 if (!data || !table->maxlen || !*lenp || (*ppos && !write)) {
  *lenp = 0;
  return 0;
 }

 i = (unsigned long *) data;
 min = (unsigned long *) table->extra1;
 max = (unsigned long *) table->extra2;
 vleft = table->maxlen / sizeof(unsigned long);
 left = *lenp;

 if (write) {
  if (*ppos) {
   switch (sysctl_writes_strict) {
   case SYSCTL_WRITES_STRICT:
    goto out;
   case SYSCTL_WRITES_WARN:
    warn_sysctl_write(table);
    break;
   default:
    break;
   }
  }

  if (left > PAGE_SIZE - 1)
   left = PAGE_SIZE - 1;
  p = kbuf = memdup_user_nul(buffer, left);
  if (IS_ERR(kbuf))
   return PTR_ERR(kbuf);
 }

 for (; left && vleft--; i++, first = 0) {
  unsigned long val;

  if (write) {
   bool neg;

   left -= proc_skip_spaces(&p);

   err = proc_get_long(&p, &left, &val, &neg,
          proc_wspace_sep,
          sizeof(proc_wspace_sep), NULL);
   if (err)
    break;
   if (neg)
    continue;
   if ((min && val < *min) || (max && val > *max))
    continue;
   *i = val;
  } else {
   val = convdiv * (*i) / convmul;
   if (!first) {
    err = proc_put_char(&buffer, &left, '\t');
    if (err)
     break;
   }
   err = proc_put_long(&buffer, &left, val, false);
   if (err)
    break;
  }
 }

 if (!write && !first && left && !err)
  err = proc_put_char(&buffer, &left, '\n');
 if (write && !err)
  left -= proc_skip_spaces(&p);
 if (write) {
  kfree(kbuf);
  if (first)
   return err ? : -EINVAL;
 }
 *lenp -= left;
out:
 *ppos += *lenp;
 return err;
}

static int do_proc_doulongvec_minmax(struct ctl_table *table, int write,
         void __user *buffer,
         size_t *lenp, loff_t *ppos,
         unsigned long convmul,
         unsigned long convdiv)
{
 return __do_proc_doulongvec_minmax(table->data, table, write,
   buffer, lenp, ppos, convmul, convdiv);
}
int proc_doulongvec_minmax(struct ctl_table *table, int write,
      void __user *buffer, size_t *lenp, loff_t *ppos)
{
    return do_proc_doulongvec_minmax(table, write, buffer, lenp, ppos, 1l, 1l);
}
int proc_doulongvec_ms_jiffies_minmax(struct ctl_table *table, int write,
          void __user *buffer,
          size_t *lenp, loff_t *ppos)
{
    return do_proc_doulongvec_minmax(table, write, buffer,
         lenp, ppos, HZ, 1000l);
}


static int do_proc_dointvec_jiffies_conv(bool *negp, unsigned long *lvalp,
      int *valp,
      int write, void *data)
{
 if (write) {
  if (*lvalp > LONG_MAX / HZ)
   return 1;
  *valp = *negp ? -(*lvalp*HZ) : (*lvalp*HZ);
 } else {
  int val = *valp;
  unsigned long lval;
  if (val < 0) {
   *negp = true;
   lval = -(unsigned long)val;
  } else {
   *negp = false;
   lval = (unsigned long)val;
  }
  *lvalp = lval / HZ;
 }
 return 0;
}

static int do_proc_dointvec_userhz_jiffies_conv(bool *negp, unsigned long *lvalp,
      int *valp,
      int write, void *data)
{
 if (write) {
  if (USER_HZ < HZ && *lvalp > (LONG_MAX / HZ) * USER_HZ)
   return 1;
  *valp = clock_t_to_jiffies(*negp ? -*lvalp : *lvalp);
 } else {
  int val = *valp;
  unsigned long lval;
  if (val < 0) {
   *negp = true;
   lval = -(unsigned long)val;
  } else {
   *negp = false;
   lval = (unsigned long)val;
  }
  *lvalp = jiffies_to_clock_t(lval);
 }
 return 0;
}

static int do_proc_dointvec_ms_jiffies_conv(bool *negp, unsigned long *lvalp,
         int *valp,
         int write, void *data)
{
 if (write) {
  unsigned long jif = msecs_to_jiffies(*negp ? -*lvalp : *lvalp);

  if (jif > INT_MAX)
   return 1;
  *valp = (int)jif;
 } else {
  int val = *valp;
  unsigned long lval;
  if (val < 0) {
   *negp = true;
   lval = -(unsigned long)val;
  } else {
   *negp = false;
   lval = (unsigned long)val;
  }
  *lvalp = jiffies_to_msecs(lval);
 }
 return 0;
}
int proc_dointvec_jiffies(struct ctl_table *table, int write,
     void __user *buffer, size_t *lenp, loff_t *ppos)
{
    return do_proc_dointvec(table,write,buffer,lenp,ppos,
           do_proc_dointvec_jiffies_conv,NULL);
}
int proc_dointvec_userhz_jiffies(struct ctl_table *table, int write,
     void __user *buffer, size_t *lenp, loff_t *ppos)
{
    return do_proc_dointvec(table,write,buffer,lenp,ppos,
           do_proc_dointvec_userhz_jiffies_conv,NULL);
}
int proc_dointvec_ms_jiffies(struct ctl_table *table, int write,
        void __user *buffer, size_t *lenp, loff_t *ppos)
{
 return do_proc_dointvec(table, write, buffer, lenp, ppos,
    do_proc_dointvec_ms_jiffies_conv, NULL);
}

static int proc_do_cad_pid(struct ctl_table *table, int write,
      void __user *buffer, size_t *lenp, loff_t *ppos)
{
 struct pid *new_pid;
 pid_t tmp;
 int r;

 tmp = pid_vnr(cad_pid);

 r = __do_proc_dointvec(&tmp, table, write, buffer,
          lenp, ppos, NULL, NULL);
 if (r || !write)
  return r;

 new_pid = find_get_pid(tmp);
 if (!new_pid)
  return -ESRCH;

 put_pid(xchg(&cad_pid, new_pid));
 return 0;
}
int proc_do_large_bitmap(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos)
{
 int err = 0;
 bool first = 1;
 size_t left = *lenp;
 unsigned long bitmap_len = table->maxlen;
 unsigned long *bitmap = *(unsigned long **) table->data;
 unsigned long *tmp_bitmap = NULL;
 char tr_a[] = { '-', ',', '\n' }, tr_b[] = { ',', '\n', 0 }, c;

 if (!bitmap || !bitmap_len || !left || (*ppos && !write)) {
  *lenp = 0;
  return 0;
 }

 if (write) {
  char *kbuf, *p;

  if (left > PAGE_SIZE - 1)
   left = PAGE_SIZE - 1;

  p = kbuf = memdup_user_nul(buffer, left);
  if (IS_ERR(kbuf))
   return PTR_ERR(kbuf);

  tmp_bitmap = kzalloc(BITS_TO_LONGS(bitmap_len) * sizeof(unsigned long),
         GFP_KERNEL);
  if (!tmp_bitmap) {
   kfree(kbuf);
   return -ENOMEM;
  }
  proc_skip_char(&p, &left, '\n');
  while (!err && left) {
   unsigned long val_a, val_b;
   bool neg;

   err = proc_get_long(&p, &left, &val_a, &neg, tr_a,
          sizeof(tr_a), &c);
   if (err)
    break;
   if (val_a >= bitmap_len || neg) {
    err = -EINVAL;
    break;
   }

   val_b = val_a;
   if (left) {
    p++;
    left--;
   }

   if (c == '-') {
    err = proc_get_long(&p, &left, &val_b,
           &neg, tr_b, sizeof(tr_b),
           &c);
    if (err)
     break;
    if (val_b >= bitmap_len || neg ||
        val_a > val_b) {
     err = -EINVAL;
     break;
    }
    if (left) {
     p++;
     left--;
    }
   }

   bitmap_set(tmp_bitmap, val_a, val_b - val_a + 1);
   first = 0;
   proc_skip_char(&p, &left, '\n');
  }
  kfree(kbuf);
 } else {
  unsigned long bit_a, bit_b = 0;

  while (left) {
   bit_a = find_next_bit(bitmap, bitmap_len, bit_b);
   if (bit_a >= bitmap_len)
    break;
   bit_b = find_next_zero_bit(bitmap, bitmap_len,
         bit_a + 1) - 1;

   if (!first) {
    err = proc_put_char(&buffer, &left, ',');
    if (err)
     break;
   }
   err = proc_put_long(&buffer, &left, bit_a, false);
   if (err)
    break;
   if (bit_a != bit_b) {
    err = proc_put_char(&buffer, &left, '-');
    if (err)
     break;
    err = proc_put_long(&buffer, &left, bit_b, false);
    if (err)
     break;
   }

   first = 0; bit_b++;
  }
  if (!err)
   err = proc_put_char(&buffer, &left, '\n');
 }

 if (!err) {
  if (write) {
   if (*ppos)
    bitmap_or(bitmap, bitmap, tmp_bitmap, bitmap_len);
   else
    bitmap_copy(bitmap, tmp_bitmap, bitmap_len);
  }
  kfree(tmp_bitmap);
  *lenp -= left;
  *ppos += *lenp;
  return 0;
 } else {
  kfree(tmp_bitmap);
  return err;
 }
}


int proc_dostring(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos)
{
 return -ENOSYS;
}

int proc_dointvec(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos)
{
 return -ENOSYS;
}

int proc_dointvec_minmax(struct ctl_table *table, int write,
      void __user *buffer, size_t *lenp, loff_t *ppos)
{
 return -ENOSYS;
}

int proc_dointvec_jiffies(struct ctl_table *table, int write,
      void __user *buffer, size_t *lenp, loff_t *ppos)
{
 return -ENOSYS;
}

int proc_dointvec_userhz_jiffies(struct ctl_table *table, int write,
      void __user *buffer, size_t *lenp, loff_t *ppos)
{
 return -ENOSYS;
}

int proc_dointvec_ms_jiffies(struct ctl_table *table, int write,
        void __user *buffer, size_t *lenp, loff_t *ppos)
{
 return -ENOSYS;
}

int proc_doulongvec_minmax(struct ctl_table *table, int write,
      void __user *buffer, size_t *lenp, loff_t *ppos)
{
 return -ENOSYS;
}

int proc_doulongvec_ms_jiffies_minmax(struct ctl_table *table, int write,
          void __user *buffer,
          size_t *lenp, loff_t *ppos)
{
    return -ENOSYS;
}







EXPORT_SYMBOL(proc_dointvec);
EXPORT_SYMBOL(proc_dointvec_jiffies);
EXPORT_SYMBOL(proc_dointvec_minmax);
EXPORT_SYMBOL(proc_dointvec_userhz_jiffies);
EXPORT_SYMBOL(proc_dointvec_ms_jiffies);
EXPORT_SYMBOL(proc_dostring);
EXPORT_SYMBOL(proc_doulongvec_minmax);
EXPORT_SYMBOL(proc_doulongvec_ms_jiffies_minmax);





asmlinkage long sys_ni_syscall(void);




asmlinkage long sys_ni_syscall(void)
{
 return -ENOSYS;
}

cond_syscall(sys_quotactl);
cond_syscall(sys32_quotactl);
cond_syscall(sys_acct);
cond_syscall(sys_lookup_dcookie);
cond_syscall(compat_sys_lookup_dcookie);
cond_syscall(sys_swapon);
cond_syscall(sys_swapoff);
cond_syscall(sys_kexec_load);
cond_syscall(compat_sys_kexec_load);
cond_syscall(sys_kexec_file_load);
cond_syscall(sys_init_module);
cond_syscall(sys_finit_module);
cond_syscall(sys_delete_module);
cond_syscall(sys_socketpair);
cond_syscall(sys_bind);
cond_syscall(sys_listen);
cond_syscall(sys_accept);
cond_syscall(sys_accept4);
cond_syscall(sys_connect);
cond_syscall(sys_getsockname);
cond_syscall(sys_getpeername);
cond_syscall(sys_sendto);
cond_syscall(sys_send);
cond_syscall(sys_recvfrom);
cond_syscall(sys_recv);
cond_syscall(sys_socket);
cond_syscall(sys_setsockopt);
cond_syscall(compat_sys_setsockopt);
cond_syscall(sys_getsockopt);
cond_syscall(compat_sys_getsockopt);
cond_syscall(sys_shutdown);
cond_syscall(sys_sendmsg);
cond_syscall(sys_sendmmsg);
cond_syscall(compat_sys_sendmsg);
cond_syscall(compat_sys_sendmmsg);
cond_syscall(sys_recvmsg);
cond_syscall(sys_recvmmsg);
cond_syscall(compat_sys_recvmsg);
cond_syscall(compat_sys_recv);
cond_syscall(compat_sys_recvfrom);
cond_syscall(compat_sys_recvmmsg);
cond_syscall(sys_socketcall);
cond_syscall(sys_futex);
cond_syscall(compat_sys_futex);
cond_syscall(sys_set_robust_list);
cond_syscall(compat_sys_set_robust_list);
cond_syscall(sys_get_robust_list);
cond_syscall(compat_sys_get_robust_list);
cond_syscall(sys_epoll_create);
cond_syscall(sys_epoll_create1);
cond_syscall(sys_epoll_ctl);
cond_syscall(sys_epoll_wait);
cond_syscall(sys_epoll_pwait);
cond_syscall(compat_sys_epoll_pwait);
cond_syscall(sys_semget);
cond_syscall(sys_semop);
cond_syscall(sys_semtimedop);
cond_syscall(compat_sys_semtimedop);
cond_syscall(sys_semctl);
cond_syscall(compat_sys_semctl);
cond_syscall(sys_msgget);
cond_syscall(sys_msgsnd);
cond_syscall(compat_sys_msgsnd);
cond_syscall(sys_msgrcv);
cond_syscall(compat_sys_msgrcv);
cond_syscall(sys_msgctl);
cond_syscall(compat_sys_msgctl);
cond_syscall(sys_shmget);
cond_syscall(sys_shmat);
cond_syscall(compat_sys_shmat);
cond_syscall(sys_shmdt);
cond_syscall(sys_shmctl);
cond_syscall(compat_sys_shmctl);
cond_syscall(sys_mq_open);
cond_syscall(sys_mq_unlink);
cond_syscall(sys_mq_timedsend);
cond_syscall(sys_mq_timedreceive);
cond_syscall(sys_mq_notify);
cond_syscall(sys_mq_getsetattr);
cond_syscall(compat_sys_mq_open);
cond_syscall(compat_sys_mq_timedsend);
cond_syscall(compat_sys_mq_timedreceive);
cond_syscall(compat_sys_mq_notify);
cond_syscall(compat_sys_mq_getsetattr);
cond_syscall(sys_mbind);
cond_syscall(sys_get_mempolicy);
cond_syscall(sys_set_mempolicy);
cond_syscall(compat_sys_mbind);
cond_syscall(compat_sys_get_mempolicy);
cond_syscall(compat_sys_set_mempolicy);
cond_syscall(sys_add_key);
cond_syscall(sys_request_key);
cond_syscall(sys_keyctl);
cond_syscall(compat_sys_keyctl);
cond_syscall(compat_sys_socketcall);
cond_syscall(sys_inotify_init);
cond_syscall(sys_inotify_init1);
cond_syscall(sys_inotify_add_watch);
cond_syscall(sys_inotify_rm_watch);
cond_syscall(sys_migrate_pages);
cond_syscall(sys_move_pages);
cond_syscall(sys_chown16);
cond_syscall(sys_fchown16);
cond_syscall(sys_getegid16);
cond_syscall(sys_geteuid16);
cond_syscall(sys_getgid16);
cond_syscall(sys_getgroups16);
cond_syscall(sys_getresgid16);
cond_syscall(sys_getresuid16);
cond_syscall(sys_getuid16);
cond_syscall(sys_lchown16);
cond_syscall(sys_setfsgid16);
cond_syscall(sys_setfsuid16);
cond_syscall(sys_setgid16);
cond_syscall(sys_setgroups16);
cond_syscall(sys_setregid16);
cond_syscall(sys_setresgid16);
cond_syscall(sys_setresuid16);
cond_syscall(sys_setreuid16);
cond_syscall(sys_setuid16);
cond_syscall(sys_sgetmask);
cond_syscall(sys_ssetmask);
cond_syscall(sys_vm86old);
cond_syscall(sys_vm86);
cond_syscall(sys_modify_ldt);
cond_syscall(sys_ipc);
cond_syscall(compat_sys_ipc);
cond_syscall(compat_sys_sysctl);
cond_syscall(sys_flock);
cond_syscall(sys_io_setup);
cond_syscall(sys_io_destroy);
cond_syscall(sys_io_submit);
cond_syscall(sys_io_cancel);
cond_syscall(sys_io_getevents);
cond_syscall(sys_sysfs);
cond_syscall(sys_syslog);
cond_syscall(sys_process_vm_readv);
cond_syscall(sys_process_vm_writev);
cond_syscall(compat_sys_process_vm_readv);
cond_syscall(compat_sys_process_vm_writev);
cond_syscall(sys_uselib);
cond_syscall(sys_fadvise64);
cond_syscall(sys_fadvise64_64);
cond_syscall(sys_madvise);
cond_syscall(sys_setuid);
cond_syscall(sys_setregid);
cond_syscall(sys_setgid);
cond_syscall(sys_setreuid);
cond_syscall(sys_setresuid);
cond_syscall(sys_getresuid);
cond_syscall(sys_setresgid);
cond_syscall(sys_getresgid);
cond_syscall(sys_setgroups);
cond_syscall(sys_getgroups);
cond_syscall(sys_setfsuid);
cond_syscall(sys_setfsgid);
cond_syscall(sys_capget);
cond_syscall(sys_capset);
cond_syscall(sys_copy_file_range);


cond_syscall(sys_pciconfig_read);
cond_syscall(sys_pciconfig_write);
cond_syscall(sys_pciconfig_iobase);
cond_syscall(compat_sys_s390_ipc);
cond_syscall(ppc_rtas);
cond_syscall(sys_spu_run);
cond_syscall(sys_spu_create);
cond_syscall(sys_subpage_prot);
cond_syscall(sys_s390_pci_mmio_read);
cond_syscall(sys_s390_pci_mmio_write);


cond_syscall(sys_mprotect);
cond_syscall(sys_msync);
cond_syscall(sys_mlock);
cond_syscall(sys_munlock);
cond_syscall(sys_mlockall);
cond_syscall(sys_munlockall);
cond_syscall(sys_mlock2);
cond_syscall(sys_mincore);
cond_syscall(sys_madvise);
cond_syscall(sys_mremap);
cond_syscall(sys_remap_file_pages);
cond_syscall(compat_sys_move_pages);
cond_syscall(compat_sys_migrate_pages);


cond_syscall(sys_bdflush);
cond_syscall(sys_ioprio_set);
cond_syscall(sys_ioprio_get);


cond_syscall(sys_signalfd);
cond_syscall(sys_signalfd4);
cond_syscall(compat_sys_signalfd);
cond_syscall(compat_sys_signalfd4);
cond_syscall(sys_timerfd_create);
cond_syscall(sys_timerfd_settime);
cond_syscall(sys_timerfd_gettime);
cond_syscall(compat_sys_timerfd_settime);
cond_syscall(compat_sys_timerfd_gettime);
cond_syscall(sys_eventfd);
cond_syscall(sys_eventfd2);
cond_syscall(sys_memfd_create);
cond_syscall(sys_userfaultfd);


cond_syscall(sys_perf_event_open);


cond_syscall(sys_fanotify_init);
cond_syscall(sys_fanotify_mark);
cond_syscall(compat_sys_fanotify_mark);


cond_syscall(sys_name_to_handle_at);
cond_syscall(sys_open_by_handle_at);
cond_syscall(compat_sys_open_by_handle_at);


cond_syscall(sys_kcmp);


cond_syscall(sys_seccomp);


cond_syscall(sys_bpf);


cond_syscall(sys_execveat);


cond_syscall(sys_membarrier);






static DEFINE_PER_CPU(__u32, taskstats_seqnum);
static int family_registered;
struct kmem_cache *taskstats_cache;

static struct genl_family family = {
 .id = GENL_ID_GENERATE,
 .name = TASKSTATS_GENL_NAME,
 .version = TASKSTATS_GENL_VERSION,
 .maxattr = TASKSTATS_CMD_ATTR_MAX,
};

static const struct nla_policy taskstats_cmd_get_policy[TASKSTATS_CMD_ATTR_MAX+1] = {
 [TASKSTATS_CMD_ATTR_PID] = { .type = NLA_U32 },
 [TASKSTATS_CMD_ATTR_TGID] = { .type = NLA_U32 },
 [TASKSTATS_CMD_ATTR_REGISTER_CPUMASK] = { .type = NLA_STRING },
 [TASKSTATS_CMD_ATTR_DEREGISTER_CPUMASK] = { .type = NLA_STRING },};

static const struct nla_policy cgroupstats_cmd_get_policy[CGROUPSTATS_CMD_ATTR_MAX+1] = {
 [CGROUPSTATS_CMD_ATTR_FD] = { .type = NLA_U32 },
};

struct listener {
 struct list_head list;
 pid_t pid;
 char valid;
};

struct listener_list {
 struct rw_semaphore sem;
 struct list_head list;
};
static DEFINE_PER_CPU(struct listener_list, listener_array);

enum actions {
 REGISTER,
 DEREGISTER,
 CPU_DONT_CARE
};

static int prepare_reply(struct genl_info *info, u8 cmd, struct sk_buff **skbp,
    size_t size)
{
 struct sk_buff *skb;
 void *reply;




 skb = genlmsg_new(size, GFP_KERNEL);
 if (!skb)
  return -ENOMEM;

 if (!info) {
  int seq = this_cpu_inc_return(taskstats_seqnum) - 1;

  reply = genlmsg_put(skb, 0, seq, &family, 0, cmd);
 } else
  reply = genlmsg_put_reply(skb, info, &family, 0, cmd);
 if (reply == NULL) {
  nlmsg_free(skb);
  return -EINVAL;
 }

 *skbp = skb;
 return 0;
}




static int send_reply(struct sk_buff *skb, struct genl_info *info)
{
 struct genlmsghdr *genlhdr = nlmsg_data(nlmsg_hdr(skb));
 void *reply = genlmsg_data(genlhdr);

 genlmsg_end(skb, reply);

 return genlmsg_reply(skb, info);
}




static void send_cpu_listeners(struct sk_buff *skb,
     struct listener_list *listeners)
{
 struct genlmsghdr *genlhdr = nlmsg_data(nlmsg_hdr(skb));
 struct listener *s, *tmp;
 struct sk_buff *skb_next, *skb_cur = skb;
 void *reply = genlmsg_data(genlhdr);
 int rc, delcount = 0;

 genlmsg_end(skb, reply);

 rc = 0;
 down_read(&listeners->sem);
 list_for_each_entry(s, &listeners->list, list) {
  skb_next = NULL;
  if (!list_is_last(&s->list, &listeners->list)) {
   skb_next = skb_clone(skb_cur, GFP_KERNEL);
   if (!skb_next)
    break;
  }
  rc = genlmsg_unicast(&init_net, skb_cur, s->pid);
  if (rc == -ECONNREFUSED) {
   s->valid = 0;
   delcount++;
  }
  skb_cur = skb_next;
 }
 up_read(&listeners->sem);

 if (skb_cur)
  nlmsg_free(skb_cur);

 if (!delcount)
  return;


 down_write(&listeners->sem);
 list_for_each_entry_safe(s, tmp, &listeners->list, list) {
  if (!s->valid) {
   list_del(&s->list);
   kfree(s);
  }
 }
 up_write(&listeners->sem);
}

static void fill_stats(struct user_namespace *user_ns,
         struct pid_namespace *pid_ns,
         struct task_struct *tsk, struct taskstats *stats)
{
 memset(stats, 0, sizeof(*stats));







 delayacct_add_tsk(stats, tsk);


 stats->version = TASKSTATS_VERSION;
 stats->nvcsw = tsk->nvcsw;
 stats->nivcsw = tsk->nivcsw;
 bacct_add_tsk(user_ns, pid_ns, stats, tsk);


 xacct_add_tsk(stats, tsk);
}

static int fill_stats_for_pid(pid_t pid, struct taskstats *stats)
{
 struct task_struct *tsk;

 rcu_read_lock();
 tsk = find_task_by_vpid(pid);
 if (tsk)
  get_task_struct(tsk);
 rcu_read_unlock();
 if (!tsk)
  return -ESRCH;
 fill_stats(current_user_ns(), task_active_pid_ns(current), tsk, stats);
 put_task_struct(tsk);
 return 0;
}

static int fill_stats_for_tgid(pid_t tgid, struct taskstats *stats)
{
 struct task_struct *tsk, *first;
 unsigned long flags;
 int rc = -ESRCH;





 rcu_read_lock();
 first = find_task_by_vpid(tgid);

 if (!first || !lock_task_sighand(first, &flags))
  goto out;

 if (first->signal->stats)
  memcpy(stats, first->signal->stats, sizeof(*stats));
 else
  memset(stats, 0, sizeof(*stats));

 tsk = first;
 do {
  if (tsk->exit_state)
   continue;






  delayacct_add_tsk(stats, tsk);

  stats->nvcsw += tsk->nvcsw;
  stats->nivcsw += tsk->nivcsw;
 } while_each_thread(first, tsk);

 unlock_task_sighand(first, &flags);
 rc = 0;
out:
 rcu_read_unlock();

 stats->version = TASKSTATS_VERSION;




 return rc;
}

static void fill_tgid_exit(struct task_struct *tsk)
{
 unsigned long flags;

 spin_lock_irqsave(&tsk->sighand->siglock, flags);
 if (!tsk->signal->stats)
  goto ret;







 delayacct_add_tsk(tsk->signal->stats, tsk);
ret:
 spin_unlock_irqrestore(&tsk->sighand->siglock, flags);
 return;
}

static int add_del_listener(pid_t pid, const struct cpumask *mask, int isadd)
{
 struct listener_list *listeners;
 struct listener *s, *tmp, *s2;
 unsigned int cpu;
 int ret = 0;

 if (!cpumask_subset(mask, cpu_possible_mask))
  return -EINVAL;

 if (current_user_ns() != &init_user_ns)
  return -EINVAL;

 if (task_active_pid_ns(current) != &init_pid_ns)
  return -EINVAL;

 if (isadd == REGISTER) {
  for_each_cpu(cpu, mask) {
   s = kmalloc_node(sizeof(struct listener),
     GFP_KERNEL, cpu_to_node(cpu));
   if (!s) {
    ret = -ENOMEM;
    goto cleanup;
   }
   s->pid = pid;
   s->valid = 1;

   listeners = &per_cpu(listener_array, cpu);
   down_write(&listeners->sem);
   list_for_each_entry(s2, &listeners->list, list) {
    if (s2->pid == pid && s2->valid)
     goto exists;
   }
   list_add(&s->list, &listeners->list);
   s = NULL;
exists:
   up_write(&listeners->sem);
   kfree(s);
  }
  return 0;
 }


cleanup:
 for_each_cpu(cpu, mask) {
  listeners = &per_cpu(listener_array, cpu);
  down_write(&listeners->sem);
  list_for_each_entry_safe(s, tmp, &listeners->list, list) {
   if (s->pid == pid) {
    list_del(&s->list);
    kfree(s);
    break;
   }
  }
  up_write(&listeners->sem);
 }
 return ret;
}

static int parse(struct nlattr *na, struct cpumask *mask)
{
 char *data;
 int len;
 int ret;

 if (na == NULL)
  return 1;
 len = nla_len(na);
 if (len > TASKSTATS_CPUMASK_MAXLEN)
  return -E2BIG;
 if (len < 1)
  return -EINVAL;
 data = kmalloc(len, GFP_KERNEL);
 if (!data)
  return -ENOMEM;
 nla_strlcpy(data, na, len);
 ret = cpulist_parse(data, mask);
 kfree(data);
 return ret;
}

static struct taskstats *mk_reply(struct sk_buff *skb, int type, u32 pid)
{
 struct nlattr *na, *ret;
 int aggr;

 aggr = (type == TASKSTATS_TYPE_PID)
   ? TASKSTATS_TYPE_AGGR_PID
   : TASKSTATS_TYPE_AGGR_TGID;

 na = nla_nest_start(skb, aggr);
 if (!na)
  goto err;

 if (nla_put(skb, type, sizeof(pid), &pid) < 0) {
  nla_nest_cancel(skb, na);
  goto err;
 }
 ret = nla_reserve_64bit(skb, TASKSTATS_TYPE_STATS,
    sizeof(struct taskstats), TASKSTATS_TYPE_NULL);
 if (!ret) {
  nla_nest_cancel(skb, na);
  goto err;
 }
 nla_nest_end(skb, na);

 return nla_data(ret);
err:
 return NULL;
}

static int cgroupstats_user_cmd(struct sk_buff *skb, struct genl_info *info)
{
 int rc = 0;
 struct sk_buff *rep_skb;
 struct cgroupstats *stats;
 struct nlattr *na;
 size_t size;
 u32 fd;
 struct fd f;

 na = info->attrs[CGROUPSTATS_CMD_ATTR_FD];
 if (!na)
  return -EINVAL;

 fd = nla_get_u32(info->attrs[CGROUPSTATS_CMD_ATTR_FD]);
 f = fdget(fd);
 if (!f.file)
  return 0;

 size = nla_total_size(sizeof(struct cgroupstats));

 rc = prepare_reply(info, CGROUPSTATS_CMD_NEW, &rep_skb,
    size);
 if (rc < 0)
  goto err;

 na = nla_reserve(rep_skb, CGROUPSTATS_TYPE_CGROUP_STATS,
    sizeof(struct cgroupstats));
 if (na == NULL) {
  nlmsg_free(rep_skb);
  rc = -EMSGSIZE;
  goto err;
 }

 stats = nla_data(na);
 memset(stats, 0, sizeof(*stats));

 rc = cgroupstats_build(stats, f.file->f_path.dentry);
 if (rc < 0) {
  nlmsg_free(rep_skb);
  goto err;
 }

 rc = send_reply(rep_skb, info);

err:
 fdput(f);
 return rc;
}

static int cmd_attr_register_cpumask(struct genl_info *info)
{
 cpumask_var_t mask;
 int rc;

 if (!alloc_cpumask_var(&mask, GFP_KERNEL))
  return -ENOMEM;
 rc = parse(info->attrs[TASKSTATS_CMD_ATTR_REGISTER_CPUMASK], mask);
 if (rc < 0)
  goto out;
 rc = add_del_listener(info->snd_portid, mask, REGISTER);
out:
 free_cpumask_var(mask);
 return rc;
}

static int cmd_attr_deregister_cpumask(struct genl_info *info)
{
 cpumask_var_t mask;
 int rc;

 if (!alloc_cpumask_var(&mask, GFP_KERNEL))
  return -ENOMEM;
 rc = parse(info->attrs[TASKSTATS_CMD_ATTR_DEREGISTER_CPUMASK], mask);
 if (rc < 0)
  goto out;
 rc = add_del_listener(info->snd_portid, mask, DEREGISTER);
out:
 free_cpumask_var(mask);
 return rc;
}

static size_t taskstats_packet_size(void)
{
 size_t size;

 size = nla_total_size(sizeof(u32)) +
  nla_total_size_64bit(sizeof(struct taskstats)) +
  nla_total_size(0);

 return size;
}

static int cmd_attr_pid(struct genl_info *info)
{
 struct taskstats *stats;
 struct sk_buff *rep_skb;
 size_t size;
 u32 pid;
 int rc;

 size = taskstats_packet_size();

 rc = prepare_reply(info, TASKSTATS_CMD_NEW, &rep_skb, size);
 if (rc < 0)
  return rc;

 rc = -EINVAL;
 pid = nla_get_u32(info->attrs[TASKSTATS_CMD_ATTR_PID]);
 stats = mk_reply(rep_skb, TASKSTATS_TYPE_PID, pid);
 if (!stats)
  goto err;

 rc = fill_stats_for_pid(pid, stats);
 if (rc < 0)
  goto err;
 return send_reply(rep_skb, info);
err:
 nlmsg_free(rep_skb);
 return rc;
}

static int cmd_attr_tgid(struct genl_info *info)
{
 struct taskstats *stats;
 struct sk_buff *rep_skb;
 size_t size;
 u32 tgid;
 int rc;

 size = taskstats_packet_size();

 rc = prepare_reply(info, TASKSTATS_CMD_NEW, &rep_skb, size);
 if (rc < 0)
  return rc;

 rc = -EINVAL;
 tgid = nla_get_u32(info->attrs[TASKSTATS_CMD_ATTR_TGID]);
 stats = mk_reply(rep_skb, TASKSTATS_TYPE_TGID, tgid);
 if (!stats)
  goto err;

 rc = fill_stats_for_tgid(tgid, stats);
 if (rc < 0)
  goto err;
 return send_reply(rep_skb, info);
err:
 nlmsg_free(rep_skb);
 return rc;
}

static int taskstats_user_cmd(struct sk_buff *skb, struct genl_info *info)
{
 if (info->attrs[TASKSTATS_CMD_ATTR_REGISTER_CPUMASK])
  return cmd_attr_register_cpumask(info);
 else if (info->attrs[TASKSTATS_CMD_ATTR_DEREGISTER_CPUMASK])
  return cmd_attr_deregister_cpumask(info);
 else if (info->attrs[TASKSTATS_CMD_ATTR_PID])
  return cmd_attr_pid(info);
 else if (info->attrs[TASKSTATS_CMD_ATTR_TGID])
  return cmd_attr_tgid(info);
 else
  return -EINVAL;
}

static struct taskstats *taskstats_tgid_alloc(struct task_struct *tsk)
{
 struct signal_struct *sig = tsk->signal;
 struct taskstats *stats;

 if (sig->stats || thread_group_empty(tsk))
  goto ret;


 stats = kmem_cache_zalloc(taskstats_cache, GFP_KERNEL);

 spin_lock_irq(&tsk->sighand->siglock);
 if (!sig->stats) {
  sig->stats = stats;
  stats = NULL;
 }
 spin_unlock_irq(&tsk->sighand->siglock);

 if (stats)
  kmem_cache_free(taskstats_cache, stats);
ret:
 return sig->stats;
}


void taskstats_exit(struct task_struct *tsk, int group_dead)
{
 int rc;
 struct listener_list *listeners;
 struct taskstats *stats;
 struct sk_buff *rep_skb;
 size_t size;
 int is_thread_group;

 if (!family_registered)
  return;




 size = taskstats_packet_size();

 is_thread_group = !!taskstats_tgid_alloc(tsk);
 if (is_thread_group) {

  size = 2 * size;

  fill_tgid_exit(tsk);
 }

 listeners = raw_cpu_ptr(&listener_array);
 if (list_empty(&listeners->list))
  return;

 rc = prepare_reply(NULL, TASKSTATS_CMD_NEW, &rep_skb, size);
 if (rc < 0)
  return;

 stats = mk_reply(rep_skb, TASKSTATS_TYPE_PID,
    task_pid_nr_ns(tsk, &init_pid_ns));
 if (!stats)
  goto err;

 fill_stats(&init_user_ns, &init_pid_ns, tsk, stats);




 if (!is_thread_group || !group_dead)
  goto send;

 stats = mk_reply(rep_skb, TASKSTATS_TYPE_TGID,
    task_tgid_nr_ns(tsk, &init_pid_ns));
 if (!stats)
  goto err;

 memcpy(stats, tsk->signal->stats, sizeof(*stats));

send:
 send_cpu_listeners(rep_skb, listeners);
 return;
err:
 nlmsg_free(rep_skb);
}

static const struct genl_ops taskstats_ops[] = {
 {
  .cmd = TASKSTATS_CMD_GET,
  .doit = taskstats_user_cmd,
  .policy = taskstats_cmd_get_policy,
  .flags = GENL_ADMIN_PERM,
 },
 {
  .cmd = CGROUPSTATS_CMD_GET,
  .doit = cgroupstats_user_cmd,
  .policy = cgroupstats_cmd_get_policy,
 },
};


void __init taskstats_init_early(void)
{
 unsigned int i;

 taskstats_cache = KMEM_CACHE(taskstats, SLAB_PANIC);
 for_each_possible_cpu(i) {
  INIT_LIST_HEAD(&(per_cpu(listener_array, i).list));
  init_rwsem(&(per_cpu(listener_array, i).sem));
 }
}

static int __init taskstats_init(void)
{
 int rc;

 rc = genl_register_family_with_ops(&family, taskstats_ops);
 if (rc)
  return rc;

 family_registered = 1;
 pr_info("registered taskstats version %d\n", TASKSTATS_GENL_VERSION);
 return 0;
}





late_initcall(taskstats_init);

static struct callback_head work_exited;
int
task_work_add(struct task_struct *task, struct callback_head *work, bool notify)
{
 struct callback_head *head;

 do {
  head = ACCESS_ONCE(task->task_works);
  if (unlikely(head == &work_exited))
   return -ESRCH;
  work->next = head;
 } while (cmpxchg(&task->task_works, head, work) != head);

 if (notify)
  set_notify_resume(task);
 return 0;
}
struct callback_head *
task_work_cancel(struct task_struct *task, task_work_func_t func)
{
 struct callback_head **pprev = &task->task_works;
 struct callback_head *work;
 unsigned long flags;






 raw_spin_lock_irqsave(&task->pi_lock, flags);
 while ((work = ACCESS_ONCE(*pprev))) {
  smp_read_barrier_depends();
  if (work->func != func)
   pprev = &work->next;
  else if (cmpxchg(pprev, work, work->next) == work)
   break;
 }
 raw_spin_unlock_irqrestore(&task->pi_lock, flags);

 return work;
}
void task_work_run(void)
{
 struct task_struct *task = current;
 struct callback_head *work, *head, *next;

 for (;;) {




  do {
   work = ACCESS_ONCE(task->task_works);
   head = !work && (task->flags & PF_EXITING) ?
    &work_exited : NULL;
  } while (cmpxchg(&task->task_works, work, head) != work);

  if (!work)
   break;





  raw_spin_unlock_wait(&task->pi_lock);
  smp_mb();

  do {
   next = work->next;
   work->func(work);
   work = next;
   cond_resched();
  } while (work);
 }
}



static u32 rand1, preh_val, posth_val, jph_val;
static int errors, handler_errors, num_tests;
static u32 (*target)(u32 value);
static u32 (*target2)(u32 value);

static noinline u32 kprobe_target(u32 value)
{
 return (value / div_factor);
}

static int kp_pre_handler(struct kprobe *p, struct pt_regs *regs)
{
 preh_val = (rand1 / div_factor);
 return 0;
}

static void kp_post_handler(struct kprobe *p, struct pt_regs *regs,
  unsigned long flags)
{
 if (preh_val != (rand1 / div_factor)) {
  handler_errors++;
  pr_err("incorrect value in post_handler\n");
 }
 posth_val = preh_val + div_factor;
}

static struct kprobe kp = {
 .symbol_name = "kprobe_target",
 .pre_handler = kp_pre_handler,
 .post_handler = kp_post_handler
};

static int test_kprobe(void)
{
 int ret;

 ret = register_kprobe(&kp);
 if (ret < 0) {
  pr_err("register_kprobe returned %d\n", ret);
  return ret;
 }

 ret = target(rand1);
 unregister_kprobe(&kp);

 if (preh_val == 0) {
  pr_err("kprobe pre_handler not called\n");
  handler_errors++;
 }

 if (posth_val == 0) {
  pr_err("kprobe post_handler not called\n");
  handler_errors++;
 }

 return 0;
}

static noinline u32 kprobe_target2(u32 value)
{
 return (value / div_factor) + 1;
}

static int kp_pre_handler2(struct kprobe *p, struct pt_regs *regs)
{
 preh_val = (rand1 / div_factor) + 1;
 return 0;
}

static void kp_post_handler2(struct kprobe *p, struct pt_regs *regs,
  unsigned long flags)
{
 if (preh_val != (rand1 / div_factor) + 1) {
  handler_errors++;
  pr_err("incorrect value in post_handler2\n");
 }
 posth_val = preh_val + div_factor;
}

static struct kprobe kp2 = {
 .symbol_name = "kprobe_target2",
 .pre_handler = kp_pre_handler2,
 .post_handler = kp_post_handler2
};

static int test_kprobes(void)
{
 int ret;
 struct kprobe *kps[2] = {&kp, &kp2};


 kp.addr = NULL;
 kp.flags = 0;
 ret = register_kprobes(kps, 2);
 if (ret < 0) {
  pr_err("register_kprobes returned %d\n", ret);
  return ret;
 }

 preh_val = 0;
 posth_val = 0;
 ret = target(rand1);

 if (preh_val == 0) {
  pr_err("kprobe pre_handler not called\n");
  handler_errors++;
 }

 if (posth_val == 0) {
  pr_err("kprobe post_handler not called\n");
  handler_errors++;
 }

 preh_val = 0;
 posth_val = 0;
 ret = target2(rand1);

 if (preh_val == 0) {
  pr_err("kprobe pre_handler2 not called\n");
  handler_errors++;
 }

 if (posth_val == 0) {
  pr_err("kprobe post_handler2 not called\n");
  handler_errors++;
 }

 unregister_kprobes(kps, 2);
 return 0;

}

static u32 j_kprobe_target(u32 value)
{
 if (value != rand1) {
  handler_errors++;
  pr_err("incorrect value in jprobe handler\n");
 }

 jph_val = rand1;
 jprobe_return();
 return 0;
}

static struct jprobe jp = {
 .entry = j_kprobe_target,
 .kp.symbol_name = "kprobe_target"
};

static int test_jprobe(void)
{
 int ret;

 ret = register_jprobe(&jp);
 if (ret < 0) {
  pr_err("register_jprobe returned %d\n", ret);
  return ret;
 }

 ret = target(rand1);
 unregister_jprobe(&jp);
 if (jph_val == 0) {
  pr_err("jprobe handler not called\n");
  handler_errors++;
 }

 return 0;
}

static struct jprobe jp2 = {
 .entry = j_kprobe_target,
 .kp.symbol_name = "kprobe_target2"
};

static int test_jprobes(void)
{
 int ret;
 struct jprobe *jps[2] = {&jp, &jp2};


 jp.kp.addr = NULL;
 jp.kp.flags = 0;
 ret = register_jprobes(jps, 2);
 if (ret < 0) {
  pr_err("register_jprobes returned %d\n", ret);
  return ret;
 }

 jph_val = 0;
 ret = target(rand1);
 if (jph_val == 0) {
  pr_err("jprobe handler not called\n");
  handler_errors++;
 }

 jph_val = 0;
 ret = target2(rand1);
 if (jph_val == 0) {
  pr_err("jprobe handler2 not called\n");
  handler_errors++;
 }
 unregister_jprobes(jps, 2);

 return 0;
}
static u32 krph_val;

static int entry_handler(struct kretprobe_instance *ri, struct pt_regs *regs)
{
 krph_val = (rand1 / div_factor);
 return 0;
}

static int return_handler(struct kretprobe_instance *ri, struct pt_regs *regs)
{
 unsigned long ret = regs_return_value(regs);

 if (ret != (rand1 / div_factor)) {
  handler_errors++;
  pr_err("incorrect value in kretprobe handler\n");
 }
 if (krph_val == 0) {
  handler_errors++;
  pr_err("call to kretprobe entry handler failed\n");
 }

 krph_val = rand1;
 return 0;
}

static struct kretprobe rp = {
 .handler = return_handler,
 .entry_handler = entry_handler,
 .kp.symbol_name = "kprobe_target"
};

static int test_kretprobe(void)
{
 int ret;

 ret = register_kretprobe(&rp);
 if (ret < 0) {
  pr_err("register_kretprobe returned %d\n", ret);
  return ret;
 }

 ret = target(rand1);
 unregister_kretprobe(&rp);
 if (krph_val != rand1) {
  pr_err("kretprobe handler not called\n");
  handler_errors++;
 }

 return 0;
}

static int return_handler2(struct kretprobe_instance *ri, struct pt_regs *regs)
{
 unsigned long ret = regs_return_value(regs);

 if (ret != (rand1 / div_factor) + 1) {
  handler_errors++;
  pr_err("incorrect value in kretprobe handler2\n");
 }
 if (krph_val == 0) {
  handler_errors++;
  pr_err("call to kretprobe entry handler failed\n");
 }

 krph_val = rand1;
 return 0;
}

static struct kretprobe rp2 = {
 .handler = return_handler2,
 .entry_handler = entry_handler,
 .kp.symbol_name = "kprobe_target2"
};

static int test_kretprobes(void)
{
 int ret;
 struct kretprobe *rps[2] = {&rp, &rp2};


 rp.kp.addr = NULL;
 rp.kp.flags = 0;
 ret = register_kretprobes(rps, 2);
 if (ret < 0) {
  pr_err("register_kretprobe returned %d\n", ret);
  return ret;
 }

 krph_val = 0;
 ret = target(rand1);
 if (krph_val != rand1) {
  pr_err("kretprobe handler not called\n");
  handler_errors++;
 }

 krph_val = 0;
 ret = target2(rand1);
 if (krph_val != rand1) {
  pr_err("kretprobe handler2 not called\n");
  handler_errors++;
 }
 unregister_kretprobes(rps, 2);
 return 0;
}

int init_test_probes(void)
{
 int ret;

 target = kprobe_target;
 target2 = kprobe_target2;

 do {
  rand1 = prandom_u32();
 } while (rand1 <= div_factor);

 pr_info("started\n");
 num_tests++;
 ret = test_kprobe();
 if (ret < 0)
  errors++;

 num_tests++;
 ret = test_kprobes();
 if (ret < 0)
  errors++;

 num_tests++;
 ret = test_jprobe();
 if (ret < 0)
  errors++;

 num_tests++;
 ret = test_jprobes();
 if (ret < 0)
  errors++;

 num_tests++;
 ret = test_kretprobe();
 if (ret < 0)
  errors++;

 num_tests++;
 ret = test_kretprobes();
 if (ret < 0)
  errors++;

 if (errors)
  pr_err("BUG: %d out of %d tests failed\n", errors, num_tests);
 else if (handler_errors)
  pr_err("BUG: %d error(s) running handlers\n", handler_errors);
 else
  pr_info("passed successfully\n");

 return 0;
}



static DEFINE_MUTEX(udelay_test_lock);
static struct dentry *udelay_test_debugfs_file;
static int udelay_test_usecs;
static int udelay_test_iterations = DEFAULT_ITERATIONS;

static int udelay_test_single(struct seq_file *s, int usecs, uint32_t iters)
{
 int min = 0, max = 0, fail_count = 0;
 uint64_t sum = 0;
 uint64_t avg;
 int i;

 int allowed_error_ns = usecs * 5;

 for (i = 0; i < iters; ++i) {
  struct timespec ts1, ts2;
  int time_passed;

  ktime_get_ts(&ts1);
  udelay(usecs);
  ktime_get_ts(&ts2);
  time_passed = timespec_to_ns(&ts2) - timespec_to_ns(&ts1);

  if (i == 0 || time_passed < min)
   min = time_passed;
  if (i == 0 || time_passed > max)
   max = time_passed;
  if ((time_passed + allowed_error_ns) / 1000 < usecs)
   ++fail_count;
  WARN_ON(time_passed < 0);
  sum += time_passed;
 }

 avg = sum;
 do_div(avg, iters);
 seq_printf(s, "%d usecs x %d: exp=%d allowed=%d min=%d avg=%lld max=%d",
   usecs, iters, usecs * 1000,
   (usecs * 1000) - allowed_error_ns, min, avg, max);
 if (fail_count)
  seq_printf(s, " FAIL=%d", fail_count);
 seq_puts(s, "\n");

 return 0;
}

static int udelay_test_show(struct seq_file *s, void *v)
{
 int usecs;
 int iters;
 int ret = 0;

 mutex_lock(&udelay_test_lock);
 usecs = udelay_test_usecs;
 iters = udelay_test_iterations;
 mutex_unlock(&udelay_test_lock);

 if (usecs > 0 && iters > 0) {
  return udelay_test_single(s, usecs, iters);
 } else if (usecs == 0) {
  struct timespec ts;

  ktime_get_ts(&ts);
  seq_printf(s, "udelay() test (lpj=%ld kt=%ld.%09ld)\n",
    loops_per_jiffy, ts.tv_sec, ts.tv_nsec);
  seq_puts(s, "usage:\n");
  seq_puts(s, "echo USECS [ITERS] > " DEBUGFS_FILENAME "\n");
  seq_puts(s, "cat " DEBUGFS_FILENAME "\n");
 }

 return ret;
}

static int udelay_test_open(struct inode *inode, struct file *file)
{
 return single_open(file, udelay_test_show, inode->i_private);
}

static ssize_t udelay_test_write(struct file *file, const char __user *buf,
  size_t count, loff_t *pos)
{
 char lbuf[32];
 int ret;
 int usecs;
 int iters;

 if (count >= sizeof(lbuf))
  return -EINVAL;

 if (copy_from_user(lbuf, buf, count))
  return -EFAULT;
 lbuf[count] = '\0';

 ret = sscanf(lbuf, "%d %d", &usecs, &iters);
 if (ret < 1)
  return -EINVAL;
 else if (ret < 2)
  iters = DEFAULT_ITERATIONS;

 mutex_lock(&udelay_test_lock);
 udelay_test_usecs = usecs;
 udelay_test_iterations = iters;
 mutex_unlock(&udelay_test_lock);

 return count;
}

static const struct file_operations udelay_test_debugfs_ops = {
 .owner = THIS_MODULE,
 .open = udelay_test_open,
 .read = seq_read,
 .write = udelay_test_write,
 .llseek = seq_lseek,
 .release = single_release,
};

static int __init udelay_test_init(void)
{
 mutex_lock(&udelay_test_lock);
 udelay_test_debugfs_file = debugfs_create_file(DEBUGFS_FILENAME,
   S_IRUSR, NULL, NULL, &udelay_test_debugfs_ops);
 mutex_unlock(&udelay_test_lock);

 return 0;
}

module_init(udelay_test_init);

static void __exit udelay_test_exit(void)
{
 mutex_lock(&udelay_test_lock);
 debugfs_remove(udelay_test_debugfs_file);
 mutex_unlock(&udelay_test_lock);
}

module_exit(udelay_test_exit);

MODULE_AUTHOR("David Riley <davidriley@chromium.org>");
MODULE_LICENSE("GPL");







static struct tick_device tick_broadcast_device;
static cpumask_var_t tick_broadcast_mask;
static cpumask_var_t tick_broadcast_on;
static cpumask_var_t tmpmask;
static DEFINE_RAW_SPINLOCK(tick_broadcast_lock);
static int tick_broadcast_forced;

static void tick_broadcast_clear_oneshot(int cpu);
static void tick_resume_broadcast_oneshot(struct clock_event_device *bc);
static inline void tick_broadcast_clear_oneshot(int cpu) { }
static inline void tick_resume_broadcast_oneshot(struct clock_event_device *bc) { }




struct tick_device *tick_get_broadcast_device(void)
{
 return &tick_broadcast_device;
}

struct cpumask *tick_get_broadcast_mask(void)
{
 return tick_broadcast_mask;
}




static void tick_broadcast_start_periodic(struct clock_event_device *bc)
{
 if (bc)
  tick_setup_periodic(bc, 1);
}




static bool tick_check_broadcast_device(struct clock_event_device *curdev,
     struct clock_event_device *newdev)
{
 if ((newdev->features & CLOCK_EVT_FEAT_DUMMY) ||
     (newdev->features & CLOCK_EVT_FEAT_PERCPU) ||
     (newdev->features & CLOCK_EVT_FEAT_C3STOP))
  return false;

 if (tick_broadcast_device.mode == TICKDEV_MODE_ONESHOT &&
     !(newdev->features & CLOCK_EVT_FEAT_ONESHOT))
  return false;

 return !curdev || newdev->rating > curdev->rating;
}




void tick_install_broadcast_device(struct clock_event_device *dev)
{
 struct clock_event_device *cur = tick_broadcast_device.evtdev;

 if (!tick_check_broadcast_device(cur, dev))
  return;

 if (!try_module_get(dev->owner))
  return;

 clockevents_exchange_device(cur, dev);
 if (cur)
  cur->event_handler = clockevents_handle_noop;
 tick_broadcast_device.evtdev = dev;
 if (!cpumask_empty(tick_broadcast_mask))
  tick_broadcast_start_periodic(dev);
 if (dev->features & CLOCK_EVT_FEAT_ONESHOT)
  tick_clock_notify();
}




int tick_is_broadcast_device(struct clock_event_device *dev)
{
 return (dev && tick_broadcast_device.evtdev == dev);
}

int tick_broadcast_update_freq(struct clock_event_device *dev, u32 freq)
{
 int ret = -ENODEV;

 if (tick_is_broadcast_device(dev)) {
  raw_spin_lock(&tick_broadcast_lock);
  ret = __clockevents_update_freq(dev, freq);
  raw_spin_unlock(&tick_broadcast_lock);
 }
 return ret;
}


static void err_broadcast(const struct cpumask *mask)
{
 pr_crit_once("Failed to broadcast timer tick. Some CPUs may be unresponsive.\n");
}

static void tick_device_setup_broadcast_func(struct clock_event_device *dev)
{
 if (!dev->broadcast)
  dev->broadcast = tick_broadcast;
 if (!dev->broadcast) {
  pr_warn_once("%s depends on broadcast, but no broadcast function available\n",
        dev->name);
  dev->broadcast = err_broadcast;
 }
}





int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
{
 struct clock_event_device *bc = tick_broadcast_device.evtdev;
 unsigned long flags;
 int ret = 0;

 raw_spin_lock_irqsave(&tick_broadcast_lock, flags);







 if (!tick_device_is_functional(dev)) {
  dev->event_handler = tick_handle_periodic;
  tick_device_setup_broadcast_func(dev);
  cpumask_set_cpu(cpu, tick_broadcast_mask);
  if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
   tick_broadcast_start_periodic(bc);
  else
   tick_broadcast_setup_oneshot(bc);
  ret = 1;
 } else {




  if (!(dev->features & CLOCK_EVT_FEAT_C3STOP))
   cpumask_clear_cpu(cpu, tick_broadcast_mask);
  else
   tick_device_setup_broadcast_func(dev);





  if (!cpumask_test_cpu(cpu, tick_broadcast_on))
   cpumask_clear_cpu(cpu, tick_broadcast_mask);

  switch (tick_broadcast_device.mode) {
  case TICKDEV_MODE_ONESHOT:
   tick_broadcast_clear_oneshot(cpu);
   ret = 0;
   break;

  case TICKDEV_MODE_PERIODIC:





   if (cpumask_empty(tick_broadcast_mask) && bc)
    clockevents_shutdown(bc);
   if (bc && !(bc->features & CLOCK_EVT_FEAT_HRTIMER))
    ret = cpumask_test_cpu(cpu, tick_broadcast_mask);
   break;
  default:
   break;
  }
 }
 raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 return ret;
}

int tick_receive_broadcast(void)
{
 struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 struct clock_event_device *evt = td->evtdev;

 if (!evt)
  return -ENODEV;

 if (!evt->event_handler)
  return -EINVAL;

 evt->event_handler(evt);
 return 0;
}




static bool tick_do_broadcast(struct cpumask *mask)
{
 int cpu = smp_processor_id();
 struct tick_device *td;
 bool local = false;




 if (cpumask_test_cpu(cpu, mask)) {
  struct clock_event_device *bc = tick_broadcast_device.evtdev;

  cpumask_clear_cpu(cpu, mask);
  local = !(bc->features & CLOCK_EVT_FEAT_HRTIMER);
 }

 if (!cpumask_empty(mask)) {






  td = &per_cpu(tick_cpu_device, cpumask_first(mask));
  td->evtdev->broadcast(mask);
 }
 return local;
}





static bool tick_do_periodic_broadcast(void)
{
 cpumask_and(tmpmask, cpu_online_mask, tick_broadcast_mask);
 return tick_do_broadcast(tmpmask);
}




static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
{
 struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 bool bc_local;

 raw_spin_lock(&tick_broadcast_lock);


 if (clockevent_state_shutdown(tick_broadcast_device.evtdev)) {
  raw_spin_unlock(&tick_broadcast_lock);
  return;
 }

 bc_local = tick_do_periodic_broadcast();

 if (clockevent_state_oneshot(dev)) {
  ktime_t next = ktime_add(dev->next_event, tick_period);

  clockevents_program_event(dev, next, true);
 }
 raw_spin_unlock(&tick_broadcast_lock);






 if (bc_local)
  td->evtdev->event_handler(td->evtdev);
}
void tick_broadcast_control(enum tick_broadcast_mode mode)
{
 struct clock_event_device *bc, *dev;
 struct tick_device *td;
 int cpu, bc_stopped;

 td = this_cpu_ptr(&tick_cpu_device);
 dev = td->evtdev;




 if (!dev || !(dev->features & CLOCK_EVT_FEAT_C3STOP))
  return;

 if (!tick_device_is_functional(dev))
  return;

 raw_spin_lock(&tick_broadcast_lock);
 cpu = smp_processor_id();
 bc = tick_broadcast_device.evtdev;
 bc_stopped = cpumask_empty(tick_broadcast_mask);

 switch (mode) {
 case TICK_BROADCAST_FORCE:
  tick_broadcast_forced = 1;
 case TICK_BROADCAST_ON:
  cpumask_set_cpu(cpu, tick_broadcast_on);
  if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_mask)) {
   if (bc && !(bc->features & CLOCK_EVT_FEAT_HRTIMER) &&
       tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
    clockevents_shutdown(dev);
  }
  break;

 case TICK_BROADCAST_OFF:
  if (tick_broadcast_forced)
   break;
  cpumask_clear_cpu(cpu, tick_broadcast_on);
  if (!tick_device_is_functional(dev))
   break;
  if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_mask)) {
   if (tick_broadcast_device.mode ==
       TICKDEV_MODE_PERIODIC)
    tick_setup_periodic(dev, 0);
  }
  break;
 }

 if (bc) {
  if (cpumask_empty(tick_broadcast_mask)) {
   if (!bc_stopped)
    clockevents_shutdown(bc);
  } else if (bc_stopped) {
   if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
    tick_broadcast_start_periodic(bc);
   else
    tick_broadcast_setup_oneshot(bc);
  }
 }
 raw_spin_unlock(&tick_broadcast_lock);
}
EXPORT_SYMBOL_GPL(tick_broadcast_control);




void tick_set_periodic_handler(struct clock_event_device *dev, int broadcast)
{
 if (!broadcast)
  dev->event_handler = tick_handle_periodic;
 else
  dev->event_handler = tick_handle_periodic_broadcast;
}




void tick_shutdown_broadcast(unsigned int cpu)
{
 struct clock_event_device *bc;
 unsigned long flags;

 raw_spin_lock_irqsave(&tick_broadcast_lock, flags);

 bc = tick_broadcast_device.evtdev;
 cpumask_clear_cpu(cpu, tick_broadcast_mask);
 cpumask_clear_cpu(cpu, tick_broadcast_on);

 if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC) {
  if (bc && cpumask_empty(tick_broadcast_mask))
   clockevents_shutdown(bc);
 }

 raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
}

void tick_suspend_broadcast(void)
{
 struct clock_event_device *bc;
 unsigned long flags;

 raw_spin_lock_irqsave(&tick_broadcast_lock, flags);

 bc = tick_broadcast_device.evtdev;
 if (bc)
  clockevents_shutdown(bc);

 raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
}
bool tick_resume_check_broadcast(void)
{
 if (tick_broadcast_device.mode == TICKDEV_MODE_ONESHOT)
  return false;
 else
  return cpumask_test_cpu(smp_processor_id(), tick_broadcast_mask);
}

void tick_resume_broadcast(void)
{
 struct clock_event_device *bc;
 unsigned long flags;

 raw_spin_lock_irqsave(&tick_broadcast_lock, flags);

 bc = tick_broadcast_device.evtdev;

 if (bc) {
  clockevents_tick_resume(bc);

  switch (tick_broadcast_device.mode) {
  case TICKDEV_MODE_PERIODIC:
   if (!cpumask_empty(tick_broadcast_mask))
    tick_broadcast_start_periodic(bc);
   break;
  case TICKDEV_MODE_ONESHOT:
   if (!cpumask_empty(tick_broadcast_mask))
    tick_resume_broadcast_oneshot(bc);
   break;
  }
 }
 raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
}


static cpumask_var_t tick_broadcast_oneshot_mask;
static cpumask_var_t tick_broadcast_pending_mask;
static cpumask_var_t tick_broadcast_force_mask;




struct cpumask *tick_get_broadcast_oneshot_mask(void)
{
 return tick_broadcast_oneshot_mask;
}
int tick_check_broadcast_expired(void)
{
 return cpumask_test_cpu(smp_processor_id(), tick_broadcast_force_mask);
}




static void tick_broadcast_set_affinity(struct clock_event_device *bc,
     const struct cpumask *cpumask)
{
 if (!(bc->features & CLOCK_EVT_FEAT_DYNIRQ))
  return;

 if (cpumask_equal(bc->cpumask, cpumask))
  return;

 bc->cpumask = cpumask;
 irq_set_affinity(bc->irq, bc->cpumask);
}

static void tick_broadcast_set_event(struct clock_event_device *bc, int cpu,
         ktime_t expires)
{
 if (!clockevent_state_oneshot(bc))
  clockevents_switch_state(bc, CLOCK_EVT_STATE_ONESHOT);

 clockevents_program_event(bc, expires, 1);
 tick_broadcast_set_affinity(bc, cpumask_of(cpu));
}

static void tick_resume_broadcast_oneshot(struct clock_event_device *bc)
{
 clockevents_switch_state(bc, CLOCK_EVT_STATE_ONESHOT);
}





void tick_check_oneshot_broadcast_this_cpu(void)
{
 if (cpumask_test_cpu(smp_processor_id(), tick_broadcast_oneshot_mask)) {
  struct tick_device *td = this_cpu_ptr(&tick_cpu_device);






  if (td->mode == TICKDEV_MODE_ONESHOT) {
   clockevents_switch_state(td->evtdev,
           CLOCK_EVT_STATE_ONESHOT);
  }
 }
}




static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
{
 struct tick_device *td;
 ktime_t now, next_event;
 int cpu, next_cpu = 0;
 bool bc_local;

 raw_spin_lock(&tick_broadcast_lock);
 dev->next_event.tv64 = KTIME_MAX;
 next_event.tv64 = KTIME_MAX;
 cpumask_clear(tmpmask);
 now = ktime_get();

 for_each_cpu(cpu, tick_broadcast_oneshot_mask) {
  td = &per_cpu(tick_cpu_device, cpu);
  if (td->evtdev->next_event.tv64 <= now.tv64) {
   cpumask_set_cpu(cpu, tmpmask);





   cpumask_set_cpu(cpu, tick_broadcast_pending_mask);
  } else if (td->evtdev->next_event.tv64 < next_event.tv64) {
   next_event.tv64 = td->evtdev->next_event.tv64;
   next_cpu = cpu;
  }
 }





 cpumask_clear_cpu(smp_processor_id(), tick_broadcast_pending_mask);


 cpumask_or(tmpmask, tmpmask, tick_broadcast_force_mask);
 cpumask_clear(tick_broadcast_force_mask);





 if (WARN_ON_ONCE(!cpumask_subset(tmpmask, cpu_online_mask)))
  cpumask_and(tmpmask, tmpmask, cpu_online_mask);




 bc_local = tick_do_broadcast(tmpmask);
 if (next_event.tv64 != KTIME_MAX)
  tick_broadcast_set_event(dev, next_cpu, next_event);

 raw_spin_unlock(&tick_broadcast_lock);

 if (bc_local) {
  td = this_cpu_ptr(&tick_cpu_device);
  td->evtdev->event_handler(td->evtdev);
 }
}

static int broadcast_needs_cpu(struct clock_event_device *bc, int cpu)
{
 if (!(bc->features & CLOCK_EVT_FEAT_HRTIMER))
  return 0;
 if (bc->next_event.tv64 == KTIME_MAX)
  return 0;
 return bc->bound_on == cpu ? -EBUSY : 0;
}

static void broadcast_shutdown_local(struct clock_event_device *bc,
         struct clock_event_device *dev)
{





 if (bc->features & CLOCK_EVT_FEAT_HRTIMER) {
  if (broadcast_needs_cpu(bc, smp_processor_id()))
   return;
  if (dev->next_event.tv64 < bc->next_event.tv64)
   return;
 }
 clockevents_switch_state(dev, CLOCK_EVT_STATE_SHUTDOWN);
}

int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
{
 struct clock_event_device *bc, *dev;
 int cpu, ret = 0;
 ktime_t now;





 if (!tick_broadcast_device.evtdev)
  return -EBUSY;

 dev = this_cpu_ptr(&tick_cpu_device)->evtdev;

 raw_spin_lock(&tick_broadcast_lock);
 bc = tick_broadcast_device.evtdev;
 cpu = smp_processor_id();

 if (state == TICK_BROADCAST_ENTER) {







  ret = broadcast_needs_cpu(bc, cpu);
  if (ret)
   goto out;





  if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC) {

   if (bc->features & CLOCK_EVT_FEAT_HRTIMER)
    ret = -EBUSY;
   goto out;
  }

  if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {
   WARN_ON_ONCE(cpumask_test_cpu(cpu, tick_broadcast_pending_mask));


   broadcast_shutdown_local(bc, dev);
   if (cpumask_test_cpu(cpu, tick_broadcast_force_mask)) {
    ret = -EBUSY;
   } else if (dev->next_event.tv64 < bc->next_event.tv64) {
    tick_broadcast_set_event(bc, cpu, dev->next_event);







    ret = broadcast_needs_cpu(bc, cpu);
    if (ret) {
     cpumask_clear_cpu(cpu,
      tick_broadcast_oneshot_mask);
    }
   }
  }
 } else {
  if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_oneshot_mask)) {
   clockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT);
   if (cpumask_test_and_clear_cpu(cpu,
           tick_broadcast_pending_mask))
    goto out;




   if (dev->next_event.tv64 == KTIME_MAX)
    goto out;
   now = ktime_get();
   if (dev->next_event.tv64 <= now.tv64) {
    cpumask_set_cpu(cpu, tick_broadcast_force_mask);
    goto out;
   }




   tick_program_event(dev->next_event, 1);
  }
 }
out:
 raw_spin_unlock(&tick_broadcast_lock);
 return ret;
}






static void tick_broadcast_clear_oneshot(int cpu)
{
 cpumask_clear_cpu(cpu, tick_broadcast_oneshot_mask);
 cpumask_clear_cpu(cpu, tick_broadcast_pending_mask);
}

static void tick_broadcast_init_next_event(struct cpumask *mask,
        ktime_t expires)
{
 struct tick_device *td;
 int cpu;

 for_each_cpu(cpu, mask) {
  td = &per_cpu(tick_cpu_device, cpu);
  if (td->evtdev)
   td->evtdev->next_event = expires;
 }
}




void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
{
 int cpu = smp_processor_id();


 if (bc->event_handler != tick_handle_oneshot_broadcast) {
  int was_periodic = clockevent_state_periodic(bc);

  bc->event_handler = tick_handle_oneshot_broadcast;







  cpumask_copy(tmpmask, tick_broadcast_mask);
  cpumask_clear_cpu(cpu, tmpmask);
  cpumask_or(tick_broadcast_oneshot_mask,
      tick_broadcast_oneshot_mask, tmpmask);

  if (was_periodic && !cpumask_empty(tmpmask)) {
   clockevents_switch_state(bc, CLOCK_EVT_STATE_ONESHOT);
   tick_broadcast_init_next_event(tmpmask,
             tick_next_period);
   tick_broadcast_set_event(bc, cpu, tick_next_period);
  } else
   bc->next_event.tv64 = KTIME_MAX;
 } else {







  tick_broadcast_clear_oneshot(cpu);
 }
}




void tick_broadcast_switch_to_oneshot(void)
{
 struct clock_event_device *bc;
 unsigned long flags;

 raw_spin_lock_irqsave(&tick_broadcast_lock, flags);

 tick_broadcast_device.mode = TICKDEV_MODE_ONESHOT;
 bc = tick_broadcast_device.evtdev;
 if (bc)
  tick_broadcast_setup_oneshot(bc);

 raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
}

void hotplug_cpu__broadcast_tick_pull(int deadcpu)
{
 struct clock_event_device *bc;
 unsigned long flags;

 raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 bc = tick_broadcast_device.evtdev;

 if (bc && broadcast_needs_cpu(bc, deadcpu)) {

  clockevents_program_event(bc, bc->next_event, 1);
 }
 raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
}




void tick_shutdown_broadcast_oneshot(unsigned int cpu)
{
 unsigned long flags;

 raw_spin_lock_irqsave(&tick_broadcast_lock, flags);





 cpumask_clear_cpu(cpu, tick_broadcast_oneshot_mask);
 cpumask_clear_cpu(cpu, tick_broadcast_pending_mask);
 cpumask_clear_cpu(cpu, tick_broadcast_force_mask);

 raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
}




int tick_broadcast_oneshot_active(void)
{
 return tick_broadcast_device.mode == TICKDEV_MODE_ONESHOT;
}




bool tick_broadcast_oneshot_available(void)
{
 struct clock_event_device *bc = tick_broadcast_device.evtdev;

 return bc ? bc->features & CLOCK_EVT_FEAT_ONESHOT : false;
}

int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
{
 struct clock_event_device *bc = tick_broadcast_device.evtdev;

 if (!bc || (bc->features & CLOCK_EVT_FEAT_HRTIMER))
  return -EBUSY;

 return 0;
}

void __init tick_broadcast_init(void)
{
 zalloc_cpumask_var(&tick_broadcast_mask, GFP_NOWAIT);
 zalloc_cpumask_var(&tick_broadcast_on, GFP_NOWAIT);
 zalloc_cpumask_var(&tmpmask, GFP_NOWAIT);
 zalloc_cpumask_var(&tick_broadcast_oneshot_mask, GFP_NOWAIT);
 zalloc_cpumask_var(&tick_broadcast_pending_mask, GFP_NOWAIT);
 zalloc_cpumask_var(&tick_broadcast_force_mask, GFP_NOWAIT);
}







static struct hrtimer bctimer;

static int bc_shutdown(struct clock_event_device *evt)
{
 hrtimer_try_to_cancel(&bctimer);
 return 0;
}





static int bc_set_next(ktime_t expires, struct clock_event_device *bc)
{
 int bc_moved;
 RCU_NONIDLE({
   bc_moved = hrtimer_try_to_cancel(&bctimer) >= 0;
   if (bc_moved)
    hrtimer_start(&bctimer, expires,
           HRTIMER_MODE_ABS_PINNED);});
 if (bc_moved) {

  bc->bound_on = smp_processor_id();
 } else if (bc->bound_on == smp_processor_id()) {
  hrtimer_set_expires(&bctimer, expires);
 }
 return 0;
}

static struct clock_event_device ce_broadcast_hrtimer = {
 .set_state_shutdown = bc_shutdown,
 .set_next_ktime = bc_set_next,
 .features = CLOCK_EVT_FEAT_ONESHOT |
      CLOCK_EVT_FEAT_KTIME |
      CLOCK_EVT_FEAT_HRTIMER,
 .rating = 0,
 .bound_on = -1,
 .min_delta_ns = 1,
 .max_delta_ns = KTIME_MAX,
 .min_delta_ticks = 1,
 .max_delta_ticks = ULONG_MAX,
 .mult = 1,
 .shift = 0,
 .cpumask = cpu_all_mask,
};

static enum hrtimer_restart bc_handler(struct hrtimer *t)
{
 ce_broadcast_hrtimer.event_handler(&ce_broadcast_hrtimer);

 if (clockevent_state_oneshot(&ce_broadcast_hrtimer))
  if (ce_broadcast_hrtimer.next_event.tv64 != KTIME_MAX)
   return HRTIMER_RESTART;

 return HRTIMER_NORESTART;
}

void tick_setup_hrtimer_broadcast(void)
{
 hrtimer_init(&bctimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
 bctimer.function = bc_handler;
 clockevents_register_device(&ce_broadcast_hrtimer);
}






DEFINE_PER_CPU(struct tick_device, tick_cpu_device);



ktime_t tick_next_period;
ktime_t tick_period;
int tick_do_timer_cpu __read_mostly = TICK_DO_TIMER_BOOT;




struct tick_device *tick_get_device(int cpu)
{
 return &per_cpu(tick_cpu_device, cpu);
}




int tick_is_oneshot_available(void)
{
 struct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);

 if (!dev || !(dev->features & CLOCK_EVT_FEAT_ONESHOT))
  return 0;
 if (!(dev->features & CLOCK_EVT_FEAT_C3STOP))
  return 1;
 return tick_broadcast_oneshot_available();
}




static void tick_periodic(int cpu)
{
 if (tick_do_timer_cpu == cpu) {
  write_seqlock(&jiffies_lock);


  tick_next_period = ktime_add(tick_next_period, tick_period);

  do_timer(1);
  write_sequnlock(&jiffies_lock);
  update_wall_time();
 }

 update_process_times(user_mode(get_irq_regs()));
 profile_tick(CPU_PROFILING);
}




void tick_handle_periodic(struct clock_event_device *dev)
{
 int cpu = smp_processor_id();
 ktime_t next = dev->next_event;

 tick_periodic(cpu);






 if (dev->event_handler != tick_handle_periodic)
  return;

 if (!clockevent_state_oneshot(dev))
  return;
 for (;;) {




  next = ktime_add(next, tick_period);

  if (!clockevents_program_event(dev, next, false))
   return;
  if (timekeeping_valid_for_hres())
   tick_periodic(cpu);
 }
}




void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
{
 tick_set_periodic_handler(dev, broadcast);


 if (!tick_device_is_functional(dev))
  return;

 if ((dev->features & CLOCK_EVT_FEAT_PERIODIC) &&
     !tick_broadcast_oneshot_active()) {
  clockevents_switch_state(dev, CLOCK_EVT_STATE_PERIODIC);
 } else {
  unsigned long seq;
  ktime_t next;

  do {
   seq = read_seqbegin(&jiffies_lock);
   next = tick_next_period;
  } while (read_seqretry(&jiffies_lock, seq));

  clockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT);

  for (;;) {
   if (!clockevents_program_event(dev, next, false))
    return;
   next = ktime_add(next, tick_period);
  }
 }
}




static void tick_setup_device(struct tick_device *td,
         struct clock_event_device *newdev, int cpu,
         const struct cpumask *cpumask)
{
 ktime_t next_event;
 void (*handler)(struct clock_event_device *) = NULL;




 if (!td->evtdev) {




  if (tick_do_timer_cpu == TICK_DO_TIMER_BOOT) {
   if (!tick_nohz_full_cpu(cpu))
    tick_do_timer_cpu = cpu;
   else
    tick_do_timer_cpu = TICK_DO_TIMER_NONE;
   tick_next_period = ktime_get();
   tick_period = ktime_set(0, NSEC_PER_SEC / HZ);
  }




  td->mode = TICKDEV_MODE_PERIODIC;
 } else {
  handler = td->evtdev->event_handler;
  next_event = td->evtdev->next_event;
  td->evtdev->event_handler = clockevents_handle_noop;
 }

 td->evtdev = newdev;





 if (!cpumask_equal(newdev->cpumask, cpumask))
  irq_set_affinity(newdev->irq, cpumask);
 if (tick_device_uses_broadcast(newdev, cpu))
  return;

 if (td->mode == TICKDEV_MODE_PERIODIC)
  tick_setup_periodic(newdev, 0);
 else
  tick_setup_oneshot(newdev, handler, next_event);
}

void tick_install_replacement(struct clock_event_device *newdev)
{
 struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 int cpu = smp_processor_id();

 clockevents_exchange_device(td->evtdev, newdev);
 tick_setup_device(td, newdev, cpu, cpumask_of(cpu));
 if (newdev->features & CLOCK_EVT_FEAT_ONESHOT)
  tick_oneshot_notify();
}

static bool tick_check_percpu(struct clock_event_device *curdev,
         struct clock_event_device *newdev, int cpu)
{
 if (!cpumask_test_cpu(cpu, newdev->cpumask))
  return false;
 if (cpumask_equal(newdev->cpumask, cpumask_of(cpu)))
  return true;

 if (newdev->irq >= 0 && !irq_can_set_affinity(newdev->irq))
  return false;

 if (curdev && cpumask_equal(curdev->cpumask, cpumask_of(cpu)))
  return false;
 return true;
}

static bool tick_check_preferred(struct clock_event_device *curdev,
     struct clock_event_device *newdev)
{

 if (!(newdev->features & CLOCK_EVT_FEAT_ONESHOT)) {
  if (curdev && (curdev->features & CLOCK_EVT_FEAT_ONESHOT))
   return false;
  if (tick_oneshot_mode_active())
   return false;
 }





 return !curdev ||
  newdev->rating > curdev->rating ||
        !cpumask_equal(curdev->cpumask, newdev->cpumask);
}





bool tick_check_replacement(struct clock_event_device *curdev,
       struct clock_event_device *newdev)
{
 if (!tick_check_percpu(curdev, newdev, smp_processor_id()))
  return false;

 return tick_check_preferred(curdev, newdev);
}





void tick_check_new_device(struct clock_event_device *newdev)
{
 struct clock_event_device *curdev;
 struct tick_device *td;
 int cpu;

 cpu = smp_processor_id();
 td = &per_cpu(tick_cpu_device, cpu);
 curdev = td->evtdev;


 if (!tick_check_percpu(curdev, newdev, cpu))
  goto out_bc;


 if (!tick_check_preferred(curdev, newdev))
  goto out_bc;

 if (!try_module_get(newdev->owner))
  return;






 if (tick_is_broadcast_device(curdev)) {
  clockevents_shutdown(curdev);
  curdev = NULL;
 }
 clockevents_exchange_device(curdev, newdev);
 tick_setup_device(td, newdev, cpu, cpumask_of(cpu));
 if (newdev->features & CLOCK_EVT_FEAT_ONESHOT)
  tick_oneshot_notify();
 return;

out_bc:



 tick_install_broadcast_device(newdev);
}
int tick_broadcast_oneshot_control(enum tick_broadcast_state state)
{
 struct tick_device *td = this_cpu_ptr(&tick_cpu_device);

 if (!(td->evtdev->features & CLOCK_EVT_FEAT_C3STOP))
  return 0;

 return __tick_broadcast_oneshot_control(state);
}
EXPORT_SYMBOL_GPL(tick_broadcast_oneshot_control);







void tick_handover_do_timer(void)
{
 if (tick_do_timer_cpu == smp_processor_id()) {
  int cpu = cpumask_first(cpu_online_mask);

  tick_do_timer_cpu = (cpu < nr_cpu_ids) ? cpu :
   TICK_DO_TIMER_NONE;
 }
}
void tick_shutdown(unsigned int cpu)
{
 struct tick_device *td = &per_cpu(tick_cpu_device, cpu);
 struct clock_event_device *dev = td->evtdev;

 td->mode = TICKDEV_MODE_PERIODIC;
 if (dev) {




  clockevent_set_state(dev, CLOCK_EVT_STATE_DETACHED);
  clockevents_exchange_device(dev, NULL);
  dev->event_handler = clockevents_handle_noop;
  td->evtdev = NULL;
 }
}
void tick_suspend_local(void)
{
 struct tick_device *td = this_cpu_ptr(&tick_cpu_device);

 clockevents_shutdown(td->evtdev);
}
void tick_resume_local(void)
{
 struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 bool broadcast = tick_resume_check_broadcast();

 clockevents_tick_resume(td->evtdev);
 if (!broadcast) {
  if (td->mode == TICKDEV_MODE_PERIODIC)
   tick_setup_periodic(td->evtdev, 0);
  else
   tick_resume_oneshot();
 }
}
void tick_suspend(void)
{
 tick_suspend_local();
 tick_suspend_broadcast();
}
void tick_resume(void)
{
 tick_resume_broadcast();
 tick_resume_local();
}

static DEFINE_RAW_SPINLOCK(tick_freeze_lock);
static unsigned int tick_freeze_depth;
void tick_freeze(void)
{
 raw_spin_lock(&tick_freeze_lock);

 tick_freeze_depth++;
 if (tick_freeze_depth == num_online_cpus()) {
  trace_suspend_resume(TPS("timekeeping_freeze"),
         smp_processor_id(), true);
  timekeeping_suspend();
 } else {
  tick_suspend_local();
 }

 raw_spin_unlock(&tick_freeze_lock);
}
void tick_unfreeze(void)
{
 raw_spin_lock(&tick_freeze_lock);

 if (tick_freeze_depth == num_online_cpus()) {
  timekeeping_resume();
  trace_suspend_resume(TPS("timekeeping_freeze"),
         smp_processor_id(), false);
 } else {
  tick_resume_local();
 }

 tick_freeze_depth--;

 raw_spin_unlock(&tick_freeze_lock);
}




void __init tick_init(void)
{
 tick_broadcast_init();
 tick_nohz_init();
}





int tick_program_event(ktime_t expires, int force)
{
 struct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);

 if (unlikely(expires.tv64 == KTIME_MAX)) {



  clockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT_STOPPED);
  return 0;
 }

 if (unlikely(clockevent_state_oneshot_stopped(dev))) {




  clockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT);
 }

 return clockevents_program_event(dev, expires, force);
}




void tick_resume_oneshot(void)
{
 struct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);

 clockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT);
 clockevents_program_event(dev, ktime_get(), true);
}




void tick_setup_oneshot(struct clock_event_device *newdev,
   void (*handler)(struct clock_event_device *),
   ktime_t next_event)
{
 newdev->event_handler = handler;
 clockevents_switch_state(newdev, CLOCK_EVT_STATE_ONESHOT);
 clockevents_program_event(newdev, next_event, true);
}




int tick_switch_to_oneshot(void (*handler)(struct clock_event_device *))
{
 struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 struct clock_event_device *dev = td->evtdev;

 if (!dev || !(dev->features & CLOCK_EVT_FEAT_ONESHOT) ||
      !tick_device_is_functional(dev)) {

  printk(KERN_INFO "Clockevents: "
         "could not switch to one-shot mode:");
  if (!dev) {
   printk(" no tick device\n");
  } else {
   if (!tick_device_is_functional(dev))
    printk(" %s is not functional.\n", dev->name);
   else
    printk(" %s does not support one-shot mode.\n",
           dev->name);
  }
  return -EINVAL;
 }

 td->mode = TICKDEV_MODE_ONESHOT;
 dev->event_handler = handler;
 clockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT);
 tick_broadcast_switch_to_oneshot();
 return 0;
}






int tick_oneshot_mode_active(void)
{
 unsigned long flags;
 int ret;

 local_irq_save(flags);
 ret = __this_cpu_read(tick_cpu_device.mode) == TICKDEV_MODE_ONESHOT;
 local_irq_restore(flags);

 return ret;
}






int tick_init_highres(void)
{
 return tick_switch_to_oneshot(hrtimer_interrupt);
}







static DEFINE_PER_CPU(struct tick_sched, tick_cpu_sched);

struct tick_sched *tick_get_tick_sched(int cpu)
{
 return &per_cpu(tick_cpu_sched, cpu);
}




static ktime_t last_jiffies_update;




static void tick_do_update_jiffies64(ktime_t now)
{
 unsigned long ticks = 0;
 ktime_t delta;




 delta = ktime_sub(now, last_jiffies_update);
 if (delta.tv64 < tick_period.tv64)
  return;


 write_seqlock(&jiffies_lock);

 delta = ktime_sub(now, last_jiffies_update);
 if (delta.tv64 >= tick_period.tv64) {

  delta = ktime_sub(delta, tick_period);
  last_jiffies_update = ktime_add(last_jiffies_update,
      tick_period);


  if (unlikely(delta.tv64 >= tick_period.tv64)) {
   s64 incr = ktime_to_ns(tick_period);

   ticks = ktime_divns(delta, incr);

   last_jiffies_update = ktime_add_ns(last_jiffies_update,
          incr * ticks);
  }
  do_timer(++ticks);


  tick_next_period = ktime_add(last_jiffies_update, tick_period);
 } else {
  write_sequnlock(&jiffies_lock);
  return;
 }
 write_sequnlock(&jiffies_lock);
 update_wall_time();
}




static ktime_t tick_init_jiffy_update(void)
{
 ktime_t period;

 write_seqlock(&jiffies_lock);

 if (last_jiffies_update.tv64 == 0)
  last_jiffies_update = tick_next_period;
 period = last_jiffies_update;
 write_sequnlock(&jiffies_lock);
 return period;
}


static void tick_sched_do_timer(ktime_t now)
{
 int cpu = smp_processor_id();








 if (unlikely(tick_do_timer_cpu == TICK_DO_TIMER_NONE)
     && !tick_nohz_full_cpu(cpu))
  tick_do_timer_cpu = cpu;


 if (tick_do_timer_cpu == cpu)
  tick_do_update_jiffies64(now);
}

static void tick_sched_handle(struct tick_sched *ts, struct pt_regs *regs)
{
 if (ts->tick_stopped) {
  touch_softlockup_watchdog_sched();
  if (is_idle_task(current))
   ts->idle_jiffies++;
 }
 update_process_times(user_mode(regs));
 profile_tick(CPU_PROFILING);
}

cpumask_var_t tick_nohz_full_mask;
cpumask_var_t housekeeping_mask;
bool tick_nohz_full_running;
static atomic_t tick_dep_mask;

static bool check_tick_dependency(atomic_t *dep)
{
 int val = atomic_read(dep);

 if (val & TICK_DEP_MASK_POSIX_TIMER) {
  trace_tick_stop(0, TICK_DEP_MASK_POSIX_TIMER);
  return true;
 }

 if (val & TICK_DEP_MASK_PERF_EVENTS) {
  trace_tick_stop(0, TICK_DEP_MASK_PERF_EVENTS);
  return true;
 }

 if (val & TICK_DEP_MASK_SCHED) {
  trace_tick_stop(0, TICK_DEP_MASK_SCHED);
  return true;
 }

 if (val & TICK_DEP_MASK_CLOCK_UNSTABLE) {
  trace_tick_stop(0, TICK_DEP_MASK_CLOCK_UNSTABLE);
  return true;
 }

 return false;
}

static bool can_stop_full_tick(struct tick_sched *ts)
{
 WARN_ON_ONCE(!irqs_disabled());

 if (check_tick_dependency(&tick_dep_mask))
  return false;

 if (check_tick_dependency(&ts->tick_dep_mask))
  return false;

 if (check_tick_dependency(&current->tick_dep_mask))
  return false;

 if (check_tick_dependency(&current->signal->tick_dep_mask))
  return false;

 return true;
}

static void nohz_full_kick_func(struct irq_work *work)
{

}

static DEFINE_PER_CPU(struct irq_work, nohz_full_kick_work) = {
 .func = nohz_full_kick_func,
};







static void tick_nohz_full_kick(void)
{
 if (!tick_nohz_full_cpu(smp_processor_id()))
  return;

 irq_work_queue(this_cpu_ptr(&nohz_full_kick_work));
}





void tick_nohz_full_kick_cpu(int cpu)
{
 if (!tick_nohz_full_cpu(cpu))
  return;

 irq_work_queue_on(&per_cpu(nohz_full_kick_work, cpu), cpu);
}





static void tick_nohz_full_kick_all(void)
{
 int cpu;

 if (!tick_nohz_full_running)
  return;

 preempt_disable();
 for_each_cpu_and(cpu, tick_nohz_full_mask, cpu_online_mask)
  tick_nohz_full_kick_cpu(cpu);
 preempt_enable();
}

static void tick_nohz_dep_set_all(atomic_t *dep,
      enum tick_dep_bits bit)
{
 int prev;

 prev = atomic_fetch_or(BIT(bit), dep);
 if (!prev)
  tick_nohz_full_kick_all();
}





void tick_nohz_dep_set(enum tick_dep_bits bit)
{
 tick_nohz_dep_set_all(&tick_dep_mask, bit);
}

void tick_nohz_dep_clear(enum tick_dep_bits bit)
{
 atomic_andnot(BIT(bit), &tick_dep_mask);
}





void tick_nohz_dep_set_cpu(int cpu, enum tick_dep_bits bit)
{
 int prev;
 struct tick_sched *ts;

 ts = per_cpu_ptr(&tick_cpu_sched, cpu);

 prev = atomic_fetch_or(BIT(bit), &ts->tick_dep_mask);
 if (!prev) {
  preempt_disable();

  if (cpu == smp_processor_id()) {
   tick_nohz_full_kick();
  } else {

   if (!WARN_ON_ONCE(in_nmi()))
    tick_nohz_full_kick_cpu(cpu);
  }
  preempt_enable();
 }
}

void tick_nohz_dep_clear_cpu(int cpu, enum tick_dep_bits bit)
{
 struct tick_sched *ts = per_cpu_ptr(&tick_cpu_sched, cpu);

 atomic_andnot(BIT(bit), &ts->tick_dep_mask);
}





void tick_nohz_dep_set_task(struct task_struct *tsk, enum tick_dep_bits bit)
{




 tick_nohz_dep_set_all(&tsk->tick_dep_mask, bit);
}

void tick_nohz_dep_clear_task(struct task_struct *tsk, enum tick_dep_bits bit)
{
 atomic_andnot(BIT(bit), &tsk->tick_dep_mask);
}





void tick_nohz_dep_set_signal(struct signal_struct *sig, enum tick_dep_bits bit)
{
 tick_nohz_dep_set_all(&sig->tick_dep_mask, bit);
}

void tick_nohz_dep_clear_signal(struct signal_struct *sig, enum tick_dep_bits bit)
{
 atomic_andnot(BIT(bit), &sig->tick_dep_mask);
}






void __tick_nohz_task_switch(void)
{
 unsigned long flags;
 struct tick_sched *ts;

 local_irq_save(flags);

 if (!tick_nohz_full_cpu(smp_processor_id()))
  goto out;

 ts = this_cpu_ptr(&tick_cpu_sched);

 if (ts->tick_stopped) {
  if (atomic_read(&current->tick_dep_mask) ||
      atomic_read(&current->signal->tick_dep_mask))
   tick_nohz_full_kick();
 }
out:
 local_irq_restore(flags);
}


static int __init tick_nohz_full_setup(char *str)
{
 alloc_bootmem_cpumask_var(&tick_nohz_full_mask);
 if (cpulist_parse(str, tick_nohz_full_mask) < 0) {
  pr_warn("NO_HZ: Incorrect nohz_full cpumask\n");
  free_bootmem_cpumask_var(tick_nohz_full_mask);
  return 1;
 }
 tick_nohz_full_running = true;

 return 1;
}
__setup("nohz_full=", tick_nohz_full_setup);

static int tick_nohz_cpu_down_callback(struct notifier_block *nfb,
           unsigned long action,
           void *hcpu)
{
 unsigned int cpu = (unsigned long)hcpu;

 switch (action & ~CPU_TASKS_FROZEN) {
 case CPU_DOWN_PREPARE:





  if (tick_nohz_full_running && tick_do_timer_cpu == cpu)
   return NOTIFY_BAD;
  break;
 }
 return NOTIFY_OK;
}

static int tick_nohz_init_all(void)
{
 int err = -1;

 if (!alloc_cpumask_var(&tick_nohz_full_mask, GFP_KERNEL)) {
  WARN(1, "NO_HZ: Can't allocate full dynticks cpumask\n");
  return err;
 }
 err = 0;
 cpumask_setall(tick_nohz_full_mask);
 tick_nohz_full_running = true;
 return err;
}

void __init tick_nohz_init(void)
{
 int cpu;

 if (!tick_nohz_full_running) {
  if (tick_nohz_init_all() < 0)
   return;
 }

 if (!alloc_cpumask_var(&housekeeping_mask, GFP_KERNEL)) {
  WARN(1, "NO_HZ: Can't allocate not-full dynticks cpumask\n");
  cpumask_clear(tick_nohz_full_mask);
  tick_nohz_full_running = false;
  return;
 }






 if (!arch_irq_work_has_interrupt()) {
  pr_warn("NO_HZ: Can't run full dynticks because arch doesn't support irq work self-IPIs\n");
  cpumask_clear(tick_nohz_full_mask);
  cpumask_copy(housekeeping_mask, cpu_possible_mask);
  tick_nohz_full_running = false;
  return;
 }

 cpu = smp_processor_id();

 if (cpumask_test_cpu(cpu, tick_nohz_full_mask)) {
  pr_warn("NO_HZ: Clearing %d from nohz_full range for timekeeping\n",
   cpu);
  cpumask_clear_cpu(cpu, tick_nohz_full_mask);
 }

 cpumask_andnot(housekeeping_mask,
         cpu_possible_mask, tick_nohz_full_mask);

 for_each_cpu(cpu, tick_nohz_full_mask)
  context_tracking_cpu_set(cpu);

 cpu_notifier(tick_nohz_cpu_down_callback, 0);
 pr_info("NO_HZ: Full dynticks CPUs: %*pbl.\n",
  cpumask_pr_args(tick_nohz_full_mask));





 WARN_ON_ONCE(cpumask_empty(housekeeping_mask));
}







bool tick_nohz_enabled __read_mostly = true;
unsigned long tick_nohz_active __read_mostly;



static int __init setup_tick_nohz(char *str)
{
 return (kstrtobool(str, &tick_nohz_enabled) == 0);
}

__setup("nohz=", setup_tick_nohz);

int tick_nohz_tick_stopped(void)
{
 return __this_cpu_read(tick_cpu_sched.tick_stopped);
}
static void tick_nohz_update_jiffies(ktime_t now)
{
 unsigned long flags;

 __this_cpu_write(tick_cpu_sched.idle_waketime, now);

 local_irq_save(flags);
 tick_do_update_jiffies64(now);
 local_irq_restore(flags);

 touch_softlockup_watchdog_sched();
}




static void
update_ts_time_stats(int cpu, struct tick_sched *ts, ktime_t now, u64 *last_update_time)
{
 ktime_t delta;

 if (ts->idle_active) {
  delta = ktime_sub(now, ts->idle_entrytime);
  if (nr_iowait_cpu(cpu) > 0)
   ts->iowait_sleeptime = ktime_add(ts->iowait_sleeptime, delta);
  else
   ts->idle_sleeptime = ktime_add(ts->idle_sleeptime, delta);
  ts->idle_entrytime = now;
 }

 if (last_update_time)
  *last_update_time = ktime_to_us(now);

}

static void tick_nohz_stop_idle(struct tick_sched *ts, ktime_t now)
{
 update_ts_time_stats(smp_processor_id(), ts, now, NULL);
 ts->idle_active = 0;

 sched_clock_idle_wakeup_event(0);
}

static ktime_t tick_nohz_start_idle(struct tick_sched *ts)
{
 ktime_t now = ktime_get();

 ts->idle_entrytime = now;
 ts->idle_active = 1;
 sched_clock_idle_sleep_event();
 return now;
}
u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time)
{
 struct tick_sched *ts = &per_cpu(tick_cpu_sched, cpu);
 ktime_t now, idle;

 if (!tick_nohz_active)
  return -1;

 now = ktime_get();
 if (last_update_time) {
  update_ts_time_stats(cpu, ts, now, last_update_time);
  idle = ts->idle_sleeptime;
 } else {
  if (ts->idle_active && !nr_iowait_cpu(cpu)) {
   ktime_t delta = ktime_sub(now, ts->idle_entrytime);

   idle = ktime_add(ts->idle_sleeptime, delta);
  } else {
   idle = ts->idle_sleeptime;
  }
 }

 return ktime_to_us(idle);

}
EXPORT_SYMBOL_GPL(get_cpu_idle_time_us);
u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time)
{
 struct tick_sched *ts = &per_cpu(tick_cpu_sched, cpu);
 ktime_t now, iowait;

 if (!tick_nohz_active)
  return -1;

 now = ktime_get();
 if (last_update_time) {
  update_ts_time_stats(cpu, ts, now, last_update_time);
  iowait = ts->iowait_sleeptime;
 } else {
  if (ts->idle_active && nr_iowait_cpu(cpu) > 0) {
   ktime_t delta = ktime_sub(now, ts->idle_entrytime);

   iowait = ktime_add(ts->iowait_sleeptime, delta);
  } else {
   iowait = ts->iowait_sleeptime;
  }
 }

 return ktime_to_us(iowait);
}
EXPORT_SYMBOL_GPL(get_cpu_iowait_time_us);

static void tick_nohz_restart(struct tick_sched *ts, ktime_t now)
{
 hrtimer_cancel(&ts->sched_timer);
 hrtimer_set_expires(&ts->sched_timer, ts->last_tick);


 hrtimer_forward(&ts->sched_timer, now, tick_period);

 if (ts->nohz_mode == NOHZ_MODE_HIGHRES)
  hrtimer_start_expires(&ts->sched_timer, HRTIMER_MODE_ABS_PINNED);
 else
  tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
}

static ktime_t tick_nohz_stop_sched_tick(struct tick_sched *ts,
      ktime_t now, int cpu)
{
 struct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);
 u64 basemono, next_tick, next_tmr, next_rcu, delta, expires;
 unsigned long seq, basejiff;
 ktime_t tick;


 do {
  seq = read_seqbegin(&jiffies_lock);
  basemono = last_jiffies_update.tv64;
  basejiff = jiffies;
 } while (read_seqretry(&jiffies_lock, seq));
 ts->last_jiffies = basejiff;

 if (rcu_needs_cpu(basemono, &next_rcu) ||
     arch_needs_cpu() || irq_work_needs_cpu()) {
  next_tick = basemono + TICK_NSEC;
 } else {







  next_tmr = get_next_timer_interrupt(basejiff, basemono);
  ts->next_timer = next_tmr;

  next_tick = next_rcu < next_tmr ? next_rcu : next_tmr;
 }





 delta = next_tick - basemono;
 if (delta <= (u64)TICK_NSEC) {
  tick.tv64 = 0;




  if (!ts->tick_stopped)
   goto out;
  if (delta == 0) {
   tick_nohz_restart(ts, now);
   goto out;
  }
 }
 delta = timekeeping_max_deferment();
 if (cpu == tick_do_timer_cpu) {
  tick_do_timer_cpu = TICK_DO_TIMER_NONE;
  ts->do_timer_last = 1;
 } else if (tick_do_timer_cpu != TICK_DO_TIMER_NONE) {
  delta = KTIME_MAX;
  ts->do_timer_last = 0;
 } else if (!ts->do_timer_last) {
  delta = KTIME_MAX;
 }


 if (!ts->inidle)
  delta = min(delta, scheduler_tick_max_deferment());


 if (delta < (KTIME_MAX - basemono))
  expires = basemono + delta;
 else
  expires = KTIME_MAX;

 expires = min_t(u64, expires, next_tick);
 tick.tv64 = expires;


 if (ts->tick_stopped && (expires == dev->next_event.tv64))
  goto out;
 if (!ts->tick_stopped) {
  nohz_balance_enter_idle(cpu);
  calc_load_enter_idle();
  cpu_load_update_nohz_start();

  ts->last_tick = hrtimer_get_expires(&ts->sched_timer);
  ts->tick_stopped = 1;
  trace_tick_stop(1, TICK_DEP_MASK_NONE);
 }





 if (unlikely(expires == KTIME_MAX)) {
  if (ts->nohz_mode == NOHZ_MODE_HIGHRES)
   hrtimer_cancel(&ts->sched_timer);
  goto out;
 }

 if (ts->nohz_mode == NOHZ_MODE_HIGHRES)
  hrtimer_start(&ts->sched_timer, tick, HRTIMER_MODE_ABS_PINNED);
 else
  tick_program_event(tick, 1);
out:

 ts->sleep_length = ktime_sub(dev->next_event, now);
 return tick;
}

static void tick_nohz_restart_sched_tick(struct tick_sched *ts, ktime_t now)
{

 tick_do_update_jiffies64(now);
 cpu_load_update_nohz_stop();

 calc_load_exit_idle();
 touch_softlockup_watchdog_sched();



 ts->tick_stopped = 0;
 ts->idle_exittime = now;

 tick_nohz_restart(ts, now);
}

static void tick_nohz_full_update_tick(struct tick_sched *ts)
{
 int cpu = smp_processor_id();

 if (!tick_nohz_full_cpu(cpu))
  return;

 if (!ts->tick_stopped && ts->nohz_mode == NOHZ_MODE_INACTIVE)
  return;

 if (can_stop_full_tick(ts))
  tick_nohz_stop_sched_tick(ts, ktime_get(), cpu);
 else if (ts->tick_stopped)
  tick_nohz_restart_sched_tick(ts, ktime_get());
}

static bool can_stop_idle_tick(int cpu, struct tick_sched *ts)
{







 if (unlikely(!cpu_online(cpu))) {
  if (cpu == tick_do_timer_cpu)
   tick_do_timer_cpu = TICK_DO_TIMER_NONE;
  return false;
 }

 if (unlikely(ts->nohz_mode == NOHZ_MODE_INACTIVE)) {
  ts->sleep_length = (ktime_t) { .tv64 = NSEC_PER_SEC/HZ };
  return false;
 }

 if (need_resched())
  return false;

 if (unlikely(local_softirq_pending() && cpu_online(cpu))) {
  static int ratelimit;

  if (ratelimit < 10 &&
      (local_softirq_pending() & SOFTIRQ_STOP_IDLE_MASK)) {
   pr_warn("NOHZ: local_softirq_pending %02x\n",
    (unsigned int) local_softirq_pending());
   ratelimit++;
  }
  return false;
 }

 if (tick_nohz_full_enabled()) {




  if (tick_do_timer_cpu == cpu)
   return false;




  if (tick_do_timer_cpu == TICK_DO_TIMER_NONE)
   return false;
 }

 return true;
}

static void __tick_nohz_idle_enter(struct tick_sched *ts)
{
 ktime_t now, expires;
 int cpu = smp_processor_id();

 now = tick_nohz_start_idle(ts);

 if (can_stop_idle_tick(cpu, ts)) {
  int was_stopped = ts->tick_stopped;

  ts->idle_calls++;

  expires = tick_nohz_stop_sched_tick(ts, now, cpu);
  if (expires.tv64 > 0LL) {
   ts->idle_sleeps++;
   ts->idle_expires = expires;
  }

  if (!was_stopped && ts->tick_stopped)
   ts->idle_jiffies = ts->last_jiffies;
 }
}
void tick_nohz_idle_enter(void)
{
 struct tick_sched *ts;

 WARN_ON_ONCE(irqs_disabled());







 set_cpu_sd_state_idle();

 local_irq_disable();

 ts = this_cpu_ptr(&tick_cpu_sched);
 ts->inidle = 1;
 __tick_nohz_idle_enter(ts);

 local_irq_enable();
}
void tick_nohz_irq_exit(void)
{
 struct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);

 if (ts->inidle)
  __tick_nohz_idle_enter(ts);
 else
  tick_nohz_full_update_tick(ts);
}






ktime_t tick_nohz_get_sleep_length(void)
{
 struct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);

 return ts->sleep_length;
}

static void tick_nohz_account_idle_ticks(struct tick_sched *ts)
{
 unsigned long ticks;

 if (vtime_accounting_cpu_enabled())
  return;





 ticks = jiffies - ts->idle_jiffies;



 if (ticks && ticks < LONG_MAX)
  account_idle_ticks(ticks);
}
void tick_nohz_idle_exit(void)
{
 struct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);
 ktime_t now;

 local_irq_disable();

 WARN_ON_ONCE(!ts->inidle);

 ts->inidle = 0;

 if (ts->idle_active || ts->tick_stopped)
  now = ktime_get();

 if (ts->idle_active)
  tick_nohz_stop_idle(ts, now);

 if (ts->tick_stopped) {
  tick_nohz_restart_sched_tick(ts, now);
  tick_nohz_account_idle_ticks(ts);
 }

 local_irq_enable();
}




static void tick_nohz_handler(struct clock_event_device *dev)
{
 struct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);
 struct pt_regs *regs = get_irq_regs();
 ktime_t now = ktime_get();

 dev->next_event.tv64 = KTIME_MAX;

 tick_sched_do_timer(now);
 tick_sched_handle(ts, regs);


 if (unlikely(ts->tick_stopped))
  return;

 hrtimer_forward(&ts->sched_timer, now, tick_period);
 tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
}

static inline void tick_nohz_activate(struct tick_sched *ts, int mode)
{
 if (!tick_nohz_enabled)
  return;
 ts->nohz_mode = mode;

 if (!test_and_set_bit(0, &tick_nohz_active))
  timers_update_migration(true);
}




static void tick_nohz_switch_to_nohz(void)
{
 struct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);
 ktime_t next;

 if (!tick_nohz_enabled)
  return;

 if (tick_switch_to_oneshot(tick_nohz_handler))
  return;





 hrtimer_init(&ts->sched_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);

 next = tick_init_jiffy_update();

 hrtimer_set_expires(&ts->sched_timer, next);
 hrtimer_forward_now(&ts->sched_timer, tick_period);
 tick_program_event(hrtimer_get_expires(&ts->sched_timer), 1);
 tick_nohz_activate(ts, NOHZ_MODE_LOWRES);
}
static void tick_nohz_kick_tick(struct tick_sched *ts, ktime_t now)
{

 ktime_t delta;





 delta = ktime_sub(hrtimer_get_expires(&ts->sched_timer), now);
 if (delta.tv64 <= tick_period.tv64)
  return;

 tick_nohz_restart(ts, now);
}

static inline void tick_nohz_irq_enter(void)
{
 struct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);
 ktime_t now;

 if (!ts->idle_active && !ts->tick_stopped)
  return;
 now = ktime_get();
 if (ts->idle_active)
  tick_nohz_stop_idle(ts, now);
 if (ts->tick_stopped) {
  tick_nohz_update_jiffies(now);
  tick_nohz_kick_tick(ts, now);
 }
}


static inline void tick_nohz_switch_to_nohz(void) { }
static inline void tick_nohz_irq_enter(void) { }
static inline void tick_nohz_activate(struct tick_sched *ts, int mode) { }





void tick_irq_enter(void)
{
 tick_check_oneshot_broadcast_this_cpu();
 tick_nohz_irq_enter();
}








static enum hrtimer_restart tick_sched_timer(struct hrtimer *timer)
{
 struct tick_sched *ts =
  container_of(timer, struct tick_sched, sched_timer);
 struct pt_regs *regs = get_irq_regs();
 ktime_t now = ktime_get();

 tick_sched_do_timer(now);





 if (regs)
  tick_sched_handle(ts, regs);


 if (unlikely(ts->tick_stopped))
  return HRTIMER_NORESTART;

 hrtimer_forward(timer, now, tick_period);

 return HRTIMER_RESTART;
}

static int sched_skew_tick;

static int __init skew_tick(char *str)
{
 get_option(&str, &sched_skew_tick);

 return 0;
}
early_param("skew_tick", skew_tick);




void tick_setup_sched_timer(void)
{
 struct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);
 ktime_t now = ktime_get();




 hrtimer_init(&ts->sched_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
 ts->sched_timer.function = tick_sched_timer;


 hrtimer_set_expires(&ts->sched_timer, tick_init_jiffy_update());


 if (sched_skew_tick) {
  u64 offset = ktime_to_ns(tick_period) >> 1;
  do_div(offset, num_possible_cpus());
  offset *= smp_processor_id();
  hrtimer_add_expires_ns(&ts->sched_timer, offset);
 }

 hrtimer_forward(&ts->sched_timer, now, tick_period);
 hrtimer_start_expires(&ts->sched_timer, HRTIMER_MODE_ABS_PINNED);
 tick_nohz_activate(ts, NOHZ_MODE_HIGHRES);
}

void tick_cancel_sched_timer(int cpu)
{
 struct tick_sched *ts = &per_cpu(tick_cpu_sched, cpu);

 if (ts->sched_timer.base)
  hrtimer_cancel(&ts->sched_timer);

 memset(ts, 0, sizeof(*ts));
}




void tick_clock_notify(void)
{
 int cpu;

 for_each_possible_cpu(cpu)
  set_bit(0, &per_cpu(tick_cpu_sched, cpu).check_clocks);
}




void tick_oneshot_notify(void)
{
 struct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);

 set_bit(0, &ts->check_clocks);
}
int tick_check_oneshot_change(int allow_nohz)
{
 struct tick_sched *ts = this_cpu_ptr(&tick_cpu_sched);

 if (!test_and_clear_bit(0, &ts->check_clocks))
  return 0;

 if (ts->nohz_mode != NOHZ_MODE_INACTIVE)
  return 0;

 if (!timekeeping_valid_for_hres() || !tick_is_oneshot_available())
  return 0;

 if (!allow_nohz)
  return 1;

 tick_nohz_switch_to_nohz();
 return 0;
}







struct timezone sys_tz;

EXPORT_SYMBOL(sys_tz);








SYSCALL_DEFINE1(time, time_t __user *, tloc)
{
 time_t i = get_seconds();

 if (tloc) {
  if (put_user(i,tloc))
   return -EFAULT;
 }
 force_successful_syscall_return();
 return i;
}
SYSCALL_DEFINE1(stime, time_t __user *, tptr)
{
 struct timespec tv;
 int err;

 if (get_user(tv.tv_sec, tptr))
  return -EFAULT;

 tv.tv_nsec = 0;

 err = security_settime(&tv, NULL);
 if (err)
  return err;

 do_settimeofday(&tv);
 return 0;
}


SYSCALL_DEFINE2(gettimeofday, struct timeval __user *, tv,
  struct timezone __user *, tz)
{
 if (likely(tv != NULL)) {
  struct timeval ktv;
  do_gettimeofday(&ktv);
  if (copy_to_user(tv, &ktv, sizeof(ktv)))
   return -EFAULT;
 }
 if (unlikely(tz != NULL)) {
  if (copy_to_user(tz, &sys_tz, sizeof(sys_tz)))
   return -EFAULT;
 }
 return 0;
}





int persistent_clock_is_local;
static inline void warp_clock(void)
{
 if (sys_tz.tz_minuteswest != 0) {
  struct timespec adjust;

  persistent_clock_is_local = 1;
  adjust.tv_sec = sys_tz.tz_minuteswest * 60;
  adjust.tv_nsec = 0;
  timekeeping_inject_offset(&adjust);
 }
}
int do_sys_settimeofday64(const struct timespec64 *tv, const struct timezone *tz)
{
 static int firsttime = 1;
 int error = 0;

 if (tv && !timespec64_valid(tv))
  return -EINVAL;

 error = security_settime64(tv, tz);
 if (error)
  return error;

 if (tz) {

  if (tz->tz_minuteswest > 15*60 || tz->tz_minuteswest < -15*60)
   return -EINVAL;

  sys_tz = *tz;
  update_vsyscall_tz();
  if (firsttime) {
   firsttime = 0;
   if (!tv)
    warp_clock();
  }
 }
 if (tv)
  return do_settimeofday64(tv);
 return 0;
}

SYSCALL_DEFINE2(settimeofday, struct timeval __user *, tv,
  struct timezone __user *, tz)
{
 struct timeval user_tv;
 struct timespec new_ts;
 struct timezone new_tz;

 if (tv) {
  if (copy_from_user(&user_tv, tv, sizeof(*tv)))
   return -EFAULT;

  if (!timeval_valid(&user_tv))
   return -EINVAL;

  new_ts.tv_sec = user_tv.tv_sec;
  new_ts.tv_nsec = user_tv.tv_usec * NSEC_PER_USEC;
 }
 if (tz) {
  if (copy_from_user(&new_tz, tz, sizeof(*tz)))
   return -EFAULT;
 }

 return do_sys_settimeofday(tv ? &new_ts : NULL, tz ? &new_tz : NULL);
}

SYSCALL_DEFINE1(adjtimex, struct timex __user *, txc_p)
{
 struct timex txc;
 int ret;





 if(copy_from_user(&txc, txc_p, sizeof(struct timex)))
  return -EFAULT;
 ret = do_adjtimex(&txc);
 return copy_to_user(txc_p, &txc, sizeof(struct timex)) ? -EFAULT : ret;
}
struct timespec current_fs_time(struct super_block *sb)
{
 struct timespec now = current_kernel_time();
 return timespec_trunc(now, sb->s_time_gran);
}
EXPORT_SYMBOL(current_fs_time);







unsigned int jiffies_to_msecs(const unsigned long j)
{
 return (MSEC_PER_SEC / HZ) * j;
 return (j + (HZ / MSEC_PER_SEC) - 1)/(HZ / MSEC_PER_SEC);
 return (HZ_TO_MSEC_MUL32 * j) >> HZ_TO_MSEC_SHR32;
 return (j * HZ_TO_MSEC_NUM) / HZ_TO_MSEC_DEN;
}
EXPORT_SYMBOL(jiffies_to_msecs);

unsigned int jiffies_to_usecs(const unsigned long j)
{




 BUILD_BUG_ON(HZ > USEC_PER_SEC);

 return (USEC_PER_SEC / HZ) * j;
 return (HZ_TO_USEC_MUL32 * j) >> HZ_TO_USEC_SHR32;
 return (j * HZ_TO_USEC_NUM) / HZ_TO_USEC_DEN;
}
EXPORT_SYMBOL(jiffies_to_usecs);
struct timespec timespec_trunc(struct timespec t, unsigned gran)
{

 if (gran == 1) {

 } else if (gran == NSEC_PER_SEC) {
  t.tv_nsec = 0;
 } else if (gran > 1 && gran < NSEC_PER_SEC) {
  t.tv_nsec -= t.tv_nsec % gran;
 } else {
  WARN(1, "illegal file time granularity: %u", gran);
 }
 return t;
}
EXPORT_SYMBOL(timespec_trunc);
time64_t mktime64(const unsigned int year0, const unsigned int mon0,
  const unsigned int day, const unsigned int hour,
  const unsigned int min, const unsigned int sec)
{
 unsigned int mon = mon0, year = year0;


 if (0 >= (int) (mon -= 2)) {
  mon += 12;
  year -= 1;
 }

 return ((((time64_t)
    (year/4 - year/100 + year/400 + 367*mon/12 + day) +
    year*365 - 719499
     )*24 + hour
   )*60 + min
 )*60 + sec;
}
EXPORT_SYMBOL(mktime64);
void set_normalized_timespec(struct timespec *ts, time_t sec, s64 nsec)
{
 while (nsec >= NSEC_PER_SEC) {





  asm("" : "+rm"(nsec));
  nsec -= NSEC_PER_SEC;
  ++sec;
 }
 while (nsec < 0) {
  asm("" : "+rm"(nsec));
  nsec += NSEC_PER_SEC;
  --sec;
 }
 ts->tv_sec = sec;
 ts->tv_nsec = nsec;
}
EXPORT_SYMBOL(set_normalized_timespec);







struct timespec ns_to_timespec(const s64 nsec)
{
 struct timespec ts;
 s32 rem;

 if (!nsec)
  return (struct timespec) {0, 0};

 ts.tv_sec = div_s64_rem(nsec, NSEC_PER_SEC, &rem);
 if (unlikely(rem < 0)) {
  ts.tv_sec--;
  rem += NSEC_PER_SEC;
 }
 ts.tv_nsec = rem;

 return ts;
}
EXPORT_SYMBOL(ns_to_timespec);







struct timeval ns_to_timeval(const s64 nsec)
{
 struct timespec ts = ns_to_timespec(nsec);
 struct timeval tv;

 tv.tv_sec = ts.tv_sec;
 tv.tv_usec = (suseconds_t) ts.tv_nsec / 1000;

 return tv;
}
EXPORT_SYMBOL(ns_to_timeval);

void set_normalized_timespec64(struct timespec64 *ts, time64_t sec, s64 nsec)
{
 while (nsec >= NSEC_PER_SEC) {





  asm("" : "+rm"(nsec));
  nsec -= NSEC_PER_SEC;
  ++sec;
 }
 while (nsec < 0) {
  asm("" : "+rm"(nsec));
  nsec += NSEC_PER_SEC;
  --sec;
 }
 ts->tv_sec = sec;
 ts->tv_nsec = nsec;
}
EXPORT_SYMBOL(set_normalized_timespec64);







struct timespec64 ns_to_timespec64(const s64 nsec)
{
 struct timespec64 ts;
 s32 rem;

 if (!nsec)
  return (struct timespec64) {0, 0};

 ts.tv_sec = div_s64_rem(nsec, NSEC_PER_SEC, &rem);
 if (unlikely(rem < 0)) {
  ts.tv_sec--;
  rem += NSEC_PER_SEC;
 }
 ts.tv_nsec = rem;

 return ts;
}
EXPORT_SYMBOL(ns_to_timespec64);
unsigned long __msecs_to_jiffies(const unsigned int m)
{



 if ((int)m < 0)
  return MAX_JIFFY_OFFSET;
 return _msecs_to_jiffies(m);
}
EXPORT_SYMBOL(__msecs_to_jiffies);

unsigned long __usecs_to_jiffies(const unsigned int u)
{
 if (u > jiffies_to_usecs(MAX_JIFFY_OFFSET))
  return MAX_JIFFY_OFFSET;
 return _usecs_to_jiffies(u);
}
EXPORT_SYMBOL(__usecs_to_jiffies);
static unsigned long
__timespec64_to_jiffies(u64 sec, long nsec)
{
 nsec = nsec + TICK_NSEC - 1;

 if (sec >= MAX_SEC_IN_JIFFIES){
  sec = MAX_SEC_IN_JIFFIES;
  nsec = 0;
 }
 return ((sec * SEC_CONVERSION) +
  (((u64)nsec * NSEC_CONVERSION) >>
   (NSEC_JIFFIE_SC - SEC_JIFFIE_SC))) >> SEC_JIFFIE_SC;

}

static unsigned long
__timespec_to_jiffies(unsigned long sec, long nsec)
{
 return __timespec64_to_jiffies((u64)sec, nsec);
}

unsigned long
timespec64_to_jiffies(const struct timespec64 *value)
{
 return __timespec64_to_jiffies(value->tv_sec, value->tv_nsec);
}
EXPORT_SYMBOL(timespec64_to_jiffies);

void
jiffies_to_timespec64(const unsigned long jiffies, struct timespec64 *value)
{




 u32 rem;
 value->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,
        NSEC_PER_SEC, &rem);
 value->tv_nsec = rem;
}
EXPORT_SYMBOL(jiffies_to_timespec64);
unsigned long
timeval_to_jiffies(const struct timeval *value)
{
 return __timespec_to_jiffies(value->tv_sec,
         value->tv_usec * NSEC_PER_USEC);
}
EXPORT_SYMBOL(timeval_to_jiffies);

void jiffies_to_timeval(const unsigned long jiffies, struct timeval *value)
{




 u32 rem;

 value->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,
        NSEC_PER_SEC, &rem);
 value->tv_usec = rem / NSEC_PER_USEC;
}
EXPORT_SYMBOL(jiffies_to_timeval);




clock_t jiffies_to_clock_t(unsigned long x)
{
 return x * (USER_HZ / HZ);
 return x / (HZ / USER_HZ);
 return div_u64((u64)x * TICK_NSEC, NSEC_PER_SEC / USER_HZ);
}
EXPORT_SYMBOL(jiffies_to_clock_t);

unsigned long clock_t_to_jiffies(unsigned long x)
{
 if (x >= ~0UL / (HZ / USER_HZ))
  return ~0UL;
 return x * (HZ / USER_HZ);

 if (x >= ~0UL / HZ * USER_HZ)
  return ~0UL;


 return div_u64((u64)x * HZ, USER_HZ);
}
EXPORT_SYMBOL(clock_t_to_jiffies);

u64 jiffies_64_to_clock_t(u64 x)
{
 x = div_u64(x * USER_HZ, HZ);
 x = div_u64(x, HZ / USER_HZ);






 x = div_u64(x * TICK_NSEC, (NSEC_PER_SEC / USER_HZ));
 return x;
}
EXPORT_SYMBOL(jiffies_64_to_clock_t);

u64 nsec_to_clock_t(u64 x)
{
 return div_u64(x, NSEC_PER_SEC / USER_HZ);
 return div_u64(x * USER_HZ / 512, NSEC_PER_SEC / 512);





 return div_u64(x * 9, (9ull * NSEC_PER_SEC + (USER_HZ / 2)) / USER_HZ);
}
u64 nsecs_to_jiffies64(u64 n)
{

 return div_u64(n, NSEC_PER_SEC / HZ);

 return div_u64(n * HZ / 512, NSEC_PER_SEC / 512);




 return div_u64(n * 9, (9ull * NSEC_PER_SEC + HZ / 2) / HZ);
}
EXPORT_SYMBOL(nsecs_to_jiffies64);
unsigned long nsecs_to_jiffies(u64 n)
{
 return (unsigned long)nsecs_to_jiffies64(n);
}
EXPORT_SYMBOL_GPL(nsecs_to_jiffies);





struct timespec timespec_add_safe(const struct timespec lhs,
      const struct timespec rhs)
{
 struct timespec res;

 set_normalized_timespec(&res, lhs.tv_sec + rhs.tv_sec,
    lhs.tv_nsec + rhs.tv_nsec);

 if (res.tv_sec < lhs.tv_sec || res.tv_sec < rhs.tv_sec)
  res.tv_sec = TIME_T_MAX;

 return res;
}






struct timespec64 timespec64_add_safe(const struct timespec64 lhs,
    const struct timespec64 rhs)
{
 struct timespec64 res;

 set_normalized_timespec64(&res, lhs.tv_sec + rhs.tv_sec,
   lhs.tv_nsec + rhs.tv_nsec);

 if (unlikely(res.tv_sec < lhs.tv_sec || res.tv_sec < rhs.tv_sec)) {
  res.tv_sec = TIME64_MAX;
  res.tv_nsec = 0;
 }

 return res;
}





static int __isleap(long year)
{
 return (year) % 4 == 0 && ((year) % 100 != 0 || (year) % 400 == 0);
}


static long math_div(long a, long b)
{
 return a / b - (a % b < 0);
}


static long leaps_between(long y1, long y2)
{
 long leaps1 = math_div(y1 - 1, 4) - math_div(y1 - 1, 100)
  + math_div(y1 - 1, 400);
 long leaps2 = math_div(y2 - 1, 4) - math_div(y2 - 1, 100)
  + math_div(y2 - 1, 400);
 return leaps2 - leaps1;
}


static const unsigned short __mon_yday[2][13] = {

 {0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 365},

 {0, 31, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335, 366}
};

void time_to_tm(time_t totalsecs, int offset, struct tm *result)
{
 long days, rem, y;
 const unsigned short *ip;

 days = totalsecs / SECS_PER_DAY;
 rem = totalsecs % SECS_PER_DAY;
 rem += offset;
 while (rem < 0) {
  rem += SECS_PER_DAY;
  --days;
 }
 while (rem >= SECS_PER_DAY) {
  rem -= SECS_PER_DAY;
  ++days;
 }

 result->tm_hour = rem / SECS_PER_HOUR;
 rem %= SECS_PER_HOUR;
 result->tm_min = rem / 60;
 result->tm_sec = rem % 60;


 result->tm_wday = (4 + days) % 7;
 if (result->tm_wday < 0)
  result->tm_wday += 7;

 y = 1970;

 while (days < 0 || days >= (__isleap(y) ? 366 : 365)) {

  long yg = y + math_div(days, 365);


  days -= (yg - y) * 365 + leaps_between(y, yg);
  y = yg;
 }

 result->tm_year = y - 1900;

 result->tm_yday = days;

 ip = __mon_yday[__isleap(y)];
 for (y = 11; days < ip[y]; y--)
  continue;
 days -= ip[y];

 result->tm_mon = y;
 result->tm_mday = days + 1;
}
EXPORT_SYMBOL(time_to_tm);

void timecounter_init(struct timecounter *tc,
        const struct cyclecounter *cc,
        u64 start_tstamp)
{
 tc->cc = cc;
 tc->cycle_last = cc->read(cc);
 tc->nsec = start_tstamp;
 tc->mask = (1ULL << cc->shift) - 1;
 tc->frac = 0;
}
EXPORT_SYMBOL_GPL(timecounter_init);
static u64 timecounter_read_delta(struct timecounter *tc)
{
 cycle_t cycle_now, cycle_delta;
 u64 ns_offset;


 cycle_now = tc->cc->read(tc->cc);


 cycle_delta = (cycle_now - tc->cycle_last) & tc->cc->mask;


 ns_offset = cyclecounter_cyc2ns(tc->cc, cycle_delta,
     tc->mask, &tc->frac);


 tc->cycle_last = cycle_now;

 return ns_offset;
}

u64 timecounter_read(struct timecounter *tc)
{
 u64 nsec;


 nsec = timecounter_read_delta(tc);
 nsec += tc->nsec;
 tc->nsec = nsec;

 return nsec;
}
EXPORT_SYMBOL_GPL(timecounter_read);





static u64 cc_cyc2ns_backwards(const struct cyclecounter *cc,
          cycle_t cycles, u64 mask, u64 frac)
{
 u64 ns = (u64) cycles;

 ns = ((ns * cc->mult) - frac) >> cc->shift;

 return ns;
}

u64 timecounter_cyc2time(struct timecounter *tc,
    cycle_t cycle_tstamp)
{
 u64 delta = (cycle_tstamp - tc->cycle_last) & tc->cc->mask;
 u64 nsec = tc->nsec, frac = tc->frac;






 if (delta > tc->cc->mask / 2) {
  delta = (tc->cycle_last - cycle_tstamp) & tc->cc->mask;
  nsec -= cc_cyc2ns_backwards(tc->cc, delta, tc->mask, frac);
 } else {
  nsec += cyclecounter_cyc2ns(tc->cc, delta, tc->mask, &frac);
 }

 return nsec;
}
EXPORT_SYMBOL_GPL(timecounter_cyc2time);







static struct {
 seqcount_t seq;
 struct timekeeper timekeeper;
} tk_core ____cacheline_aligned;

static DEFINE_RAW_SPINLOCK(timekeeper_lock);
static struct timekeeper shadow_timekeeper;
struct tk_fast {
 seqcount_t seq;
 struct tk_read_base base[2];
};

static struct tk_fast tk_fast_mono ____cacheline_aligned;
static struct tk_fast tk_fast_raw ____cacheline_aligned;


int __read_mostly timekeeping_suspended;

static inline void tk_normalize_xtime(struct timekeeper *tk)
{
 while (tk->tkr_mono.xtime_nsec >= ((u64)NSEC_PER_SEC << tk->tkr_mono.shift)) {
  tk->tkr_mono.xtime_nsec -= (u64)NSEC_PER_SEC << tk->tkr_mono.shift;
  tk->xtime_sec++;
 }
}

static inline struct timespec64 tk_xtime(struct timekeeper *tk)
{
 struct timespec64 ts;

 ts.tv_sec = tk->xtime_sec;
 ts.tv_nsec = (long)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);
 return ts;
}

static void tk_set_xtime(struct timekeeper *tk, const struct timespec64 *ts)
{
 tk->xtime_sec = ts->tv_sec;
 tk->tkr_mono.xtime_nsec = (u64)ts->tv_nsec << tk->tkr_mono.shift;
}

static void tk_xtime_add(struct timekeeper *tk, const struct timespec64 *ts)
{
 tk->xtime_sec += ts->tv_sec;
 tk->tkr_mono.xtime_nsec += (u64)ts->tv_nsec << tk->tkr_mono.shift;
 tk_normalize_xtime(tk);
}

static void tk_set_wall_to_mono(struct timekeeper *tk, struct timespec64 wtm)
{
 struct timespec64 tmp;





 set_normalized_timespec64(&tmp, -tk->wall_to_monotonic.tv_sec,
     -tk->wall_to_monotonic.tv_nsec);
 WARN_ON_ONCE(tk->offs_real.tv64 != timespec64_to_ktime(tmp).tv64);
 tk->wall_to_monotonic = wtm;
 set_normalized_timespec64(&tmp, -wtm.tv_sec, -wtm.tv_nsec);
 tk->offs_real = timespec64_to_ktime(tmp);
 tk->offs_tai = ktime_add(tk->offs_real, ktime_set(tk->tai_offset, 0));
}

static inline void tk_update_sleep_time(struct timekeeper *tk, ktime_t delta)
{
 tk->offs_boot = ktime_add(tk->offs_boot, delta);
}


static void timekeeping_check_update(struct timekeeper *tk, cycle_t offset)
{

 cycle_t max_cycles = tk->tkr_mono.clock->max_cycles;
 const char *name = tk->tkr_mono.clock->name;

 if (offset > max_cycles) {
  printk_deferred("WARNING: timekeeping: Cycle offset (%lld) is larger than allowed by the '%s' clock's max_cycles value (%lld): time overflow danger\n",
    offset, name, max_cycles);
  printk_deferred("         timekeeping: Your kernel is sick, but tries to cope by capping time updates\n");
 } else {
  if (offset > (max_cycles >> 1)) {
   printk_deferred("INFO: timekeeping: Cycle offset (%lld) is larger than the '%s' clock's 50%% safety margin (%lld)\n",
     offset, name, max_cycles >> 1);
   printk_deferred("      timekeeping: Your kernel is still fine, but is feeling a bit nervous\n");
  }
 }

 if (tk->underflow_seen) {
  if (jiffies - tk->last_warning > WARNING_FREQ) {
   printk_deferred("WARNING: Underflow in clocksource '%s' observed, time update ignored.\n", name);
   printk_deferred("         Please report this, consider using a different clocksource, if possible.\n");
   printk_deferred("         Your kernel is probably still fine.\n");
   tk->last_warning = jiffies;
  }
  tk->underflow_seen = 0;
 }

 if (tk->overflow_seen) {
  if (jiffies - tk->last_warning > WARNING_FREQ) {
   printk_deferred("WARNING: Overflow in clocksource '%s' observed, time update capped.\n", name);
   printk_deferred("         Please report this, consider using a different clocksource, if possible.\n");
   printk_deferred("         Your kernel is probably still fine.\n");
   tk->last_warning = jiffies;
  }
  tk->overflow_seen = 0;
 }
}

static inline cycle_t timekeeping_get_delta(struct tk_read_base *tkr)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 cycle_t now, last, mask, max, delta;
 unsigned int seq;
 do {
  seq = read_seqcount_begin(&tk_core.seq);
  now = tkr->read(tkr->clock);
  last = tkr->cycle_last;
  mask = tkr->mask;
  max = tkr->clock->max_cycles;
 } while (read_seqcount_retry(&tk_core.seq, seq));

 delta = clocksource_delta(now, last, mask);





 if (unlikely((~delta & mask) < (mask >> 3))) {
  tk->underflow_seen = 1;
  delta = 0;
 }


 if (unlikely(delta > max)) {
  tk->overflow_seen = 1;
  delta = tkr->clock->max_cycles;
 }

 return delta;
}
static inline void timekeeping_check_update(struct timekeeper *tk, cycle_t offset)
{
}
static inline cycle_t timekeeping_get_delta(struct tk_read_base *tkr)
{
 cycle_t cycle_now, delta;


 cycle_now = tkr->read(tkr->clock);


 delta = clocksource_delta(cycle_now, tkr->cycle_last, tkr->mask);

 return delta;
}
static void tk_setup_internals(struct timekeeper *tk, struct clocksource *clock)
{
 cycle_t interval;
 u64 tmp, ntpinterval;
 struct clocksource *old_clock;

 ++tk->cs_was_changed_seq;
 old_clock = tk->tkr_mono.clock;
 tk->tkr_mono.clock = clock;
 tk->tkr_mono.read = clock->read;
 tk->tkr_mono.mask = clock->mask;
 tk->tkr_mono.cycle_last = tk->tkr_mono.read(clock);

 tk->tkr_raw.clock = clock;
 tk->tkr_raw.read = clock->read;
 tk->tkr_raw.mask = clock->mask;
 tk->tkr_raw.cycle_last = tk->tkr_mono.cycle_last;


 tmp = NTP_INTERVAL_LENGTH;
 tmp <<= clock->shift;
 ntpinterval = tmp;
 tmp += clock->mult/2;
 do_div(tmp, clock->mult);
 if (tmp == 0)
  tmp = 1;

 interval = (cycle_t) tmp;
 tk->cycle_interval = interval;


 tk->xtime_interval = (u64) interval * clock->mult;
 tk->xtime_remainder = ntpinterval - tk->xtime_interval;
 tk->raw_interval =
  ((u64) interval * clock->mult) >> clock->shift;


 if (old_clock) {
  int shift_change = clock->shift - old_clock->shift;
  if (shift_change < 0)
   tk->tkr_mono.xtime_nsec >>= -shift_change;
  else
   tk->tkr_mono.xtime_nsec <<= shift_change;
 }
 tk->tkr_raw.xtime_nsec = 0;

 tk->tkr_mono.shift = clock->shift;
 tk->tkr_raw.shift = clock->shift;

 tk->ntp_error = 0;
 tk->ntp_error_shift = NTP_SCALE_SHIFT - clock->shift;
 tk->ntp_tick = ntpinterval << tk->ntp_error_shift;






 tk->tkr_mono.mult = clock->mult;
 tk->tkr_raw.mult = clock->mult;
 tk->ntp_err_mult = 0;
}



static u32 default_arch_gettimeoffset(void) { return 0; }
u32 (*arch_gettimeoffset)(void) = default_arch_gettimeoffset;
static inline u32 arch_gettimeoffset(void) { return 0; }

static inline s64 timekeeping_delta_to_ns(struct tk_read_base *tkr,
       cycle_t delta)
{
 s64 nsec;

 nsec = delta * tkr->mult + tkr->xtime_nsec;
 nsec >>= tkr->shift;


 return nsec + arch_gettimeoffset();
}

static inline s64 timekeeping_get_ns(struct tk_read_base *tkr)
{
 cycle_t delta;

 delta = timekeeping_get_delta(tkr);
 return timekeeping_delta_to_ns(tkr, delta);
}

static inline s64 timekeeping_cycles_to_ns(struct tk_read_base *tkr,
         cycle_t cycles)
{
 cycle_t delta;


 delta = clocksource_delta(cycles, tkr->cycle_last, tkr->mask);
 return timekeeping_delta_to_ns(tkr, delta);
}
static void update_fast_timekeeper(struct tk_read_base *tkr, struct tk_fast *tkf)
{
 struct tk_read_base *base = tkf->base;


 raw_write_seqcount_latch(&tkf->seq);


 memcpy(base, tkr, sizeof(*base));


 raw_write_seqcount_latch(&tkf->seq);


 memcpy(base + 1, base, sizeof(*base));
}
static __always_inline u64 __ktime_get_fast_ns(struct tk_fast *tkf)
{
 struct tk_read_base *tkr;
 unsigned int seq;
 u64 now;

 do {
  seq = raw_read_seqcount_latch(&tkf->seq);
  tkr = tkf->base + (seq & 0x01);
  now = ktime_to_ns(tkr->base) + timekeeping_get_ns(tkr);
 } while (read_seqcount_retry(&tkf->seq, seq));

 return now;
}

u64 ktime_get_mono_fast_ns(void)
{
 return __ktime_get_fast_ns(&tk_fast_mono);
}
EXPORT_SYMBOL_GPL(ktime_get_mono_fast_ns);

u64 ktime_get_raw_fast_ns(void)
{
 return __ktime_get_fast_ns(&tk_fast_raw);
}
EXPORT_SYMBOL_GPL(ktime_get_raw_fast_ns);


static cycle_t cycles_at_suspend;

static cycle_t dummy_clock_read(struct clocksource *cs)
{
 return cycles_at_suspend;
}
static void halt_fast_timekeeper(struct timekeeper *tk)
{
 static struct tk_read_base tkr_dummy;
 struct tk_read_base *tkr = &tk->tkr_mono;

 memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));
 cycles_at_suspend = tkr->read(tkr->clock);
 tkr_dummy.read = dummy_clock_read;
 update_fast_timekeeper(&tkr_dummy, &tk_fast_mono);

 tkr = &tk->tkr_raw;
 memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));
 tkr_dummy.read = dummy_clock_read;
 update_fast_timekeeper(&tkr_dummy, &tk_fast_raw);
}


static inline void update_vsyscall(struct timekeeper *tk)
{
 struct timespec xt, wm;

 xt = timespec64_to_timespec(tk_xtime(tk));
 wm = timespec64_to_timespec(tk->wall_to_monotonic);
 update_vsyscall_old(&xt, &wm, tk->tkr_mono.clock, tk->tkr_mono.mult,
       tk->tkr_mono.cycle_last);
}

static inline void old_vsyscall_fixup(struct timekeeper *tk)
{
 s64 remainder;
 remainder = tk->tkr_mono.xtime_nsec & ((1ULL << tk->tkr_mono.shift) - 1);
 tk->tkr_mono.xtime_nsec -= remainder;
 tk->tkr_mono.xtime_nsec += 1ULL << tk->tkr_mono.shift;
 tk->ntp_error += remainder << tk->ntp_error_shift;
 tk->ntp_error -= (1ULL << tk->tkr_mono.shift) << tk->ntp_error_shift;
}

static RAW_NOTIFIER_HEAD(pvclock_gtod_chain);

static void update_pvclock_gtod(struct timekeeper *tk, bool was_set)
{
 raw_notifier_call_chain(&pvclock_gtod_chain, was_set, tk);
}




int pvclock_gtod_register_notifier(struct notifier_block *nb)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned long flags;
 int ret;

 raw_spin_lock_irqsave(&timekeeper_lock, flags);
 ret = raw_notifier_chain_register(&pvclock_gtod_chain, nb);
 update_pvclock_gtod(tk, true);
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);

 return ret;
}
EXPORT_SYMBOL_GPL(pvclock_gtod_register_notifier);





int pvclock_gtod_unregister_notifier(struct notifier_block *nb)
{
 unsigned long flags;
 int ret;

 raw_spin_lock_irqsave(&timekeeper_lock, flags);
 ret = raw_notifier_chain_unregister(&pvclock_gtod_chain, nb);
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);

 return ret;
}
EXPORT_SYMBOL_GPL(pvclock_gtod_unregister_notifier);




static inline void tk_update_leap_state(struct timekeeper *tk)
{
 tk->next_leap_ktime = ntp_get_next_leap();
 if (tk->next_leap_ktime.tv64 != KTIME_MAX)

  tk->next_leap_ktime = ktime_sub(tk->next_leap_ktime, tk->offs_real);
}




static inline void tk_update_ktime_data(struct timekeeper *tk)
{
 u64 seconds;
 u32 nsec;
 seconds = (u64)(tk->xtime_sec + tk->wall_to_monotonic.tv_sec);
 nsec = (u32) tk->wall_to_monotonic.tv_nsec;
 tk->tkr_mono.base = ns_to_ktime(seconds * NSEC_PER_SEC + nsec);


 tk->tkr_raw.base = timespec64_to_ktime(tk->raw_time);






 nsec += (u32)(tk->tkr_mono.xtime_nsec >> tk->tkr_mono.shift);
 if (nsec >= NSEC_PER_SEC)
  seconds++;
 tk->ktime_sec = seconds;
}


static void timekeeping_update(struct timekeeper *tk, unsigned int action)
{
 if (action & TK_CLEAR_NTP) {
  tk->ntp_error = 0;
  ntp_clear();
 }

 tk_update_leap_state(tk);
 tk_update_ktime_data(tk);

 update_vsyscall(tk);
 update_pvclock_gtod(tk, action & TK_CLOCK_WAS_SET);

 update_fast_timekeeper(&tk->tkr_mono, &tk_fast_mono);
 update_fast_timekeeper(&tk->tkr_raw, &tk_fast_raw);

 if (action & TK_CLOCK_WAS_SET)
  tk->clock_was_set_seq++;





 if (action & TK_MIRROR)
  memcpy(&shadow_timekeeper, &tk_core.timekeeper,
         sizeof(tk_core.timekeeper));
}
static void timekeeping_forward_now(struct timekeeper *tk)
{
 struct clocksource *clock = tk->tkr_mono.clock;
 cycle_t cycle_now, delta;
 s64 nsec;

 cycle_now = tk->tkr_mono.read(clock);
 delta = clocksource_delta(cycle_now, tk->tkr_mono.cycle_last, tk->tkr_mono.mask);
 tk->tkr_mono.cycle_last = cycle_now;
 tk->tkr_raw.cycle_last = cycle_now;

 tk->tkr_mono.xtime_nsec += delta * tk->tkr_mono.mult;


 tk->tkr_mono.xtime_nsec += (u64)arch_gettimeoffset() << tk->tkr_mono.shift;

 tk_normalize_xtime(tk);

 nsec = clocksource_cyc2ns(delta, tk->tkr_raw.mult, tk->tkr_raw.shift);
 timespec64_add_ns(&tk->raw_time, nsec);
}
int __getnstimeofday64(struct timespec64 *ts)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned long seq;
 s64 nsecs = 0;

 do {
  seq = read_seqcount_begin(&tk_core.seq);

  ts->tv_sec = tk->xtime_sec;
  nsecs = timekeeping_get_ns(&tk->tkr_mono);

 } while (read_seqcount_retry(&tk_core.seq, seq));

 ts->tv_nsec = 0;
 timespec64_add_ns(ts, nsecs);





 if (unlikely(timekeeping_suspended))
  return -EAGAIN;
 return 0;
}
EXPORT_SYMBOL(__getnstimeofday64);







void getnstimeofday64(struct timespec64 *ts)
{
 WARN_ON(__getnstimeofday64(ts));
}
EXPORT_SYMBOL(getnstimeofday64);

ktime_t ktime_get(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned int seq;
 ktime_t base;
 s64 nsecs;

 WARN_ON(timekeeping_suspended);

 do {
  seq = read_seqcount_begin(&tk_core.seq);
  base = tk->tkr_mono.base;
  nsecs = timekeeping_get_ns(&tk->tkr_mono);

 } while (read_seqcount_retry(&tk_core.seq, seq));

 return ktime_add_ns(base, nsecs);
}
EXPORT_SYMBOL_GPL(ktime_get);

u32 ktime_get_resolution_ns(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned int seq;
 u32 nsecs;

 WARN_ON(timekeeping_suspended);

 do {
  seq = read_seqcount_begin(&tk_core.seq);
  nsecs = tk->tkr_mono.mult >> tk->tkr_mono.shift;
 } while (read_seqcount_retry(&tk_core.seq, seq));

 return nsecs;
}
EXPORT_SYMBOL_GPL(ktime_get_resolution_ns);

static ktime_t *offsets[TK_OFFS_MAX] = {
 [TK_OFFS_REAL] = &tk_core.timekeeper.offs_real,
 [TK_OFFS_BOOT] = &tk_core.timekeeper.offs_boot,
 [TK_OFFS_TAI] = &tk_core.timekeeper.offs_tai,
};

ktime_t ktime_get_with_offset(enum tk_offsets offs)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned int seq;
 ktime_t base, *offset = offsets[offs];
 s64 nsecs;

 WARN_ON(timekeeping_suspended);

 do {
  seq = read_seqcount_begin(&tk_core.seq);
  base = ktime_add(tk->tkr_mono.base, *offset);
  nsecs = timekeeping_get_ns(&tk->tkr_mono);

 } while (read_seqcount_retry(&tk_core.seq, seq));

 return ktime_add_ns(base, nsecs);

}
EXPORT_SYMBOL_GPL(ktime_get_with_offset);






ktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs)
{
 ktime_t *offset = offsets[offs];
 unsigned long seq;
 ktime_t tconv;

 do {
  seq = read_seqcount_begin(&tk_core.seq);
  tconv = ktime_add(tmono, *offset);
 } while (read_seqcount_retry(&tk_core.seq, seq));

 return tconv;
}
EXPORT_SYMBOL_GPL(ktime_mono_to_any);




ktime_t ktime_get_raw(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned int seq;
 ktime_t base;
 s64 nsecs;

 do {
  seq = read_seqcount_begin(&tk_core.seq);
  base = tk->tkr_raw.base;
  nsecs = timekeeping_get_ns(&tk->tkr_raw);

 } while (read_seqcount_retry(&tk_core.seq, seq));

 return ktime_add_ns(base, nsecs);
}
EXPORT_SYMBOL_GPL(ktime_get_raw);
void ktime_get_ts64(struct timespec64 *ts)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 struct timespec64 tomono;
 s64 nsec;
 unsigned int seq;

 WARN_ON(timekeeping_suspended);

 do {
  seq = read_seqcount_begin(&tk_core.seq);
  ts->tv_sec = tk->xtime_sec;
  nsec = timekeeping_get_ns(&tk->tkr_mono);
  tomono = tk->wall_to_monotonic;

 } while (read_seqcount_retry(&tk_core.seq, seq));

 ts->tv_sec += tomono.tv_sec;
 ts->tv_nsec = 0;
 timespec64_add_ns(ts, nsec + tomono.tv_nsec);
}
EXPORT_SYMBOL_GPL(ktime_get_ts64);
time64_t ktime_get_seconds(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;

 WARN_ON(timekeeping_suspended);
 return tk->ktime_sec;
}
EXPORT_SYMBOL_GPL(ktime_get_seconds);
time64_t ktime_get_real_seconds(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 time64_t seconds;
 unsigned int seq;

 if (IS_ENABLED(CONFIG_64BIT))
  return tk->xtime_sec;

 do {
  seq = read_seqcount_begin(&tk_core.seq);
  seconds = tk->xtime_sec;

 } while (read_seqcount_retry(&tk_core.seq, seq));

 return seconds;
}
EXPORT_SYMBOL_GPL(ktime_get_real_seconds);






time64_t __ktime_get_real_seconds(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;

 return tk->xtime_sec;
}





void ktime_get_snapshot(struct system_time_snapshot *systime_snapshot)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned long seq;
 ktime_t base_raw;
 ktime_t base_real;
 s64 nsec_raw;
 s64 nsec_real;
 cycle_t now;

 WARN_ON_ONCE(timekeeping_suspended);

 do {
  seq = read_seqcount_begin(&tk_core.seq);

  now = tk->tkr_mono.read(tk->tkr_mono.clock);
  systime_snapshot->cs_was_changed_seq = tk->cs_was_changed_seq;
  systime_snapshot->clock_was_set_seq = tk->clock_was_set_seq;
  base_real = ktime_add(tk->tkr_mono.base,
          tk_core.timekeeper.offs_real);
  base_raw = tk->tkr_raw.base;
  nsec_real = timekeeping_cycles_to_ns(&tk->tkr_mono, now);
  nsec_raw = timekeeping_cycles_to_ns(&tk->tkr_raw, now);
 } while (read_seqcount_retry(&tk_core.seq, seq));

 systime_snapshot->cycles = now;
 systime_snapshot->real = ktime_add_ns(base_real, nsec_real);
 systime_snapshot->raw = ktime_add_ns(base_raw, nsec_raw);
}
EXPORT_SYMBOL_GPL(ktime_get_snapshot);


static int scale64_check_overflow(u64 mult, u64 div, u64 *base)
{
 u64 tmp, rem;

 tmp = div64_u64_rem(*base, div, &rem);

 if (((int)sizeof(u64)*8 - fls64(mult) < fls64(tmp)) ||
     ((int)sizeof(u64)*8 - fls64(mult) < fls64(rem)))
  return -EOVERFLOW;
 tmp *= mult;
 rem *= mult;

 do_div(rem, div);
 *base = tmp + rem;
 return 0;
}
static int adjust_historical_crosststamp(struct system_time_snapshot *history,
      cycle_t partial_history_cycles,
      cycle_t total_history_cycles,
      bool discontinuity,
      struct system_device_crosststamp *ts)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 u64 corr_raw, corr_real;
 bool interp_forward;
 int ret;

 if (total_history_cycles == 0 || partial_history_cycles == 0)
  return 0;


 interp_forward = partial_history_cycles > total_history_cycles/2 ?
  true : false;
 partial_history_cycles = interp_forward ?
  total_history_cycles - partial_history_cycles :
  partial_history_cycles;





 corr_raw = (u64)ktime_to_ns(
  ktime_sub(ts->sys_monoraw, history->raw));
 ret = scale64_check_overflow(partial_history_cycles,
         total_history_cycles, &corr_raw);
 if (ret)
  return ret;
 if (discontinuity) {
  corr_real = mul_u64_u32_div
   (corr_raw, tk->tkr_mono.mult, tk->tkr_raw.mult);
 } else {
  corr_real = (u64)ktime_to_ns(
   ktime_sub(ts->sys_realtime, history->real));
  ret = scale64_check_overflow(partial_history_cycles,
          total_history_cycles, &corr_real);
  if (ret)
   return ret;
 }


 if (interp_forward) {
  ts->sys_monoraw = ktime_add_ns(history->raw, corr_raw);
  ts->sys_realtime = ktime_add_ns(history->real, corr_real);
 } else {
  ts->sys_monoraw = ktime_sub_ns(ts->sys_monoraw, corr_raw);
  ts->sys_realtime = ktime_sub_ns(ts->sys_realtime, corr_real);
 }

 return 0;
}




static bool cycle_between(cycle_t before, cycle_t test, cycle_t after)
{
 if (test > before && test < after)
  return true;
 if (test < before && before > after)
  return true;
 return false;
}
int get_device_system_crosststamp(int (*get_time_fn)
      (ktime_t *device_time,
       struct system_counterval_t *sys_counterval,
       void *ctx),
      void *ctx,
      struct system_time_snapshot *history_begin,
      struct system_device_crosststamp *xtstamp)
{
 struct system_counterval_t system_counterval;
 struct timekeeper *tk = &tk_core.timekeeper;
 cycle_t cycles, now, interval_start;
 unsigned int clock_was_set_seq = 0;
 ktime_t base_real, base_raw;
 s64 nsec_real, nsec_raw;
 u8 cs_was_changed_seq;
 unsigned long seq;
 bool do_interp;
 int ret;

 do {
  seq = read_seqcount_begin(&tk_core.seq);




  ret = get_time_fn(&xtstamp->device, &system_counterval, ctx);
  if (ret)
   return ret;






  if (tk->tkr_mono.clock != system_counterval.cs)
   return -ENODEV;
  cycles = system_counterval.cycles;





  now = tk->tkr_mono.read(tk->tkr_mono.clock);
  interval_start = tk->tkr_mono.cycle_last;
  if (!cycle_between(interval_start, cycles, now)) {
   clock_was_set_seq = tk->clock_was_set_seq;
   cs_was_changed_seq = tk->cs_was_changed_seq;
   cycles = interval_start;
   do_interp = true;
  } else {
   do_interp = false;
  }

  base_real = ktime_add(tk->tkr_mono.base,
          tk_core.timekeeper.offs_real);
  base_raw = tk->tkr_raw.base;

  nsec_real = timekeeping_cycles_to_ns(&tk->tkr_mono,
           system_counterval.cycles);
  nsec_raw = timekeeping_cycles_to_ns(&tk->tkr_raw,
          system_counterval.cycles);
 } while (read_seqcount_retry(&tk_core.seq, seq));

 xtstamp->sys_realtime = ktime_add_ns(base_real, nsec_real);
 xtstamp->sys_monoraw = ktime_add_ns(base_raw, nsec_raw);





 if (do_interp) {
  cycle_t partial_history_cycles, total_history_cycles;
  bool discontinuity;






  if (!history_begin ||
      !cycle_between(history_begin->cycles,
       system_counterval.cycles, cycles) ||
      history_begin->cs_was_changed_seq != cs_was_changed_seq)
   return -EINVAL;
  partial_history_cycles = cycles - system_counterval.cycles;
  total_history_cycles = cycles - history_begin->cycles;
  discontinuity =
   history_begin->clock_was_set_seq != clock_was_set_seq;

  ret = adjust_historical_crosststamp(history_begin,
          partial_history_cycles,
          total_history_cycles,
          discontinuity, xtstamp);
  if (ret)
   return ret;
 }

 return 0;
}
EXPORT_SYMBOL_GPL(get_device_system_crosststamp);







void do_gettimeofday(struct timeval *tv)
{
 struct timespec64 now;

 getnstimeofday64(&now);
 tv->tv_sec = now.tv_sec;
 tv->tv_usec = now.tv_nsec/1000;
}
EXPORT_SYMBOL(do_gettimeofday);







int do_settimeofday64(const struct timespec64 *ts)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 struct timespec64 ts_delta, xt;
 unsigned long flags;
 int ret = 0;

 if (!timespec64_valid_strict(ts))
  return -EINVAL;

 raw_spin_lock_irqsave(&timekeeper_lock, flags);
 write_seqcount_begin(&tk_core.seq);

 timekeeping_forward_now(tk);

 xt = tk_xtime(tk);
 ts_delta.tv_sec = ts->tv_sec - xt.tv_sec;
 ts_delta.tv_nsec = ts->tv_nsec - xt.tv_nsec;

 if (timespec64_compare(&tk->wall_to_monotonic, &ts_delta) > 0) {
  ret = -EINVAL;
  goto out;
 }

 tk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, ts_delta));

 tk_set_xtime(tk, ts);
out:
 timekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);

 write_seqcount_end(&tk_core.seq);
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);


 clock_was_set();

 return ret;
}
EXPORT_SYMBOL(do_settimeofday64);







int timekeeping_inject_offset(struct timespec *ts)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned long flags;
 struct timespec64 ts64, tmp;
 int ret = 0;

 if (!timespec_inject_offset_valid(ts))
  return -EINVAL;

 ts64 = timespec_to_timespec64(*ts);

 raw_spin_lock_irqsave(&timekeeper_lock, flags);
 write_seqcount_begin(&tk_core.seq);

 timekeeping_forward_now(tk);


 tmp = timespec64_add(tk_xtime(tk), ts64);
 if (timespec64_compare(&tk->wall_to_monotonic, &ts64) > 0 ||
     !timespec64_valid_strict(&tmp)) {
  ret = -EINVAL;
  goto error;
 }

 tk_xtime_add(tk, &ts64);
 tk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, ts64));

error:
 timekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);

 write_seqcount_end(&tk_core.seq);
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);


 clock_was_set();

 return ret;
}
EXPORT_SYMBOL(timekeeping_inject_offset);






s32 timekeeping_get_tai_offset(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned int seq;
 s32 ret;

 do {
  seq = read_seqcount_begin(&tk_core.seq);
  ret = tk->tai_offset;
 } while (read_seqcount_retry(&tk_core.seq, seq));

 return ret;
}





static void __timekeeping_set_tai_offset(struct timekeeper *tk, s32 tai_offset)
{
 tk->tai_offset = tai_offset;
 tk->offs_tai = ktime_add(tk->offs_real, ktime_set(tai_offset, 0));
}





void timekeeping_set_tai_offset(s32 tai_offset)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned long flags;

 raw_spin_lock_irqsave(&timekeeper_lock, flags);
 write_seqcount_begin(&tk_core.seq);
 __timekeeping_set_tai_offset(tk, tai_offset);
 timekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);
 write_seqcount_end(&tk_core.seq);
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
 clock_was_set();
}






static int change_clocksource(void *data)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 struct clocksource *new, *old;
 unsigned long flags;

 new = (struct clocksource *) data;

 raw_spin_lock_irqsave(&timekeeper_lock, flags);
 write_seqcount_begin(&tk_core.seq);

 timekeeping_forward_now(tk);




 if (try_module_get(new->owner)) {
  if (!new->enable || new->enable(new) == 0) {
   old = tk->tkr_mono.clock;
   tk_setup_internals(tk, new);
   if (old->disable)
    old->disable(old);
   module_put(old->owner);
  } else {
   module_put(new->owner);
  }
 }
 timekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);

 write_seqcount_end(&tk_core.seq);
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);

 return 0;
}
int timekeeping_notify(struct clocksource *clock)
{
 struct timekeeper *tk = &tk_core.timekeeper;

 if (tk->tkr_mono.clock == clock)
  return 0;
 stop_machine(change_clocksource, clock, NULL);
 tick_clock_notify();
 return tk->tkr_mono.clock == clock ? 0 : -1;
}







void getrawmonotonic64(struct timespec64 *ts)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 struct timespec64 ts64;
 unsigned long seq;
 s64 nsecs;

 do {
  seq = read_seqcount_begin(&tk_core.seq);
  nsecs = timekeeping_get_ns(&tk->tkr_raw);
  ts64 = tk->raw_time;

 } while (read_seqcount_retry(&tk_core.seq, seq));

 timespec64_add_ns(&ts64, nsecs);
 *ts = ts64;
}
EXPORT_SYMBOL(getrawmonotonic64);





int timekeeping_valid_for_hres(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned long seq;
 int ret;

 do {
  seq = read_seqcount_begin(&tk_core.seq);

  ret = tk->tkr_mono.clock->flags & CLOCK_SOURCE_VALID_FOR_HRES;

 } while (read_seqcount_retry(&tk_core.seq, seq));

 return ret;
}




u64 timekeeping_max_deferment(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned long seq;
 u64 ret;

 do {
  seq = read_seqcount_begin(&tk_core.seq);

  ret = tk->tkr_mono.clock->max_idle_ns;

 } while (read_seqcount_retry(&tk_core.seq, seq));

 return ret;
}
void __weak read_persistent_clock(struct timespec *ts)
{
 ts->tv_sec = 0;
 ts->tv_nsec = 0;
}

void __weak read_persistent_clock64(struct timespec64 *ts64)
{
 struct timespec ts;

 read_persistent_clock(&ts);
 *ts64 = timespec_to_timespec64(ts);
}
void __weak read_boot_clock64(struct timespec64 *ts)
{
 ts->tv_sec = 0;
 ts->tv_nsec = 0;
}


static bool sleeptime_injected;


static bool persistent_clock_exists;




void __init timekeeping_init(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 struct clocksource *clock;
 unsigned long flags;
 struct timespec64 now, boot, tmp;

 read_persistent_clock64(&now);
 if (!timespec64_valid_strict(&now)) {
  pr_warn("WARNING: Persistent clock returned invalid value!\n"
   "         Check your CMOS/BIOS settings.\n");
  now.tv_sec = 0;
  now.tv_nsec = 0;
 } else if (now.tv_sec || now.tv_nsec)
  persistent_clock_exists = true;

 read_boot_clock64(&boot);
 if (!timespec64_valid_strict(&boot)) {
  pr_warn("WARNING: Boot clock returned invalid value!\n"
   "         Check your CMOS/BIOS settings.\n");
  boot.tv_sec = 0;
  boot.tv_nsec = 0;
 }

 raw_spin_lock_irqsave(&timekeeper_lock, flags);
 write_seqcount_begin(&tk_core.seq);
 ntp_init();

 clock = clocksource_default_clock();
 if (clock->enable)
  clock->enable(clock);
 tk_setup_internals(tk, clock);

 tk_set_xtime(tk, &now);
 tk->raw_time.tv_sec = 0;
 tk->raw_time.tv_nsec = 0;
 if (boot.tv_sec == 0 && boot.tv_nsec == 0)
  boot = tk_xtime(tk);

 set_normalized_timespec64(&tmp, -boot.tv_sec, -boot.tv_nsec);
 tk_set_wall_to_mono(tk, tmp);

 timekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);

 write_seqcount_end(&tk_core.seq);
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
}


static struct timespec64 timekeeping_suspend_time;
static void __timekeeping_inject_sleeptime(struct timekeeper *tk,
        struct timespec64 *delta)
{
 if (!timespec64_valid_strict(delta)) {
  printk_deferred(KERN_WARNING
    "__timekeeping_inject_sleeptime: Invalid "
    "sleep delta value!\n");
  return;
 }
 tk_xtime_add(tk, delta);
 tk_set_wall_to_mono(tk, timespec64_sub(tk->wall_to_monotonic, *delta));
 tk_update_sleep_time(tk, timespec64_to_ktime(*delta));
 tk_debug_account_sleep_time(delta);
}

bool timekeeping_rtc_skipresume(void)
{
 return sleeptime_injected;
}
bool timekeeping_rtc_skipsuspend(void)
{
 return persistent_clock_exists;
}
void timekeeping_inject_sleeptime64(struct timespec64 *delta)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned long flags;

 raw_spin_lock_irqsave(&timekeeper_lock, flags);
 write_seqcount_begin(&tk_core.seq);

 timekeeping_forward_now(tk);

 __timekeeping_inject_sleeptime(tk, delta);

 timekeeping_update(tk, TK_CLEAR_NTP | TK_MIRROR | TK_CLOCK_WAS_SET);

 write_seqcount_end(&tk_core.seq);
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);


 clock_was_set();
}




void timekeeping_resume(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 struct clocksource *clock = tk->tkr_mono.clock;
 unsigned long flags;
 struct timespec64 ts_new, ts_delta;
 cycle_t cycle_now, cycle_delta;

 sleeptime_injected = false;
 read_persistent_clock64(&ts_new);

 clockevents_resume();
 clocksource_resume();

 raw_spin_lock_irqsave(&timekeeper_lock, flags);
 write_seqcount_begin(&tk_core.seq);
 cycle_now = tk->tkr_mono.read(clock);
 if ((clock->flags & CLOCK_SOURCE_SUSPEND_NONSTOP) &&
  cycle_now > tk->tkr_mono.cycle_last) {
  u64 num, max = ULLONG_MAX;
  u32 mult = clock->mult;
  u32 shift = clock->shift;
  s64 nsec = 0;

  cycle_delta = clocksource_delta(cycle_now, tk->tkr_mono.cycle_last,
      tk->tkr_mono.mask);






  do_div(max, mult);
  if (cycle_delta > max) {
   num = div64_u64(cycle_delta, max);
   nsec = (((u64) max * mult) >> shift) * num;
   cycle_delta -= num * max;
  }
  nsec += ((u64) cycle_delta * mult) >> shift;

  ts_delta = ns_to_timespec64(nsec);
  sleeptime_injected = true;
 } else if (timespec64_compare(&ts_new, &timekeeping_suspend_time) > 0) {
  ts_delta = timespec64_sub(ts_new, timekeeping_suspend_time);
  sleeptime_injected = true;
 }

 if (sleeptime_injected)
  __timekeeping_inject_sleeptime(tk, &ts_delta);


 tk->tkr_mono.cycle_last = cycle_now;
 tk->tkr_raw.cycle_last = cycle_now;

 tk->ntp_error = 0;
 timekeeping_suspended = 0;
 timekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);
 write_seqcount_end(&tk_core.seq);
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);

 touch_softlockup_watchdog();

 tick_resume();
 hrtimers_resume();
}

int timekeeping_suspend(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned long flags;
 struct timespec64 delta, delta_delta;
 static struct timespec64 old_delta;

 read_persistent_clock64(&timekeeping_suspend_time);






 if (timekeeping_suspend_time.tv_sec || timekeeping_suspend_time.tv_nsec)
  persistent_clock_exists = true;

 raw_spin_lock_irqsave(&timekeeper_lock, flags);
 write_seqcount_begin(&tk_core.seq);
 timekeeping_forward_now(tk);
 timekeeping_suspended = 1;

 if (persistent_clock_exists) {






  delta = timespec64_sub(tk_xtime(tk), timekeeping_suspend_time);
  delta_delta = timespec64_sub(delta, old_delta);
  if (abs(delta_delta.tv_sec) >= 2) {




   old_delta = delta;
  } else {

   timekeeping_suspend_time =
    timespec64_add(timekeeping_suspend_time, delta_delta);
  }
 }

 timekeeping_update(tk, TK_MIRROR);
 halt_fast_timekeeper(tk);
 write_seqcount_end(&tk_core.seq);
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);

 tick_suspend();
 clocksource_suspend();
 clockevents_suspend();

 return 0;
}


static struct syscore_ops timekeeping_syscore_ops = {
 .resume = timekeeping_resume,
 .suspend = timekeeping_suspend,
};

static int __init timekeeping_init_ops(void)
{
 register_syscore_ops(&timekeeping_syscore_ops);
 return 0;
}
device_initcall(timekeeping_init_ops);




static __always_inline void timekeeping_apply_adjustment(struct timekeeper *tk,
        s64 offset,
        bool negative,
        int adj_scale)
{
 s64 interval = tk->cycle_interval;
 s32 mult_adj = 1;

 if (negative) {
  mult_adj = -mult_adj;
  interval = -interval;
  offset = -offset;
 }
 mult_adj <<= adj_scale;
 interval <<= adj_scale;
 offset <<= adj_scale;
 if ((mult_adj > 0) && (tk->tkr_mono.mult + mult_adj < mult_adj)) {

  WARN_ON_ONCE(1);
  return;
 }

 tk->tkr_mono.mult += mult_adj;
 tk->xtime_interval += interval;
 tk->tkr_mono.xtime_nsec -= offset;
 tk->ntp_error -= (interval - offset) << tk->ntp_error_shift;
}





static __always_inline void timekeeping_freqadjust(struct timekeeper *tk,
       s64 offset)
{
 s64 interval = tk->cycle_interval;
 s64 xinterval = tk->xtime_interval;
 u32 base = tk->tkr_mono.clock->mult;
 u32 max = tk->tkr_mono.clock->maxadj;
 u32 cur_adj = tk->tkr_mono.mult;
 s64 tick_error;
 bool negative;
 u32 adj_scale;


 if (tk->ntp_err_mult)
  xinterval -= tk->cycle_interval;

 tk->ntp_tick = ntp_tick_length();


 tick_error = ntp_tick_length() >> tk->ntp_error_shift;
 tick_error -= (xinterval + tk->xtime_remainder);


 if (likely((tick_error >= 0) && (tick_error <= interval)))
  return;


 negative = (tick_error < 0);


 if (negative && (cur_adj - 1) <= (base - max))
  return;
 if (!negative && (cur_adj + 1) >= (base + max))
  return;





 adj_scale = 0;
 tick_error = abs(tick_error);
 while (tick_error > interval) {
  u32 adj = 1 << (adj_scale + 1);


  if (negative && (cur_adj - adj) <= (base - max))
   break;
  if (!negative && (cur_adj + adj) >= (base + max))
   break;

  adj_scale++;
  tick_error >>= 1;
 }


 timekeeping_apply_adjustment(tk, offset, negative, adj_scale);
}





static void timekeeping_adjust(struct timekeeper *tk, s64 offset)
{

 timekeeping_freqadjust(tk, offset);


 if (!tk->ntp_err_mult && (tk->ntp_error > 0)) {
  tk->ntp_err_mult = 1;
  timekeeping_apply_adjustment(tk, offset, 0, 0);
 } else if (tk->ntp_err_mult && (tk->ntp_error <= 0)) {

  timekeeping_apply_adjustment(tk, offset, 1, 0);
  tk->ntp_err_mult = 0;
 }

 if (unlikely(tk->tkr_mono.clock->maxadj &&
  (abs(tk->tkr_mono.mult - tk->tkr_mono.clock->mult)
   > tk->tkr_mono.clock->maxadj))) {
  printk_once(KERN_WARNING
   "Adjusting %s more than 11%% (%ld vs %ld)\n",
   tk->tkr_mono.clock->name, (long)tk->tkr_mono.mult,
   (long)tk->tkr_mono.clock->mult + tk->tkr_mono.clock->maxadj);
 }
 if (unlikely((s64)tk->tkr_mono.xtime_nsec < 0)) {
  s64 neg = -(s64)tk->tkr_mono.xtime_nsec;
  tk->tkr_mono.xtime_nsec = 0;
  tk->ntp_error += neg << tk->ntp_error_shift;
 }
}
static inline unsigned int accumulate_nsecs_to_secs(struct timekeeper *tk)
{
 u64 nsecps = (u64)NSEC_PER_SEC << tk->tkr_mono.shift;
 unsigned int clock_set = 0;

 while (tk->tkr_mono.xtime_nsec >= nsecps) {
  int leap;

  tk->tkr_mono.xtime_nsec -= nsecps;
  tk->xtime_sec++;


  leap = second_overflow(tk->xtime_sec);
  if (unlikely(leap)) {
   struct timespec64 ts;

   tk->xtime_sec += leap;

   ts.tv_sec = leap;
   ts.tv_nsec = 0;
   tk_set_wall_to_mono(tk,
    timespec64_sub(tk->wall_to_monotonic, ts));

   __timekeeping_set_tai_offset(tk, tk->tai_offset - leap);

   clock_set = TK_CLOCK_WAS_SET;
  }
 }
 return clock_set;
}
static cycle_t logarithmic_accumulation(struct timekeeper *tk, cycle_t offset,
      u32 shift,
      unsigned int *clock_set)
{
 cycle_t interval = tk->cycle_interval << shift;
 u64 raw_nsecs;


 if (offset < interval)
  return offset;


 offset -= interval;
 tk->tkr_mono.cycle_last += interval;
 tk->tkr_raw.cycle_last += interval;

 tk->tkr_mono.xtime_nsec += tk->xtime_interval << shift;
 *clock_set |= accumulate_nsecs_to_secs(tk);


 raw_nsecs = (u64)tk->raw_interval << shift;
 raw_nsecs += tk->raw_time.tv_nsec;
 if (raw_nsecs >= NSEC_PER_SEC) {
  u64 raw_secs = raw_nsecs;
  raw_nsecs = do_div(raw_secs, NSEC_PER_SEC);
  tk->raw_time.tv_sec += raw_secs;
 }
 tk->raw_time.tv_nsec = raw_nsecs;


 tk->ntp_error += tk->ntp_tick << shift;
 tk->ntp_error -= (tk->xtime_interval + tk->xtime_remainder) <<
      (tk->ntp_error_shift + shift);

 return offset;
}





void update_wall_time(void)
{
 struct timekeeper *real_tk = &tk_core.timekeeper;
 struct timekeeper *tk = &shadow_timekeeper;
 cycle_t offset;
 int shift = 0, maxshift;
 unsigned int clock_set = 0;
 unsigned long flags;

 raw_spin_lock_irqsave(&timekeeper_lock, flags);


 if (unlikely(timekeeping_suspended))
  goto out;

 offset = real_tk->cycle_interval;
 offset = clocksource_delta(tk->tkr_mono.read(tk->tkr_mono.clock),
       tk->tkr_mono.cycle_last, tk->tkr_mono.mask);


 if (offset < real_tk->cycle_interval)
  goto out;


 timekeeping_check_update(real_tk, offset);
 shift = ilog2(offset) - ilog2(tk->cycle_interval);
 shift = max(0, shift);

 maxshift = (64 - (ilog2(ntp_tick_length())+1)) - 1;
 shift = min(shift, maxshift);
 while (offset >= tk->cycle_interval) {
  offset = logarithmic_accumulation(tk, offset, shift,
       &clock_set);
  if (offset < tk->cycle_interval<<shift)
   shift--;
 }


 timekeeping_adjust(tk, offset);





 old_vsyscall_fixup(tk);





 clock_set |= accumulate_nsecs_to_secs(tk);

 write_seqcount_begin(&tk_core.seq);
 timekeeping_update(tk, clock_set);
 memcpy(real_tk, tk, sizeof(*tk));

 write_seqcount_end(&tk_core.seq);
out:
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
 if (clock_set)

  clock_was_set_delayed();
}
void getboottime64(struct timespec64 *ts)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 ktime_t t = ktime_sub(tk->offs_real, tk->offs_boot);

 *ts = ktime_to_timespec64(t);
}
EXPORT_SYMBOL_GPL(getboottime64);

unsigned long get_seconds(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;

 return tk->xtime_sec;
}
EXPORT_SYMBOL(get_seconds);

struct timespec __current_kernel_time(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;

 return timespec64_to_timespec(tk_xtime(tk));
}

struct timespec64 current_kernel_time64(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 struct timespec64 now;
 unsigned long seq;

 do {
  seq = read_seqcount_begin(&tk_core.seq);

  now = tk_xtime(tk);
 } while (read_seqcount_retry(&tk_core.seq, seq));

 return now;
}
EXPORT_SYMBOL(current_kernel_time64);

struct timespec64 get_monotonic_coarse64(void)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 struct timespec64 now, mono;
 unsigned long seq;

 do {
  seq = read_seqcount_begin(&tk_core.seq);

  now = tk_xtime(tk);
  mono = tk->wall_to_monotonic;
 } while (read_seqcount_retry(&tk_core.seq, seq));

 set_normalized_timespec64(&now, now.tv_sec + mono.tv_sec,
    now.tv_nsec + mono.tv_nsec);

 return now;
}




void do_timer(unsigned long ticks)
{
 jiffies_64 += ticks;
 calc_global_load(ticks);
}
ktime_t ktime_get_update_offsets_now(unsigned int *cwsseq, ktime_t *offs_real,
         ktime_t *offs_boot, ktime_t *offs_tai)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned int seq;
 ktime_t base;
 u64 nsecs;

 do {
  seq = read_seqcount_begin(&tk_core.seq);

  base = tk->tkr_mono.base;
  nsecs = timekeeping_get_ns(&tk->tkr_mono);
  base = ktime_add_ns(base, nsecs);

  if (*cwsseq != tk->clock_was_set_seq) {
   *cwsseq = tk->clock_was_set_seq;
   *offs_real = tk->offs_real;
   *offs_boot = tk->offs_boot;
   *offs_tai = tk->offs_tai;
  }


  if (unlikely(base.tv64 >= tk->next_leap_ktime.tv64))
   *offs_real = ktime_sub(tk->offs_real, ktime_set(1, 0));

 } while (read_seqcount_retry(&tk_core.seq, seq));

 return base;
}




int do_adjtimex(struct timex *txc)
{
 struct timekeeper *tk = &tk_core.timekeeper;
 unsigned long flags;
 struct timespec64 ts;
 s32 orig_tai, tai;
 int ret;


 ret = ntp_validate_timex(txc);
 if (ret)
  return ret;

 if (txc->modes & ADJ_SETOFFSET) {
  struct timespec delta;
  delta.tv_sec = txc->time.tv_sec;
  delta.tv_nsec = txc->time.tv_usec;
  if (!(txc->modes & ADJ_NANO))
   delta.tv_nsec *= 1000;
  ret = timekeeping_inject_offset(&delta);
  if (ret)
   return ret;
 }

 getnstimeofday64(&ts);

 raw_spin_lock_irqsave(&timekeeper_lock, flags);
 write_seqcount_begin(&tk_core.seq);

 orig_tai = tai = tk->tai_offset;
 ret = __do_adjtimex(txc, &ts, &tai);

 if (tai != orig_tai) {
  __timekeeping_set_tai_offset(tk, tai);
  timekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);
 }
 tk_update_leap_state(tk);

 write_seqcount_end(&tk_core.seq);
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);

 if (tai != orig_tai)
  clock_was_set();

 ntp_notify_cmos_timer();

 return ret;
}




void hardpps(const struct timespec64 *phase_ts, const struct timespec64 *raw_ts)
{
 unsigned long flags;

 raw_spin_lock_irqsave(&timekeeper_lock, flags);
 write_seqcount_begin(&tk_core.seq);

 __hardpps(phase_ts, raw_ts);

 write_seqcount_end(&tk_core.seq);
 raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
}
EXPORT_SYMBOL(hardpps);







void xtime_update(unsigned long ticks)
{
 write_seqlock(&jiffies_lock);
 do_timer(ticks);
 write_sequnlock(&jiffies_lock);
 update_wall_time();
}


static unsigned int sleep_time_bin[32] = {0};

static int tk_debug_show_sleep_time(struct seq_file *s, void *data)
{
 unsigned int bin;
 seq_puts(s, "      time (secs)        count\n");
 seq_puts(s, "------------------------------\n");
 for (bin = 0; bin < 32; bin++) {
  if (sleep_time_bin[bin] == 0)
   continue;
  seq_printf(s, "%10u - %-10u %4u\n",
   bin ? 1 << (bin - 1) : 0, 1 << bin,
    sleep_time_bin[bin]);
 }
 return 0;
}

static int tk_debug_sleep_time_open(struct inode *inode, struct file *file)
{
 return single_open(file, tk_debug_show_sleep_time, NULL);
}

static const struct file_operations tk_debug_sleep_time_fops = {
 .open = tk_debug_sleep_time_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = single_release,
};

static int __init tk_debug_sleep_time_init(void)
{
 struct dentry *d;

 d = debugfs_create_file("sleep_time", 0444, NULL, NULL,
  &tk_debug_sleep_time_fops);
 if (!d) {
  pr_err("Failed to create sleep_time debug file\n");
  return -ENOMEM;
 }

 return 0;
}
late_initcall(tk_debug_sleep_time_init);

void tk_debug_account_sleep_time(struct timespec64 *t)
{
 sleep_time_bin[fls(t->tv_sec)]++;
}




__visible u64 jiffies_64 __cacheline_aligned_in_smp = INITIAL_JIFFIES;

EXPORT_SYMBOL(jiffies_64);





struct tvec {
 struct hlist_head vec[TVN_SIZE];
};

struct tvec_root {
 struct hlist_head vec[TVR_SIZE];
};

struct tvec_base {
 spinlock_t lock;
 struct timer_list *running_timer;
 unsigned long timer_jiffies;
 unsigned long next_timer;
 unsigned long active_timers;
 unsigned long all_timers;
 int cpu;
 bool migration_enabled;
 bool nohz_active;
 struct tvec_root tv1;
 struct tvec tv2;
 struct tvec tv3;
 struct tvec tv4;
 struct tvec tv5;
} ____cacheline_aligned;


static DEFINE_PER_CPU(struct tvec_base, tvec_bases);

unsigned int sysctl_timer_migration = 1;

void timers_update_migration(bool update_nohz)
{
 bool on = sysctl_timer_migration && tick_nohz_active;
 unsigned int cpu;


 if (this_cpu_read(tvec_bases.migration_enabled) == on)
  return;

 for_each_possible_cpu(cpu) {
  per_cpu(tvec_bases.migration_enabled, cpu) = on;
  per_cpu(hrtimer_bases.migration_enabled, cpu) = on;
  if (!update_nohz)
   continue;
  per_cpu(tvec_bases.nohz_active, cpu) = true;
  per_cpu(hrtimer_bases.nohz_active, cpu) = true;
 }
}

int timer_migration_handler(struct ctl_table *table, int write,
       void __user *buffer, size_t *lenp,
       loff_t *ppos)
{
 static DEFINE_MUTEX(mutex);
 int ret;

 mutex_lock(&mutex);
 ret = proc_dointvec(table, write, buffer, lenp, ppos);
 if (!ret && write)
  timers_update_migration(false);
 mutex_unlock(&mutex);
 return ret;
}

static inline struct tvec_base *get_target_base(struct tvec_base *base,
      int pinned)
{
 if (pinned || !base->migration_enabled)
  return this_cpu_ptr(&tvec_bases);
 return per_cpu_ptr(&tvec_bases, get_nohz_timer_target());
}
static inline struct tvec_base *get_target_base(struct tvec_base *base,
      int pinned)
{
 return this_cpu_ptr(&tvec_bases);
}

static unsigned long round_jiffies_common(unsigned long j, int cpu,
  bool force_up)
{
 int rem;
 unsigned long original = j;
 j += cpu * 3;

 rem = j % HZ;
 if (rem < HZ/4 && !force_up)
  j = j - rem;
 else
  j = j - rem + HZ;


 j -= cpu * 3;





 return time_is_after_jiffies(j) ? j : original;
}
unsigned long __round_jiffies(unsigned long j, int cpu)
{
 return round_jiffies_common(j, cpu, false);
}
EXPORT_SYMBOL_GPL(__round_jiffies);
unsigned long __round_jiffies_relative(unsigned long j, int cpu)
{
 unsigned long j0 = jiffies;


 return round_jiffies_common(j + j0, cpu, false) - j0;
}
EXPORT_SYMBOL_GPL(__round_jiffies_relative);
unsigned long round_jiffies(unsigned long j)
{
 return round_jiffies_common(j, raw_smp_processor_id(), false);
}
EXPORT_SYMBOL_GPL(round_jiffies);
unsigned long round_jiffies_relative(unsigned long j)
{
 return __round_jiffies_relative(j, raw_smp_processor_id());
}
EXPORT_SYMBOL_GPL(round_jiffies_relative);
unsigned long __round_jiffies_up(unsigned long j, int cpu)
{
 return round_jiffies_common(j, cpu, true);
}
EXPORT_SYMBOL_GPL(__round_jiffies_up);
unsigned long __round_jiffies_up_relative(unsigned long j, int cpu)
{
 unsigned long j0 = jiffies;


 return round_jiffies_common(j + j0, cpu, true) - j0;
}
EXPORT_SYMBOL_GPL(__round_jiffies_up_relative);
unsigned long round_jiffies_up(unsigned long j)
{
 return round_jiffies_common(j, raw_smp_processor_id(), true);
}
EXPORT_SYMBOL_GPL(round_jiffies_up);
unsigned long round_jiffies_up_relative(unsigned long j)
{
 return __round_jiffies_up_relative(j, raw_smp_processor_id());
}
EXPORT_SYMBOL_GPL(round_jiffies_up_relative);
void set_timer_slack(struct timer_list *timer, int slack_hz)
{
 timer->slack = slack_hz;
}
EXPORT_SYMBOL_GPL(set_timer_slack);

static void
__internal_add_timer(struct tvec_base *base, struct timer_list *timer)
{
 unsigned long expires = timer->expires;
 unsigned long idx = expires - base->timer_jiffies;
 struct hlist_head *vec;

 if (idx < TVR_SIZE) {
  int i = expires & TVR_MASK;
  vec = base->tv1.vec + i;
 } else if (idx < 1 << (TVR_BITS + TVN_BITS)) {
  int i = (expires >> TVR_BITS) & TVN_MASK;
  vec = base->tv2.vec + i;
 } else if (idx < 1 << (TVR_BITS + 2 * TVN_BITS)) {
  int i = (expires >> (TVR_BITS + TVN_BITS)) & TVN_MASK;
  vec = base->tv3.vec + i;
 } else if (idx < 1 << (TVR_BITS + 3 * TVN_BITS)) {
  int i = (expires >> (TVR_BITS + 2 * TVN_BITS)) & TVN_MASK;
  vec = base->tv4.vec + i;
 } else if ((signed long) idx < 0) {




  vec = base->tv1.vec + (base->timer_jiffies & TVR_MASK);
 } else {
  int i;




  if (idx > MAX_TVAL) {
   idx = MAX_TVAL;
   expires = idx + base->timer_jiffies;
  }
  i = (expires >> (TVR_BITS + 3 * TVN_BITS)) & TVN_MASK;
  vec = base->tv5.vec + i;
 }

 hlist_add_head(&timer->entry, vec);
}

static void internal_add_timer(struct tvec_base *base, struct timer_list *timer)
{

 if (!base->all_timers++)
  base->timer_jiffies = jiffies;

 __internal_add_timer(base, timer);



 if (!(timer->flags & TIMER_DEFERRABLE)) {
  if (!base->active_timers++ ||
      time_before(timer->expires, base->next_timer))
   base->next_timer = timer->expires;
 }
 if (base->nohz_active) {
  if (!(timer->flags & TIMER_DEFERRABLE) ||
      tick_nohz_full_cpu(base->cpu))
   wake_up_nohz_cpu(base->cpu);
 }
}

void __timer_stats_timer_set_start_info(struct timer_list *timer, void *addr)
{
 if (timer->start_site)
  return;

 timer->start_site = addr;
 memcpy(timer->start_comm, current->comm, TASK_COMM_LEN);
 timer->start_pid = current->pid;
}

static void timer_stats_account_timer(struct timer_list *timer)
{
 void *site;





 site = READ_ONCE(timer->start_site);
 if (likely(!site))
  return;

 timer_stats_update_stats(timer, timer->start_pid, site,
     timer->function, timer->start_comm,
     timer->flags);
}

static void timer_stats_account_timer(struct timer_list *timer) {}


static struct debug_obj_descr timer_debug_descr;

static void *timer_debug_hint(void *addr)
{
 return ((struct timer_list *) addr)->function;
}

static bool timer_is_static_object(void *addr)
{
 struct timer_list *timer = addr;

 return (timer->entry.pprev == NULL &&
  timer->entry.next == TIMER_ENTRY_STATIC);
}





static bool timer_fixup_init(void *addr, enum debug_obj_state state)
{
 struct timer_list *timer = addr;

 switch (state) {
 case ODEBUG_STATE_ACTIVE:
  del_timer_sync(timer);
  debug_object_init(timer, &timer_debug_descr);
  return true;
 default:
  return false;
 }
}


static void stub_timer(unsigned long data)
{
 WARN_ON(1);
}






static bool timer_fixup_activate(void *addr, enum debug_obj_state state)
{
 struct timer_list *timer = addr;

 switch (state) {
 case ODEBUG_STATE_NOTAVAILABLE:
  setup_timer(timer, stub_timer, 0);
  return true;

 case ODEBUG_STATE_ACTIVE:
  WARN_ON(1);

 default:
  return false;
 }
}





static bool timer_fixup_free(void *addr, enum debug_obj_state state)
{
 struct timer_list *timer = addr;

 switch (state) {
 case ODEBUG_STATE_ACTIVE:
  del_timer_sync(timer);
  debug_object_free(timer, &timer_debug_descr);
  return true;
 default:
  return false;
 }
}





static bool timer_fixup_assert_init(void *addr, enum debug_obj_state state)
{
 struct timer_list *timer = addr;

 switch (state) {
 case ODEBUG_STATE_NOTAVAILABLE:
  setup_timer(timer, stub_timer, 0);
  return true;
 default:
  return false;
 }
}

static struct debug_obj_descr timer_debug_descr = {
 .name = "timer_list",
 .debug_hint = timer_debug_hint,
 .is_static_object = timer_is_static_object,
 .fixup_init = timer_fixup_init,
 .fixup_activate = timer_fixup_activate,
 .fixup_free = timer_fixup_free,
 .fixup_assert_init = timer_fixup_assert_init,
};

static inline void debug_timer_init(struct timer_list *timer)
{
 debug_object_init(timer, &timer_debug_descr);
}

static inline void debug_timer_activate(struct timer_list *timer)
{
 debug_object_activate(timer, &timer_debug_descr);
}

static inline void debug_timer_deactivate(struct timer_list *timer)
{
 debug_object_deactivate(timer, &timer_debug_descr);
}

static inline void debug_timer_free(struct timer_list *timer)
{
 debug_object_free(timer, &timer_debug_descr);
}

static inline void debug_timer_assert_init(struct timer_list *timer)
{
 debug_object_assert_init(timer, &timer_debug_descr);
}

static void do_init_timer(struct timer_list *timer, unsigned int flags,
     const char *name, struct lock_class_key *key);

void init_timer_on_stack_key(struct timer_list *timer, unsigned int flags,
        const char *name, struct lock_class_key *key)
{
 debug_object_init_on_stack(timer, &timer_debug_descr);
 do_init_timer(timer, flags, name, key);
}
EXPORT_SYMBOL_GPL(init_timer_on_stack_key);

void destroy_timer_on_stack(struct timer_list *timer)
{
 debug_object_free(timer, &timer_debug_descr);
}
EXPORT_SYMBOL_GPL(destroy_timer_on_stack);

static inline void debug_timer_init(struct timer_list *timer) { }
static inline void debug_timer_activate(struct timer_list *timer) { }
static inline void debug_timer_deactivate(struct timer_list *timer) { }
static inline void debug_timer_assert_init(struct timer_list *timer) { }

static inline void debug_init(struct timer_list *timer)
{
 debug_timer_init(timer);
 trace_timer_init(timer);
}

static inline void
debug_activate(struct timer_list *timer, unsigned long expires)
{
 debug_timer_activate(timer);
 trace_timer_start(timer, expires, timer->flags);
}

static inline void debug_deactivate(struct timer_list *timer)
{
 debug_timer_deactivate(timer);
 trace_timer_cancel(timer);
}

static inline void debug_assert_init(struct timer_list *timer)
{
 debug_timer_assert_init(timer);
}

static void do_init_timer(struct timer_list *timer, unsigned int flags,
     const char *name, struct lock_class_key *key)
{
 timer->entry.pprev = NULL;
 timer->flags = flags | raw_smp_processor_id();
 timer->slack = -1;
 timer->start_site = NULL;
 timer->start_pid = -1;
 memset(timer->start_comm, 0, TASK_COMM_LEN);
 lockdep_init_map(&timer->lockdep_map, name, key, 0);
}
void init_timer_key(struct timer_list *timer, unsigned int flags,
      const char *name, struct lock_class_key *key)
{
 debug_init(timer);
 do_init_timer(timer, flags, name, key);
}
EXPORT_SYMBOL(init_timer_key);

static inline void detach_timer(struct timer_list *timer, bool clear_pending)
{
 struct hlist_node *entry = &timer->entry;

 debug_deactivate(timer);

 __hlist_del(entry);
 if (clear_pending)
  entry->pprev = NULL;
 entry->next = LIST_POISON2;
}

static inline void
detach_expired_timer(struct timer_list *timer, struct tvec_base *base)
{
 detach_timer(timer, true);
 if (!(timer->flags & TIMER_DEFERRABLE))
  base->active_timers--;
 base->all_timers--;
}

static int detach_if_pending(struct timer_list *timer, struct tvec_base *base,
        bool clear_pending)
{
 if (!timer_pending(timer))
  return 0;

 detach_timer(timer, clear_pending);
 if (!(timer->flags & TIMER_DEFERRABLE)) {
  base->active_timers--;
  if (timer->expires == base->next_timer)
   base->next_timer = base->timer_jiffies;
 }

 if (!--base->all_timers)
  base->timer_jiffies = jiffies;
 return 1;
}
static struct tvec_base *lock_timer_base(struct timer_list *timer,
     unsigned long *flags)
 __acquires(timer->base->lock)
{
 for (;;) {
  u32 tf = timer->flags;
  struct tvec_base *base;

  if (!(tf & TIMER_MIGRATING)) {
   base = per_cpu_ptr(&tvec_bases, tf & TIMER_CPUMASK);
   spin_lock_irqsave(&base->lock, *flags);
   if (timer->flags == tf)
    return base;
   spin_unlock_irqrestore(&base->lock, *flags);
  }
  cpu_relax();
 }
}

static inline int
__mod_timer(struct timer_list *timer, unsigned long expires,
     bool pending_only, int pinned)
{
 struct tvec_base *base, *new_base;
 unsigned long flags;
 int ret = 0;

 timer_stats_timer_set_start_info(timer);
 BUG_ON(!timer->function);

 base = lock_timer_base(timer, &flags);

 ret = detach_if_pending(timer, base, false);
 if (!ret && pending_only)
  goto out_unlock;

 debug_activate(timer, expires);

 new_base = get_target_base(base, pinned);

 if (base != new_base) {







  if (likely(base->running_timer != timer)) {

   timer->flags |= TIMER_MIGRATING;

   spin_unlock(&base->lock);
   base = new_base;
   spin_lock(&base->lock);
   WRITE_ONCE(timer->flags,
       (timer->flags & ~TIMER_BASEMASK) | base->cpu);
  }
 }

 timer->expires = expires;
 internal_add_timer(base, timer);

out_unlock:
 spin_unlock_irqrestore(&base->lock, flags);

 return ret;
}
int mod_timer_pending(struct timer_list *timer, unsigned long expires)
{
 return __mod_timer(timer, expires, true, TIMER_NOT_PINNED);
}
EXPORT_SYMBOL(mod_timer_pending);
static inline
unsigned long apply_slack(struct timer_list *timer, unsigned long expires)
{
 unsigned long expires_limit, mask;
 int bit;

 if (timer->slack >= 0) {
  expires_limit = expires + timer->slack;
 } else {
  long delta = expires - jiffies;

  if (delta < 256)
   return expires;

  expires_limit = expires + delta / 256;
 }
 mask = expires ^ expires_limit;
 if (mask == 0)
  return expires;

 bit = __fls(mask);

 mask = (1UL << bit) - 1;

 expires_limit = expires_limit & ~(mask);

 return expires_limit;
}
int mod_timer(struct timer_list *timer, unsigned long expires)
{
 expires = apply_slack(timer, expires);






 if (timer_pending(timer) && timer->expires == expires)
  return 1;

 return __mod_timer(timer, expires, false, TIMER_NOT_PINNED);
}
EXPORT_SYMBOL(mod_timer);
int mod_timer_pinned(struct timer_list *timer, unsigned long expires)
{
 if (timer->expires == expires && timer_pending(timer))
  return 1;

 return __mod_timer(timer, expires, false, TIMER_PINNED);
}
EXPORT_SYMBOL(mod_timer_pinned);
void add_timer(struct timer_list *timer)
{
 BUG_ON(timer_pending(timer));
 mod_timer(timer, timer->expires);
}
EXPORT_SYMBOL(add_timer);
void add_timer_on(struct timer_list *timer, int cpu)
{
 struct tvec_base *new_base = per_cpu_ptr(&tvec_bases, cpu);
 struct tvec_base *base;
 unsigned long flags;

 timer_stats_timer_set_start_info(timer);
 BUG_ON(timer_pending(timer) || !timer->function);






 base = lock_timer_base(timer, &flags);
 if (base != new_base) {
  timer->flags |= TIMER_MIGRATING;

  spin_unlock(&base->lock);
  base = new_base;
  spin_lock(&base->lock);
  WRITE_ONCE(timer->flags,
      (timer->flags & ~TIMER_BASEMASK) | cpu);
 }

 debug_activate(timer, timer->expires);
 internal_add_timer(base, timer);
 spin_unlock_irqrestore(&base->lock, flags);
}
EXPORT_SYMBOL_GPL(add_timer_on);
int del_timer(struct timer_list *timer)
{
 struct tvec_base *base;
 unsigned long flags;
 int ret = 0;

 debug_assert_init(timer);

 timer_stats_timer_clear_start_info(timer);
 if (timer_pending(timer)) {
  base = lock_timer_base(timer, &flags);
  ret = detach_if_pending(timer, base, true);
  spin_unlock_irqrestore(&base->lock, flags);
 }

 return ret;
}
EXPORT_SYMBOL(del_timer);
int try_to_del_timer_sync(struct timer_list *timer)
{
 struct tvec_base *base;
 unsigned long flags;
 int ret = -1;

 debug_assert_init(timer);

 base = lock_timer_base(timer, &flags);

 if (base->running_timer != timer) {
  timer_stats_timer_clear_start_info(timer);
  ret = detach_if_pending(timer, base, true);
 }
 spin_unlock_irqrestore(&base->lock, flags);

 return ret;
}
EXPORT_SYMBOL(try_to_del_timer_sync);

int del_timer_sync(struct timer_list *timer)
{
 unsigned long flags;





 local_irq_save(flags);
 lock_map_acquire(&timer->lockdep_map);
 lock_map_release(&timer->lockdep_map);
 local_irq_restore(flags);




 WARN_ON(in_irq() && !(timer->flags & TIMER_IRQSAFE));
 for (;;) {
  int ret = try_to_del_timer_sync(timer);
  if (ret >= 0)
   return ret;
  cpu_relax();
 }
}
EXPORT_SYMBOL(del_timer_sync);

static int cascade(struct tvec_base *base, struct tvec *tv, int index)
{

 struct timer_list *timer;
 struct hlist_node *tmp;
 struct hlist_head tv_list;

 hlist_move_list(tv->vec + index, &tv_list);





 hlist_for_each_entry_safe(timer, tmp, &tv_list, entry) {

  __internal_add_timer(base, timer);
 }

 return index;
}

static void call_timer_fn(struct timer_list *timer, void (*fn)(unsigned long),
     unsigned long data)
{
 int count = preempt_count();








 struct lockdep_map lockdep_map;

 lockdep_copy_map(&lockdep_map, &timer->lockdep_map);





 lock_map_acquire(&lockdep_map);

 trace_timer_expire_entry(timer);
 fn(data);
 trace_timer_expire_exit(timer);

 lock_map_release(&lockdep_map);

 if (count != preempt_count()) {
  WARN_ONCE(1, "timer: %pF preempt leak: %08x -> %08x\n",
     fn, count, preempt_count());






  preempt_count_set(count);
 }
}

static inline void __run_timers(struct tvec_base *base)
{
 struct timer_list *timer;

 spin_lock_irq(&base->lock);

 while (time_after_eq(jiffies, base->timer_jiffies)) {
  struct hlist_head work_list;
  struct hlist_head *head = &work_list;
  int index;

  if (!base->all_timers) {
   base->timer_jiffies = jiffies;
   break;
  }

  index = base->timer_jiffies & TVR_MASK;




  if (!index &&
   (!cascade(base, &base->tv2, INDEX(0))) &&
    (!cascade(base, &base->tv3, INDEX(1))) &&
     !cascade(base, &base->tv4, INDEX(2)))
   cascade(base, &base->tv5, INDEX(3));
  ++base->timer_jiffies;
  hlist_move_list(base->tv1.vec + index, head);
  while (!hlist_empty(head)) {
   void (*fn)(unsigned long);
   unsigned long data;
   bool irqsafe;

   timer = hlist_entry(head->first, struct timer_list, entry);
   fn = timer->function;
   data = timer->data;
   irqsafe = timer->flags & TIMER_IRQSAFE;

   timer_stats_account_timer(timer);

   base->running_timer = timer;
   detach_expired_timer(timer, base);

   if (irqsafe) {
    spin_unlock(&base->lock);
    call_timer_fn(timer, fn, data);
    spin_lock(&base->lock);
   } else {
    spin_unlock_irq(&base->lock);
    call_timer_fn(timer, fn, data);
    spin_lock_irq(&base->lock);
   }
  }
 }
 base->running_timer = NULL;
 spin_unlock_irq(&base->lock);
}






static unsigned long __next_timer_interrupt(struct tvec_base *base)
{
 unsigned long timer_jiffies = base->timer_jiffies;
 unsigned long expires = timer_jiffies + NEXT_TIMER_MAX_DELTA;
 int index, slot, array, found = 0;
 struct timer_list *nte;
 struct tvec *varray[4];


 index = slot = timer_jiffies & TVR_MASK;
 do {
  hlist_for_each_entry(nte, base->tv1.vec + slot, entry) {
   if (nte->flags & TIMER_DEFERRABLE)
    continue;

   found = 1;
   expires = nte->expires;

   if (!index || slot < index)
    goto cascade;
   return expires;
  }
  slot = (slot + 1) & TVR_MASK;
 } while (slot != index);

cascade:

 if (index)
  timer_jiffies += TVR_SIZE - index;
 timer_jiffies >>= TVR_BITS;


 varray[0] = &base->tv2;
 varray[1] = &base->tv3;
 varray[2] = &base->tv4;
 varray[3] = &base->tv5;

 for (array = 0; array < 4; array++) {
  struct tvec *varp = varray[array];

  index = slot = timer_jiffies & TVN_MASK;
  do {
   hlist_for_each_entry(nte, varp->vec + slot, entry) {
    if (nte->flags & TIMER_DEFERRABLE)
     continue;

    found = 1;
    if (time_before(nte->expires, expires))
     expires = nte->expires;
   }




   if (found) {

    if (!index || slot < index)
     break;
    return expires;
   }
   slot = (slot + 1) & TVN_MASK;
  } while (slot != index);

  if (index)
   timer_jiffies += TVN_SIZE - index;
  timer_jiffies >>= TVN_BITS;
 }
 return expires;
}





static u64 cmp_next_hrtimer_event(u64 basem, u64 expires)
{
 u64 nextevt = hrtimer_get_next_event();





 if (expires <= nextevt)
  return expires;





 if (nextevt <= basem)
  return basem;
 return DIV_ROUND_UP_ULL(nextevt, TICK_NSEC) * TICK_NSEC;
}
u64 get_next_timer_interrupt(unsigned long basej, u64 basem)
{
 struct tvec_base *base = this_cpu_ptr(&tvec_bases);
 u64 expires = KTIME_MAX;
 unsigned long nextevt;





 if (cpu_is_offline(smp_processor_id()))
  return expires;

 spin_lock(&base->lock);
 if (base->active_timers) {
  if (time_before_eq(base->next_timer, base->timer_jiffies))
   base->next_timer = __next_timer_interrupt(base);
  nextevt = base->next_timer;
  if (time_before_eq(nextevt, basej))
   expires = basem;
  else
   expires = basem + (nextevt - basej) * TICK_NSEC;
 }
 spin_unlock(&base->lock);

 return cmp_next_hrtimer_event(basem, expires);
}





void update_process_times(int user_tick)
{
 struct task_struct *p = current;


 account_process_tick(p, user_tick);
 run_local_timers();
 rcu_check_callbacks(user_tick);
 if (in_irq())
  irq_work_tick();
 scheduler_tick();
 run_posix_cpu_timers(p);
}




static void run_timer_softirq(struct softirq_action *h)
{
 struct tvec_base *base = this_cpu_ptr(&tvec_bases);

 if (time_after_eq(jiffies, base->timer_jiffies))
  __run_timers(base);
}




void run_local_timers(void)
{
 hrtimer_run_queues();
 raise_softirq(TIMER_SOFTIRQ);
}






SYSCALL_DEFINE1(alarm, unsigned int, seconds)
{
 return alarm_setitimer(seconds);
}


static void process_timeout(unsigned long __data)
{
 wake_up_process((struct task_struct *)__data);
}
signed long __sched schedule_timeout(signed long timeout)
{
 struct timer_list timer;
 unsigned long expire;

 switch (timeout)
 {
 case MAX_SCHEDULE_TIMEOUT:







  schedule();
  goto out;
 default:







  if (timeout < 0) {
   printk(KERN_ERR "schedule_timeout: wrong timeout "
    "value %lx\n", timeout);
   dump_stack();
   current->state = TASK_RUNNING;
   goto out;
  }
 }

 expire = timeout + jiffies;

 setup_timer_on_stack(&timer, process_timeout, (unsigned long)current);
 __mod_timer(&timer, expire, false, TIMER_NOT_PINNED);
 schedule();
 del_singleshot_timer_sync(&timer);


 destroy_timer_on_stack(&timer);

 timeout = expire - jiffies;

 out:
 return timeout < 0 ? 0 : timeout;
}
EXPORT_SYMBOL(schedule_timeout);





signed long __sched schedule_timeout_interruptible(signed long timeout)
{
 __set_current_state(TASK_INTERRUPTIBLE);
 return schedule_timeout(timeout);
}
EXPORT_SYMBOL(schedule_timeout_interruptible);

signed long __sched schedule_timeout_killable(signed long timeout)
{
 __set_current_state(TASK_KILLABLE);
 return schedule_timeout(timeout);
}
EXPORT_SYMBOL(schedule_timeout_killable);

signed long __sched schedule_timeout_uninterruptible(signed long timeout)
{
 __set_current_state(TASK_UNINTERRUPTIBLE);
 return schedule_timeout(timeout);
}
EXPORT_SYMBOL(schedule_timeout_uninterruptible);





signed long __sched schedule_timeout_idle(signed long timeout)
{
 __set_current_state(TASK_IDLE);
 return schedule_timeout(timeout);
}
EXPORT_SYMBOL(schedule_timeout_idle);

static void migrate_timer_list(struct tvec_base *new_base, struct hlist_head *head)
{
 struct timer_list *timer;
 int cpu = new_base->cpu;

 while (!hlist_empty(head)) {
  timer = hlist_entry(head->first, struct timer_list, entry);

  detach_timer(timer, false);
  timer->flags = (timer->flags & ~TIMER_BASEMASK) | cpu;
  internal_add_timer(new_base, timer);
 }
}

static void migrate_timers(int cpu)
{
 struct tvec_base *old_base;
 struct tvec_base *new_base;
 int i;

 BUG_ON(cpu_online(cpu));
 old_base = per_cpu_ptr(&tvec_bases, cpu);
 new_base = get_cpu_ptr(&tvec_bases);




 spin_lock_irq(&new_base->lock);
 spin_lock_nested(&old_base->lock, SINGLE_DEPTH_NESTING);

 BUG_ON(old_base->running_timer);

 for (i = 0; i < TVR_SIZE; i++)
  migrate_timer_list(new_base, old_base->tv1.vec + i);
 for (i = 0; i < TVN_SIZE; i++) {
  migrate_timer_list(new_base, old_base->tv2.vec + i);
  migrate_timer_list(new_base, old_base->tv3.vec + i);
  migrate_timer_list(new_base, old_base->tv4.vec + i);
  migrate_timer_list(new_base, old_base->tv5.vec + i);
 }

 old_base->active_timers = 0;
 old_base->all_timers = 0;

 spin_unlock(&old_base->lock);
 spin_unlock_irq(&new_base->lock);
 put_cpu_ptr(&tvec_bases);
}

static int timer_cpu_notify(struct notifier_block *self,
    unsigned long action, void *hcpu)
{
 switch (action) {
 case CPU_DEAD:
 case CPU_DEAD_FROZEN:
  migrate_timers((long)hcpu);
  break;
 default:
  break;
 }

 return NOTIFY_OK;
}

static inline void timer_register_cpu_notifier(void)
{
 cpu_notifier(timer_cpu_notify, 0);
}
static inline void timer_register_cpu_notifier(void) { }

static void __init init_timer_cpu(int cpu)
{
 struct tvec_base *base = per_cpu_ptr(&tvec_bases, cpu);

 base->cpu = cpu;
 spin_lock_init(&base->lock);

 base->timer_jiffies = jiffies;
 base->next_timer = base->timer_jiffies;
}

static void __init init_timer_cpus(void)
{
 int cpu;

 for_each_possible_cpu(cpu)
  init_timer_cpu(cpu);
}

void __init init_timers(void)
{
 init_timer_cpus();
 init_timer_stats();
 timer_register_cpu_notifier();
 open_softirq(TIMER_SOFTIRQ, run_timer_softirq);
}





void msleep(unsigned int msecs)
{
 unsigned long timeout = msecs_to_jiffies(msecs) + 1;

 while (timeout)
  timeout = schedule_timeout_uninterruptible(timeout);
}

EXPORT_SYMBOL(msleep);





unsigned long msleep_interruptible(unsigned int msecs)
{
 unsigned long timeout = msecs_to_jiffies(msecs) + 1;

 while (timeout && !signal_pending(current))
  timeout = schedule_timeout_interruptible(timeout);
 return jiffies_to_msecs(timeout);
}

EXPORT_SYMBOL(msleep_interruptible);

static void __sched do_usleep_range(unsigned long min, unsigned long max)
{
 ktime_t kmin;
 u64 delta;

 kmin = ktime_set(0, min * NSEC_PER_USEC);
 delta = (u64)(max - min) * NSEC_PER_USEC;
 schedule_hrtimeout_range(&kmin, delta, HRTIMER_MODE_REL);
}






void __sched usleep_range(unsigned long min, unsigned long max)
{
 __set_current_state(TASK_UNINTERRUPTIBLE);
 do_usleep_range(min, max);
}
EXPORT_SYMBOL(usleep_range);



struct timer_list_iter {
 int cpu;
 bool second_pass;
 u64 now;
};

typedef void (*print_fn_t)(struct seq_file *m, unsigned int *classes);





__printf(2, 3)
static void SEQ_printf(struct seq_file *m, const char *fmt, ...)
{
 va_list args;

 va_start(args, fmt);

 if (m)
  seq_vprintf(m, fmt, args);
 else
  vprintk(fmt, args);

 va_end(args);
}

static void print_name_offset(struct seq_file *m, void *sym)
{
 char symname[KSYM_NAME_LEN];

 if (lookup_symbol_name((unsigned long)sym, symname) < 0)
  SEQ_printf(m, "<%pK>", sym);
 else
  SEQ_printf(m, "%s", symname);
}

static void
print_timer(struct seq_file *m, struct hrtimer *taddr, struct hrtimer *timer,
     int idx, u64 now)
{
 char tmp[TASK_COMM_LEN + 1];
 SEQ_printf(m, " #%d: ", idx);
 print_name_offset(m, taddr);
 SEQ_printf(m, ", ");
 print_name_offset(m, timer->function);
 SEQ_printf(m, ", S:%02x", timer->state);
 SEQ_printf(m, ", ");
 print_name_offset(m, timer->start_site);
 memcpy(tmp, timer->start_comm, TASK_COMM_LEN);
 tmp[TASK_COMM_LEN] = 0;
 SEQ_printf(m, ", %s/%d", tmp, timer->start_pid);
 SEQ_printf(m, "\n");
 SEQ_printf(m, " # expires at %Lu-%Lu nsecs [in %Ld to %Ld nsecs]\n",
  (unsigned long long)ktime_to_ns(hrtimer_get_softexpires(timer)),
  (unsigned long long)ktime_to_ns(hrtimer_get_expires(timer)),
  (long long)(ktime_to_ns(hrtimer_get_softexpires(timer)) - now),
  (long long)(ktime_to_ns(hrtimer_get_expires(timer)) - now));
}

static void
print_active_timers(struct seq_file *m, struct hrtimer_clock_base *base,
      u64 now)
{
 struct hrtimer *timer, tmp;
 unsigned long next = 0, i;
 struct timerqueue_node *curr;
 unsigned long flags;

next_one:
 i = 0;
 raw_spin_lock_irqsave(&base->cpu_base->lock, flags);

 curr = timerqueue_getnext(&base->active);




 while (curr && i < next) {
  curr = timerqueue_iterate_next(curr);
  i++;
 }

 if (curr) {

  timer = container_of(curr, struct hrtimer, node);
  tmp = *timer;
  raw_spin_unlock_irqrestore(&base->cpu_base->lock, flags);

  print_timer(m, timer, &tmp, i, now);
  next++;
  goto next_one;
 }
 raw_spin_unlock_irqrestore(&base->cpu_base->lock, flags);
}

static void
print_base(struct seq_file *m, struct hrtimer_clock_base *base, u64 now)
{
 SEQ_printf(m, "  .base:       %pK\n", base);
 SEQ_printf(m, "  .index:      %d\n", base->index);

 SEQ_printf(m, "  .resolution: %u nsecs\n", (unsigned) hrtimer_resolution);

 SEQ_printf(m, "  .get_time:   ");
 print_name_offset(m, base->get_time);
 SEQ_printf(m, "\n");
 SEQ_printf(m, "  .offset:     %Lu nsecs\n",
     (unsigned long long) ktime_to_ns(base->offset));
 SEQ_printf(m, "active timers:\n");
 print_active_timers(m, base, now + ktime_to_ns(base->offset));
}

static void print_cpu(struct seq_file *m, int cpu, u64 now)
{
 struct hrtimer_cpu_base *cpu_base = &per_cpu(hrtimer_bases, cpu);
 int i;

 SEQ_printf(m, "cpu: %d\n", cpu);
 for (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {
  SEQ_printf(m, " clock %d:\n", i);
  print_base(m, cpu_base->clock_base + i, now);
 }
 SEQ_printf(m, "  .%-15s: %Lu\n", #x, \
     (unsigned long long)(cpu_base->x))
 SEQ_printf(m, "  .%-15s: %Lu nsecs\n", #x, \
     (unsigned long long)(ktime_to_ns(cpu_base->x)))

 P_ns(expires_next);
 P(hres_active);
 P(nr_events);
 P(nr_retries);
 P(nr_hangs);
 P(max_hang_time);

 SEQ_printf(m, "  .%-15s: %Lu\n", #x, \
     (unsigned long long)(ts->x))
 SEQ_printf(m, "  .%-15s: %Lu nsecs\n", #x, \
     (unsigned long long)(ktime_to_ns(ts->x)))
 {
  struct tick_sched *ts = tick_get_tick_sched(cpu);
  P(nohz_mode);
  P_ns(last_tick);
  P(tick_stopped);
  P(idle_jiffies);
  P(idle_calls);
  P(idle_sleeps);
  P_ns(idle_entrytime);
  P_ns(idle_waketime);
  P_ns(idle_exittime);
  P_ns(idle_sleeptime);
  P_ns(iowait_sleeptime);
  P(last_jiffies);
  P(next_timer);
  P_ns(idle_expires);
  SEQ_printf(m, "jiffies: %Lu\n",
      (unsigned long long)jiffies);
 }

 SEQ_printf(m, "\n");
}

static void
print_tickdevice(struct seq_file *m, struct tick_device *td, int cpu)
{
 struct clock_event_device *dev = td->evtdev;

 SEQ_printf(m, "Tick Device: mode:     %d\n", td->mode);
 if (cpu < 0)
  SEQ_printf(m, "Broadcast device\n");
 else
  SEQ_printf(m, "Per CPU device: %d\n", cpu);

 SEQ_printf(m, "Clock Event Device: ");
 if (!dev) {
  SEQ_printf(m, "<NULL>\n");
  return;
 }
 SEQ_printf(m, "%s\n", dev->name);
 SEQ_printf(m, " max_delta_ns:   %llu\n",
     (unsigned long long) dev->max_delta_ns);
 SEQ_printf(m, " min_delta_ns:   %llu\n",
     (unsigned long long) dev->min_delta_ns);
 SEQ_printf(m, " mult:           %u\n", dev->mult);
 SEQ_printf(m, " shift:          %u\n", dev->shift);
 SEQ_printf(m, " mode:           %d\n", clockevent_get_state(dev));
 SEQ_printf(m, " next_event:     %Ld nsecs\n",
     (unsigned long long) ktime_to_ns(dev->next_event));

 SEQ_printf(m, " set_next_event: ");
 print_name_offset(m, dev->set_next_event);
 SEQ_printf(m, "\n");

 if (dev->set_state_shutdown) {
  SEQ_printf(m, " shutdown: ");
  print_name_offset(m, dev->set_state_shutdown);
  SEQ_printf(m, "\n");
 }

 if (dev->set_state_periodic) {
  SEQ_printf(m, " periodic: ");
  print_name_offset(m, dev->set_state_periodic);
  SEQ_printf(m, "\n");
 }

 if (dev->set_state_oneshot) {
  SEQ_printf(m, " oneshot:  ");
  print_name_offset(m, dev->set_state_oneshot);
  SEQ_printf(m, "\n");
 }

 if (dev->set_state_oneshot_stopped) {
  SEQ_printf(m, " oneshot stopped: ");
  print_name_offset(m, dev->set_state_oneshot_stopped);
  SEQ_printf(m, "\n");
 }

 if (dev->tick_resume) {
  SEQ_printf(m, " resume:   ");
  print_name_offset(m, dev->tick_resume);
  SEQ_printf(m, "\n");
 }

 SEQ_printf(m, " event_handler:  ");
 print_name_offset(m, dev->event_handler);
 SEQ_printf(m, "\n");
 SEQ_printf(m, " retries:        %lu\n", dev->retries);
 SEQ_printf(m, "\n");
}

static void timer_list_show_tickdevices_header(struct seq_file *m)
{
 print_tickdevice(m, tick_get_broadcast_device(), -1);
 SEQ_printf(m, "tick_broadcast_mask: %*pb\n",
     cpumask_pr_args(tick_get_broadcast_mask()));
 SEQ_printf(m, "tick_broadcast_oneshot_mask: %*pb\n",
     cpumask_pr_args(tick_get_broadcast_oneshot_mask()));
 SEQ_printf(m, "\n");
}

static inline void timer_list_header(struct seq_file *m, u64 now)
{
 SEQ_printf(m, "Timer List Version: v0.8\n");
 SEQ_printf(m, "HRTIMER_MAX_CLOCK_BASES: %d\n", HRTIMER_MAX_CLOCK_BASES);
 SEQ_printf(m, "now at %Ld nsecs\n", (unsigned long long)now);
 SEQ_printf(m, "\n");
}

static int timer_list_show(struct seq_file *m, void *v)
{
 struct timer_list_iter *iter = v;

 if (iter->cpu == -1 && !iter->second_pass)
  timer_list_header(m, iter->now);
 else if (!iter->second_pass)
  print_cpu(m, iter->cpu, iter->now);
 else if (iter->cpu == -1 && iter->second_pass)
  timer_list_show_tickdevices_header(m);
 else
  print_tickdevice(m, tick_get_device(iter->cpu), iter->cpu);
 return 0;
}

void sysrq_timer_list_show(void)
{
 u64 now = ktime_to_ns(ktime_get());
 int cpu;

 timer_list_header(NULL, now);

 for_each_online_cpu(cpu)
  print_cpu(NULL, cpu, now);

 timer_list_show_tickdevices_header(NULL);
 for_each_online_cpu(cpu)
  print_tickdevice(NULL, tick_get_device(cpu), cpu);
 return;
}

static void *move_iter(struct timer_list_iter *iter, loff_t offset)
{
 for (; offset; offset--) {
  iter->cpu = cpumask_next(iter->cpu, cpu_online_mask);
  if (iter->cpu >= nr_cpu_ids) {
   if (!iter->second_pass) {
    iter->cpu = -1;
    iter->second_pass = true;
   } else
    return NULL;
   return NULL;
  }
 }
 return iter;
}

static void *timer_list_start(struct seq_file *file, loff_t *offset)
{
 struct timer_list_iter *iter = file->private;

 if (!*offset)
  iter->now = ktime_to_ns(ktime_get());
 iter->cpu = -1;
 iter->second_pass = false;
 return move_iter(iter, *offset);
}

static void *timer_list_next(struct seq_file *file, void *v, loff_t *offset)
{
 struct timer_list_iter *iter = file->private;
 ++*offset;
 return move_iter(iter, 1);
}

static void timer_list_stop(struct seq_file *seq, void *v)
{
}

static const struct seq_operations timer_list_sops = {
 .start = timer_list_start,
 .next = timer_list_next,
 .stop = timer_list_stop,
 .show = timer_list_show,
};

static int timer_list_open(struct inode *inode, struct file *filp)
{
 return seq_open_private(filp, &timer_list_sops,
   sizeof(struct timer_list_iter));
}

static const struct file_operations timer_list_fops = {
 .open = timer_list_open,
 .read = seq_read,
 .llseek = seq_lseek,
 .release = seq_release_private,
};

static int __init init_timer_list_procfs(void)
{
 struct proc_dir_entry *pe;

 pe = proc_create("timer_list", 0444, NULL, &timer_list_fops);
 if (!pe)
  return -ENOMEM;
 return 0;
}
__initcall(init_timer_list_procfs);







struct entry {



 struct entry *next;




 void *timer;
 void *start_func;
 void *expire_func;
 pid_t pid;




 unsigned long count;
 u32 flags;





 char comm[TASK_COMM_LEN + 1];

} ____cacheline_aligned_in_smp;




static DEFINE_RAW_SPINLOCK(table_lock);




static DEFINE_PER_CPU(raw_spinlock_t, tstats_lookup_lock);




static DEFINE_MUTEX(show_mutex);




int __read_mostly timer_stats_active;




static ktime_t time_start, time_stop;

static unsigned long nr_entries;
static struct entry entries[MAX_ENTRIES];

static atomic_t overflow_count;





 (((unsigned long)(entry)->timer ^ \
   (unsigned long)(entry)->start_func ^ \
   (unsigned long)(entry)->expire_func ^ \
   (unsigned long)(entry)->pid ) & TSTAT_HASH_MASK)


static struct entry *tstat_hash_table[TSTAT_HASH_SIZE] __read_mostly;

static void reset_entries(void)
{
 nr_entries = 0;
 memset(entries, 0, sizeof(entries));
 memset(tstat_hash_table, 0, sizeof(tstat_hash_table));
 atomic_set(&overflow_count, 0);
}

static struct entry *alloc_entry(void)
{
 if (nr_entries >= MAX_ENTRIES)
  return NULL;

 return entries + nr_entries++;
}

static int match_entries(struct entry *entry1, struct entry *entry2)
{
 return entry1->timer == entry2->timer &&
        entry1->start_func == entry2->start_func &&
        entry1->expire_func == entry2->expire_func &&
        entry1->pid == entry2->pid;
}






static struct entry *tstat_lookup(struct entry *entry, char *comm)
{
 struct entry **head, *curr, *prev;

 head = tstat_hashentry(entry);
 curr = *head;






 while (curr) {
  if (match_entries(curr, entry))
   return curr;

  curr = curr->next;
 }



 prev = NULL;
 curr = *head;

 raw_spin_lock(&table_lock);



 while (curr) {
  if (match_entries(curr, entry))
   goto out_unlock;

  prev = curr;
  curr = curr->next;
 }

 curr = alloc_entry();
 if (curr) {
  *curr = *entry;
  curr->count = 0;
  curr->next = NULL;
  memcpy(curr->comm, comm, TASK_COMM_LEN);

  smp_mb();

  if (prev)
   prev->next = curr;
  else
   *head = curr;
 }
 out_unlock:
 raw_spin_unlock(&table_lock);

 return curr;
}
void timer_stats_update_stats(void *timer, pid_t pid, void *startf,
         void *timerf, char *comm, u32 tflags)
{



 raw_spinlock_t *lock;
 struct entry *entry, input;
 unsigned long flags;

 if (likely(!timer_stats_active))
  return;

 lock = &per_cpu(tstats_lookup_lock, raw_smp_processor_id());

 input.timer = timer;
 input.start_func = startf;
 input.expire_func = timerf;
 input.pid = pid;
 input.flags = tflags;

 raw_spin_lock_irqsave(lock, flags);
 if (!timer_stats_active)
  goto out_unlock;

 entry = tstat_lookup(&input, comm);
 if (likely(entry))
  entry->count++;
 else
  atomic_inc(&overflow_count);

 out_unlock:
 raw_spin_unlock_irqrestore(lock, flags);
}

static void print_name_offset(struct seq_file *m, unsigned long addr)
{
 char symname[KSYM_NAME_LEN];

 if (lookup_symbol_name(addr, symname) < 0)
  seq_printf(m, "<%p>", (void *)addr);
 else
  seq_printf(m, "%s", symname);
}

static int tstats_show(struct seq_file *m, void *v)
{
 struct timespec period;
 struct entry *entry;
 unsigned long ms;
 long events = 0;
 ktime_t time;
 int i;

 mutex_lock(&show_mutex);



 if (timer_stats_active)
  time_stop = ktime_get();

 time = ktime_sub(time_stop, time_start);

 period = ktime_to_timespec(time);
 ms = period.tv_nsec / 1000000;

 seq_puts(m, "Timer Stats Version: v0.3\n");
 seq_printf(m, "Sample period: %ld.%03ld s\n", period.tv_sec, ms);
 if (atomic_read(&overflow_count))
  seq_printf(m, "Overflow: %d entries\n", atomic_read(&overflow_count));
 seq_printf(m, "Collection: %s\n", timer_stats_active ? "active" : "inactive");

 for (i = 0; i < nr_entries; i++) {
  entry = entries + i;
  if (entry->flags & TIMER_DEFERRABLE) {
   seq_printf(m, "%4luD, %5d %-16s ",
    entry->count, entry->pid, entry->comm);
  } else {
   seq_printf(m, " %4lu, %5d %-16s ",
    entry->count, entry->pid, entry->comm);
  }

  print_name_offset(m, (unsigned long)entry->start_func);
  seq_puts(m, " (");
  print_name_offset(m, (unsigned long)entry->expire_func);
  seq_puts(m, ")\n");

  events += entry->count;
 }

 ms += period.tv_sec * 1000;
 if (!ms)
  ms = 1;

 if (events && period.tv_sec)
  seq_printf(m, "%ld total events, %ld.%03ld events/sec\n",
      events, events * 1000 / ms,
      (events * 1000000 / ms) % 1000);
 else
  seq_printf(m, "%ld total events\n", events);

 mutex_unlock(&show_mutex);

 return 0;
}





static void sync_access(void)
{
 unsigned long flags;
 int cpu;

 for_each_online_cpu(cpu) {
  raw_spinlock_t *lock = &per_cpu(tstats_lookup_lock, cpu);

  raw_spin_lock_irqsave(lock, flags);

  raw_spin_unlock_irqrestore(lock, flags);
 }
}

static ssize_t tstats_write(struct file *file, const char __user *buf,
       size_t count, loff_t *offs)
{
 char ctl[2];

 if (count != 2 || *offs)
  return -EINVAL;

 if (copy_from_user(ctl, buf, count))
  return -EFAULT;

 mutex_lock(&show_mutex);
 switch (ctl[0]) {
 case '0':
  if (timer_stats_active) {
   timer_stats_active = 0;
   time_stop = ktime_get();
   sync_access();
  }
  break;
 case '1':
  if (!timer_stats_active) {
   reset_entries();
   time_start = ktime_get();
   smp_mb();
   timer_stats_active = 1;
  }
  break;
 default:
  count = -EINVAL;
 }
 mutex_unlock(&show_mutex);

 return count;
}

static int tstats_open(struct inode *inode, struct file *filp)
{
 return single_open(filp, tstats_show, NULL);
}

static const struct file_operations tstats_fops = {
 .open = tstats_open,
 .read = seq_read,
 .write = tstats_write,
 .llseek = seq_lseek,
 .release = single_release,
};

void __init init_timer_stats(void)
{
 int cpu;

 for_each_possible_cpu(cpu)
  raw_spin_lock_init(&per_cpu(tstats_lookup_lock, cpu));
}

static int __init init_tstats_procfs(void)
{
 struct proc_dir_entry *pe;

 pe = proc_create("timer_stats", 0644, NULL, &tstats_fops);
 if (!pe)
  return -ENOMEM;
 return 0;
}
__initcall(init_tstats_procfs);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Paul E. McKenney <paulmck@us.ibm.com>");

static char *torture_type;
static bool verbose;


static int fullstop = FULLSTOP_RMMOD;
static DEFINE_MUTEX(fullstop_mutex);
static int *torture_runnable;







static struct task_struct *onoff_task;
static long onoff_holdoff;
static long onoff_interval;
static long n_offline_attempts;
static long n_offline_successes;
static unsigned long sum_offline;
static int min_offline = -1;
static int max_offline;
static long n_online_attempts;
static long n_online_successes;
static unsigned long sum_online;
static int min_online = -1;
static int max_online;





static int
torture_onoff(void *arg)
{
 int cpu;
 unsigned long delta;
 int maxcpu = -1;
 DEFINE_TORTURE_RANDOM(rand);
 int ret;
 unsigned long starttime;

 VERBOSE_TOROUT_STRING("torture_onoff task started");
 for_each_online_cpu(cpu)
  maxcpu = cpu;
 WARN_ON(maxcpu < 0);
 if (onoff_holdoff > 0) {
  VERBOSE_TOROUT_STRING("torture_onoff begin holdoff");
  schedule_timeout_interruptible(onoff_holdoff);
  VERBOSE_TOROUT_STRING("torture_onoff end holdoff");
 }
 while (!torture_must_stop()) {
  cpu = (torture_random(&rand) >> 4) % (maxcpu + 1);
  if (cpu_online(cpu) && cpu_is_hotpluggable(cpu)) {
   if (verbose)
    pr_alert("%s" TORTURE_FLAG
      "torture_onoff task: offlining %d\n",
      torture_type, cpu);
   starttime = jiffies;
   n_offline_attempts++;
   ret = cpu_down(cpu);
   if (ret) {
    if (verbose)
     pr_alert("%s" TORTURE_FLAG
       "torture_onoff task: offline %d failed: errno %d\n",
       torture_type, cpu, ret);
   } else {
    if (verbose)
     pr_alert("%s" TORTURE_FLAG
       "torture_onoff task: offlined %d\n",
       torture_type, cpu);
    n_offline_successes++;
    delta = jiffies - starttime;
    sum_offline += delta;
    if (min_offline < 0) {
     min_offline = delta;
     max_offline = delta;
    }
    if (min_offline > delta)
     min_offline = delta;
    if (max_offline < delta)
     max_offline = delta;
   }
  } else if (cpu_is_hotpluggable(cpu)) {
   if (verbose)
    pr_alert("%s" TORTURE_FLAG
      "torture_onoff task: onlining %d\n",
      torture_type, cpu);
   starttime = jiffies;
   n_online_attempts++;
   ret = cpu_up(cpu);
   if (ret) {
    if (verbose)
     pr_alert("%s" TORTURE_FLAG
       "torture_onoff task: online %d failed: errno %d\n",
       torture_type, cpu, ret);
   } else {
    if (verbose)
     pr_alert("%s" TORTURE_FLAG
       "torture_onoff task: onlined %d\n",
       torture_type, cpu);
    n_online_successes++;
    delta = jiffies - starttime;
    sum_online += delta;
    if (min_online < 0) {
     min_online = delta;
     max_online = delta;
    }
    if (min_online > delta)
     min_online = delta;
    if (max_online < delta)
     max_online = delta;
   }
  }
  schedule_timeout_interruptible(onoff_interval);
 }
 torture_kthread_stopping("torture_onoff");
 return 0;
}





int torture_onoff_init(long ooholdoff, long oointerval)
{
 int ret = 0;

 onoff_holdoff = ooholdoff;
 onoff_interval = oointerval;
 if (onoff_interval <= 0)
  return 0;
 ret = torture_create_kthread(torture_onoff, NULL, onoff_task);
 return ret;
}
EXPORT_SYMBOL_GPL(torture_onoff_init);




static void torture_onoff_cleanup(void)
{
 if (onoff_task == NULL)
  return;
 VERBOSE_TOROUT_STRING("Stopping torture_onoff task");
 kthread_stop(onoff_task);
 onoff_task = NULL;
}
EXPORT_SYMBOL_GPL(torture_onoff_cleanup);




void torture_onoff_stats(void)
{
 pr_cont("onoff: %ld/%ld:%ld/%ld %d,%d:%d,%d %lu:%lu (HZ=%d) ",
  n_online_successes, n_online_attempts,
  n_offline_successes, n_offline_attempts,
  min_online, max_online,
  min_offline, max_offline,
  sum_online, sum_offline, HZ);
}
EXPORT_SYMBOL_GPL(torture_onoff_stats);




bool torture_onoff_failures(void)
{
 return n_online_successes != n_online_attempts ||
        n_offline_successes != n_offline_attempts;
 return false;
}
EXPORT_SYMBOL_GPL(torture_onoff_failures);






unsigned long
torture_random(struct torture_random_state *trsp)
{
 if (--trsp->trs_count < 0) {
  trsp->trs_state += (unsigned long)local_clock();
  trsp->trs_count = TORTURE_RANDOM_REFRESH;
 }
 trsp->trs_state = trsp->trs_state * TORTURE_RANDOM_MULT +
  TORTURE_RANDOM_ADD;
 return swahw32(trsp->trs_state);
}
EXPORT_SYMBOL_GPL(torture_random);






struct shuffle_task {
 struct list_head st_l;
 struct task_struct *st_t;
};

static long shuffle_interval;
static struct task_struct *shuffler_task;
static cpumask_var_t shuffle_tmp_mask;
static int shuffle_idle_cpu;
static struct list_head shuffle_task_list = LIST_HEAD_INIT(shuffle_task_list);
static DEFINE_MUTEX(shuffle_task_mutex);





void torture_shuffle_task_register(struct task_struct *tp)
{
 struct shuffle_task *stp;

 if (WARN_ON_ONCE(tp == NULL))
  return;
 stp = kmalloc(sizeof(*stp), GFP_KERNEL);
 if (WARN_ON_ONCE(stp == NULL))
  return;
 stp->st_t = tp;
 mutex_lock(&shuffle_task_mutex);
 list_add(&stp->st_l, &shuffle_task_list);
 mutex_unlock(&shuffle_task_mutex);
}
EXPORT_SYMBOL_GPL(torture_shuffle_task_register);




static void torture_shuffle_task_unregister_all(void)
{
 struct shuffle_task *stp;
 struct shuffle_task *p;

 mutex_lock(&shuffle_task_mutex);
 list_for_each_entry_safe(stp, p, &shuffle_task_list, st_l) {
  list_del(&stp->st_l);
  kfree(stp);
 }
 mutex_unlock(&shuffle_task_mutex);
}





static void torture_shuffle_tasks(void)
{
 struct shuffle_task *stp;

 cpumask_setall(shuffle_tmp_mask);
 get_online_cpus();


 if (num_online_cpus() == 1) {
  put_online_cpus();
  return;
 }


 shuffle_idle_cpu = cpumask_next(shuffle_idle_cpu, shuffle_tmp_mask);
 if (shuffle_idle_cpu >= nr_cpu_ids)
  shuffle_idle_cpu = -1;
 else
  cpumask_clear_cpu(shuffle_idle_cpu, shuffle_tmp_mask);

 mutex_lock(&shuffle_task_mutex);
 list_for_each_entry(stp, &shuffle_task_list, st_l)
  set_cpus_allowed_ptr(stp->st_t, shuffle_tmp_mask);
 mutex_unlock(&shuffle_task_mutex);

 put_online_cpus();
}





static int torture_shuffle(void *arg)
{
 VERBOSE_TOROUT_STRING("torture_shuffle task started");
 do {
  schedule_timeout_interruptible(shuffle_interval);
  torture_shuffle_tasks();
  torture_shutdown_absorb("torture_shuffle");
 } while (!torture_must_stop());
 torture_kthread_stopping("torture_shuffle");
 return 0;
}




int torture_shuffle_init(long shuffint)
{
 shuffle_interval = shuffint;

 shuffle_idle_cpu = -1;

 if (!alloc_cpumask_var(&shuffle_tmp_mask, GFP_KERNEL)) {
  VERBOSE_TOROUT_ERRSTRING("Failed to alloc mask");
  return -ENOMEM;
 }


 return torture_create_kthread(torture_shuffle, NULL, shuffler_task);
}
EXPORT_SYMBOL_GPL(torture_shuffle_init);




static void torture_shuffle_cleanup(void)
{
 torture_shuffle_task_unregister_all();
 if (shuffler_task) {
  VERBOSE_TOROUT_STRING("Stopping torture_shuffle task");
  kthread_stop(shuffler_task);
  free_cpumask_var(shuffle_tmp_mask);
 }
 shuffler_task = NULL;
}
EXPORT_SYMBOL_GPL(torture_shuffle_cleanup);





static int shutdown_secs;
static struct task_struct *shutdown_task;
static unsigned long shutdown_time;
static void (*torture_shutdown_hook)(void);





void torture_shutdown_absorb(const char *title)
{
 while (READ_ONCE(fullstop) == FULLSTOP_SHUTDOWN) {
  pr_notice("torture thread %s parking due to system shutdown\n",
     title);
  schedule_timeout_uninterruptible(MAX_SCHEDULE_TIMEOUT);
 }
}
EXPORT_SYMBOL_GPL(torture_shutdown_absorb);





static int torture_shutdown(void *arg)
{
 long delta;
 unsigned long jiffies_snap;

 VERBOSE_TOROUT_STRING("torture_shutdown task started");
 jiffies_snap = jiffies;
 while (ULONG_CMP_LT(jiffies_snap, shutdown_time) &&
        !torture_must_stop()) {
  delta = shutdown_time - jiffies_snap;
  if (verbose)
   pr_alert("%s" TORTURE_FLAG
     "torture_shutdown task: %lu jiffies remaining\n",
     torture_type, delta);
  schedule_timeout_interruptible(delta);
  jiffies_snap = jiffies;
 }
 if (torture_must_stop()) {
  torture_kthread_stopping("torture_shutdown");
  return 0;
 }



 VERBOSE_TOROUT_STRING("torture_shutdown task shutting down system");
 shutdown_task = NULL;
 if (torture_shutdown_hook)
  torture_shutdown_hook();
 else
  VERBOSE_TOROUT_STRING("No torture_shutdown_hook(), skipping.");
 ftrace_dump(DUMP_ALL);
 kernel_power_off();
 return 0;
}




int torture_shutdown_init(int ssecs, void (*cleanup)(void))
{
 int ret = 0;

 shutdown_secs = ssecs;
 torture_shutdown_hook = cleanup;
 if (shutdown_secs > 0) {
  shutdown_time = jiffies + shutdown_secs * HZ;
  ret = torture_create_kthread(torture_shutdown, NULL,
          shutdown_task);
 }
 return ret;
}
EXPORT_SYMBOL_GPL(torture_shutdown_init);




static int torture_shutdown_notify(struct notifier_block *unused1,
       unsigned long unused2, void *unused3)
{
 mutex_lock(&fullstop_mutex);
 if (READ_ONCE(fullstop) == FULLSTOP_DONTSTOP) {
  VERBOSE_TOROUT_STRING("Unscheduled system shutdown detected");
  WRITE_ONCE(fullstop, FULLSTOP_SHUTDOWN);
 } else {
  pr_warn("Concurrent rmmod and shutdown illegal!\n");
 }
 mutex_unlock(&fullstop_mutex);
 return NOTIFY_DONE;
}

static struct notifier_block torture_shutdown_nb = {
 .notifier_call = torture_shutdown_notify,
};





static void torture_shutdown_cleanup(void)
{
 unregister_reboot_notifier(&torture_shutdown_nb);
 if (shutdown_task != NULL) {
  VERBOSE_TOROUT_STRING("Stopping torture_shutdown task");
  kthread_stop(shutdown_task);
 }
 shutdown_task = NULL;
}






static struct task_struct *stutter_task;
static int stutter_pause_test;
static int stutter;





void stutter_wait(const char *title)
{
 cond_resched_rcu_qs();
 while (READ_ONCE(stutter_pause_test) ||
        (torture_runnable && !READ_ONCE(*torture_runnable))) {
  if (stutter_pause_test)
   if (READ_ONCE(stutter_pause_test) == 1)
    schedule_timeout_interruptible(1);
   else
    while (READ_ONCE(stutter_pause_test))
     cond_resched();
  else
   schedule_timeout_interruptible(round_jiffies_relative(HZ));
  torture_shutdown_absorb(title);
 }
}
EXPORT_SYMBOL_GPL(stutter_wait);





static int torture_stutter(void *arg)
{
 VERBOSE_TOROUT_STRING("torture_stutter task started");
 do {
  if (!torture_must_stop()) {
   if (stutter > 1) {
    schedule_timeout_interruptible(stutter - 1);
    WRITE_ONCE(stutter_pause_test, 2);
   }
   schedule_timeout_interruptible(1);
   WRITE_ONCE(stutter_pause_test, 1);
  }
  if (!torture_must_stop())
   schedule_timeout_interruptible(stutter);
  WRITE_ONCE(stutter_pause_test, 0);
  torture_shutdown_absorb("torture_stutter");
 } while (!torture_must_stop());
 torture_kthread_stopping("torture_stutter");
 return 0;
}




int torture_stutter_init(int s)
{
 int ret;

 stutter = s;
 ret = torture_create_kthread(torture_stutter, NULL, stutter_task);
 return ret;
}
EXPORT_SYMBOL_GPL(torture_stutter_init);




static void torture_stutter_cleanup(void)
{
 if (!stutter_task)
  return;
 VERBOSE_TOROUT_STRING("Stopping torture_stutter task");
 kthread_stop(stutter_task);
 stutter_task = NULL;
}
bool torture_init_begin(char *ttype, bool v, int *runnable)
{
 mutex_lock(&fullstop_mutex);
 if (torture_type != NULL) {
  pr_alert("torture_init_begin: Refusing %s init: %s running.\n",
    ttype, torture_type);
  pr_alert("torture_init_begin: One torture test at a time!\n");
  mutex_unlock(&fullstop_mutex);
  return false;
 }
 torture_type = ttype;
 verbose = v;
 torture_runnable = runnable;
 fullstop = FULLSTOP_DONTSTOP;
 return true;
}
EXPORT_SYMBOL_GPL(torture_init_begin);




void torture_init_end(void)
{
 mutex_unlock(&fullstop_mutex);
 register_reboot_notifier(&torture_shutdown_nb);
}
EXPORT_SYMBOL_GPL(torture_init_end);
bool torture_cleanup_begin(void)
{
 mutex_lock(&fullstop_mutex);
 if (READ_ONCE(fullstop) == FULLSTOP_SHUTDOWN) {
  pr_warn("Concurrent rmmod and shutdown illegal!\n");
  mutex_unlock(&fullstop_mutex);
  schedule_timeout_uninterruptible(10);
  return true;
 }
 WRITE_ONCE(fullstop, FULLSTOP_RMMOD);
 mutex_unlock(&fullstop_mutex);
 torture_shutdown_cleanup();
 torture_shuffle_cleanup();
 torture_stutter_cleanup();
 torture_onoff_cleanup();
 return false;
}
EXPORT_SYMBOL_GPL(torture_cleanup_begin);

void torture_cleanup_end(void)
{
 mutex_lock(&fullstop_mutex);
 torture_type = NULL;
 mutex_unlock(&fullstop_mutex);
}
EXPORT_SYMBOL_GPL(torture_cleanup_end);




bool torture_must_stop(void)
{
 return torture_must_stop_irq() || kthread_should_stop();
}
EXPORT_SYMBOL_GPL(torture_must_stop);





bool torture_must_stop_irq(void)
{
 return READ_ONCE(fullstop) != FULLSTOP_DONTSTOP;
}
EXPORT_SYMBOL_GPL(torture_must_stop_irq);
void torture_kthread_stopping(char *title)
{
 char buf[128];

 snprintf(buf, sizeof(buf), "Stopping %s", title);
 VERBOSE_TOROUT_STRING(buf);
 while (!kthread_should_stop()) {
  torture_shutdown_absorb(title);
  schedule_timeout_uninterruptible(1);
 }
}
EXPORT_SYMBOL_GPL(torture_kthread_stopping);






int _torture_create_kthread(int (*fn)(void *arg), void *arg, char *s, char *m,
       char *f, struct task_struct **tp)
{
 int ret = 0;

 VERBOSE_TOROUT_STRING(m);
 *tp = kthread_run(fn, arg, "%s", s);
 if (IS_ERR(*tp)) {
  ret = PTR_ERR(*tp);
  VERBOSE_TOROUT_ERRSTRING(f);
  *tp = NULL;
 }
 torture_shuffle_task_register(*tp);
 return ret;
}
EXPORT_SYMBOL_GPL(_torture_create_kthread);




void _torture_stop_kthread(char *m, struct task_struct **tp)
{
 if (*tp == NULL)
  return;
 VERBOSE_TOROUT_STRING(m);
 kthread_stop(*tp);
 *tp = NULL;
}
EXPORT_SYMBOL_GPL(_torture_stop_kthread);

extern struct tracepoint * const __start___tracepoints_ptrs[];
extern struct tracepoint * const __stop___tracepoints_ptrs[];


static const int tracepoint_debug;




static DEFINE_MUTEX(tracepoint_module_list_mutex);


static LIST_HEAD(tracepoint_module_list);





static DEFINE_MUTEX(tracepoints_mutex);






struct tp_probes {
 struct rcu_head rcu;
 struct tracepoint_func probes[0];
};

static inline void *allocate_probes(int count)
{
 struct tp_probes *p = kmalloc(count * sizeof(struct tracepoint_func)
   + sizeof(struct tp_probes), GFP_KERNEL);
 return p == NULL ? NULL : p->probes;
}

static void rcu_free_old_probes(struct rcu_head *head)
{
 kfree(container_of(head, struct tp_probes, rcu));
}

static inline void release_probes(struct tracepoint_func *old)
{
 if (old) {
  struct tp_probes *tp_probes = container_of(old,
   struct tp_probes, probes[0]);
  call_rcu_sched(&tp_probes->rcu, rcu_free_old_probes);
 }
}

static void debug_print_probes(struct tracepoint_func *funcs)
{
 int i;

 if (!tracepoint_debug || !funcs)
  return;

 for (i = 0; funcs[i].func; i++)
  printk(KERN_DEBUG "Probe %d : %p\n", i, funcs[i].func);
}

static struct tracepoint_func *
func_add(struct tracepoint_func **funcs, struct tracepoint_func *tp_func,
  int prio)
{
 struct tracepoint_func *old, *new;
 int nr_probes = 0;
 int pos = -1;

 if (WARN_ON(!tp_func->func))
  return ERR_PTR(-EINVAL);

 debug_print_probes(*funcs);
 old = *funcs;
 if (old) {

  for (nr_probes = 0; old[nr_probes].func; nr_probes++) {

   if (pos < 0 && old[nr_probes].prio < prio)
    pos = nr_probes;
   if (old[nr_probes].func == tp_func->func &&
       old[nr_probes].data == tp_func->data)
    return ERR_PTR(-EEXIST);
  }
 }

 new = allocate_probes(nr_probes + 2);
 if (new == NULL)
  return ERR_PTR(-ENOMEM);
 if (old) {
  if (pos < 0) {
   pos = nr_probes;
   memcpy(new, old, nr_probes * sizeof(struct tracepoint_func));
  } else {

   memcpy(new, old, pos * sizeof(struct tracepoint_func));

   memcpy(new + pos + 1, old + pos,
          (nr_probes - pos) * sizeof(struct tracepoint_func));
  }
 } else
  pos = 0;
 new[pos] = *tp_func;
 new[nr_probes + 1].func = NULL;
 *funcs = new;
 debug_print_probes(*funcs);
 return old;
}

static void *func_remove(struct tracepoint_func **funcs,
  struct tracepoint_func *tp_func)
{
 int nr_probes = 0, nr_del = 0, i;
 struct tracepoint_func *old, *new;

 old = *funcs;

 if (!old)
  return ERR_PTR(-ENOENT);

 debug_print_probes(*funcs);

 if (tp_func->func) {
  for (nr_probes = 0; old[nr_probes].func; nr_probes++) {
   if (old[nr_probes].func == tp_func->func &&
        old[nr_probes].data == tp_func->data)
    nr_del++;
  }
 }





 if (nr_probes - nr_del == 0) {

  *funcs = NULL;
  debug_print_probes(*funcs);
  return old;
 } else {
  int j = 0;


  new = allocate_probes(nr_probes - nr_del + 1);
  if (new == NULL)
   return ERR_PTR(-ENOMEM);
  for (i = 0; old[i].func; i++)
   if (old[i].func != tp_func->func
     || old[i].data != tp_func->data)
    new[j++] = old[i];
  new[nr_probes - nr_del].func = NULL;
  *funcs = new;
 }
 debug_print_probes(*funcs);
 return old;
}




static int tracepoint_add_func(struct tracepoint *tp,
          struct tracepoint_func *func, int prio)
{
 struct tracepoint_func *old, *tp_funcs;

 if (tp->regfunc && !static_key_enabled(&tp->key))
  tp->regfunc();

 tp_funcs = rcu_dereference_protected(tp->funcs,
   lockdep_is_held(&tracepoints_mutex));
 old = func_add(&tp_funcs, func, prio);
 if (IS_ERR(old)) {
  WARN_ON_ONCE(1);
  return PTR_ERR(old);
 }
 rcu_assign_pointer(tp->funcs, tp_funcs);
 if (!static_key_enabled(&tp->key))
  static_key_slow_inc(&tp->key);
 release_probes(old);
 return 0;
}







static int tracepoint_remove_func(struct tracepoint *tp,
  struct tracepoint_func *func)
{
 struct tracepoint_func *old, *tp_funcs;

 tp_funcs = rcu_dereference_protected(tp->funcs,
   lockdep_is_held(&tracepoints_mutex));
 old = func_remove(&tp_funcs, func);
 if (IS_ERR(old)) {
  WARN_ON_ONCE(1);
  return PTR_ERR(old);
 }

 if (!tp_funcs) {

  if (tp->unregfunc && static_key_enabled(&tp->key))
   tp->unregfunc();

  if (static_key_enabled(&tp->key))
   static_key_slow_dec(&tp->key);
 }
 rcu_assign_pointer(tp->funcs, tp_funcs);
 release_probes(old);
 return 0;
}
int tracepoint_probe_register_prio(struct tracepoint *tp, void *probe,
       void *data, int prio)
{
 struct tracepoint_func tp_func;
 int ret;

 mutex_lock(&tracepoints_mutex);
 tp_func.func = probe;
 tp_func.data = data;
 tp_func.prio = prio;
 ret = tracepoint_add_func(tp, &tp_func, prio);
 mutex_unlock(&tracepoints_mutex);
 return ret;
}
EXPORT_SYMBOL_GPL(tracepoint_probe_register_prio);
int tracepoint_probe_register(struct tracepoint *tp, void *probe, void *data)
{
 return tracepoint_probe_register_prio(tp, probe, data, TRACEPOINT_DEFAULT_PRIO);
}
EXPORT_SYMBOL_GPL(tracepoint_probe_register);
int tracepoint_probe_unregister(struct tracepoint *tp, void *probe, void *data)
{
 struct tracepoint_func tp_func;
 int ret;

 mutex_lock(&tracepoints_mutex);
 tp_func.func = probe;
 tp_func.data = data;
 ret = tracepoint_remove_func(tp, &tp_func);
 mutex_unlock(&tracepoints_mutex);
 return ret;
}
EXPORT_SYMBOL_GPL(tracepoint_probe_unregister);

bool trace_module_has_bad_taint(struct module *mod)
{
 return mod->taints & ~((1 << TAINT_OOT_MODULE) | (1 << TAINT_CRAP) |
          (1 << TAINT_UNSIGNED_MODULE));
}

static BLOCKING_NOTIFIER_HEAD(tracepoint_notify_list);
int register_tracepoint_module_notifier(struct notifier_block *nb)
{
 struct tp_module *tp_mod;
 int ret;

 mutex_lock(&tracepoint_module_list_mutex);
 ret = blocking_notifier_chain_register(&tracepoint_notify_list, nb);
 if (ret)
  goto end;
 list_for_each_entry(tp_mod, &tracepoint_module_list, list)
  (void) nb->notifier_call(nb, MODULE_STATE_COMING, tp_mod);
end:
 mutex_unlock(&tracepoint_module_list_mutex);
 return ret;
}
EXPORT_SYMBOL_GPL(register_tracepoint_module_notifier);
int unregister_tracepoint_module_notifier(struct notifier_block *nb)
{
 struct tp_module *tp_mod;
 int ret;

 mutex_lock(&tracepoint_module_list_mutex);
 ret = blocking_notifier_chain_unregister(&tracepoint_notify_list, nb);
 if (ret)
  goto end;
 list_for_each_entry(tp_mod, &tracepoint_module_list, list)
  (void) nb->notifier_call(nb, MODULE_STATE_GOING, tp_mod);
end:
 mutex_unlock(&tracepoint_module_list_mutex);
 return ret;

}
EXPORT_SYMBOL_GPL(unregister_tracepoint_module_notifier);





static void tp_module_going_check_quiescent(struct tracepoint * const *begin,
  struct tracepoint * const *end)
{
 struct tracepoint * const *iter;

 if (!begin)
  return;
 for (iter = begin; iter < end; iter++)
  WARN_ON_ONCE((*iter)->funcs);
}

static int tracepoint_module_coming(struct module *mod)
{
 struct tp_module *tp_mod;
 int ret = 0;

 if (!mod->num_tracepoints)
  return 0;






 if (trace_module_has_bad_taint(mod))
  return 0;
 mutex_lock(&tracepoint_module_list_mutex);
 tp_mod = kmalloc(sizeof(struct tp_module), GFP_KERNEL);
 if (!tp_mod) {
  ret = -ENOMEM;
  goto end;
 }
 tp_mod->mod = mod;
 list_add_tail(&tp_mod->list, &tracepoint_module_list);
 blocking_notifier_call_chain(&tracepoint_notify_list,
   MODULE_STATE_COMING, tp_mod);
end:
 mutex_unlock(&tracepoint_module_list_mutex);
 return ret;
}

static void tracepoint_module_going(struct module *mod)
{
 struct tp_module *tp_mod;

 if (!mod->num_tracepoints)
  return;

 mutex_lock(&tracepoint_module_list_mutex);
 list_for_each_entry(tp_mod, &tracepoint_module_list, list) {
  if (tp_mod->mod == mod) {
   blocking_notifier_call_chain(&tracepoint_notify_list,
     MODULE_STATE_GOING, tp_mod);
   list_del(&tp_mod->list);
   kfree(tp_mod);




   tp_module_going_check_quiescent(mod->tracepoints_ptrs,
    mod->tracepoints_ptrs + mod->num_tracepoints);
   break;
  }
 }






 mutex_unlock(&tracepoint_module_list_mutex);
}

static int tracepoint_module_notify(struct notifier_block *self,
  unsigned long val, void *data)
{
 struct module *mod = data;
 int ret = 0;

 switch (val) {
 case MODULE_STATE_COMING:
  ret = tracepoint_module_coming(mod);
  break;
 case MODULE_STATE_LIVE:
  break;
 case MODULE_STATE_GOING:
  tracepoint_module_going(mod);
  break;
 case MODULE_STATE_UNFORMED:
  break;
 }
 return ret;
}

static struct notifier_block tracepoint_module_nb = {
 .notifier_call = tracepoint_module_notify,
 .priority = 0,
};

static __init int init_tracepoints(void)
{
 int ret;

 ret = register_module_notifier(&tracepoint_module_nb);
 if (ret)
  pr_warn("Failed to register tracepoint module enter notifier\n");

 return ret;
}
__initcall(init_tracepoints);

static void for_each_tracepoint_range(struct tracepoint * const *begin,
  struct tracepoint * const *end,
  void (*fct)(struct tracepoint *tp, void *priv),
  void *priv)
{
 struct tracepoint * const *iter;

 if (!begin)
  return;
 for (iter = begin; iter < end; iter++)
  fct(*iter, priv);
}






void for_each_kernel_tracepoint(void (*fct)(struct tracepoint *tp, void *priv),
  void *priv)
{
 for_each_tracepoint_range(__start___tracepoints_ptrs,
  __stop___tracepoints_ptrs, fct, priv);
}
EXPORT_SYMBOL_GPL(for_each_kernel_tracepoint);



static int sys_tracepoint_refcount;

void syscall_regfunc(void)
{
 struct task_struct *p, *t;

 if (!sys_tracepoint_refcount) {
  read_lock(&tasklist_lock);
  for_each_process_thread(p, t) {
   set_tsk_thread_flag(t, TIF_SYSCALL_TRACEPOINT);
  }
  read_unlock(&tasklist_lock);
 }
 sys_tracepoint_refcount++;
}

void syscall_unregfunc(void)
{
 struct task_struct *p, *t;

 sys_tracepoint_refcount--;
 if (!sys_tracepoint_refcount) {
  read_lock(&tasklist_lock);
  for_each_process_thread(p, t) {
   clear_tsk_thread_flag(t, TIF_SYSCALL_TRACEPOINT);
  }
  read_unlock(&tasklist_lock);
 }
}




void bacct_add_tsk(struct user_namespace *user_ns,
     struct pid_namespace *pid_ns,
     struct taskstats *stats, struct task_struct *tsk)
{
 const struct cred *tcred;
 cputime_t utime, stime, utimescaled, stimescaled;
 u64 delta;

 BUILD_BUG_ON(TS_COMM_LEN < TASK_COMM_LEN);


 delta = ktime_get_ns() - tsk->start_time;

 do_div(delta, NSEC_PER_USEC);
 stats->ac_etime = delta;

 do_div(delta, USEC_PER_SEC);
 stats->ac_btime = get_seconds() - delta;
 if (thread_group_leader(tsk)) {
  stats->ac_exitcode = tsk->exit_code;
  if (tsk->flags & PF_FORKNOEXEC)
   stats->ac_flag |= AFORK;
 }
 if (tsk->flags & PF_SUPERPRIV)
  stats->ac_flag |= ASU;
 if (tsk->flags & PF_DUMPCORE)
  stats->ac_flag |= ACORE;
 if (tsk->flags & PF_SIGNALED)
  stats->ac_flag |= AXSIG;
 stats->ac_nice = task_nice(tsk);
 stats->ac_sched = tsk->policy;
 stats->ac_pid = task_pid_nr_ns(tsk, pid_ns);
 rcu_read_lock();
 tcred = __task_cred(tsk);
 stats->ac_uid = from_kuid_munged(user_ns, tcred->uid);
 stats->ac_gid = from_kgid_munged(user_ns, tcred->gid);
 stats->ac_ppid = pid_alive(tsk) ?
  task_tgid_nr_ns(rcu_dereference(tsk->real_parent), pid_ns) : 0;
 rcu_read_unlock();

 task_cputime(tsk, &utime, &stime);
 stats->ac_utime = cputime_to_usecs(utime);
 stats->ac_stime = cputime_to_usecs(stime);

 task_cputime_scaled(tsk, &utimescaled, &stimescaled);
 stats->ac_utimescaled = cputime_to_usecs(utimescaled);
 stats->ac_stimescaled = cputime_to_usecs(stimescaled);

 stats->ac_minflt = tsk->min_flt;
 stats->ac_majflt = tsk->maj_flt;

 strncpy(stats->ac_comm, tsk->comm, sizeof(stats->ac_comm));
}






void xacct_add_tsk(struct taskstats *stats, struct task_struct *p)
{
 struct mm_struct *mm;


 stats->coremem = p->acct_rss_mem1 * PAGE_SIZE;
 do_div(stats->coremem, 1000 * KB);
 stats->virtmem = p->acct_vm_mem1 * PAGE_SIZE;
 do_div(stats->virtmem, 1000 * KB);
 mm = get_task_mm(p);
 if (mm) {

  stats->hiwater_rss = get_mm_hiwater_rss(mm) * PAGE_SIZE / KB;
  stats->hiwater_vm = get_mm_hiwater_vm(mm) * PAGE_SIZE / KB;
  mmput(mm);
 }
 stats->read_char = p->ioac.rchar & KB_MASK;
 stats->write_char = p->ioac.wchar & KB_MASK;
 stats->read_syscalls = p->ioac.syscr & KB_MASK;
 stats->write_syscalls = p->ioac.syscw & KB_MASK;
 stats->read_bytes = p->ioac.read_bytes & KB_MASK;
 stats->write_bytes = p->ioac.write_bytes & KB_MASK;
 stats->cancelled_write_bytes = p->ioac.cancelled_write_bytes & KB_MASK;
 stats->read_bytes = 0;
 stats->write_bytes = 0;
 stats->cancelled_write_bytes = 0;
}

static void __acct_update_integrals(struct task_struct *tsk,
        cputime_t utime, cputime_t stime)
{
 cputime_t time, dtime;
 u64 delta;

 if (!likely(tsk->mm))
  return;

 time = stime + utime;
 dtime = time - tsk->acct_timexpd;

 delta = cputime_to_nsecs(dtime);

 if (delta < TICK_NSEC)
  return;

 tsk->acct_timexpd = time;





 tsk->acct_rss_mem1 += delta * get_mm_rss(tsk->mm) >> 10;
 tsk->acct_vm_mem1 += delta * tsk->mm->total_vm >> 10;
}





void acct_update_integrals(struct task_struct *tsk)
{
 cputime_t utime, stime;
 unsigned long flags;

 local_irq_save(flags);
 task_cputime(tsk, &utime, &stime);
 __acct_update_integrals(tsk, utime, stime);
 local_irq_restore(flags);
}





void acct_account_cputime(struct task_struct *tsk)
{
 __acct_update_integrals(tsk, tsk->utime, tsk->stime);
}





void acct_clear_integrals(struct task_struct *tsk)
{
 tsk->acct_timexpd = 0;
 tsk->acct_rss_mem1 = 0;
 tsk->acct_vm_mem1 = 0;
}







SYSCALL_DEFINE3(chown16, const char __user *, filename, old_uid_t, user, old_gid_t, group)
{
 return sys_chown(filename, low2highuid(user), low2highgid(group));
}

SYSCALL_DEFINE3(lchown16, const char __user *, filename, old_uid_t, user, old_gid_t, group)
{
 return sys_lchown(filename, low2highuid(user), low2highgid(group));
}

SYSCALL_DEFINE3(fchown16, unsigned int, fd, old_uid_t, user, old_gid_t, group)
{
 return sys_fchown(fd, low2highuid(user), low2highgid(group));
}

SYSCALL_DEFINE2(setregid16, old_gid_t, rgid, old_gid_t, egid)
{
 return sys_setregid(low2highgid(rgid), low2highgid(egid));
}

SYSCALL_DEFINE1(setgid16, old_gid_t, gid)
{
 return sys_setgid(low2highgid(gid));
}

SYSCALL_DEFINE2(setreuid16, old_uid_t, ruid, old_uid_t, euid)
{
 return sys_setreuid(low2highuid(ruid), low2highuid(euid));
}

SYSCALL_DEFINE1(setuid16, old_uid_t, uid)
{
 return sys_setuid(low2highuid(uid));
}

SYSCALL_DEFINE3(setresuid16, old_uid_t, ruid, old_uid_t, euid, old_uid_t, suid)
{
 return sys_setresuid(low2highuid(ruid), low2highuid(euid),
     low2highuid(suid));
}

SYSCALL_DEFINE3(getresuid16, old_uid_t __user *, ruidp, old_uid_t __user *, euidp, old_uid_t __user *, suidp)
{
 const struct cred *cred = current_cred();
 int retval;
 old_uid_t ruid, euid, suid;

 ruid = high2lowuid(from_kuid_munged(cred->user_ns, cred->uid));
 euid = high2lowuid(from_kuid_munged(cred->user_ns, cred->euid));
 suid = high2lowuid(from_kuid_munged(cred->user_ns, cred->suid));

 if (!(retval = put_user(ruid, ruidp)) &&
     !(retval = put_user(euid, euidp)))
  retval = put_user(suid, suidp);

 return retval;
}

SYSCALL_DEFINE3(setresgid16, old_gid_t, rgid, old_gid_t, egid, old_gid_t, sgid)
{
 return sys_setresgid(low2highgid(rgid), low2highgid(egid),
     low2highgid(sgid));
}


SYSCALL_DEFINE3(getresgid16, old_gid_t __user *, rgidp, old_gid_t __user *, egidp, old_gid_t __user *, sgidp)
{
 const struct cred *cred = current_cred();
 int retval;
 old_gid_t rgid, egid, sgid;

 rgid = high2lowgid(from_kgid_munged(cred->user_ns, cred->gid));
 egid = high2lowgid(from_kgid_munged(cred->user_ns, cred->egid));
 sgid = high2lowgid(from_kgid_munged(cred->user_ns, cred->sgid));

 if (!(retval = put_user(rgid, rgidp)) &&
     !(retval = put_user(egid, egidp)))
  retval = put_user(sgid, sgidp);

 return retval;
}

SYSCALL_DEFINE1(setfsuid16, old_uid_t, uid)
{
 return sys_setfsuid(low2highuid(uid));
}

SYSCALL_DEFINE1(setfsgid16, old_gid_t, gid)
{
 return sys_setfsgid(low2highgid(gid));
}

static int groups16_to_user(old_gid_t __user *grouplist,
    struct group_info *group_info)
{
 struct user_namespace *user_ns = current_user_ns();
 int i;
 old_gid_t group;
 kgid_t kgid;

 for (i = 0; i < group_info->ngroups; i++) {
  kgid = GROUP_AT(group_info, i);
  group = high2lowgid(from_kgid_munged(user_ns, kgid));
  if (put_user(group, grouplist+i))
   return -EFAULT;
 }

 return 0;
}

static int groups16_from_user(struct group_info *group_info,
    old_gid_t __user *grouplist)
{
 struct user_namespace *user_ns = current_user_ns();
 int i;
 old_gid_t group;
 kgid_t kgid;

 for (i = 0; i < group_info->ngroups; i++) {
  if (get_user(group, grouplist+i))
   return -EFAULT;

  kgid = make_kgid(user_ns, low2highgid(group));
  if (!gid_valid(kgid))
   return -EINVAL;

  GROUP_AT(group_info, i) = kgid;
 }

 return 0;
}

SYSCALL_DEFINE2(getgroups16, int, gidsetsize, old_gid_t __user *, grouplist)
{
 const struct cred *cred = current_cred();
 int i;

 if (gidsetsize < 0)
  return -EINVAL;

 i = cred->group_info->ngroups;
 if (gidsetsize) {
  if (i > gidsetsize) {
   i = -EINVAL;
   goto out;
  }
  if (groups16_to_user(grouplist, cred->group_info)) {
   i = -EFAULT;
   goto out;
  }
 }
out:
 return i;
}

SYSCALL_DEFINE2(setgroups16, int, gidsetsize, old_gid_t __user *, grouplist)
{
 struct group_info *group_info;
 int retval;

 if (!may_setgroups())
  return -EPERM;
 if ((unsigned)gidsetsize > NGROUPS_MAX)
  return -EINVAL;

 group_info = groups_alloc(gidsetsize);
 if (!group_info)
  return -ENOMEM;
 retval = groups16_from_user(group_info, grouplist);
 if (retval) {
  put_group_info(group_info);
  return retval;
 }

 retval = set_current_groups(group_info);
 put_group_info(group_info);

 return retval;
}

SYSCALL_DEFINE0(getuid16)
{
 return high2lowuid(from_kuid_munged(current_user_ns(), current_uid()));
}

SYSCALL_DEFINE0(geteuid16)
{
 return high2lowuid(from_kuid_munged(current_user_ns(), current_euid()));
}

SYSCALL_DEFINE0(getgid16)
{
 return high2lowgid(from_kgid_munged(current_user_ns(), current_gid()));
}

SYSCALL_DEFINE0(getegid16)
{
 return high2lowgid(from_kgid_munged(current_user_ns(), current_egid()));
}





int smp_call_function_single(int cpu, void (*func) (void *info), void *info,
    int wait)
{
 unsigned long flags;

 WARN_ON(cpu != 0);

 local_irq_save(flags);
 func(info);
 local_irq_restore(flags);

 return 0;
}
EXPORT_SYMBOL(smp_call_function_single);

int smp_call_function_single_async(int cpu, struct call_single_data *csd)
{
 unsigned long flags;

 local_irq_save(flags);
 csd->func(csd->info);
 local_irq_restore(flags);
 return 0;
}
EXPORT_SYMBOL(smp_call_function_single_async);

int on_each_cpu(smp_call_func_t func, void *info, int wait)
{
 unsigned long flags;

 local_irq_save(flags);
 func(info);
 local_irq_restore(flags);
 return 0;
}
EXPORT_SYMBOL(on_each_cpu);







void on_each_cpu_mask(const struct cpumask *mask,
        smp_call_func_t func, void *info, bool wait)
{
 unsigned long flags;

 if (cpumask_test_cpu(0, mask)) {
  local_irq_save(flags);
  func(info);
  local_irq_restore(flags);
 }
}
EXPORT_SYMBOL(on_each_cpu_mask);





void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),
        smp_call_func_t func, void *info, bool wait,
        gfp_t gfp_flags)
{
 unsigned long flags;

 preempt_disable();
 if (cond_func(0, info)) {
  local_irq_save(flags);
  func(info);
  local_irq_restore(flags);
 }
 preempt_enable();
}
EXPORT_SYMBOL(on_each_cpu_cond);



static struct rb_root uprobes_tree = RB_ROOT;





static DEFINE_SPINLOCK(uprobes_treelock);


static struct mutex uprobes_mmap_mutex[UPROBES_HASH_SZ];

static struct percpu_rw_semaphore dup_mmap_sem;



struct uprobe {
 struct rb_node rb_node;
 atomic_t ref;
 struct rw_semaphore register_rwsem;
 struct rw_semaphore consumer_rwsem;
 struct list_head pending_list;
 struct uprobe_consumer *consumers;
 struct inode *inode;
 loff_t offset;
 unsigned long flags;
 struct arch_uprobe arch;
};
struct xol_area {
 wait_queue_head_t wq;
 atomic_t slot_count;
 unsigned long *bitmap;

 struct vm_special_mapping xol_mapping;
 struct page *pages[2];





 unsigned long vaddr;
};
static bool valid_vma(struct vm_area_struct *vma, bool is_register)
{
 vm_flags_t flags = VM_HUGETLB | VM_MAYEXEC | VM_MAYSHARE;

 if (is_register)
  flags |= VM_WRITE;

 return vma->vm_file && (vma->vm_flags & flags) == VM_MAYEXEC;
}

static unsigned long offset_to_vaddr(struct vm_area_struct *vma, loff_t offset)
{
 return vma->vm_start + offset - ((loff_t)vma->vm_pgoff << PAGE_SHIFT);
}

static loff_t vaddr_to_offset(struct vm_area_struct *vma, unsigned long vaddr)
{
 return ((loff_t)vma->vm_pgoff << PAGE_SHIFT) + (vaddr - vma->vm_start);
}
static int __replace_page(struct vm_area_struct *vma, unsigned long addr,
    struct page *page, struct page *kpage)
{
 struct mm_struct *mm = vma->vm_mm;
 spinlock_t *ptl;
 pte_t *ptep;
 int err;

 const unsigned long mmun_start = addr;
 const unsigned long mmun_end = addr + PAGE_SIZE;
 struct mem_cgroup *memcg;

 err = mem_cgroup_try_charge(kpage, vma->vm_mm, GFP_KERNEL, &memcg,
   false);
 if (err)
  return err;


 lock_page(page);

 mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
 err = -EAGAIN;
 ptep = page_check_address(page, mm, addr, &ptl, 0);
 if (!ptep)
  goto unlock;

 get_page(kpage);
 page_add_new_anon_rmap(kpage, vma, addr, false);
 mem_cgroup_commit_charge(kpage, memcg, false, false);
 lru_cache_add_active_or_unevictable(kpage, vma);

 if (!PageAnon(page)) {
  dec_mm_counter(mm, mm_counter_file(page));
  inc_mm_counter(mm, MM_ANONPAGES);
 }

 flush_cache_page(vma, addr, pte_pfn(*ptep));
 ptep_clear_flush_notify(vma, addr, ptep);
 set_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma->vm_page_prot));

 page_remove_rmap(page, false);
 if (!page_mapped(page))
  try_to_free_swap(page);
 pte_unmap_unlock(ptep, ptl);

 if (vma->vm_flags & VM_LOCKED)
  munlock_vma_page(page);
 put_page(page);

 err = 0;
 unlock:
 mem_cgroup_cancel_charge(kpage, memcg, false);
 mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 unlock_page(page);
 return err;
}







bool __weak is_swbp_insn(uprobe_opcode_t *insn)
{
 return *insn == UPROBE_SWBP_INSN;
}
bool __weak is_trap_insn(uprobe_opcode_t *insn)
{
 return is_swbp_insn(insn);
}

static void copy_from_page(struct page *page, unsigned long vaddr, void *dst, int len)
{
 void *kaddr = kmap_atomic(page);
 memcpy(dst, kaddr + (vaddr & ~PAGE_MASK), len);
 kunmap_atomic(kaddr);
}

static void copy_to_page(struct page *page, unsigned long vaddr, const void *src, int len)
{
 void *kaddr = kmap_atomic(page);
 memcpy(kaddr + (vaddr & ~PAGE_MASK), src, len);
 kunmap_atomic(kaddr);
}

static int verify_opcode(struct page *page, unsigned long vaddr, uprobe_opcode_t *new_opcode)
{
 uprobe_opcode_t old_opcode;
 bool is_swbp;
 copy_from_page(page, vaddr, &old_opcode, UPROBE_SWBP_INSN_SIZE);
 is_swbp = is_swbp_insn(&old_opcode);

 if (is_swbp_insn(new_opcode)) {
  if (is_swbp)
   return 0;
 } else {
  if (!is_swbp)
   return 0;
 }

 return 1;
}
int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr,
   uprobe_opcode_t opcode)
{
 struct page *old_page, *new_page;
 struct vm_area_struct *vma;
 int ret;

retry:

 ret = get_user_pages_remote(NULL, mm, vaddr, 1, 0, 1, &old_page, &vma);
 if (ret <= 0)
  return ret;

 ret = verify_opcode(old_page, vaddr, &opcode);
 if (ret <= 0)
  goto put_old;

 ret = anon_vma_prepare(vma);
 if (ret)
  goto put_old;

 ret = -ENOMEM;
 new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, vaddr);
 if (!new_page)
  goto put_old;

 __SetPageUptodate(new_page);
 copy_highpage(new_page, old_page);
 copy_to_page(new_page, vaddr, &opcode, UPROBE_SWBP_INSN_SIZE);

 ret = __replace_page(vma, vaddr, old_page, new_page);
 put_page(new_page);
put_old:
 put_page(old_page);

 if (unlikely(ret == -EAGAIN))
  goto retry;
 return ret;
}
int __weak set_swbp(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long vaddr)
{
 return uprobe_write_opcode(mm, vaddr, UPROBE_SWBP_INSN);
}
int __weak
set_orig_insn(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long vaddr)
{
 return uprobe_write_opcode(mm, vaddr, *(uprobe_opcode_t *)&auprobe->insn);
}

static struct uprobe *get_uprobe(struct uprobe *uprobe)
{
 atomic_inc(&uprobe->ref);
 return uprobe;
}

static void put_uprobe(struct uprobe *uprobe)
{
 if (atomic_dec_and_test(&uprobe->ref))
  kfree(uprobe);
}

static int match_uprobe(struct uprobe *l, struct uprobe *r)
{
 if (l->inode < r->inode)
  return -1;

 if (l->inode > r->inode)
  return 1;

 if (l->offset < r->offset)
  return -1;

 if (l->offset > r->offset)
  return 1;

 return 0;
}

static struct uprobe *__find_uprobe(struct inode *inode, loff_t offset)
{
 struct uprobe u = { .inode = inode, .offset = offset };
 struct rb_node *n = uprobes_tree.rb_node;
 struct uprobe *uprobe;
 int match;

 while (n) {
  uprobe = rb_entry(n, struct uprobe, rb_node);
  match = match_uprobe(&u, uprobe);
  if (!match)
   return get_uprobe(uprobe);

  if (match < 0)
   n = n->rb_left;
  else
   n = n->rb_right;
 }
 return NULL;
}





static struct uprobe *find_uprobe(struct inode *inode, loff_t offset)
{
 struct uprobe *uprobe;

 spin_lock(&uprobes_treelock);
 uprobe = __find_uprobe(inode, offset);
 spin_unlock(&uprobes_treelock);

 return uprobe;
}

static struct uprobe *__insert_uprobe(struct uprobe *uprobe)
{
 struct rb_node **p = &uprobes_tree.rb_node;
 struct rb_node *parent = NULL;
 struct uprobe *u;
 int match;

 while (*p) {
  parent = *p;
  u = rb_entry(parent, struct uprobe, rb_node);
  match = match_uprobe(uprobe, u);
  if (!match)
   return get_uprobe(u);

  if (match < 0)
   p = &parent->rb_left;
  else
   p = &parent->rb_right;

 }

 u = NULL;
 rb_link_node(&uprobe->rb_node, parent, p);
 rb_insert_color(&uprobe->rb_node, &uprobes_tree);

 atomic_set(&uprobe->ref, 2);

 return u;
}
static struct uprobe *insert_uprobe(struct uprobe *uprobe)
{
 struct uprobe *u;

 spin_lock(&uprobes_treelock);
 u = __insert_uprobe(uprobe);
 spin_unlock(&uprobes_treelock);

 return u;
}

static struct uprobe *alloc_uprobe(struct inode *inode, loff_t offset)
{
 struct uprobe *uprobe, *cur_uprobe;

 uprobe = kzalloc(sizeof(struct uprobe), GFP_KERNEL);
 if (!uprobe)
  return NULL;

 uprobe->inode = igrab(inode);
 uprobe->offset = offset;
 init_rwsem(&uprobe->register_rwsem);
 init_rwsem(&uprobe->consumer_rwsem);


 cur_uprobe = insert_uprobe(uprobe);

 if (cur_uprobe) {
  kfree(uprobe);
  uprobe = cur_uprobe;
  iput(inode);
 }

 return uprobe;
}

static void consumer_add(struct uprobe *uprobe, struct uprobe_consumer *uc)
{
 down_write(&uprobe->consumer_rwsem);
 uc->next = uprobe->consumers;
 uprobe->consumers = uc;
 up_write(&uprobe->consumer_rwsem);
}






static bool consumer_del(struct uprobe *uprobe, struct uprobe_consumer *uc)
{
 struct uprobe_consumer **con;
 bool ret = false;

 down_write(&uprobe->consumer_rwsem);
 for (con = &uprobe->consumers; *con; con = &(*con)->next) {
  if (*con == uc) {
   *con = uc->next;
   ret = true;
   break;
  }
 }
 up_write(&uprobe->consumer_rwsem);

 return ret;
}

static int __copy_insn(struct address_space *mapping, struct file *filp,
   void *insn, int nbytes, loff_t offset)
{
 struct page *page;





 if (mapping->a_ops->readpage)
  page = read_mapping_page(mapping, offset >> PAGE_SHIFT, filp);
 else
  page = shmem_read_mapping_page(mapping, offset >> PAGE_SHIFT);
 if (IS_ERR(page))
  return PTR_ERR(page);

 copy_from_page(page, offset, insn, nbytes);
 put_page(page);

 return 0;
}

static int copy_insn(struct uprobe *uprobe, struct file *filp)
{
 struct address_space *mapping = uprobe->inode->i_mapping;
 loff_t offs = uprobe->offset;
 void *insn = &uprobe->arch.insn;
 int size = sizeof(uprobe->arch.insn);
 int len, err = -EIO;


 do {
  if (offs >= i_size_read(uprobe->inode))
   break;

  len = min_t(int, size, PAGE_SIZE - (offs & ~PAGE_MASK));
  err = __copy_insn(mapping, filp, insn, len, offs);
  if (err)
   break;

  insn += len;
  offs += len;
  size -= len;
 } while (size);

 return err;
}

static int prepare_uprobe(struct uprobe *uprobe, struct file *file,
    struct mm_struct *mm, unsigned long vaddr)
{
 int ret = 0;

 if (test_bit(UPROBE_COPY_INSN, &uprobe->flags))
  return ret;


 down_write(&uprobe->consumer_rwsem);
 if (test_bit(UPROBE_COPY_INSN, &uprobe->flags))
  goto out;

 ret = copy_insn(uprobe, file);
 if (ret)
  goto out;

 ret = -ENOTSUPP;
 if (is_trap_insn((uprobe_opcode_t *)&uprobe->arch.insn))
  goto out;

 ret = arch_uprobe_analyze_insn(&uprobe->arch, mm, vaddr);
 if (ret)
  goto out;


 BUG_ON((uprobe->offset & ~PAGE_MASK) +
   UPROBE_SWBP_INSN_SIZE > PAGE_SIZE);

 smp_wmb();
 set_bit(UPROBE_COPY_INSN, &uprobe->flags);

 out:
 up_write(&uprobe->consumer_rwsem);

 return ret;
}

static inline bool consumer_filter(struct uprobe_consumer *uc,
       enum uprobe_filter_ctx ctx, struct mm_struct *mm)
{
 return !uc->filter || uc->filter(uc, ctx, mm);
}

static bool filter_chain(struct uprobe *uprobe,
    enum uprobe_filter_ctx ctx, struct mm_struct *mm)
{
 struct uprobe_consumer *uc;
 bool ret = false;

 down_read(&uprobe->consumer_rwsem);
 for (uc = uprobe->consumers; uc; uc = uc->next) {
  ret = consumer_filter(uc, ctx, mm);
  if (ret)
   break;
 }
 up_read(&uprobe->consumer_rwsem);

 return ret;
}

static int
install_breakpoint(struct uprobe *uprobe, struct mm_struct *mm,
   struct vm_area_struct *vma, unsigned long vaddr)
{
 bool first_uprobe;
 int ret;

 ret = prepare_uprobe(uprobe, vma->vm_file, mm, vaddr);
 if (ret)
  return ret;





 first_uprobe = !test_bit(MMF_HAS_UPROBES, &mm->flags);
 if (first_uprobe)
  set_bit(MMF_HAS_UPROBES, &mm->flags);

 ret = set_swbp(&uprobe->arch, mm, vaddr);
 if (!ret)
  clear_bit(MMF_RECALC_UPROBES, &mm->flags);
 else if (first_uprobe)
  clear_bit(MMF_HAS_UPROBES, &mm->flags);

 return ret;
}

static int
remove_breakpoint(struct uprobe *uprobe, struct mm_struct *mm, unsigned long vaddr)
{
 set_bit(MMF_RECALC_UPROBES, &mm->flags);
 return set_orig_insn(&uprobe->arch, mm, vaddr);
}

static inline bool uprobe_is_active(struct uprobe *uprobe)
{
 return !RB_EMPTY_NODE(&uprobe->rb_node);
}





static void delete_uprobe(struct uprobe *uprobe)
{
 if (WARN_ON(!uprobe_is_active(uprobe)))
  return;

 spin_lock(&uprobes_treelock);
 rb_erase(&uprobe->rb_node, &uprobes_tree);
 spin_unlock(&uprobes_treelock);
 RB_CLEAR_NODE(&uprobe->rb_node);
 iput(uprobe->inode);
 put_uprobe(uprobe);
}

struct map_info {
 struct map_info *next;
 struct mm_struct *mm;
 unsigned long vaddr;
};

static inline struct map_info *free_map_info(struct map_info *info)
{
 struct map_info *next = info->next;
 kfree(info);
 return next;
}

static struct map_info *
build_map_info(struct address_space *mapping, loff_t offset, bool is_register)
{
 unsigned long pgoff = offset >> PAGE_SHIFT;
 struct vm_area_struct *vma;
 struct map_info *curr = NULL;
 struct map_info *prev = NULL;
 struct map_info *info;
 int more = 0;

 again:
 i_mmap_lock_read(mapping);
 vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
  if (!valid_vma(vma, is_register))
   continue;

  if (!prev && !more) {




   prev = kmalloc(sizeof(struct map_info),
     GFP_NOWAIT | __GFP_NOMEMALLOC | __GFP_NOWARN);
   if (prev)
    prev->next = NULL;
  }
  if (!prev) {
   more++;
   continue;
  }

  if (!atomic_inc_not_zero(&vma->vm_mm->mm_users))
   continue;

  info = prev;
  prev = prev->next;
  info->next = curr;
  curr = info;

  info->mm = vma->vm_mm;
  info->vaddr = offset_to_vaddr(vma, offset);
 }
 i_mmap_unlock_read(mapping);

 if (!more)
  goto out;

 prev = curr;
 while (curr) {
  mmput(curr->mm);
  curr = curr->next;
 }

 do {
  info = kmalloc(sizeof(struct map_info), GFP_KERNEL);
  if (!info) {
   curr = ERR_PTR(-ENOMEM);
   goto out;
  }
  info->next = prev;
  prev = info;
 } while (--more);

 goto again;
 out:
 while (prev)
  prev = free_map_info(prev);
 return curr;
}

static int
register_for_each_vma(struct uprobe *uprobe, struct uprobe_consumer *new)
{
 bool is_register = !!new;
 struct map_info *info;
 int err = 0;

 percpu_down_write(&dup_mmap_sem);
 info = build_map_info(uprobe->inode->i_mapping,
     uprobe->offset, is_register);
 if (IS_ERR(info)) {
  err = PTR_ERR(info);
  goto out;
 }

 while (info) {
  struct mm_struct *mm = info->mm;
  struct vm_area_struct *vma;

  if (err && is_register)
   goto free;

  down_write(&mm->mmap_sem);
  vma = find_vma(mm, info->vaddr);
  if (!vma || !valid_vma(vma, is_register) ||
      file_inode(vma->vm_file) != uprobe->inode)
   goto unlock;

  if (vma->vm_start > info->vaddr ||
      vaddr_to_offset(vma, info->vaddr) != uprobe->offset)
   goto unlock;

  if (is_register) {

   if (consumer_filter(new,
     UPROBE_FILTER_REGISTER, mm))
    err = install_breakpoint(uprobe, mm, vma, info->vaddr);
  } else if (test_bit(MMF_HAS_UPROBES, &mm->flags)) {
   if (!filter_chain(uprobe,
     UPROBE_FILTER_UNREGISTER, mm))
    err |= remove_breakpoint(uprobe, mm, info->vaddr);
  }

 unlock:
  up_write(&mm->mmap_sem);
 free:
  mmput(mm);
  info = free_map_info(info);
 }
 out:
 percpu_up_write(&dup_mmap_sem);
 return err;
}

static int __uprobe_register(struct uprobe *uprobe, struct uprobe_consumer *uc)
{
 consumer_add(uprobe, uc);
 return register_for_each_vma(uprobe, uc);
}

static void __uprobe_unregister(struct uprobe *uprobe, struct uprobe_consumer *uc)
{
 int err;

 if (WARN_ON(!consumer_del(uprobe, uc)))
  return;

 err = register_for_each_vma(uprobe, NULL);

 if (!uprobe->consumers && !err)
  delete_uprobe(uprobe);
}
int uprobe_register(struct inode *inode, loff_t offset, struct uprobe_consumer *uc)
{
 struct uprobe *uprobe;
 int ret;


 if (!uc->handler && !uc->ret_handler)
  return -EINVAL;


 if (!inode->i_mapping->a_ops->readpage && !shmem_mapping(inode->i_mapping))
  return -EIO;

 if (offset > i_size_read(inode))
  return -EINVAL;

 retry:
 uprobe = alloc_uprobe(inode, offset);
 if (!uprobe)
  return -ENOMEM;




 down_write(&uprobe->register_rwsem);
 ret = -EAGAIN;
 if (likely(uprobe_is_active(uprobe))) {
  ret = __uprobe_register(uprobe, uc);
  if (ret)
   __uprobe_unregister(uprobe, uc);
 }
 up_write(&uprobe->register_rwsem);
 put_uprobe(uprobe);

 if (unlikely(ret == -EAGAIN))
  goto retry;
 return ret;
}
EXPORT_SYMBOL_GPL(uprobe_register);
int uprobe_apply(struct inode *inode, loff_t offset,
   struct uprobe_consumer *uc, bool add)
{
 struct uprobe *uprobe;
 struct uprobe_consumer *con;
 int ret = -ENOENT;

 uprobe = find_uprobe(inode, offset);
 if (WARN_ON(!uprobe))
  return ret;

 down_write(&uprobe->register_rwsem);
 for (con = uprobe->consumers; con && con != uc ; con = con->next)
  ;
 if (con)
  ret = register_for_each_vma(uprobe, add ? uc : NULL);
 up_write(&uprobe->register_rwsem);
 put_uprobe(uprobe);

 return ret;
}







void uprobe_unregister(struct inode *inode, loff_t offset, struct uprobe_consumer *uc)
{
 struct uprobe *uprobe;

 uprobe = find_uprobe(inode, offset);
 if (WARN_ON(!uprobe))
  return;

 down_write(&uprobe->register_rwsem);
 __uprobe_unregister(uprobe, uc);
 up_write(&uprobe->register_rwsem);
 put_uprobe(uprobe);
}
EXPORT_SYMBOL_GPL(uprobe_unregister);

static int unapply_uprobe(struct uprobe *uprobe, struct mm_struct *mm)
{
 struct vm_area_struct *vma;
 int err = 0;

 down_read(&mm->mmap_sem);
 for (vma = mm->mmap; vma; vma = vma->vm_next) {
  unsigned long vaddr;
  loff_t offset;

  if (!valid_vma(vma, false) ||
      file_inode(vma->vm_file) != uprobe->inode)
   continue;

  offset = (loff_t)vma->vm_pgoff << PAGE_SHIFT;
  if (uprobe->offset < offset ||
      uprobe->offset >= offset + vma->vm_end - vma->vm_start)
   continue;

  vaddr = offset_to_vaddr(vma, uprobe->offset);
  err |= remove_breakpoint(uprobe, mm, vaddr);
 }
 up_read(&mm->mmap_sem);

 return err;
}

static struct rb_node *
find_node_in_range(struct inode *inode, loff_t min, loff_t max)
{
 struct rb_node *n = uprobes_tree.rb_node;

 while (n) {
  struct uprobe *u = rb_entry(n, struct uprobe, rb_node);

  if (inode < u->inode) {
   n = n->rb_left;
  } else if (inode > u->inode) {
   n = n->rb_right;
  } else {
   if (max < u->offset)
    n = n->rb_left;
   else if (min > u->offset)
    n = n->rb_right;
   else
    break;
  }
 }

 return n;
}




static void build_probe_list(struct inode *inode,
    struct vm_area_struct *vma,
    unsigned long start, unsigned long end,
    struct list_head *head)
{
 loff_t min, max;
 struct rb_node *n, *t;
 struct uprobe *u;

 INIT_LIST_HEAD(head);
 min = vaddr_to_offset(vma, start);
 max = min + (end - start) - 1;

 spin_lock(&uprobes_treelock);
 n = find_node_in_range(inode, min, max);
 if (n) {
  for (t = n; t; t = rb_prev(t)) {
   u = rb_entry(t, struct uprobe, rb_node);
   if (u->inode != inode || u->offset < min)
    break;
   list_add(&u->pending_list, head);
   get_uprobe(u);
  }
  for (t = n; (t = rb_next(t)); ) {
   u = rb_entry(t, struct uprobe, rb_node);
   if (u->inode != inode || u->offset > max)
    break;
   list_add(&u->pending_list, head);
   get_uprobe(u);
  }
 }
 spin_unlock(&uprobes_treelock);
}







int uprobe_mmap(struct vm_area_struct *vma)
{
 struct list_head tmp_list;
 struct uprobe *uprobe, *u;
 struct inode *inode;

 if (no_uprobe_events() || !valid_vma(vma, true))
  return 0;

 inode = file_inode(vma->vm_file);
 if (!inode)
  return 0;

 mutex_lock(uprobes_mmap_hash(inode));
 build_probe_list(inode, vma, vma->vm_start, vma->vm_end, &tmp_list);





 list_for_each_entry_safe(uprobe, u, &tmp_list, pending_list) {
  if (!fatal_signal_pending(current) &&
      filter_chain(uprobe, UPROBE_FILTER_MMAP, vma->vm_mm)) {
   unsigned long vaddr = offset_to_vaddr(vma, uprobe->offset);
   install_breakpoint(uprobe, vma->vm_mm, vma, vaddr);
  }
  put_uprobe(uprobe);
 }
 mutex_unlock(uprobes_mmap_hash(inode));

 return 0;
}

static bool
vma_has_uprobes(struct vm_area_struct *vma, unsigned long start, unsigned long end)
{
 loff_t min, max;
 struct inode *inode;
 struct rb_node *n;

 inode = file_inode(vma->vm_file);

 min = vaddr_to_offset(vma, start);
 max = min + (end - start) - 1;

 spin_lock(&uprobes_treelock);
 n = find_node_in_range(inode, min, max);
 spin_unlock(&uprobes_treelock);

 return !!n;
}




void uprobe_munmap(struct vm_area_struct *vma, unsigned long start, unsigned long end)
{
 if (no_uprobe_events() || !valid_vma(vma, false))
  return;

 if (!atomic_read(&vma->vm_mm->mm_users))
  return;

 if (!test_bit(MMF_HAS_UPROBES, &vma->vm_mm->flags) ||
      test_bit(MMF_RECALC_UPROBES, &vma->vm_mm->flags))
  return;

 if (vma_has_uprobes(vma, start, end))
  set_bit(MMF_RECALC_UPROBES, &vma->vm_mm->flags);
}


static int xol_add_vma(struct mm_struct *mm, struct xol_area *area)
{
 struct vm_area_struct *vma;
 int ret;

 if (down_write_killable(&mm->mmap_sem))
  return -EINTR;

 if (mm->uprobes_state.xol_area) {
  ret = -EALREADY;
  goto fail;
 }

 if (!area->vaddr) {

  area->vaddr = get_unmapped_area(NULL, TASK_SIZE - PAGE_SIZE,
      PAGE_SIZE, 0, 0);
  if (area->vaddr & ~PAGE_MASK) {
   ret = area->vaddr;
   goto fail;
  }
 }

 vma = _install_special_mapping(mm, area->vaddr, PAGE_SIZE,
    VM_EXEC|VM_MAYEXEC|VM_DONTCOPY|VM_IO,
    &area->xol_mapping);
 if (IS_ERR(vma)) {
  ret = PTR_ERR(vma);
  goto fail;
 }

 ret = 0;
 smp_wmb();
 mm->uprobes_state.xol_area = area;
 fail:
 up_write(&mm->mmap_sem);

 return ret;
}

static struct xol_area *__create_xol_area(unsigned long vaddr)
{
 struct mm_struct *mm = current->mm;
 uprobe_opcode_t insn = UPROBE_SWBP_INSN;
 struct xol_area *area;

 area = kmalloc(sizeof(*area), GFP_KERNEL);
 if (unlikely(!area))
  goto out;

 area->bitmap = kzalloc(BITS_TO_LONGS(UINSNS_PER_PAGE) * sizeof(long), GFP_KERNEL);
 if (!area->bitmap)
  goto free_area;

 area->xol_mapping.name = "[uprobes]";
 area->xol_mapping.fault = NULL;
 area->xol_mapping.pages = area->pages;
 area->pages[0] = alloc_page(GFP_HIGHUSER);
 if (!area->pages[0])
  goto free_bitmap;
 area->pages[1] = NULL;

 area->vaddr = vaddr;
 init_waitqueue_head(&area->wq);

 set_bit(0, area->bitmap);
 atomic_set(&area->slot_count, 1);
 copy_to_page(area->pages[0], 0, &insn, UPROBE_SWBP_INSN_SIZE);

 if (!xol_add_vma(mm, area))
  return area;

 __free_page(area->pages[0]);
 free_bitmap:
 kfree(area->bitmap);
 free_area:
 kfree(area);
 out:
 return NULL;
}







static struct xol_area *get_xol_area(void)
{
 struct mm_struct *mm = current->mm;
 struct xol_area *area;

 if (!mm->uprobes_state.xol_area)
  __create_xol_area(0);

 area = mm->uprobes_state.xol_area;
 smp_read_barrier_depends();
 return area;
}




void uprobe_clear_state(struct mm_struct *mm)
{
 struct xol_area *area = mm->uprobes_state.xol_area;

 if (!area)
  return;

 put_page(area->pages[0]);
 kfree(area->bitmap);
 kfree(area);
}

void uprobe_start_dup_mmap(void)
{
 percpu_down_read(&dup_mmap_sem);
}

void uprobe_end_dup_mmap(void)
{
 percpu_up_read(&dup_mmap_sem);
}

void uprobe_dup_mmap(struct mm_struct *oldmm, struct mm_struct *newmm)
{
 newmm->uprobes_state.xol_area = NULL;

 if (test_bit(MMF_HAS_UPROBES, &oldmm->flags)) {
  set_bit(MMF_HAS_UPROBES, &newmm->flags);

  set_bit(MMF_RECALC_UPROBES, &newmm->flags);
 }
}




static unsigned long xol_take_insn_slot(struct xol_area *area)
{
 unsigned long slot_addr;
 int slot_nr;

 do {
  slot_nr = find_first_zero_bit(area->bitmap, UINSNS_PER_PAGE);
  if (slot_nr < UINSNS_PER_PAGE) {
   if (!test_and_set_bit(slot_nr, area->bitmap))
    break;

   slot_nr = UINSNS_PER_PAGE;
   continue;
  }
  wait_event(area->wq, (atomic_read(&area->slot_count) < UINSNS_PER_PAGE));
 } while (slot_nr >= UINSNS_PER_PAGE);

 slot_addr = area->vaddr + (slot_nr * UPROBE_XOL_SLOT_BYTES);
 atomic_inc(&area->slot_count);

 return slot_addr;
}





static unsigned long xol_get_insn_slot(struct uprobe *uprobe)
{
 struct xol_area *area;
 unsigned long xol_vaddr;

 area = get_xol_area();
 if (!area)
  return 0;

 xol_vaddr = xol_take_insn_slot(area);
 if (unlikely(!xol_vaddr))
  return 0;

 arch_uprobe_copy_ixol(area->pages[0], xol_vaddr,
         &uprobe->arch.ixol, sizeof(uprobe->arch.ixol));

 return xol_vaddr;
}






static void xol_free_insn_slot(struct task_struct *tsk)
{
 struct xol_area *area;
 unsigned long vma_end;
 unsigned long slot_addr;

 if (!tsk->mm || !tsk->mm->uprobes_state.xol_area || !tsk->utask)
  return;

 slot_addr = tsk->utask->xol_vaddr;
 if (unlikely(!slot_addr))
  return;

 area = tsk->mm->uprobes_state.xol_area;
 vma_end = area->vaddr + PAGE_SIZE;
 if (area->vaddr <= slot_addr && slot_addr < vma_end) {
  unsigned long offset;
  int slot_nr;

  offset = slot_addr - area->vaddr;
  slot_nr = offset / UPROBE_XOL_SLOT_BYTES;
  if (slot_nr >= UINSNS_PER_PAGE)
   return;

  clear_bit(slot_nr, area->bitmap);
  atomic_dec(&area->slot_count);
  smp_mb__after_atomic();
  if (waitqueue_active(&area->wq))
   wake_up(&area->wq);

  tsk->utask->xol_vaddr = 0;
 }
}

void __weak arch_uprobe_copy_ixol(struct page *page, unsigned long vaddr,
      void *src, unsigned long len)
{

 copy_to_page(page, vaddr, src, len);







 flush_dcache_page(page);
}







unsigned long __weak uprobe_get_swbp_addr(struct pt_regs *regs)
{
 return instruction_pointer(regs) - UPROBE_SWBP_INSN_SIZE;
}

unsigned long uprobe_get_trap_addr(struct pt_regs *regs)
{
 struct uprobe_task *utask = current->utask;

 if (unlikely(utask && utask->active_uprobe))
  return utask->vaddr;

 return instruction_pointer(regs);
}

static struct return_instance *free_ret_instance(struct return_instance *ri)
{
 struct return_instance *next = ri->next;
 put_uprobe(ri->uprobe);
 kfree(ri);
 return next;
}





void uprobe_free_utask(struct task_struct *t)
{
 struct uprobe_task *utask = t->utask;
 struct return_instance *ri;

 if (!utask)
  return;

 if (utask->active_uprobe)
  put_uprobe(utask->active_uprobe);

 ri = utask->return_instances;
 while (ri)
  ri = free_ret_instance(ri);

 xol_free_insn_slot(t);
 kfree(utask);
 t->utask = NULL;
}
static struct uprobe_task *get_utask(void)
{
 if (!current->utask)
  current->utask = kzalloc(sizeof(struct uprobe_task), GFP_KERNEL);
 return current->utask;
}

static int dup_utask(struct task_struct *t, struct uprobe_task *o_utask)
{
 struct uprobe_task *n_utask;
 struct return_instance **p, *o, *n;

 n_utask = kzalloc(sizeof(struct uprobe_task), GFP_KERNEL);
 if (!n_utask)
  return -ENOMEM;
 t->utask = n_utask;

 p = &n_utask->return_instances;
 for (o = o_utask->return_instances; o; o = o->next) {
  n = kmalloc(sizeof(struct return_instance), GFP_KERNEL);
  if (!n)
   return -ENOMEM;

  *n = *o;
  get_uprobe(n->uprobe);
  n->next = NULL;

  *p = n;
  p = &n->next;
  n_utask->depth++;
 }

 return 0;
}

static void uprobe_warn(struct task_struct *t, const char *msg)
{
 pr_warn("uprobe: %s:%d failed to %s\n",
   current->comm, current->pid, msg);
}

static void dup_xol_work(struct callback_head *work)
{
 if (current->flags & PF_EXITING)
  return;

 if (!__create_xol_area(current->utask->dup_xol_addr) &&
   !fatal_signal_pending(current))
  uprobe_warn(current, "dup xol area");
}




void uprobe_copy_process(struct task_struct *t, unsigned long flags)
{
 struct uprobe_task *utask = current->utask;
 struct mm_struct *mm = current->mm;
 struct xol_area *area;

 t->utask = NULL;

 if (!utask || !utask->return_instances)
  return;

 if (mm == t->mm && !(flags & CLONE_VFORK))
  return;

 if (dup_utask(t, utask))
  return uprobe_warn(t, "dup ret instances");


 area = mm->uprobes_state.xol_area;
 if (!area)
  return uprobe_warn(t, "dup xol area");

 if (mm == t->mm)
  return;

 t->utask->dup_xol_addr = area->vaddr;
 init_task_work(&t->utask->dup_xol_work, dup_xol_work);
 task_work_add(t, &t->utask->dup_xol_work, true);
}







static unsigned long get_trampoline_vaddr(void)
{
 struct xol_area *area;
 unsigned long trampoline_vaddr = -1;

 area = current->mm->uprobes_state.xol_area;
 smp_read_barrier_depends();
 if (area)
  trampoline_vaddr = area->vaddr;

 return trampoline_vaddr;
}

static void cleanup_return_instances(struct uprobe_task *utask, bool chained,
     struct pt_regs *regs)
{
 struct return_instance *ri = utask->return_instances;
 enum rp_check ctx = chained ? RP_CHECK_CHAIN_CALL : RP_CHECK_CALL;

 while (ri && !arch_uretprobe_is_alive(ri, ctx, regs)) {
  ri = free_ret_instance(ri);
  utask->depth--;
 }
 utask->return_instances = ri;
}

static void prepare_uretprobe(struct uprobe *uprobe, struct pt_regs *regs)
{
 struct return_instance *ri;
 struct uprobe_task *utask;
 unsigned long orig_ret_vaddr, trampoline_vaddr;
 bool chained;

 if (!get_xol_area())
  return;

 utask = get_utask();
 if (!utask)
  return;

 if (utask->depth >= MAX_URETPROBE_DEPTH) {
  printk_ratelimited(KERN_INFO "uprobe: omit uretprobe due to"
    " nestedness limit pid/tgid=%d/%d\n",
    current->pid, current->tgid);
  return;
 }

 ri = kmalloc(sizeof(struct return_instance), GFP_KERNEL);
 if (!ri)
  return;

 trampoline_vaddr = get_trampoline_vaddr();
 orig_ret_vaddr = arch_uretprobe_hijack_return_addr(trampoline_vaddr, regs);
 if (orig_ret_vaddr == -1)
  goto fail;


 chained = (orig_ret_vaddr == trampoline_vaddr);
 cleanup_return_instances(utask, chained, regs);






 if (chained) {
  if (!utask->return_instances) {




   uprobe_warn(current, "handle tail call");
   goto fail;
  }
  orig_ret_vaddr = utask->return_instances->orig_ret_vaddr;
 }

 ri->uprobe = get_uprobe(uprobe);
 ri->func = instruction_pointer(regs);
 ri->stack = user_stack_pointer(regs);
 ri->orig_ret_vaddr = orig_ret_vaddr;
 ri->chained = chained;

 utask->depth++;
 ri->next = utask->return_instances;
 utask->return_instances = ri;

 return;
 fail:
 kfree(ri);
}


static int
pre_ssout(struct uprobe *uprobe, struct pt_regs *regs, unsigned long bp_vaddr)
{
 struct uprobe_task *utask;
 unsigned long xol_vaddr;
 int err;

 utask = get_utask();
 if (!utask)
  return -ENOMEM;

 xol_vaddr = xol_get_insn_slot(uprobe);
 if (!xol_vaddr)
  return -ENOMEM;

 utask->xol_vaddr = xol_vaddr;
 utask->vaddr = bp_vaddr;

 err = arch_uprobe_pre_xol(&uprobe->arch, regs);
 if (unlikely(err)) {
  xol_free_insn_slot(current);
  return err;
 }

 utask->active_uprobe = uprobe;
 utask->state = UTASK_SSTEP;
 return 0;
}
bool uprobe_deny_signal(void)
{
 struct task_struct *t = current;
 struct uprobe_task *utask = t->utask;

 if (likely(!utask || !utask->active_uprobe))
  return false;

 WARN_ON_ONCE(utask->state != UTASK_SSTEP);

 if (signal_pending(t)) {
  spin_lock_irq(&t->sighand->siglock);
  clear_tsk_thread_flag(t, TIF_SIGPENDING);
  spin_unlock_irq(&t->sighand->siglock);

  if (__fatal_signal_pending(t) || arch_uprobe_xol_was_trapped(t)) {
   utask->state = UTASK_SSTEP_TRAPPED;
   set_tsk_thread_flag(t, TIF_UPROBE);
  }
 }

 return true;
}

static void mmf_recalc_uprobes(struct mm_struct *mm)
{
 struct vm_area_struct *vma;

 for (vma = mm->mmap; vma; vma = vma->vm_next) {
  if (!valid_vma(vma, false))
   continue;






  if (vma_has_uprobes(vma, vma->vm_start, vma->vm_end))
   return;
 }

 clear_bit(MMF_HAS_UPROBES, &mm->flags);
}

static int is_trap_at_addr(struct mm_struct *mm, unsigned long vaddr)
{
 struct page *page;
 uprobe_opcode_t opcode;
 int result;

 pagefault_disable();
 result = __get_user(opcode, (uprobe_opcode_t __user *)vaddr);
 pagefault_enable();

 if (likely(result == 0))
  goto out;







 result = get_user_pages_remote(NULL, mm, vaddr, 1, 0, 1, &page, NULL);
 if (result < 0)
  return result;

 copy_from_page(page, vaddr, &opcode, UPROBE_SWBP_INSN_SIZE);
 put_page(page);
 out:

 return is_trap_insn(&opcode);
}

static struct uprobe *find_active_uprobe(unsigned long bp_vaddr, int *is_swbp)
{
 struct mm_struct *mm = current->mm;
 struct uprobe *uprobe = NULL;
 struct vm_area_struct *vma;

 down_read(&mm->mmap_sem);
 vma = find_vma(mm, bp_vaddr);
 if (vma && vma->vm_start <= bp_vaddr) {
  if (valid_vma(vma, false)) {
   struct inode *inode = file_inode(vma->vm_file);
   loff_t offset = vaddr_to_offset(vma, bp_vaddr);

   uprobe = find_uprobe(inode, offset);
  }

  if (!uprobe)
   *is_swbp = is_trap_at_addr(mm, bp_vaddr);
 } else {
  *is_swbp = -EFAULT;
 }

 if (!uprobe && test_and_clear_bit(MMF_RECALC_UPROBES, &mm->flags))
  mmf_recalc_uprobes(mm);
 up_read(&mm->mmap_sem);

 return uprobe;
}

static void handler_chain(struct uprobe *uprobe, struct pt_regs *regs)
{
 struct uprobe_consumer *uc;
 int remove = UPROBE_HANDLER_REMOVE;
 bool need_prep = false;

 down_read(&uprobe->register_rwsem);
 for (uc = uprobe->consumers; uc; uc = uc->next) {
  int rc = 0;

  if (uc->handler) {
   rc = uc->handler(uc, regs);
   WARN(rc & ~UPROBE_HANDLER_MASK,
    "bad rc=0x%x from %pf()\n", rc, uc->handler);
  }

  if (uc->ret_handler)
   need_prep = true;

  remove &= rc;
 }

 if (need_prep && !remove)
  prepare_uretprobe(uprobe, regs);

 if (remove && uprobe->consumers) {
  WARN_ON(!uprobe_is_active(uprobe));
  unapply_uprobe(uprobe, current->mm);
 }
 up_read(&uprobe->register_rwsem);
}

static void
handle_uretprobe_chain(struct return_instance *ri, struct pt_regs *regs)
{
 struct uprobe *uprobe = ri->uprobe;
 struct uprobe_consumer *uc;

 down_read(&uprobe->register_rwsem);
 for (uc = uprobe->consumers; uc; uc = uc->next) {
  if (uc->ret_handler)
   uc->ret_handler(uc, ri->func, regs);
 }
 up_read(&uprobe->register_rwsem);
}

static struct return_instance *find_next_ret_chain(struct return_instance *ri)
{
 bool chained;

 do {
  chained = ri->chained;
  ri = ri->next;
 } while (chained);

 return ri;
}

static void handle_trampoline(struct pt_regs *regs)
{
 struct uprobe_task *utask;
 struct return_instance *ri, *next;
 bool valid;

 utask = current->utask;
 if (!utask)
  goto sigill;

 ri = utask->return_instances;
 if (!ri)
  goto sigill;

 do {






  next = find_next_ret_chain(ri);
  valid = !next || arch_uretprobe_is_alive(next, RP_CHECK_RET, regs);

  instruction_pointer_set(regs, ri->orig_ret_vaddr);
  do {
   if (valid)
    handle_uretprobe_chain(ri, regs);
   ri = free_ret_instance(ri);
   utask->depth--;
  } while (ri != next);
 } while (!valid);

 utask->return_instances = ri;
 return;

 sigill:
 uprobe_warn(current, "handle uretprobe, sending SIGILL.");
 force_sig_info(SIGILL, SEND_SIG_FORCED, current);

}

bool __weak arch_uprobe_ignore(struct arch_uprobe *aup, struct pt_regs *regs)
{
 return false;
}

bool __weak arch_uretprobe_is_alive(struct return_instance *ret, enum rp_check ctx,
     struct pt_regs *regs)
{
 return true;
}





static void handle_swbp(struct pt_regs *regs)
{
 struct uprobe *uprobe;
 unsigned long bp_vaddr;
 int uninitialized_var(is_swbp);

 bp_vaddr = uprobe_get_swbp_addr(regs);
 if (bp_vaddr == get_trampoline_vaddr())
  return handle_trampoline(regs);

 uprobe = find_active_uprobe(bp_vaddr, &is_swbp);
 if (!uprobe) {
  if (is_swbp > 0) {

   send_sig(SIGTRAP, current, 0);
  } else {
   instruction_pointer_set(regs, bp_vaddr);
  }
  return;
 }


 instruction_pointer_set(regs, bp_vaddr);






 smp_rmb();
 if (unlikely(!test_bit(UPROBE_COPY_INSN, &uprobe->flags)))
  goto out;


 if (!get_utask())
  goto out;

 if (arch_uprobe_ignore(&uprobe->arch, regs))
  goto out;

 handler_chain(uprobe, regs);

 if (arch_uprobe_skip_sstep(&uprobe->arch, regs))
  goto out;

 if (!pre_ssout(uprobe, regs, bp_vaddr))
  return;


out:
 put_uprobe(uprobe);
}





static void handle_singlestep(struct uprobe_task *utask, struct pt_regs *regs)
{
 struct uprobe *uprobe;
 int err = 0;

 uprobe = utask->active_uprobe;
 if (utask->state == UTASK_SSTEP_ACK)
  err = arch_uprobe_post_xol(&uprobe->arch, regs);
 else if (utask->state == UTASK_SSTEP_TRAPPED)
  arch_uprobe_abort_xol(&uprobe->arch, regs);
 else
  WARN_ON_ONCE(1);

 put_uprobe(uprobe);
 utask->active_uprobe = NULL;
 utask->state = UTASK_RUNNING;
 xol_free_insn_slot(current);

 spin_lock_irq(&current->sighand->siglock);
 recalc_sigpending();
 spin_unlock_irq(&current->sighand->siglock);

 if (unlikely(err)) {
  uprobe_warn(current, "execute the probed insn, sending SIGILL.");
  force_sig_info(SIGILL, SEND_SIG_FORCED, current);
 }
}
void uprobe_notify_resume(struct pt_regs *regs)
{
 struct uprobe_task *utask;

 clear_thread_flag(TIF_UPROBE);

 utask = current->utask;
 if (utask && utask->active_uprobe)
  handle_singlestep(utask, regs);
 else
  handle_swbp(regs);
}





int uprobe_pre_sstep_notifier(struct pt_regs *regs)
{
 if (!current->mm)
  return 0;

 if (!test_bit(MMF_HAS_UPROBES, &current->mm->flags) &&
     (!current->utask || !current->utask->return_instances))
  return 0;

 set_thread_flag(TIF_UPROBE);
 return 1;
}





int uprobe_post_sstep_notifier(struct pt_regs *regs)
{
 struct uprobe_task *utask = current->utask;

 if (!current->mm || !utask || !utask->active_uprobe)

  return 0;

 utask->state = UTASK_SSTEP_ACK;
 set_thread_flag(TIF_UPROBE);
 return 1;
}

static struct notifier_block uprobe_exception_nb = {
 .notifier_call = arch_uprobe_exception_notify,
 .priority = INT_MAX-1,
};

static int __init init_uprobes(void)
{
 int i;

 for (i = 0; i < UPROBES_HASH_SZ; i++)
  mutex_init(&uprobes_mmap_mutex[i]);

 if (percpu_init_rwsem(&dup_mmap_sem))
  return -ENOMEM;

 return register_die_notifier(&uprobe_exception_nb);
}
__initcall(init_uprobes);





static struct snapshot_data {
 struct snapshot_handle handle;
 int swap;
 int mode;
 bool frozen;
 bool ready;
 bool platform_support;
 bool free_bitmaps;
} snapshot_state;

atomic_t snapshot_device_available = ATOMIC_INIT(1);

static int snapshot_open(struct inode *inode, struct file *filp)
{
 struct snapshot_data *data;
 int error;

 if (!hibernation_available())
  return -EPERM;

 lock_system_sleep();

 if (!atomic_add_unless(&snapshot_device_available, -1, 0)) {
  error = -EBUSY;
  goto Unlock;
 }

 if ((filp->f_flags & O_ACCMODE) == O_RDWR) {
  atomic_inc(&snapshot_device_available);
  error = -ENOSYS;
  goto Unlock;
 }
 nonseekable_open(inode, filp);
 data = &snapshot_state;
 filp->private_data = data;
 memset(&data->handle, 0, sizeof(struct snapshot_handle));
 if ((filp->f_flags & O_ACCMODE) == O_RDONLY) {

  data->swap = swsusp_resume_device ?
   swap_type_of(swsusp_resume_device, 0, NULL) : -1;
  data->mode = O_RDONLY;
  data->free_bitmaps = false;
  error = pm_notifier_call_chain(PM_HIBERNATION_PREPARE);
  if (error)
   pm_notifier_call_chain(PM_POST_HIBERNATION);
 } else {




  wait_for_device_probe();

  data->swap = -1;
  data->mode = O_WRONLY;
  error = pm_notifier_call_chain(PM_RESTORE_PREPARE);
  if (!error) {
   error = create_basic_memory_bitmaps();
   data->free_bitmaps = !error;
  }
  if (error)
   pm_notifier_call_chain(PM_POST_RESTORE);
 }
 if (error)
  atomic_inc(&snapshot_device_available);

 data->frozen = false;
 data->ready = false;
 data->platform_support = false;

 Unlock:
 unlock_system_sleep();

 return error;
}

static int snapshot_release(struct inode *inode, struct file *filp)
{
 struct snapshot_data *data;

 lock_system_sleep();

 swsusp_free();
 data = filp->private_data;
 free_all_swap_pages(data->swap);
 if (data->frozen) {
  pm_restore_gfp_mask();
  free_basic_memory_bitmaps();
  thaw_processes();
 } else if (data->free_bitmaps) {
  free_basic_memory_bitmaps();
 }
 pm_notifier_call_chain(data->mode == O_RDONLY ?
   PM_POST_HIBERNATION : PM_POST_RESTORE);
 atomic_inc(&snapshot_device_available);

 unlock_system_sleep();

 return 0;
}

static ssize_t snapshot_read(struct file *filp, char __user *buf,
                             size_t count, loff_t *offp)
{
 struct snapshot_data *data;
 ssize_t res;
 loff_t pg_offp = *offp & ~PAGE_MASK;

 lock_system_sleep();

 data = filp->private_data;
 if (!data->ready) {
  res = -ENODATA;
  goto Unlock;
 }
 if (!pg_offp) {
  res = snapshot_read_next(&data->handle);
  if (res <= 0)
   goto Unlock;
 } else {
  res = PAGE_SIZE - pg_offp;
 }

 res = simple_read_from_buffer(buf, count, &pg_offp,
   data_of(data->handle), res);
 if (res > 0)
  *offp += res;

 Unlock:
 unlock_system_sleep();

 return res;
}

static ssize_t snapshot_write(struct file *filp, const char __user *buf,
                              size_t count, loff_t *offp)
{
 struct snapshot_data *data;
 ssize_t res;
 loff_t pg_offp = *offp & ~PAGE_MASK;

 lock_system_sleep();

 data = filp->private_data;

 if (!pg_offp) {
  res = snapshot_write_next(&data->handle);
  if (res <= 0)
   goto unlock;
 } else {
  res = PAGE_SIZE - pg_offp;
 }

 res = simple_write_to_buffer(data_of(data->handle), res, &pg_offp,
   buf, count);
 if (res > 0)
  *offp += res;
unlock:
 unlock_system_sleep();

 return res;
}

static long snapshot_ioctl(struct file *filp, unsigned int cmd,
       unsigned long arg)
{
 int error = 0;
 struct snapshot_data *data;
 loff_t size;
 sector_t offset;

 if (_IOC_TYPE(cmd) != SNAPSHOT_IOC_MAGIC)
  return -ENOTTY;
 if (_IOC_NR(cmd) > SNAPSHOT_IOC_MAXNR)
  return -ENOTTY;
 if (!capable(CAP_SYS_ADMIN))
  return -EPERM;

 if (!mutex_trylock(&pm_mutex))
  return -EBUSY;

 lock_device_hotplug();
 data = filp->private_data;

 switch (cmd) {

 case SNAPSHOT_FREEZE:
  if (data->frozen)
   break;

  printk("Syncing filesystems ... ");
  sys_sync();
  printk("done.\n");

  error = freeze_processes();
  if (error)
   break;

  error = create_basic_memory_bitmaps();
  if (error)
   thaw_processes();
  else
   data->frozen = true;

  break;

 case SNAPSHOT_UNFREEZE:
  if (!data->frozen || data->ready)
   break;
  pm_restore_gfp_mask();
  free_basic_memory_bitmaps();
  data->free_bitmaps = false;
  thaw_processes();
  data->frozen = false;
  break;

 case SNAPSHOT_CREATE_IMAGE:
  if (data->mode != O_RDONLY || !data->frozen || data->ready) {
   error = -EPERM;
   break;
  }
  pm_restore_gfp_mask();
  error = hibernation_snapshot(data->platform_support);
  if (!error) {
   error = put_user(in_suspend, (int __user *)arg);
   data->ready = !freezer_test_done && !error;
   freezer_test_done = false;
  }
  break;

 case SNAPSHOT_ATOMIC_RESTORE:
  snapshot_write_finalize(&data->handle);
  if (data->mode != O_WRONLY || !data->frozen ||
      !snapshot_image_loaded(&data->handle)) {
   error = -EPERM;
   break;
  }
  error = hibernation_restore(data->platform_support);
  break;

 case SNAPSHOT_FREE:
  swsusp_free();
  memset(&data->handle, 0, sizeof(struct snapshot_handle));
  data->ready = false;
  thaw_kernel_threads();
  break;

 case SNAPSHOT_PREF_IMAGE_SIZE:
  image_size = arg;
  break;

 case SNAPSHOT_GET_IMAGE_SIZE:
  if (!data->ready) {
   error = -ENODATA;
   break;
  }
  size = snapshot_get_image_size();
  size <<= PAGE_SHIFT;
  error = put_user(size, (loff_t __user *)arg);
  break;

 case SNAPSHOT_AVAIL_SWAP_SIZE:
  size = count_swap_pages(data->swap, 1);
  size <<= PAGE_SHIFT;
  error = put_user(size, (loff_t __user *)arg);
  break;

 case SNAPSHOT_ALLOC_SWAP_PAGE:
  if (data->swap < 0 || data->swap >= MAX_SWAPFILES) {
   error = -ENODEV;
   break;
  }
  offset = alloc_swapdev_block(data->swap);
  if (offset) {
   offset <<= PAGE_SHIFT;
   error = put_user(offset, (loff_t __user *)arg);
  } else {
   error = -ENOSPC;
  }
  break;

 case SNAPSHOT_FREE_SWAP_PAGES:
  if (data->swap < 0 || data->swap >= MAX_SWAPFILES) {
   error = -ENODEV;
   break;
  }
  free_all_swap_pages(data->swap);
  break;

 case SNAPSHOT_S2RAM:
  if (!data->frozen) {
   error = -EPERM;
   break;
  }




  error = suspend_devices_and_enter(PM_SUSPEND_MEM);
  data->ready = false;
  break;

 case SNAPSHOT_PLATFORM_SUPPORT:
  data->platform_support = !!arg;
  break;

 case SNAPSHOT_POWER_OFF:
  if (data->platform_support)
   error = hibernation_platform_enter();
  break;

 case SNAPSHOT_SET_SWAP_AREA:
  if (swsusp_swap_in_use()) {
   error = -EPERM;
  } else {
   struct resume_swap_area swap_area;
   dev_t swdev;

   error = copy_from_user(&swap_area, (void __user *)arg,
     sizeof(struct resume_swap_area));
   if (error) {
    error = -EFAULT;
    break;
   }





   swdev = new_decode_dev(swap_area.dev);
   if (swdev) {
    offset = swap_area.offset;
    data->swap = swap_type_of(swdev, offset, NULL);
    if (data->swap < 0)
     error = -ENODEV;
   } else {
    data->swap = -1;
    error = -EINVAL;
   }
  }
  break;

 default:
  error = -ENOTTY;

 }

 unlock_device_hotplug();
 mutex_unlock(&pm_mutex);

 return error;
}


struct compat_resume_swap_area {
 compat_loff_t offset;
 u32 dev;
} __packed;

static long
snapshot_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
 BUILD_BUG_ON(sizeof(loff_t) != sizeof(compat_loff_t));

 switch (cmd) {
 case SNAPSHOT_GET_IMAGE_SIZE:
 case SNAPSHOT_AVAIL_SWAP_SIZE:
 case SNAPSHOT_ALLOC_SWAP_PAGE: {
  compat_loff_t __user *uoffset = compat_ptr(arg);
  loff_t offset;
  mm_segment_t old_fs;
  int err;

  old_fs = get_fs();
  set_fs(KERNEL_DS);
  err = snapshot_ioctl(file, cmd, (unsigned long) &offset);
  set_fs(old_fs);
  if (!err && put_user(offset, uoffset))
   err = -EFAULT;
  return err;
 }

 case SNAPSHOT_CREATE_IMAGE:
  return snapshot_ioctl(file, cmd,
          (unsigned long) compat_ptr(arg));

 case SNAPSHOT_SET_SWAP_AREA: {
  struct compat_resume_swap_area __user *u_swap_area =
   compat_ptr(arg);
  struct resume_swap_area swap_area;
  mm_segment_t old_fs;
  int err;

  err = get_user(swap_area.offset, &u_swap_area->offset);
  err |= get_user(swap_area.dev, &u_swap_area->dev);
  if (err)
   return -EFAULT;
  old_fs = get_fs();
  set_fs(KERNEL_DS);
  err = snapshot_ioctl(file, SNAPSHOT_SET_SWAP_AREA,
         (unsigned long) &swap_area);
  set_fs(old_fs);
  return err;
 }

 default:
  return snapshot_ioctl(file, cmd, arg);
 }
}


static const struct file_operations snapshot_fops = {
 .open = snapshot_open,
 .release = snapshot_release,
 .read = snapshot_read,
 .write = snapshot_write,
 .llseek = no_llseek,
 .unlocked_ioctl = snapshot_ioctl,
 .compat_ioctl = snapshot_compat_ioctl,
};

static struct miscdevice snapshot_device = {
 .minor = SNAPSHOT_MINOR,
 .name = "snapshot",
 .fops = &snapshot_fops,
};

static int __init snapshot_device_init(void)
{
 return misc_register(&snapshot_device);
};

device_initcall(snapshot_device_init);








static struct kmem_cache *user_ns_cachep __read_mostly;
static DEFINE_MUTEX(userns_state_mutex);

static bool new_idmap_permitted(const struct file *file,
    struct user_namespace *ns, int cap_setid,
    struct uid_gid_map *map);

static void set_cred_user_ns(struct cred *cred, struct user_namespace *user_ns)
{



 cred->securebits = SECUREBITS_DEFAULT;
 cred->cap_inheritable = CAP_EMPTY_SET;
 cred->cap_permitted = CAP_FULL_SET;
 cred->cap_effective = CAP_FULL_SET;
 cred->cap_ambient = CAP_EMPTY_SET;
 cred->cap_bset = CAP_FULL_SET;
 key_put(cred->request_key_auth);
 cred->request_key_auth = NULL;

 cred->user_ns = user_ns;
}
int create_user_ns(struct cred *new)
{
 struct user_namespace *ns, *parent_ns = new->user_ns;
 kuid_t owner = new->euid;
 kgid_t group = new->egid;
 int ret;

 if (parent_ns->level > 32)
  return -EUSERS;







 if (current_chrooted())
  return -EPERM;





 if (!kuid_has_mapping(parent_ns, owner) ||
     !kgid_has_mapping(parent_ns, group))
  return -EPERM;

 ns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);
 if (!ns)
  return -ENOMEM;

 ret = ns_alloc_inum(&ns->ns);
 if (ret) {
  kmem_cache_free(user_ns_cachep, ns);
  return ret;
 }
 ns->ns.ops = &userns_operations;

 atomic_set(&ns->count, 1);

 ns->parent = parent_ns;
 ns->level = parent_ns->level + 1;
 ns->owner = owner;
 ns->group = group;


 mutex_lock(&userns_state_mutex);
 ns->flags = parent_ns->flags;
 mutex_unlock(&userns_state_mutex);

 set_cred_user_ns(new, ns);

 init_rwsem(&ns->persistent_keyring_register_sem);
 return 0;
}

int unshare_userns(unsigned long unshare_flags, struct cred **new_cred)
{
 struct cred *cred;
 int err = -ENOMEM;

 if (!(unshare_flags & CLONE_NEWUSER))
  return 0;

 cred = prepare_creds();
 if (cred) {
  err = create_user_ns(cred);
  if (err)
   put_cred(cred);
  else
   *new_cred = cred;
 }

 return err;
}

void free_user_ns(struct user_namespace *ns)
{
 struct user_namespace *parent;

 do {
  parent = ns->parent;
  key_put(ns->persistent_keyring_register);
  ns_free_inum(&ns->ns);
  kmem_cache_free(user_ns_cachep, ns);
  ns = parent;
 } while (atomic_dec_and_test(&parent->count));
}
EXPORT_SYMBOL(free_user_ns);

static u32 map_id_range_down(struct uid_gid_map *map, u32 id, u32 count)
{
 unsigned idx, extents;
 u32 first, last, id2;

 id2 = id + count - 1;


 extents = map->nr_extents;
 smp_rmb();
 for (idx = 0; idx < extents; idx++) {
  first = map->extent[idx].first;
  last = first + map->extent[idx].count - 1;
  if (id >= first && id <= last &&
      (id2 >= first && id2 <= last))
   break;
 }

 if (idx < extents)
  id = (id - first) + map->extent[idx].lower_first;
 else
  id = (u32) -1;

 return id;
}

static u32 map_id_down(struct uid_gid_map *map, u32 id)
{
 unsigned idx, extents;
 u32 first, last;


 extents = map->nr_extents;
 smp_rmb();
 for (idx = 0; idx < extents; idx++) {
  first = map->extent[idx].first;
  last = first + map->extent[idx].count - 1;
  if (id >= first && id <= last)
   break;
 }

 if (idx < extents)
  id = (id - first) + map->extent[idx].lower_first;
 else
  id = (u32) -1;

 return id;
}

static u32 map_id_up(struct uid_gid_map *map, u32 id)
{
 unsigned idx, extents;
 u32 first, last;


 extents = map->nr_extents;
 smp_rmb();
 for (idx = 0; idx < extents; idx++) {
  first = map->extent[idx].lower_first;
  last = first + map->extent[idx].count - 1;
  if (id >= first && id <= last)
   break;
 }

 if (idx < extents)
  id = (id - first) + map->extent[idx].first;
 else
  id = (u32) -1;

 return id;
}
kuid_t make_kuid(struct user_namespace *ns, uid_t uid)
{

 return KUIDT_INIT(map_id_down(&ns->uid_map, uid));
}
EXPORT_SYMBOL(make_kuid);
uid_t from_kuid(struct user_namespace *targ, kuid_t kuid)
{

 return map_id_up(&targ->uid_map, __kuid_val(kuid));
}
EXPORT_SYMBOL(from_kuid);
uid_t from_kuid_munged(struct user_namespace *targ, kuid_t kuid)
{
 uid_t uid;
 uid = from_kuid(targ, kuid);

 if (uid == (uid_t) -1)
  uid = overflowuid;
 return uid;
}
EXPORT_SYMBOL(from_kuid_munged);
kgid_t make_kgid(struct user_namespace *ns, gid_t gid)
{

 return KGIDT_INIT(map_id_down(&ns->gid_map, gid));
}
EXPORT_SYMBOL(make_kgid);
gid_t from_kgid(struct user_namespace *targ, kgid_t kgid)
{

 return map_id_up(&targ->gid_map, __kgid_val(kgid));
}
EXPORT_SYMBOL(from_kgid);
gid_t from_kgid_munged(struct user_namespace *targ, kgid_t kgid)
{
 gid_t gid;
 gid = from_kgid(targ, kgid);

 if (gid == (gid_t) -1)
  gid = overflowgid;
 return gid;
}
EXPORT_SYMBOL(from_kgid_munged);
kprojid_t make_kprojid(struct user_namespace *ns, projid_t projid)
{

 return KPROJIDT_INIT(map_id_down(&ns->projid_map, projid));
}
EXPORT_SYMBOL(make_kprojid);
projid_t from_kprojid(struct user_namespace *targ, kprojid_t kprojid)
{

 return map_id_up(&targ->projid_map, __kprojid_val(kprojid));
}
EXPORT_SYMBOL(from_kprojid);
projid_t from_kprojid_munged(struct user_namespace *targ, kprojid_t kprojid)
{
 projid_t projid;
 projid = from_kprojid(targ, kprojid);

 if (projid == (projid_t) -1)
  projid = OVERFLOW_PROJID;
 return projid;
}
EXPORT_SYMBOL(from_kprojid_munged);


static int uid_m_show(struct seq_file *seq, void *v)
{
 struct user_namespace *ns = seq->private;
 struct uid_gid_extent *extent = v;
 struct user_namespace *lower_ns;
 uid_t lower;

 lower_ns = seq_user_ns(seq);
 if ((lower_ns == ns) && lower_ns->parent)
  lower_ns = lower_ns->parent;

 lower = from_kuid(lower_ns, KUIDT_INIT(extent->lower_first));

 seq_printf(seq, "%10u %10u %10u\n",
  extent->first,
  lower,
  extent->count);

 return 0;
}

static int gid_m_show(struct seq_file *seq, void *v)
{
 struct user_namespace *ns = seq->private;
 struct uid_gid_extent *extent = v;
 struct user_namespace *lower_ns;
 gid_t lower;

 lower_ns = seq_user_ns(seq);
 if ((lower_ns == ns) && lower_ns->parent)
  lower_ns = lower_ns->parent;

 lower = from_kgid(lower_ns, KGIDT_INIT(extent->lower_first));

 seq_printf(seq, "%10u %10u %10u\n",
  extent->first,
  lower,
  extent->count);

 return 0;
}

static int projid_m_show(struct seq_file *seq, void *v)
{
 struct user_namespace *ns = seq->private;
 struct uid_gid_extent *extent = v;
 struct user_namespace *lower_ns;
 projid_t lower;

 lower_ns = seq_user_ns(seq);
 if ((lower_ns == ns) && lower_ns->parent)
  lower_ns = lower_ns->parent;

 lower = from_kprojid(lower_ns, KPROJIDT_INIT(extent->lower_first));

 seq_printf(seq, "%10u %10u %10u\n",
  extent->first,
  lower,
  extent->count);

 return 0;
}

static void *m_start(struct seq_file *seq, loff_t *ppos,
       struct uid_gid_map *map)
{
 struct uid_gid_extent *extent = NULL;
 loff_t pos = *ppos;

 if (pos < map->nr_extents)
  extent = &map->extent[pos];

 return extent;
}

static void *uid_m_start(struct seq_file *seq, loff_t *ppos)
{
 struct user_namespace *ns = seq->private;

 return m_start(seq, ppos, &ns->uid_map);
}

static void *gid_m_start(struct seq_file *seq, loff_t *ppos)
{
 struct user_namespace *ns = seq->private;

 return m_start(seq, ppos, &ns->gid_map);
}

static void *projid_m_start(struct seq_file *seq, loff_t *ppos)
{
 struct user_namespace *ns = seq->private;

 return m_start(seq, ppos, &ns->projid_map);
}

static void *m_next(struct seq_file *seq, void *v, loff_t *pos)
{
 (*pos)++;
 return seq->op->start(seq, pos);
}

static void m_stop(struct seq_file *seq, void *v)
{
 return;
}

const struct seq_operations proc_uid_seq_operations = {
 .start = uid_m_start,
 .stop = m_stop,
 .next = m_next,
 .show = uid_m_show,
};

const struct seq_operations proc_gid_seq_operations = {
 .start = gid_m_start,
 .stop = m_stop,
 .next = m_next,
 .show = gid_m_show,
};

const struct seq_operations proc_projid_seq_operations = {
 .start = projid_m_start,
 .stop = m_stop,
 .next = m_next,
 .show = projid_m_show,
};

static bool mappings_overlap(struct uid_gid_map *new_map,
        struct uid_gid_extent *extent)
{
 u32 upper_first, lower_first, upper_last, lower_last;
 unsigned idx;

 upper_first = extent->first;
 lower_first = extent->lower_first;
 upper_last = upper_first + extent->count - 1;
 lower_last = lower_first + extent->count - 1;

 for (idx = 0; idx < new_map->nr_extents; idx++) {
  u32 prev_upper_first, prev_lower_first;
  u32 prev_upper_last, prev_lower_last;
  struct uid_gid_extent *prev;

  prev = &new_map->extent[idx];

  prev_upper_first = prev->first;
  prev_lower_first = prev->lower_first;
  prev_upper_last = prev_upper_first + prev->count - 1;
  prev_lower_last = prev_lower_first + prev->count - 1;


  if ((prev_upper_first <= upper_last) &&
      (prev_upper_last >= upper_first))
   return true;


  if ((prev_lower_first <= lower_last) &&
      (prev_lower_last >= lower_first))
   return true;
 }
 return false;
}

static ssize_t map_write(struct file *file, const char __user *buf,
    size_t count, loff_t *ppos,
    int cap_setid,
    struct uid_gid_map *map,
    struct uid_gid_map *parent_map)
{
 struct seq_file *seq = file->private_data;
 struct user_namespace *ns = seq->private;
 struct uid_gid_map new_map;
 unsigned idx;
 struct uid_gid_extent *extent = NULL;
 char *kbuf = NULL, *pos, *next_line;
 ssize_t ret = -EINVAL;
 mutex_lock(&userns_state_mutex);

 ret = -EPERM;

 if (map->nr_extents != 0)
  goto out;




 if (cap_valid(cap_setid) && !file_ns_capable(file, ns, CAP_SYS_ADMIN))
  goto out;


 ret = -EINVAL;
 if ((*ppos != 0) || (count >= PAGE_SIZE))
  goto out;


 kbuf = memdup_user_nul(buf, count);
 if (IS_ERR(kbuf)) {
  ret = PTR_ERR(kbuf);
  kbuf = NULL;
  goto out;
 }


 ret = -EINVAL;
 pos = kbuf;
 new_map.nr_extents = 0;
 for (; pos; pos = next_line) {
  extent = &new_map.extent[new_map.nr_extents];


  next_line = strchr(pos, '\n');
  if (next_line) {
   *next_line = '\0';
   next_line++;
   if (*next_line == '\0')
    next_line = NULL;
  }

  pos = skip_spaces(pos);
  extent->first = simple_strtoul(pos, &pos, 10);
  if (!isspace(*pos))
   goto out;

  pos = skip_spaces(pos);
  extent->lower_first = simple_strtoul(pos, &pos, 10);
  if (!isspace(*pos))
   goto out;

  pos = skip_spaces(pos);
  extent->count = simple_strtoul(pos, &pos, 10);
  if (*pos && !isspace(*pos))
   goto out;


  pos = skip_spaces(pos);
  if (*pos != '\0')
   goto out;


  if ((extent->first == (u32) -1) ||
      (extent->lower_first == (u32) -1))
   goto out;




  if ((extent->first + extent->count) <= extent->first)
   goto out;
  if ((extent->lower_first + extent->count) <=
       extent->lower_first)
   goto out;


  if (mappings_overlap(&new_map, extent))
   goto out;

  new_map.nr_extents++;


  if ((new_map.nr_extents == UID_GID_MAP_MAX_EXTENTS) &&
      (next_line != NULL))
   goto out;
 }

 if (new_map.nr_extents == 0)
  goto out;

 ret = -EPERM;

 if (!new_idmap_permitted(file, ns, cap_setid, &new_map))
  goto out;




 for (idx = 0; idx < new_map.nr_extents; idx++) {
  u32 lower_first;
  extent = &new_map.extent[idx];

  lower_first = map_id_range_down(parent_map,
      extent->lower_first,
      extent->count);




  if (lower_first == (u32) -1)
   goto out;

  extent->lower_first = lower_first;
 }


 memcpy(map->extent, new_map.extent,
  new_map.nr_extents*sizeof(new_map.extent[0]));
 smp_wmb();
 map->nr_extents = new_map.nr_extents;

 *ppos = count;
 ret = count;
out:
 mutex_unlock(&userns_state_mutex);
 kfree(kbuf);
 return ret;
}

ssize_t proc_uid_map_write(struct file *file, const char __user *buf,
      size_t size, loff_t *ppos)
{
 struct seq_file *seq = file->private_data;
 struct user_namespace *ns = seq->private;
 struct user_namespace *seq_ns = seq_user_ns(seq);

 if (!ns->parent)
  return -EPERM;

 if ((seq_ns != ns) && (seq_ns != ns->parent))
  return -EPERM;

 return map_write(file, buf, size, ppos, CAP_SETUID,
    &ns->uid_map, &ns->parent->uid_map);
}

ssize_t proc_gid_map_write(struct file *file, const char __user *buf,
      size_t size, loff_t *ppos)
{
 struct seq_file *seq = file->private_data;
 struct user_namespace *ns = seq->private;
 struct user_namespace *seq_ns = seq_user_ns(seq);

 if (!ns->parent)
  return -EPERM;

 if ((seq_ns != ns) && (seq_ns != ns->parent))
  return -EPERM;

 return map_write(file, buf, size, ppos, CAP_SETGID,
    &ns->gid_map, &ns->parent->gid_map);
}

ssize_t proc_projid_map_write(struct file *file, const char __user *buf,
         size_t size, loff_t *ppos)
{
 struct seq_file *seq = file->private_data;
 struct user_namespace *ns = seq->private;
 struct user_namespace *seq_ns = seq_user_ns(seq);

 if (!ns->parent)
  return -EPERM;

 if ((seq_ns != ns) && (seq_ns != ns->parent))
  return -EPERM;


 return map_write(file, buf, size, ppos, -1,
    &ns->projid_map, &ns->parent->projid_map);
}

static bool new_idmap_permitted(const struct file *file,
    struct user_namespace *ns, int cap_setid,
    struct uid_gid_map *new_map)
{
 const struct cred *cred = file->f_cred;



 if ((new_map->nr_extents == 1) && (new_map->extent[0].count == 1) &&
     uid_eq(ns->owner, cred->euid)) {
  u32 id = new_map->extent[0].lower_first;
  if (cap_setid == CAP_SETUID) {
   kuid_t uid = make_kuid(ns->parent, id);
   if (uid_eq(uid, cred->euid))
    return true;
  } else if (cap_setid == CAP_SETGID) {
   kgid_t gid = make_kgid(ns->parent, id);
   if (!(ns->flags & USERNS_SETGROUPS_ALLOWED) &&
       gid_eq(gid, cred->egid))
    return true;
  }
 }


 if (!cap_valid(cap_setid))
  return true;





 if (ns_capable(ns->parent, cap_setid) &&
     file_ns_capable(file, ns->parent, cap_setid))
  return true;

 return false;
}

int proc_setgroups_show(struct seq_file *seq, void *v)
{
 struct user_namespace *ns = seq->private;
 unsigned long userns_flags = ACCESS_ONCE(ns->flags);

 seq_printf(seq, "%s\n",
     (userns_flags & USERNS_SETGROUPS_ALLOWED) ?
     "allow" : "deny");
 return 0;
}

ssize_t proc_setgroups_write(struct file *file, const char __user *buf,
        size_t count, loff_t *ppos)
{
 struct seq_file *seq = file->private_data;
 struct user_namespace *ns = seq->private;
 char kbuf[8], *pos;
 bool setgroups_allowed;
 ssize_t ret;


 ret = -EINVAL;
 if ((*ppos != 0) || (count >= sizeof(kbuf)))
  goto out;


 ret = -EFAULT;
 if (copy_from_user(kbuf, buf, count))
  goto out;
 kbuf[count] = '\0';
 pos = kbuf;


 ret = -EINVAL;
 if (strncmp(pos, "allow", 5) == 0) {
  pos += 5;
  setgroups_allowed = true;
 }
 else if (strncmp(pos, "deny", 4) == 0) {
  pos += 4;
  setgroups_allowed = false;
 }
 else
  goto out;


 pos = skip_spaces(pos);
 if (*pos != '\0')
  goto out;

 ret = -EPERM;
 mutex_lock(&userns_state_mutex);
 if (setgroups_allowed) {



  if (!(ns->flags & USERNS_SETGROUPS_ALLOWED))
   goto out_unlock;
 } else {



  if (ns->gid_map.nr_extents != 0)
   goto out_unlock;
  ns->flags &= ~USERNS_SETGROUPS_ALLOWED;
 }
 mutex_unlock(&userns_state_mutex);


 *ppos = count;
 ret = count;
out:
 return ret;
out_unlock:
 mutex_unlock(&userns_state_mutex);
 goto out;
}

bool userns_may_setgroups(const struct user_namespace *ns)
{
 bool allowed;

 mutex_lock(&userns_state_mutex);



 allowed = ns->gid_map.nr_extents != 0;

 allowed = allowed && (ns->flags & USERNS_SETGROUPS_ALLOWED);
 mutex_unlock(&userns_state_mutex);

 return allowed;
}

static inline struct user_namespace *to_user_ns(struct ns_common *ns)
{
 return container_of(ns, struct user_namespace, ns);
}

static struct ns_common *userns_get(struct task_struct *task)
{
 struct user_namespace *user_ns;

 rcu_read_lock();
 user_ns = get_user_ns(__task_cred(task)->user_ns);
 rcu_read_unlock();

 return user_ns ? &user_ns->ns : NULL;
}

static void userns_put(struct ns_common *ns)
{
 put_user_ns(to_user_ns(ns));
}

static int userns_install(struct nsproxy *nsproxy, struct ns_common *ns)
{
 struct user_namespace *user_ns = to_user_ns(ns);
 struct cred *cred;




 if (user_ns == current_user_ns())
  return -EINVAL;


 if (!thread_group_empty(current))
  return -EINVAL;

 if (current->fs->users != 1)
  return -EINVAL;

 if (!ns_capable(user_ns, CAP_SYS_ADMIN))
  return -EPERM;

 cred = prepare_creds();
 if (!cred)
  return -ENOMEM;

 put_user_ns(cred->user_ns);
 set_cred_user_ns(cred, get_user_ns(user_ns));

 return commit_creds(cred);
}

const struct proc_ns_operations userns_operations = {
 .name = "user",
 .type = CLONE_NEWUSER,
 .get = userns_get,
 .put = userns_put,
 .install = userns_install,
};

static __init int user_namespaces_init(void)
{
 user_ns_cachep = KMEM_CACHE(user_namespace, SLAB_PANIC);
 return 0;
}
subsys_initcall(user_namespaces_init);


static DEFINE_PER_CPU(struct hlist_head, return_notifier_list);






void user_return_notifier_register(struct user_return_notifier *urn)
{
 set_tsk_thread_flag(current, TIF_USER_RETURN_NOTIFY);
 hlist_add_head(&urn->link, this_cpu_ptr(&return_notifier_list));
}
EXPORT_SYMBOL_GPL(user_return_notifier_register);





void user_return_notifier_unregister(struct user_return_notifier *urn)
{
 hlist_del(&urn->link);
 if (hlist_empty(this_cpu_ptr(&return_notifier_list)))
  clear_tsk_thread_flag(current, TIF_USER_RETURN_NOTIFY);
}
EXPORT_SYMBOL_GPL(user_return_notifier_unregister);


void fire_user_return_notifiers(void)
{
 struct user_return_notifier *urn;
 struct hlist_node *tmp2;
 struct hlist_head *head;

 head = &get_cpu_var(return_notifier_list);
 hlist_for_each_entry_safe(urn, tmp2, head, link)
  urn->on_user_return(urn);
 put_cpu_var(return_notifier_list);
}

static struct uts_namespace *create_uts_ns(void)
{
 struct uts_namespace *uts_ns;

 uts_ns = kmalloc(sizeof(struct uts_namespace), GFP_KERNEL);
 if (uts_ns)
  kref_init(&uts_ns->kref);
 return uts_ns;
}






static struct uts_namespace *clone_uts_ns(struct user_namespace *user_ns,
       struct uts_namespace *old_ns)
{
 struct uts_namespace *ns;
 int err;

 ns = create_uts_ns();
 if (!ns)
  return ERR_PTR(-ENOMEM);

 err = ns_alloc_inum(&ns->ns);
 if (err) {
  kfree(ns);
  return ERR_PTR(err);
 }

 ns->ns.ops = &utsns_operations;

 down_read(&uts_sem);
 memcpy(&ns->name, &old_ns->name, sizeof(ns->name));
 ns->user_ns = get_user_ns(user_ns);
 up_read(&uts_sem);
 return ns;
}







struct uts_namespace *copy_utsname(unsigned long flags,
 struct user_namespace *user_ns, struct uts_namespace *old_ns)
{
 struct uts_namespace *new_ns;

 BUG_ON(!old_ns);
 get_uts_ns(old_ns);

 if (!(flags & CLONE_NEWUTS))
  return old_ns;

 new_ns = clone_uts_ns(user_ns, old_ns);

 put_uts_ns(old_ns);
 return new_ns;
}

void free_uts_ns(struct kref *kref)
{
 struct uts_namespace *ns;

 ns = container_of(kref, struct uts_namespace, kref);
 put_user_ns(ns->user_ns);
 ns_free_inum(&ns->ns);
 kfree(ns);
}

static inline struct uts_namespace *to_uts_ns(struct ns_common *ns)
{
 return container_of(ns, struct uts_namespace, ns);
}

static struct ns_common *utsns_get(struct task_struct *task)
{
 struct uts_namespace *ns = NULL;
 struct nsproxy *nsproxy;

 task_lock(task);
 nsproxy = task->nsproxy;
 if (nsproxy) {
  ns = nsproxy->uts_ns;
  get_uts_ns(ns);
 }
 task_unlock(task);

 return ns ? &ns->ns : NULL;
}

static void utsns_put(struct ns_common *ns)
{
 put_uts_ns(to_uts_ns(ns));
}

static int utsns_install(struct nsproxy *nsproxy, struct ns_common *new)
{
 struct uts_namespace *ns = to_uts_ns(new);

 if (!ns_capable(ns->user_ns, CAP_SYS_ADMIN) ||
     !ns_capable(current_user_ns(), CAP_SYS_ADMIN))
  return -EPERM;

 get_uts_ns(ns);
 put_uts_ns(nsproxy->uts_ns);
 nsproxy->uts_ns = ns;
 return 0;
}

const struct proc_ns_operations utsns_operations = {
 .name = "uts",
 .type = CLONE_NEWUTS,
 .get = utsns_get,
 .put = utsns_put,
 .install = utsns_install,
};


static void *get_uts(struct ctl_table *table, int write)
{
 char *which = table->data;
 struct uts_namespace *uts_ns;

 uts_ns = current->nsproxy->uts_ns;
 which = (which - (char *)&init_uts_ns) + (char *)uts_ns;

 if (!write)
  down_read(&uts_sem);
 else
  down_write(&uts_sem);
 return which;
}

static void put_uts(struct ctl_table *table, int write, void *which)
{
 if (!write)
  up_read(&uts_sem);
 else
  up_write(&uts_sem);
}





static int proc_do_uts_string(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos)
{
 struct ctl_table uts_table;
 int r;
 memcpy(&uts_table, table, sizeof(uts_table));
 uts_table.data = get_uts(table, write);
 r = proc_dostring(&uts_table, write, buffer, lenp, ppos);
 put_uts(table, write, uts_table.data);

 if (write)
  proc_sys_poll_notify(table->poll);

 return r;
}

static DEFINE_CTL_TABLE_POLL(hostname_poll);
static DEFINE_CTL_TABLE_POLL(domainname_poll);

static struct ctl_table uts_kern_table[] = {
 {
  .procname = "ostype",
  .data = init_uts_ns.name.sysname,
  .maxlen = sizeof(init_uts_ns.name.sysname),
  .mode = 0444,
  .proc_handler = proc_do_uts_string,
 },
 {
  .procname = "osrelease",
  .data = init_uts_ns.name.release,
  .maxlen = sizeof(init_uts_ns.name.release),
  .mode = 0444,
  .proc_handler = proc_do_uts_string,
 },
 {
  .procname = "version",
  .data = init_uts_ns.name.version,
  .maxlen = sizeof(init_uts_ns.name.version),
  .mode = 0444,
  .proc_handler = proc_do_uts_string,
 },
 {
  .procname = "hostname",
  .data = init_uts_ns.name.nodename,
  .maxlen = sizeof(init_uts_ns.name.nodename),
  .mode = 0644,
  .proc_handler = proc_do_uts_string,
  .poll = &hostname_poll,
 },
 {
  .procname = "domainname",
  .data = init_uts_ns.name.domainname,
  .maxlen = sizeof(init_uts_ns.name.domainname),
  .mode = 0644,
  .proc_handler = proc_do_uts_string,
  .poll = &domainname_poll,
 },
 {}
};

static struct ctl_table uts_root_table[] = {
 {
  .procname = "kernel",
  .mode = 0555,
  .child = uts_kern_table,
 },
 {}
};





void uts_proc_notify(enum uts_proc proc)
{
 struct ctl_table *table = &uts_kern_table[proc];

 proc_sys_poll_notify(table->poll);
}

static int __init utsname_sysctl_init(void)
{
 register_sysctl_table(uts_root_table);
 return 0;
}

device_initcall(utsname_sysctl_init);
struct reg_state {
 enum bpf_reg_type type;
 union {

  s64 imm;


  struct {
   u32 id;
   u16 off;
   u16 range;
  };




  struct bpf_map *map_ptr;
 };
};

enum bpf_stack_slot_type {
 STACK_INVALID,
 STACK_SPILL,
 STACK_MISC
};





struct verifier_state {
 struct reg_state regs[MAX_BPF_REG];
 u8 stack_slot_type[MAX_BPF_STACK];
 struct reg_state spilled_regs[MAX_BPF_STACK / BPF_REG_SIZE];
};


struct verifier_state_list {
 struct verifier_state state;
 struct verifier_state_list *next;
};


struct verifier_stack_elem {




 struct verifier_state st;
 int insn_idx;
 int prev_insn_idx;
 struct verifier_stack_elem *next;
};





struct verifier_env {
 struct bpf_prog *prog;
 struct verifier_stack_elem *head;
 int stack_size;
 struct verifier_state cur_state;
 struct verifier_state_list **explored_states;
 struct bpf_map *used_maps[MAX_USED_MAPS];
 u32 used_map_cnt;
 bool allow_ptr_leaks;
};


struct bpf_call_arg_meta {
 struct bpf_map *map_ptr;
 bool raw_mode;
 int regno;
 int access_size;
};




static u32 log_level, log_size, log_len;
static char *log_buf;

static DEFINE_MUTEX(bpf_verifier_lock);





static __printf(1, 2) void verbose(const char *fmt, ...)
{
 va_list args;

 if (log_level == 0 || log_len >= log_size - 1)
  return;

 va_start(args, fmt);
 log_len += vscnprintf(log_buf + log_len, log_size - log_len, fmt, args);
 va_end(args);
}


static const char * const reg_type_str[] = {
 [NOT_INIT] = "?",
 [UNKNOWN_VALUE] = "inv",
 [PTR_TO_CTX] = "ctx",
 [CONST_PTR_TO_MAP] = "map_ptr",
 [PTR_TO_MAP_VALUE] = "map_value",
 [PTR_TO_MAP_VALUE_OR_NULL] = "map_value_or_null",
 [FRAME_PTR] = "fp",
 [PTR_TO_STACK] = "fp",
 [CONST_IMM] = "imm",
 [PTR_TO_PACKET] = "pkt",
 [PTR_TO_PACKET_END] = "pkt_end",
};

static void print_verifier_state(struct verifier_state *state)
{
 struct reg_state *reg;
 enum bpf_reg_type t;
 int i;

 for (i = 0; i < MAX_BPF_REG; i++) {
  reg = &state->regs[i];
  t = reg->type;
  if (t == NOT_INIT)
   continue;
  verbose(" R%d=%s", i, reg_type_str[t]);
  if (t == CONST_IMM || t == PTR_TO_STACK)
   verbose("%lld", reg->imm);
  else if (t == PTR_TO_PACKET)
   verbose("(id=%d,off=%d,r=%d)",
    reg->id, reg->off, reg->range);
  else if (t == UNKNOWN_VALUE && reg->imm)
   verbose("%lld", reg->imm);
  else if (t == CONST_PTR_TO_MAP || t == PTR_TO_MAP_VALUE ||
    t == PTR_TO_MAP_VALUE_OR_NULL)
   verbose("(ks=%d,vs=%d)",
    reg->map_ptr->key_size,
    reg->map_ptr->value_size);
 }
 for (i = 0; i < MAX_BPF_STACK; i += BPF_REG_SIZE) {
  if (state->stack_slot_type[i] == STACK_SPILL)
   verbose(" fp%d=%s", -MAX_BPF_STACK + i,
    reg_type_str[state->spilled_regs[i / BPF_REG_SIZE].type]);
 }
 verbose("\n");
}

static const char *const bpf_class_string[] = {
 [BPF_LD] = "ld",
 [BPF_LDX] = "ldx",
 [BPF_ST] = "st",
 [BPF_STX] = "stx",
 [BPF_ALU] = "alu",
 [BPF_JMP] = "jmp",
 [BPF_RET] = "BUG",
 [BPF_ALU64] = "alu64",
};

static const char *const bpf_alu_string[16] = {
 [BPF_ADD >> 4] = "+=",
 [BPF_SUB >> 4] = "-=",
 [BPF_MUL >> 4] = "*=",
 [BPF_DIV >> 4] = "/=",
 [BPF_OR >> 4] = "|=",
 [BPF_AND >> 4] = "&=",
 [BPF_LSH >> 4] = "<<=",
 [BPF_RSH >> 4] = ">>=",
 [BPF_NEG >> 4] = "neg",
 [BPF_MOD >> 4] = "%=",
 [BPF_XOR >> 4] = "^=",
 [BPF_MOV >> 4] = "=",
 [BPF_ARSH >> 4] = "s>>=",
 [BPF_END >> 4] = "endian",
};

static const char *const bpf_ldst_string[] = {
 [BPF_W >> 3] = "u32",
 [BPF_H >> 3] = "u16",
 [BPF_B >> 3] = "u8",
 [BPF_DW >> 3] = "u64",
};

static const char *const bpf_jmp_string[16] = {
 [BPF_JA >> 4] = "jmp",
 [BPF_JEQ >> 4] = "==",
 [BPF_JGT >> 4] = ">",
 [BPF_JGE >> 4] = ">=",
 [BPF_JSET >> 4] = "&",
 [BPF_JNE >> 4] = "!=",
 [BPF_JSGT >> 4] = "s>",
 [BPF_JSGE >> 4] = "s>=",
 [BPF_CALL >> 4] = "call",
 [BPF_EXIT >> 4] = "exit",
};

static void print_bpf_insn(struct bpf_insn *insn)
{
 u8 class = BPF_CLASS(insn->code);

 if (class == BPF_ALU || class == BPF_ALU64) {
  if (BPF_SRC(insn->code) == BPF_X)
   verbose("(%02x) %sr%d %s %sr%d\n",
    insn->code, class == BPF_ALU ? "(u32) " : "",
    insn->dst_reg,
    bpf_alu_string[BPF_OP(insn->code) >> 4],
    class == BPF_ALU ? "(u32) " : "",
    insn->src_reg);
  else
   verbose("(%02x) %sr%d %s %s%d\n",
    insn->code, class == BPF_ALU ? "(u32) " : "",
    insn->dst_reg,
    bpf_alu_string[BPF_OP(insn->code) >> 4],
    class == BPF_ALU ? "(u32) " : "",
    insn->imm);
 } else if (class == BPF_STX) {
  if (BPF_MODE(insn->code) == BPF_MEM)
   verbose("(%02x) *(%s *)(r%d %+d) = r%d\n",
    insn->code,
    bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
    insn->dst_reg,
    insn->off, insn->src_reg);
  else if (BPF_MODE(insn->code) == BPF_XADD)
   verbose("(%02x) lock *(%s *)(r%d %+d) += r%d\n",
    insn->code,
    bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
    insn->dst_reg, insn->off,
    insn->src_reg);
  else
   verbose("BUG_%02x\n", insn->code);
 } else if (class == BPF_ST) {
  if (BPF_MODE(insn->code) != BPF_MEM) {
   verbose("BUG_st_%02x\n", insn->code);
   return;
  }
  verbose("(%02x) *(%s *)(r%d %+d) = %d\n",
   insn->code,
   bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
   insn->dst_reg,
   insn->off, insn->imm);
 } else if (class == BPF_LDX) {
  if (BPF_MODE(insn->code) != BPF_MEM) {
   verbose("BUG_ldx_%02x\n", insn->code);
   return;
  }
  verbose("(%02x) r%d = *(%s *)(r%d %+d)\n",
   insn->code, insn->dst_reg,
   bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
   insn->src_reg, insn->off);
 } else if (class == BPF_LD) {
  if (BPF_MODE(insn->code) == BPF_ABS) {
   verbose("(%02x) r0 = *(%s *)skb[%d]\n",
    insn->code,
    bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
    insn->imm);
  } else if (BPF_MODE(insn->code) == BPF_IND) {
   verbose("(%02x) r0 = *(%s *)skb[r%d + %d]\n",
    insn->code,
    bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
    insn->src_reg, insn->imm);
  } else if (BPF_MODE(insn->code) == BPF_IMM) {
   verbose("(%02x) r%d = 0x%x\n",
    insn->code, insn->dst_reg, insn->imm);
  } else {
   verbose("BUG_ld_%02x\n", insn->code);
   return;
  }
 } else if (class == BPF_JMP) {
  u8 opcode = BPF_OP(insn->code);

  if (opcode == BPF_CALL) {
   verbose("(%02x) call %d\n", insn->code, insn->imm);
  } else if (insn->code == (BPF_JMP | BPF_JA)) {
   verbose("(%02x) goto pc%+d\n",
    insn->code, insn->off);
  } else if (insn->code == (BPF_JMP | BPF_EXIT)) {
   verbose("(%02x) exit\n", insn->code);
  } else if (BPF_SRC(insn->code) == BPF_X) {
   verbose("(%02x) if r%d %s r%d goto pc%+d\n",
    insn->code, insn->dst_reg,
    bpf_jmp_string[BPF_OP(insn->code) >> 4],
    insn->src_reg, insn->off);
  } else {
   verbose("(%02x) if r%d %s 0x%x goto pc%+d\n",
    insn->code, insn->dst_reg,
    bpf_jmp_string[BPF_OP(insn->code) >> 4],
    insn->imm, insn->off);
  }
 } else {
  verbose("(%02x) %s\n", insn->code, bpf_class_string[class]);
 }
}

static int pop_stack(struct verifier_env *env, int *prev_insn_idx)
{
 struct verifier_stack_elem *elem;
 int insn_idx;

 if (env->head == NULL)
  return -1;

 memcpy(&env->cur_state, &env->head->st, sizeof(env->cur_state));
 insn_idx = env->head->insn_idx;
 if (prev_insn_idx)
  *prev_insn_idx = env->head->prev_insn_idx;
 elem = env->head->next;
 kfree(env->head);
 env->head = elem;
 env->stack_size--;
 return insn_idx;
}

static struct verifier_state *push_stack(struct verifier_env *env, int insn_idx,
      int prev_insn_idx)
{
 struct verifier_stack_elem *elem;

 elem = kmalloc(sizeof(struct verifier_stack_elem), GFP_KERNEL);
 if (!elem)
  goto err;

 memcpy(&elem->st, &env->cur_state, sizeof(env->cur_state));
 elem->insn_idx = insn_idx;
 elem->prev_insn_idx = prev_insn_idx;
 elem->next = env->head;
 env->head = elem;
 env->stack_size++;
 if (env->stack_size > BPF_COMPLEXITY_LIMIT_STACK) {
  verbose("BPF program is too complex\n");
  goto err;
 }
 return &elem->st;
err:

 while (pop_stack(env, NULL) >= 0);
 return NULL;
}

static const int caller_saved[CALLER_SAVED_REGS] = {
 BPF_REG_0, BPF_REG_1, BPF_REG_2, BPF_REG_3, BPF_REG_4, BPF_REG_5
};

static void init_reg_state(struct reg_state *regs)
{
 int i;

 for (i = 0; i < MAX_BPF_REG; i++) {
  regs[i].type = NOT_INIT;
  regs[i].imm = 0;
 }


 regs[BPF_REG_FP].type = FRAME_PTR;


 regs[BPF_REG_1].type = PTR_TO_CTX;
}

static void mark_reg_unknown_value(struct reg_state *regs, u32 regno)
{
 BUG_ON(regno >= MAX_BPF_REG);
 regs[regno].type = UNKNOWN_VALUE;
 regs[regno].imm = 0;
}

enum reg_arg_type {
 SRC_OP,
 DST_OP,
 DST_OP_NO_MARK
};

static int check_reg_arg(struct reg_state *regs, u32 regno,
    enum reg_arg_type t)
{
 if (regno >= MAX_BPF_REG) {
  verbose("R%d is invalid\n", regno);
  return -EINVAL;
 }

 if (t == SRC_OP) {

  if (regs[regno].type == NOT_INIT) {
   verbose("R%d !read_ok\n", regno);
   return -EACCES;
  }
 } else {

  if (regno == BPF_REG_FP) {
   verbose("frame pointer is read only\n");
   return -EACCES;
  }
  if (t == DST_OP)
   mark_reg_unknown_value(regs, regno);
 }
 return 0;
}

static int bpf_size_to_bytes(int bpf_size)
{
 if (bpf_size == BPF_W)
  return 4;
 else if (bpf_size == BPF_H)
  return 2;
 else if (bpf_size == BPF_B)
  return 1;
 else if (bpf_size == BPF_DW)
  return 8;
 else
  return -EINVAL;
}

static bool is_spillable_regtype(enum bpf_reg_type type)
{
 switch (type) {
 case PTR_TO_MAP_VALUE:
 case PTR_TO_MAP_VALUE_OR_NULL:
 case PTR_TO_STACK:
 case PTR_TO_CTX:
 case PTR_TO_PACKET:
 case PTR_TO_PACKET_END:
 case FRAME_PTR:
 case CONST_PTR_TO_MAP:
  return true;
 default:
  return false;
 }
}




static int check_stack_write(struct verifier_state *state, int off, int size,
        int value_regno)
{
 int i;




 if (value_regno >= 0 &&
     is_spillable_regtype(state->regs[value_regno].type)) {


  if (size != BPF_REG_SIZE) {
   verbose("invalid size of register spill\n");
   return -EACCES;
  }


  state->spilled_regs[(MAX_BPF_STACK + off) / BPF_REG_SIZE] =
   state->regs[value_regno];

  for (i = 0; i < BPF_REG_SIZE; i++)
   state->stack_slot_type[MAX_BPF_STACK + off + i] = STACK_SPILL;
 } else {

  state->spilled_regs[(MAX_BPF_STACK + off) / BPF_REG_SIZE] =
   (struct reg_state) {};

  for (i = 0; i < size; i++)
   state->stack_slot_type[MAX_BPF_STACK + off + i] = STACK_MISC;
 }
 return 0;
}

static int check_stack_read(struct verifier_state *state, int off, int size,
       int value_regno)
{
 u8 *slot_type;
 int i;

 slot_type = &state->stack_slot_type[MAX_BPF_STACK + off];

 if (slot_type[0] == STACK_SPILL) {
  if (size != BPF_REG_SIZE) {
   verbose("invalid size of register spill\n");
   return -EACCES;
  }
  for (i = 1; i < BPF_REG_SIZE; i++) {
   if (slot_type[i] != STACK_SPILL) {
    verbose("corrupted spill memory\n");
    return -EACCES;
   }
  }

  if (value_regno >= 0)

   state->regs[value_regno] =
    state->spilled_regs[(MAX_BPF_STACK + off) / BPF_REG_SIZE];
  return 0;
 } else {
  for (i = 0; i < size; i++) {
   if (slot_type[i] != STACK_MISC) {
    verbose("invalid read from stack off %d+%d size %d\n",
     off, i, size);
    return -EACCES;
   }
  }
  if (value_regno >= 0)

   mark_reg_unknown_value(state->regs, value_regno);
  return 0;
 }
}


static int check_map_access(struct verifier_env *env, u32 regno, int off,
       int size)
{
 struct bpf_map *map = env->cur_state.regs[regno].map_ptr;

 if (off < 0 || off + size > map->value_size) {
  verbose("invalid access to map value, value_size=%d off=%d size=%d\n",
   map->value_size, off, size);
  return -EACCES;
 }
 return 0;
}


static int check_packet_access(struct verifier_env *env, u32 regno, int off,
          int size)
{
 struct reg_state *regs = env->cur_state.regs;
 struct reg_state *reg = &regs[regno];

 off += reg->off;
 if (off < 0 || off + size > reg->range) {
  verbose("invalid access to packet, off=%d size=%d, R%d(id=%d,off=%d,r=%d)\n",
   off, size, regno, reg->id, reg->off, reg->range);
  return -EACCES;
 }
 return 0;
}


static int check_ctx_access(struct verifier_env *env, int off, int size,
       enum bpf_access_type t, enum bpf_reg_type *reg_type)
{
 if (env->prog->aux->ops->is_valid_access &&
     env->prog->aux->ops->is_valid_access(off, size, t, reg_type)) {

  if (env->prog->aux->max_ctx_offset < off + size)
   env->prog->aux->max_ctx_offset = off + size;
  return 0;
 }

 verbose("invalid bpf_context access off=%d size=%d\n", off, size);
 return -EACCES;
}

static bool is_pointer_value(struct verifier_env *env, int regno)
{
 if (env->allow_ptr_leaks)
  return false;

 switch (env->cur_state.regs[regno].type) {
 case UNKNOWN_VALUE:
 case CONST_IMM:
  return false;
 default:
  return true;
 }
}

static int check_ptr_alignment(struct verifier_env *env, struct reg_state *reg,
          int off, int size)
{
 if (reg->type != PTR_TO_PACKET) {
  if (off % size != 0) {
   verbose("misaligned access off %d size %d\n", off, size);
   return -EACCES;
  } else {
   return 0;
  }
 }

 switch (env->prog->type) {
 case BPF_PROG_TYPE_SCHED_CLS:
 case BPF_PROG_TYPE_SCHED_ACT:
  break;
 default:
  verbose("verifier is misconfigured\n");
  return -EACCES;
 }

 if (IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))

  return 0;

 if (reg->id && size != 1) {
  verbose("Unknown packet alignment. Only byte-sized access allowed\n");
  return -EACCES;
 }


 if ((NET_IP_ALIGN + reg->off + off) % size != 0) {
  verbose("misaligned packet access off %d+%d+%d size %d\n",
   NET_IP_ALIGN, reg->off, off, size);
  return -EACCES;
 }
 return 0;
}







static int check_mem_access(struct verifier_env *env, u32 regno, int off,
       int bpf_size, enum bpf_access_type t,
       int value_regno)
{
 struct verifier_state *state = &env->cur_state;
 struct reg_state *reg = &state->regs[regno];
 int size, err = 0;

 if (reg->type == PTR_TO_STACK)
  off += reg->imm;

 size = bpf_size_to_bytes(bpf_size);
 if (size < 0)
  return size;

 err = check_ptr_alignment(env, reg, off, size);
 if (err)
  return err;

 if (reg->type == PTR_TO_MAP_VALUE) {
  if (t == BPF_WRITE && value_regno >= 0 &&
      is_pointer_value(env, value_regno)) {
   verbose("R%d leaks addr into map\n", value_regno);
   return -EACCES;
  }
  err = check_map_access(env, regno, off, size);
  if (!err && t == BPF_READ && value_regno >= 0)
   mark_reg_unknown_value(state->regs, value_regno);

 } else if (reg->type == PTR_TO_CTX) {
  enum bpf_reg_type reg_type = UNKNOWN_VALUE;

  if (t == BPF_WRITE && value_regno >= 0 &&
      is_pointer_value(env, value_regno)) {
   verbose("R%d leaks addr into ctx\n", value_regno);
   return -EACCES;
  }
  err = check_ctx_access(env, off, size, t, &reg_type);
  if (!err && t == BPF_READ && value_regno >= 0) {
   mark_reg_unknown_value(state->regs, value_regno);
   if (env->allow_ptr_leaks)

    state->regs[value_regno].type = reg_type;
  }

 } else if (reg->type == FRAME_PTR || reg->type == PTR_TO_STACK) {
  if (off >= 0 || off < -MAX_BPF_STACK) {
   verbose("invalid stack off=%d size=%d\n", off, size);
   return -EACCES;
  }
  if (t == BPF_WRITE) {
   if (!env->allow_ptr_leaks &&
       state->stack_slot_type[MAX_BPF_STACK + off] == STACK_SPILL &&
       size != BPF_REG_SIZE) {
    verbose("attempt to corrupt spilled pointer on stack\n");
    return -EACCES;
   }
   err = check_stack_write(state, off, size, value_regno);
  } else {
   err = check_stack_read(state, off, size, value_regno);
  }
 } else if (state->regs[regno].type == PTR_TO_PACKET) {
  if (t == BPF_WRITE) {
   verbose("cannot write into packet\n");
   return -EACCES;
  }
  err = check_packet_access(env, regno, off, size);
  if (!err && t == BPF_READ && value_regno >= 0)
   mark_reg_unknown_value(state->regs, value_regno);
 } else {
  verbose("R%d invalid mem access '%s'\n",
   regno, reg_type_str[reg->type]);
  return -EACCES;
 }

 if (!err && size <= 2 && value_regno >= 0 && env->allow_ptr_leaks &&
     state->regs[value_regno].type == UNKNOWN_VALUE) {




  state->regs[value_regno].imm = 64 - size * 8;
 }
 return err;
}

static int check_xadd(struct verifier_env *env, struct bpf_insn *insn)
{
 struct reg_state *regs = env->cur_state.regs;
 int err;

 if ((BPF_SIZE(insn->code) != BPF_W && BPF_SIZE(insn->code) != BPF_DW) ||
     insn->imm != 0) {
  verbose("BPF_XADD uses reserved fields\n");
  return -EINVAL;
 }


 err = check_reg_arg(regs, insn->src_reg, SRC_OP);
 if (err)
  return err;


 err = check_reg_arg(regs, insn->dst_reg, SRC_OP);
 if (err)
  return err;


 err = check_mem_access(env, insn->dst_reg, insn->off,
          BPF_SIZE(insn->code), BPF_READ, -1);
 if (err)
  return err;


 return check_mem_access(env, insn->dst_reg, insn->off,
    BPF_SIZE(insn->code), BPF_WRITE, -1);
}





static int check_stack_boundary(struct verifier_env *env, int regno,
    int access_size, bool zero_size_allowed,
    struct bpf_call_arg_meta *meta)
{
 struct verifier_state *state = &env->cur_state;
 struct reg_state *regs = state->regs;
 int off, i;

 if (regs[regno].type != PTR_TO_STACK) {
  if (zero_size_allowed && access_size == 0 &&
      regs[regno].type == CONST_IMM &&
      regs[regno].imm == 0)
   return 0;

  verbose("R%d type=%s expected=%s\n", regno,
   reg_type_str[regs[regno].type],
   reg_type_str[PTR_TO_STACK]);
  return -EACCES;
 }

 off = regs[regno].imm;
 if (off >= 0 || off < -MAX_BPF_STACK || off + access_size > 0 ||
     access_size <= 0) {
  verbose("invalid stack type R%d off=%d access_size=%d\n",
   regno, off, access_size);
  return -EACCES;
 }

 if (meta && meta->raw_mode) {
  meta->access_size = access_size;
  meta->regno = regno;
  return 0;
 }

 for (i = 0; i < access_size; i++) {
  if (state->stack_slot_type[MAX_BPF_STACK + off + i] != STACK_MISC) {
   verbose("invalid indirect read from stack off %d+%d size %d\n",
    off, i, access_size);
   return -EACCES;
  }
 }
 return 0;
}

static int check_func_arg(struct verifier_env *env, u32 regno,
     enum bpf_arg_type arg_type,
     struct bpf_call_arg_meta *meta)
{
 struct reg_state *reg = env->cur_state.regs + regno;
 enum bpf_reg_type expected_type;
 int err = 0;

 if (arg_type == ARG_DONTCARE)
  return 0;

 if (reg->type == NOT_INIT) {
  verbose("R%d !read_ok\n", regno);
  return -EACCES;
 }

 if (arg_type == ARG_ANYTHING) {
  if (is_pointer_value(env, regno)) {
   verbose("R%d leaks addr into helper function\n", regno);
   return -EACCES;
  }
  return 0;
 }

 if (arg_type == ARG_PTR_TO_MAP_KEY ||
     arg_type == ARG_PTR_TO_MAP_VALUE) {
  expected_type = PTR_TO_STACK;
 } else if (arg_type == ARG_CONST_STACK_SIZE ||
     arg_type == ARG_CONST_STACK_SIZE_OR_ZERO) {
  expected_type = CONST_IMM;
 } else if (arg_type == ARG_CONST_MAP_PTR) {
  expected_type = CONST_PTR_TO_MAP;
 } else if (arg_type == ARG_PTR_TO_CTX) {
  expected_type = PTR_TO_CTX;
 } else if (arg_type == ARG_PTR_TO_STACK ||
     arg_type == ARG_PTR_TO_RAW_STACK) {
  expected_type = PTR_TO_STACK;




  if (reg->type == CONST_IMM && reg->imm == 0)
   expected_type = CONST_IMM;
  meta->raw_mode = arg_type == ARG_PTR_TO_RAW_STACK;
 } else {
  verbose("unsupported arg_type %d\n", arg_type);
  return -EFAULT;
 }

 if (reg->type != expected_type) {
  verbose("R%d type=%s expected=%s\n", regno,
   reg_type_str[reg->type], reg_type_str[expected_type]);
  return -EACCES;
 }

 if (arg_type == ARG_CONST_MAP_PTR) {

  meta->map_ptr = reg->map_ptr;
 } else if (arg_type == ARG_PTR_TO_MAP_KEY) {




  if (!meta->map_ptr) {





   verbose("invalid map_ptr to access map->key\n");
   return -EACCES;
  }
  err = check_stack_boundary(env, regno, meta->map_ptr->key_size,
        false, NULL);
 } else if (arg_type == ARG_PTR_TO_MAP_VALUE) {



  if (!meta->map_ptr) {

   verbose("invalid map_ptr to access map->value\n");
   return -EACCES;
  }
  err = check_stack_boundary(env, regno,
        meta->map_ptr->value_size,
        false, NULL);
 } else if (arg_type == ARG_CONST_STACK_SIZE ||
     arg_type == ARG_CONST_STACK_SIZE_OR_ZERO) {
  bool zero_size_allowed = (arg_type == ARG_CONST_STACK_SIZE_OR_ZERO);





  if (regno == 0) {

   verbose("ARG_CONST_STACK_SIZE cannot be first argument\n");
   return -EACCES;
  }
  err = check_stack_boundary(env, regno - 1, reg->imm,
        zero_size_allowed, meta);
 }

 return err;
}

static int check_map_func_compatibility(struct bpf_map *map, int func_id)
{
 if (!map)
  return 0;


 switch (map->map_type) {
 case BPF_MAP_TYPE_PROG_ARRAY:
  if (func_id != BPF_FUNC_tail_call)
   goto error;
  break;
 case BPF_MAP_TYPE_PERF_EVENT_ARRAY:
  if (func_id != BPF_FUNC_perf_event_read &&
      func_id != BPF_FUNC_perf_event_output)
   goto error;
  break;
 case BPF_MAP_TYPE_STACK_TRACE:
  if (func_id != BPF_FUNC_get_stackid)
   goto error;
  break;
 default:
  break;
 }


 switch (func_id) {
 case BPF_FUNC_tail_call:
  if (map->map_type != BPF_MAP_TYPE_PROG_ARRAY)
   goto error;
  break;
 case BPF_FUNC_perf_event_read:
 case BPF_FUNC_perf_event_output:
  if (map->map_type != BPF_MAP_TYPE_PERF_EVENT_ARRAY)
   goto error;
  break;
 case BPF_FUNC_get_stackid:
  if (map->map_type != BPF_MAP_TYPE_STACK_TRACE)
   goto error;
  break;
 default:
  break;
 }

 return 0;
error:
 verbose("cannot pass map_type %d into func %d\n",
  map->map_type, func_id);
 return -EINVAL;
}

static int check_raw_mode(const struct bpf_func_proto *fn)
{
 int count = 0;

 if (fn->arg1_type == ARG_PTR_TO_RAW_STACK)
  count++;
 if (fn->arg2_type == ARG_PTR_TO_RAW_STACK)
  count++;
 if (fn->arg3_type == ARG_PTR_TO_RAW_STACK)
  count++;
 if (fn->arg4_type == ARG_PTR_TO_RAW_STACK)
  count++;
 if (fn->arg5_type == ARG_PTR_TO_RAW_STACK)
  count++;

 return count > 1 ? -EINVAL : 0;
}

static void clear_all_pkt_pointers(struct verifier_env *env)
{
 struct verifier_state *state = &env->cur_state;
 struct reg_state *regs = state->regs, *reg;
 int i;

 for (i = 0; i < MAX_BPF_REG; i++)
  if (regs[i].type == PTR_TO_PACKET ||
      regs[i].type == PTR_TO_PACKET_END)
   mark_reg_unknown_value(regs, i);

 for (i = 0; i < MAX_BPF_STACK; i += BPF_REG_SIZE) {
  if (state->stack_slot_type[i] != STACK_SPILL)
   continue;
  reg = &state->spilled_regs[i / BPF_REG_SIZE];
  if (reg->type != PTR_TO_PACKET &&
      reg->type != PTR_TO_PACKET_END)
   continue;
  reg->type = UNKNOWN_VALUE;
  reg->imm = 0;
 }
}

static int check_call(struct verifier_env *env, int func_id)
{
 struct verifier_state *state = &env->cur_state;
 const struct bpf_func_proto *fn = NULL;
 struct reg_state *regs = state->regs;
 struct reg_state *reg;
 struct bpf_call_arg_meta meta;
 bool changes_data;
 int i, err;


 if (func_id < 0 || func_id >= __BPF_FUNC_MAX_ID) {
  verbose("invalid func %d\n", func_id);
  return -EINVAL;
 }

 if (env->prog->aux->ops->get_func_proto)
  fn = env->prog->aux->ops->get_func_proto(func_id);

 if (!fn) {
  verbose("unknown func %d\n", func_id);
  return -EINVAL;
 }


 if (!env->prog->gpl_compatible && fn->gpl_only) {
  verbose("cannot call GPL only function from proprietary program\n");
  return -EINVAL;
 }

 changes_data = bpf_helper_changes_skb_data(fn->func);

 memset(&meta, 0, sizeof(meta));




 err = check_raw_mode(fn);
 if (err) {
  verbose("kernel subsystem misconfigured func %d\n", func_id);
  return err;
 }


 err = check_func_arg(env, BPF_REG_1, fn->arg1_type, &meta);
 if (err)
  return err;
 err = check_func_arg(env, BPF_REG_2, fn->arg2_type, &meta);
 if (err)
  return err;
 err = check_func_arg(env, BPF_REG_3, fn->arg3_type, &meta);
 if (err)
  return err;
 err = check_func_arg(env, BPF_REG_4, fn->arg4_type, &meta);
 if (err)
  return err;
 err = check_func_arg(env, BPF_REG_5, fn->arg5_type, &meta);
 if (err)
  return err;




 for (i = 0; i < meta.access_size; i++) {
  err = check_mem_access(env, meta.regno, i, BPF_B, BPF_WRITE, -1);
  if (err)
   return err;
 }


 for (i = 0; i < CALLER_SAVED_REGS; i++) {
  reg = regs + caller_saved[i];
  reg->type = NOT_INIT;
  reg->imm = 0;
 }


 if (fn->ret_type == RET_INTEGER) {
  regs[BPF_REG_0].type = UNKNOWN_VALUE;
 } else if (fn->ret_type == RET_VOID) {
  regs[BPF_REG_0].type = NOT_INIT;
 } else if (fn->ret_type == RET_PTR_TO_MAP_VALUE_OR_NULL) {
  regs[BPF_REG_0].type = PTR_TO_MAP_VALUE_OR_NULL;




  if (meta.map_ptr == NULL) {
   verbose("kernel subsystem misconfigured verifier\n");
   return -EINVAL;
  }
  regs[BPF_REG_0].map_ptr = meta.map_ptr;
 } else {
  verbose("unknown return type %d of func %d\n",
   fn->ret_type, func_id);
  return -EINVAL;
 }

 err = check_map_func_compatibility(meta.map_ptr, func_id);
 if (err)
  return err;

 if (changes_data)
  clear_all_pkt_pointers(env);
 return 0;
}

static int check_packet_ptr_add(struct verifier_env *env, struct bpf_insn *insn)
{
 struct reg_state *regs = env->cur_state.regs;
 struct reg_state *dst_reg = &regs[insn->dst_reg];
 struct reg_state *src_reg = &regs[insn->src_reg];
 struct reg_state tmp_reg;
 s32 imm;

 if (BPF_SRC(insn->code) == BPF_K) {

  imm = insn->imm;

add_imm:
  if (imm <= 0) {
   verbose("addition of negative constant to packet pointer is not allowed\n");
   return -EACCES;
  }
  if (imm >= MAX_PACKET_OFF ||
      imm + dst_reg->off >= MAX_PACKET_OFF) {
   verbose("constant %d is too large to add to packet pointer\n",
    imm);
   return -EACCES;
  }



  dst_reg->off += imm;
 } else {
  if (src_reg->type == PTR_TO_PACKET) {

   tmp_reg = *dst_reg;
   *dst_reg = *src_reg;
   src_reg = &tmp_reg;






  }

  if (src_reg->type == CONST_IMM) {

   imm = src_reg->imm;
   goto add_imm;
  }





  if (src_reg->type != UNKNOWN_VALUE) {
   verbose("cannot add '%s' to ptr_to_packet\n",
    reg_type_str[src_reg->type]);
   return -EACCES;
  }
  if (src_reg->imm < 48) {
   verbose("cannot add integer value with %lld upper zero bits to ptr_to_packet\n",
    src_reg->imm);
   return -EACCES;
  }



  dst_reg->id++;


  dst_reg->off = 0;
  dst_reg->range = 0;
 }
 return 0;
}

static int evaluate_reg_alu(struct verifier_env *env, struct bpf_insn *insn)
{
 struct reg_state *regs = env->cur_state.regs;
 struct reg_state *dst_reg = &regs[insn->dst_reg];
 u8 opcode = BPF_OP(insn->code);
 s64 imm_log2;






 if (BPF_SRC(insn->code) == BPF_X) {
  struct reg_state *src_reg = &regs[insn->src_reg];

  if (src_reg->type == UNKNOWN_VALUE && src_reg->imm > 0 &&
      dst_reg->imm && opcode == BPF_ADD) {







   dst_reg->imm = min(dst_reg->imm, src_reg->imm);
   dst_reg->imm--;
   return 0;
  }
  if (src_reg->type == CONST_IMM && src_reg->imm > 0 &&
      dst_reg->imm && opcode == BPF_ADD) {





   imm_log2 = __ilog2_u64((long long)src_reg->imm);
   dst_reg->imm = min(dst_reg->imm, 63 - imm_log2);
   dst_reg->imm--;
   return 0;
  }

  dst_reg->imm = 0;
  return 0;
 }





 imm_log2 = __ilog2_u64((long long)insn->imm);

 if (dst_reg->imm && opcode == BPF_LSH) {





  dst_reg->imm -= insn->imm;
 } else if (dst_reg->imm && opcode == BPF_MUL) {





  dst_reg->imm -= imm_log2 + 1;
 } else if (opcode == BPF_AND) {

  dst_reg->imm = 63 - imm_log2;
 } else if (dst_reg->imm && opcode == BPF_ADD) {

  dst_reg->imm = min(dst_reg->imm, 63 - imm_log2);
  dst_reg->imm--;
 } else if (opcode == BPF_RSH) {





  dst_reg->imm += insn->imm;
  if (unlikely(dst_reg->imm > 64))




   dst_reg->imm = 64;
 } else {



  dst_reg->imm = 0;
 }

 if (dst_reg->imm < 0) {




  dst_reg->imm = 0;
 }
 return 0;
}

static int evaluate_reg_imm_alu(struct verifier_env *env, struct bpf_insn *insn)
{
 struct reg_state *regs = env->cur_state.regs;
 struct reg_state *dst_reg = &regs[insn->dst_reg];
 struct reg_state *src_reg = &regs[insn->src_reg];
 u8 opcode = BPF_OP(insn->code);




 if (opcode == BPF_ADD && BPF_SRC(insn->code) == BPF_K)
  dst_reg->imm += insn->imm;
 else if (opcode == BPF_ADD && BPF_SRC(insn->code) == BPF_X &&
   src_reg->type == CONST_IMM)
  dst_reg->imm += src_reg->imm;
 else
  mark_reg_unknown_value(regs, insn->dst_reg);
 return 0;
}


static int check_alu_op(struct verifier_env *env, struct bpf_insn *insn)
{
 struct reg_state *regs = env->cur_state.regs, *dst_reg;
 u8 opcode = BPF_OP(insn->code);
 int err;

 if (opcode == BPF_END || opcode == BPF_NEG) {
  if (opcode == BPF_NEG) {
   if (BPF_SRC(insn->code) != 0 ||
       insn->src_reg != BPF_REG_0 ||
       insn->off != 0 || insn->imm != 0) {
    verbose("BPF_NEG uses reserved fields\n");
    return -EINVAL;
   }
  } else {
   if (insn->src_reg != BPF_REG_0 || insn->off != 0 ||
       (insn->imm != 16 && insn->imm != 32 && insn->imm != 64)) {
    verbose("BPF_END uses reserved fields\n");
    return -EINVAL;
   }
  }


  err = check_reg_arg(regs, insn->dst_reg, SRC_OP);
  if (err)
   return err;

  if (is_pointer_value(env, insn->dst_reg)) {
   verbose("R%d pointer arithmetic prohibited\n",
    insn->dst_reg);
   return -EACCES;
  }


  err = check_reg_arg(regs, insn->dst_reg, DST_OP);
  if (err)
   return err;

 } else if (opcode == BPF_MOV) {

  if (BPF_SRC(insn->code) == BPF_X) {
   if (insn->imm != 0 || insn->off != 0) {
    verbose("BPF_MOV uses reserved fields\n");
    return -EINVAL;
   }


   err = check_reg_arg(regs, insn->src_reg, SRC_OP);
   if (err)
    return err;
  } else {
   if (insn->src_reg != BPF_REG_0 || insn->off != 0) {
    verbose("BPF_MOV uses reserved fields\n");
    return -EINVAL;
   }
  }


  err = check_reg_arg(regs, insn->dst_reg, DST_OP);
  if (err)
   return err;

  if (BPF_SRC(insn->code) == BPF_X) {
   if (BPF_CLASS(insn->code) == BPF_ALU64) {



    regs[insn->dst_reg] = regs[insn->src_reg];
   } else {
    if (is_pointer_value(env, insn->src_reg)) {
     verbose("R%d partial copy of pointer\n",
      insn->src_reg);
     return -EACCES;
    }
    regs[insn->dst_reg].type = UNKNOWN_VALUE;
    regs[insn->dst_reg].map_ptr = NULL;
   }
  } else {



   regs[insn->dst_reg].type = CONST_IMM;
   regs[insn->dst_reg].imm = insn->imm;
  }

 } else if (opcode > BPF_END) {
  verbose("invalid BPF_ALU opcode %x\n", opcode);
  return -EINVAL;

 } else {

  if (BPF_SRC(insn->code) == BPF_X) {
   if (insn->imm != 0 || insn->off != 0) {
    verbose("BPF_ALU uses reserved fields\n");
    return -EINVAL;
   }

   err = check_reg_arg(regs, insn->src_reg, SRC_OP);
   if (err)
    return err;
  } else {
   if (insn->src_reg != BPF_REG_0 || insn->off != 0) {
    verbose("BPF_ALU uses reserved fields\n");
    return -EINVAL;
   }
  }


  err = check_reg_arg(regs, insn->dst_reg, SRC_OP);
  if (err)
   return err;

  if ((opcode == BPF_MOD || opcode == BPF_DIV) &&
      BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {
   verbose("div by zero\n");
   return -EINVAL;
  }

  if ((opcode == BPF_LSH || opcode == BPF_RSH ||
       opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {
   int size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;

   if (insn->imm < 0 || insn->imm >= size) {
    verbose("invalid shift %d\n", insn->imm);
    return -EINVAL;
   }
  }


  err = check_reg_arg(regs, insn->dst_reg, DST_OP_NO_MARK);
  if (err)
   return err;

  dst_reg = &regs[insn->dst_reg];


  if (opcode == BPF_ADD && BPF_CLASS(insn->code) == BPF_ALU64 &&
      dst_reg->type == FRAME_PTR && BPF_SRC(insn->code) == BPF_K) {
   dst_reg->type = PTR_TO_STACK;
   dst_reg->imm = insn->imm;
   return 0;
  } else if (opcode == BPF_ADD &&
      BPF_CLASS(insn->code) == BPF_ALU64 &&
      (dst_reg->type == PTR_TO_PACKET ||
       (BPF_SRC(insn->code) == BPF_X &&
        regs[insn->src_reg].type == PTR_TO_PACKET))) {

   return check_packet_ptr_add(env, insn);
  } else if (BPF_CLASS(insn->code) == BPF_ALU64 &&
      dst_reg->type == UNKNOWN_VALUE &&
      env->allow_ptr_leaks) {

   return evaluate_reg_alu(env, insn);
  } else if (BPF_CLASS(insn->code) == BPF_ALU64 &&
      dst_reg->type == CONST_IMM &&
      env->allow_ptr_leaks) {

   return evaluate_reg_imm_alu(env, insn);
  } else if (is_pointer_value(env, insn->dst_reg)) {
   verbose("R%d pointer arithmetic prohibited\n",
    insn->dst_reg);
   return -EACCES;
  } else if (BPF_SRC(insn->code) == BPF_X &&
      is_pointer_value(env, insn->src_reg)) {
   verbose("R%d pointer arithmetic prohibited\n",
    insn->src_reg);
   return -EACCES;
  }


  mark_reg_unknown_value(regs, insn->dst_reg);
 }

 return 0;
}

static void find_good_pkt_pointers(struct verifier_env *env,
       struct reg_state *dst_reg)
{
 struct verifier_state *state = &env->cur_state;
 struct reg_state *regs = state->regs, *reg;
 int i;
 for (i = 0; i < MAX_BPF_REG; i++)
  if (regs[i].type == PTR_TO_PACKET && regs[i].id == dst_reg->id)
   regs[i].range = dst_reg->off;

 for (i = 0; i < MAX_BPF_STACK; i += BPF_REG_SIZE) {
  if (state->stack_slot_type[i] != STACK_SPILL)
   continue;
  reg = &state->spilled_regs[i / BPF_REG_SIZE];
  if (reg->type == PTR_TO_PACKET && reg->id == dst_reg->id)
   reg->range = dst_reg->off;
 }
}

static int check_cond_jmp_op(struct verifier_env *env,
        struct bpf_insn *insn, int *insn_idx)
{
 struct reg_state *regs = env->cur_state.regs, *dst_reg;
 struct verifier_state *other_branch;
 u8 opcode = BPF_OP(insn->code);
 int err;

 if (opcode > BPF_EXIT) {
  verbose("invalid BPF_JMP opcode %x\n", opcode);
  return -EINVAL;
 }

 if (BPF_SRC(insn->code) == BPF_X) {
  if (insn->imm != 0) {
   verbose("BPF_JMP uses reserved fields\n");
   return -EINVAL;
  }


  err = check_reg_arg(regs, insn->src_reg, SRC_OP);
  if (err)
   return err;

  if (is_pointer_value(env, insn->src_reg)) {
   verbose("R%d pointer comparison prohibited\n",
    insn->src_reg);
   return -EACCES;
  }
 } else {
  if (insn->src_reg != BPF_REG_0) {
   verbose("BPF_JMP uses reserved fields\n");
   return -EINVAL;
  }
 }


 err = check_reg_arg(regs, insn->dst_reg, SRC_OP);
 if (err)
  return err;

 dst_reg = &regs[insn->dst_reg];


 if (BPF_SRC(insn->code) == BPF_K &&
     (opcode == BPF_JEQ || opcode == BPF_JNE) &&
     dst_reg->type == CONST_IMM && dst_reg->imm == insn->imm) {
  if (opcode == BPF_JEQ) {



   *insn_idx += insn->off;
   return 0;
  } else {




   return 0;
  }
 }

 other_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx);
 if (!other_branch)
  return -EFAULT;


 if (BPF_SRC(insn->code) == BPF_K &&
     insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&
     dst_reg->type == PTR_TO_MAP_VALUE_OR_NULL) {
  if (opcode == BPF_JEQ) {



   regs[insn->dst_reg].type = PTR_TO_MAP_VALUE;

   mark_reg_unknown_value(other_branch->regs,
            insn->dst_reg);
  } else {
   other_branch->regs[insn->dst_reg].type = PTR_TO_MAP_VALUE;
   mark_reg_unknown_value(regs, insn->dst_reg);
  }
 } else if (BPF_SRC(insn->code) == BPF_X && opcode == BPF_JGT &&
     dst_reg->type == PTR_TO_PACKET &&
     regs[insn->src_reg].type == PTR_TO_PACKET_END) {
  find_good_pkt_pointers(env, dst_reg);
 } else if (is_pointer_value(env, insn->dst_reg)) {
  verbose("R%d pointer comparison prohibited\n", insn->dst_reg);
  return -EACCES;
 }
 if (log_level)
  print_verifier_state(&env->cur_state);
 return 0;
}


static struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)
{
 u64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;

 return (struct bpf_map *) (unsigned long) imm64;
}


static int check_ld_imm(struct verifier_env *env, struct bpf_insn *insn)
{
 struct reg_state *regs = env->cur_state.regs;
 int err;

 if (BPF_SIZE(insn->code) != BPF_DW) {
  verbose("invalid BPF_LD_IMM insn\n");
  return -EINVAL;
 }
 if (insn->off != 0) {
  verbose("BPF_LD_IMM64 uses reserved fields\n");
  return -EINVAL;
 }

 err = check_reg_arg(regs, insn->dst_reg, DST_OP);
 if (err)
  return err;

 if (insn->src_reg == 0)

  return 0;


 BUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);

 regs[insn->dst_reg].type = CONST_PTR_TO_MAP;
 regs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);
 return 0;
}

static bool may_access_skb(enum bpf_prog_type type)
{
 switch (type) {
 case BPF_PROG_TYPE_SOCKET_FILTER:
 case BPF_PROG_TYPE_SCHED_CLS:
 case BPF_PROG_TYPE_SCHED_ACT:
  return true;
 default:
  return false;
 }
}
static int check_ld_abs(struct verifier_env *env, struct bpf_insn *insn)
{
 struct reg_state *regs = env->cur_state.regs;
 u8 mode = BPF_MODE(insn->code);
 struct reg_state *reg;
 int i, err;

 if (!may_access_skb(env->prog->type)) {
  verbose("BPF_LD_[ABS|IND] instructions not allowed for this program type\n");
  return -EINVAL;
 }

 if (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||
     BPF_SIZE(insn->code) == BPF_DW ||
     (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {
  verbose("BPF_LD_[ABS|IND] uses reserved fields\n");
  return -EINVAL;
 }


 err = check_reg_arg(regs, BPF_REG_6, SRC_OP);
 if (err)
  return err;

 if (regs[BPF_REG_6].type != PTR_TO_CTX) {
  verbose("at the time of BPF_LD_ABS|IND R6 != pointer to skb\n");
  return -EINVAL;
 }

 if (mode == BPF_IND) {

  err = check_reg_arg(regs, insn->src_reg, SRC_OP);
  if (err)
   return err;
 }


 for (i = 0; i < CALLER_SAVED_REGS; i++) {
  reg = regs + caller_saved[i];
  reg->type = NOT_INIT;
  reg->imm = 0;
 }




 regs[BPF_REG_0].type = UNKNOWN_VALUE;
 return 0;
}
enum {
 DISCOVERED = 0x10,
 EXPLORED = 0x20,
 FALLTHROUGH = 1,
 BRANCH = 2,
};


static int *insn_stack;
static int cur_stack;
static int *insn_state;






static int push_insn(int t, int w, int e, struct verifier_env *env)
{
 if (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))
  return 0;

 if (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))
  return 0;

 if (w < 0 || w >= env->prog->len) {
  verbose("jump out of range from insn %d to %d\n", t, w);
  return -EINVAL;
 }

 if (e == BRANCH)

  env->explored_states[w] = STATE_LIST_MARK;

 if (insn_state[w] == 0) {

  insn_state[t] = DISCOVERED | e;
  insn_state[w] = DISCOVERED;
  if (cur_stack >= env->prog->len)
   return -E2BIG;
  insn_stack[cur_stack++] = w;
  return 1;
 } else if ((insn_state[w] & 0xF0) == DISCOVERED) {
  verbose("back-edge from insn %d to %d\n", t, w);
  return -EINVAL;
 } else if (insn_state[w] == EXPLORED) {

  insn_state[t] = DISCOVERED | e;
 } else {
  verbose("insn state internal bug\n");
  return -EFAULT;
 }
 return 0;
}




static int check_cfg(struct verifier_env *env)
{
 struct bpf_insn *insns = env->prog->insnsi;
 int insn_cnt = env->prog->len;
 int ret = 0;
 int i, t;

 insn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);
 if (!insn_state)
  return -ENOMEM;

 insn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);
 if (!insn_stack) {
  kfree(insn_state);
  return -ENOMEM;
 }

 insn_state[0] = DISCOVERED;
 insn_stack[0] = 0;
 cur_stack = 1;

peek_stack:
 if (cur_stack == 0)
  goto check_state;
 t = insn_stack[cur_stack - 1];

 if (BPF_CLASS(insns[t].code) == BPF_JMP) {
  u8 opcode = BPF_OP(insns[t].code);

  if (opcode == BPF_EXIT) {
   goto mark_explored;
  } else if (opcode == BPF_CALL) {
   ret = push_insn(t, t + 1, FALLTHROUGH, env);
   if (ret == 1)
    goto peek_stack;
   else if (ret < 0)
    goto err_free;
   if (t + 1 < insn_cnt)
    env->explored_states[t + 1] = STATE_LIST_MARK;
  } else if (opcode == BPF_JA) {
   if (BPF_SRC(insns[t].code) != BPF_K) {
    ret = -EINVAL;
    goto err_free;
   }

   ret = push_insn(t, t + insns[t].off + 1,
     FALLTHROUGH, env);
   if (ret == 1)
    goto peek_stack;
   else if (ret < 0)
    goto err_free;



   if (t + 1 < insn_cnt)
    env->explored_states[t + 1] = STATE_LIST_MARK;
  } else {

   ret = push_insn(t, t + 1, FALLTHROUGH, env);
   if (ret == 1)
    goto peek_stack;
   else if (ret < 0)
    goto err_free;

   ret = push_insn(t, t + insns[t].off + 1, BRANCH, env);
   if (ret == 1)
    goto peek_stack;
   else if (ret < 0)
    goto err_free;
  }
 } else {



  ret = push_insn(t, t + 1, FALLTHROUGH, env);
  if (ret == 1)
   goto peek_stack;
  else if (ret < 0)
   goto err_free;
 }

mark_explored:
 insn_state[t] = EXPLORED;
 if (cur_stack-- <= 0) {
  verbose("pop stack internal bug\n");
  ret = -EFAULT;
  goto err_free;
 }
 goto peek_stack;

check_state:
 for (i = 0; i < insn_cnt; i++) {
  if (insn_state[i] != EXPLORED) {
   verbose("unreachable insn %d\n", i);
   ret = -EINVAL;
   goto err_free;
  }
 }
 ret = 0;

err_free:
 kfree(insn_state);
 kfree(insn_stack);
 return ret;
}




static bool compare_ptrs_to_packet(struct reg_state *old, struct reg_state *cur)
{
 if (old->id != cur->id)
  return false;
 if (old->off == cur->off && old->range < cur->range)
  return true;
 if (old->off <= cur->off &&
     old->off >= old->range && cur->off >= cur->range)
  return true;

 return false;
}
static bool states_equal(struct verifier_state *old, struct verifier_state *cur)
{
 struct reg_state *rold, *rcur;
 int i;

 for (i = 0; i < MAX_BPF_REG; i++) {
  rold = &old->regs[i];
  rcur = &cur->regs[i];

  if (memcmp(rold, rcur, sizeof(*rold)) == 0)
   continue;

  if (rold->type == NOT_INIT ||
      (rold->type == UNKNOWN_VALUE && rcur->type != NOT_INIT))
   continue;

  if (rold->type == PTR_TO_PACKET && rcur->type == PTR_TO_PACKET &&
      compare_ptrs_to_packet(rold, rcur))
   continue;

  return false;
 }

 for (i = 0; i < MAX_BPF_STACK; i++) {
  if (old->stack_slot_type[i] == STACK_INVALID)
   continue;
  if (old->stack_slot_type[i] != cur->stack_slot_type[i])





   return false;
  if (i % BPF_REG_SIZE)
   continue;
  if (memcmp(&old->spilled_regs[i / BPF_REG_SIZE],
      &cur->spilled_regs[i / BPF_REG_SIZE],
      sizeof(old->spilled_regs[0])))
   return false;
  else
   continue;
 }
 return true;
}

static int is_state_visited(struct verifier_env *env, int insn_idx)
{
 struct verifier_state_list *new_sl;
 struct verifier_state_list *sl;

 sl = env->explored_states[insn_idx];
 if (!sl)



  return 0;

 while (sl != STATE_LIST_MARK) {
  if (states_equal(&sl->state, &env->cur_state))



   return 1;
  sl = sl->next;
 }







 new_sl = kmalloc(sizeof(struct verifier_state_list), GFP_USER);
 if (!new_sl)
  return -ENOMEM;


 memcpy(&new_sl->state, &env->cur_state, sizeof(env->cur_state));
 new_sl->next = env->explored_states[insn_idx];
 env->explored_states[insn_idx] = new_sl;
 return 0;
}

static int do_check(struct verifier_env *env)
{
 struct verifier_state *state = &env->cur_state;
 struct bpf_insn *insns = env->prog->insnsi;
 struct reg_state *regs = state->regs;
 int insn_cnt = env->prog->len;
 int insn_idx, prev_insn_idx = 0;
 int insn_processed = 0;
 bool do_print_state = false;

 init_reg_state(regs);
 insn_idx = 0;
 for (;;) {
  struct bpf_insn *insn;
  u8 class;
  int err;

  if (insn_idx >= insn_cnt) {
   verbose("invalid insn idx %d insn_cnt %d\n",
    insn_idx, insn_cnt);
   return -EFAULT;
  }

  insn = &insns[insn_idx];
  class = BPF_CLASS(insn->code);

  if (++insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {
   verbose("BPF program is too large. Proccessed %d insn\n",
    insn_processed);
   return -E2BIG;
  }

  err = is_state_visited(env, insn_idx);
  if (err < 0)
   return err;
  if (err == 1) {

   if (log_level) {
    if (do_print_state)
     verbose("\nfrom %d to %d: safe\n",
      prev_insn_idx, insn_idx);
    else
     verbose("%d: safe\n", insn_idx);
   }
   goto process_bpf_exit;
  }

  if (log_level && do_print_state) {
   verbose("\nfrom %d to %d:", prev_insn_idx, insn_idx);
   print_verifier_state(&env->cur_state);
   do_print_state = false;
  }

  if (log_level) {
   verbose("%d: ", insn_idx);
   print_bpf_insn(insn);
  }

  if (class == BPF_ALU || class == BPF_ALU64) {
   err = check_alu_op(env, insn);
   if (err)
    return err;

  } else if (class == BPF_LDX) {
   enum bpf_reg_type src_reg_type;




   err = check_reg_arg(regs, insn->src_reg, SRC_OP);
   if (err)
    return err;

   err = check_reg_arg(regs, insn->dst_reg, DST_OP_NO_MARK);
   if (err)
    return err;

   src_reg_type = regs[insn->src_reg].type;




   err = check_mem_access(env, insn->src_reg, insn->off,
            BPF_SIZE(insn->code), BPF_READ,
            insn->dst_reg);
   if (err)
    return err;

   if (BPF_SIZE(insn->code) != BPF_W) {
    insn_idx++;
    continue;
   }

   if (insn->imm == 0) {




    insn->imm = src_reg_type;

   } else if (src_reg_type != insn->imm &&
       (src_reg_type == PTR_TO_CTX ||
        insn->imm == PTR_TO_CTX)) {







    verbose("same insn cannot be used with different pointers\n");
    return -EINVAL;
   }

  } else if (class == BPF_STX) {
   enum bpf_reg_type dst_reg_type;

   if (BPF_MODE(insn->code) == BPF_XADD) {
    err = check_xadd(env, insn);
    if (err)
     return err;
    insn_idx++;
    continue;
   }


   err = check_reg_arg(regs, insn->src_reg, SRC_OP);
   if (err)
    return err;

   err = check_reg_arg(regs, insn->dst_reg, SRC_OP);
   if (err)
    return err;

   dst_reg_type = regs[insn->dst_reg].type;


   err = check_mem_access(env, insn->dst_reg, insn->off,
            BPF_SIZE(insn->code), BPF_WRITE,
            insn->src_reg);
   if (err)
    return err;

   if (insn->imm == 0) {
    insn->imm = dst_reg_type;
   } else if (dst_reg_type != insn->imm &&
       (dst_reg_type == PTR_TO_CTX ||
        insn->imm == PTR_TO_CTX)) {
    verbose("same insn cannot be used with different pointers\n");
    return -EINVAL;
   }

  } else if (class == BPF_ST) {
   if (BPF_MODE(insn->code) != BPF_MEM ||
       insn->src_reg != BPF_REG_0) {
    verbose("BPF_ST uses reserved fields\n");
    return -EINVAL;
   }

   err = check_reg_arg(regs, insn->dst_reg, SRC_OP);
   if (err)
    return err;


   err = check_mem_access(env, insn->dst_reg, insn->off,
            BPF_SIZE(insn->code), BPF_WRITE,
            -1);
   if (err)
    return err;

  } else if (class == BPF_JMP) {
   u8 opcode = BPF_OP(insn->code);

   if (opcode == BPF_CALL) {
    if (BPF_SRC(insn->code) != BPF_K ||
        insn->off != 0 ||
        insn->src_reg != BPF_REG_0 ||
        insn->dst_reg != BPF_REG_0) {
     verbose("BPF_CALL uses reserved fields\n");
     return -EINVAL;
    }

    err = check_call(env, insn->imm);
    if (err)
     return err;

   } else if (opcode == BPF_JA) {
    if (BPF_SRC(insn->code) != BPF_K ||
        insn->imm != 0 ||
        insn->src_reg != BPF_REG_0 ||
        insn->dst_reg != BPF_REG_0) {
     verbose("BPF_JA uses reserved fields\n");
     return -EINVAL;
    }

    insn_idx += insn->off + 1;
    continue;

   } else if (opcode == BPF_EXIT) {
    if (BPF_SRC(insn->code) != BPF_K ||
        insn->imm != 0 ||
        insn->src_reg != BPF_REG_0 ||
        insn->dst_reg != BPF_REG_0) {
     verbose("BPF_EXIT uses reserved fields\n");
     return -EINVAL;
    }







    err = check_reg_arg(regs, BPF_REG_0, SRC_OP);
    if (err)
     return err;

    if (is_pointer_value(env, BPF_REG_0)) {
     verbose("R0 leaks addr as return value\n");
     return -EACCES;
    }

process_bpf_exit:
    insn_idx = pop_stack(env, &prev_insn_idx);
    if (insn_idx < 0) {
     break;
    } else {
     do_print_state = true;
     continue;
    }
   } else {
    err = check_cond_jmp_op(env, insn, &insn_idx);
    if (err)
     return err;
   }
  } else if (class == BPF_LD) {
   u8 mode = BPF_MODE(insn->code);

   if (mode == BPF_ABS || mode == BPF_IND) {
    err = check_ld_abs(env, insn);
    if (err)
     return err;

   } else if (mode == BPF_IMM) {
    err = check_ld_imm(env, insn);
    if (err)
     return err;

    insn_idx++;
   } else {
    verbose("invalid BPF_LD mode\n");
    return -EINVAL;
   }
  } else {
   verbose("unknown insn class %d\n", class);
   return -EINVAL;
  }

  insn_idx++;
 }

 verbose("processed %d insns\n", insn_processed);
 return 0;
}




static int replace_map_fd_with_map_ptr(struct verifier_env *env)
{
 struct bpf_insn *insn = env->prog->insnsi;
 int insn_cnt = env->prog->len;
 int i, j;

 for (i = 0; i < insn_cnt; i++, insn++) {
  if (BPF_CLASS(insn->code) == BPF_LDX &&
      (BPF_MODE(insn->code) != BPF_MEM || insn->imm != 0)) {
   verbose("BPF_LDX uses reserved fields\n");
   return -EINVAL;
  }

  if (BPF_CLASS(insn->code) == BPF_STX &&
      ((BPF_MODE(insn->code) != BPF_MEM &&
        BPF_MODE(insn->code) != BPF_XADD) || insn->imm != 0)) {
   verbose("BPF_STX uses reserved fields\n");
   return -EINVAL;
  }

  if (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW)) {
   struct bpf_map *map;
   struct fd f;

   if (i == insn_cnt - 1 || insn[1].code != 0 ||
       insn[1].dst_reg != 0 || insn[1].src_reg != 0 ||
       insn[1].off != 0) {
    verbose("invalid bpf_ld_imm64 insn\n");
    return -EINVAL;
   }

   if (insn->src_reg == 0)

    goto next_insn;

   if (insn->src_reg != BPF_PSEUDO_MAP_FD) {
    verbose("unrecognized bpf_ld_imm64 insn\n");
    return -EINVAL;
   }

   f = fdget(insn->imm);
   map = __bpf_map_get(f);
   if (IS_ERR(map)) {
    verbose("fd %d is not pointing to valid bpf_map\n",
     insn->imm);
    return PTR_ERR(map);
   }


   insn[0].imm = (u32) (unsigned long) map;
   insn[1].imm = ((u64) (unsigned long) map) >> 32;


   for (j = 0; j < env->used_map_cnt; j++)
    if (env->used_maps[j] == map) {
     fdput(f);
     goto next_insn;
    }

   if (env->used_map_cnt >= MAX_USED_MAPS) {
    fdput(f);
    return -E2BIG;
   }






   map = bpf_map_inc(map, false);
   if (IS_ERR(map)) {
    fdput(f);
    return PTR_ERR(map);
   }
   env->used_maps[env->used_map_cnt++] = map;

   fdput(f);
next_insn:
   insn++;
   i++;
  }
 }





 return 0;
}


static void release_maps(struct verifier_env *env)
{
 int i;

 for (i = 0; i < env->used_map_cnt; i++)
  bpf_map_put(env->used_maps[i]);
}


static void convert_pseudo_ld_imm64(struct verifier_env *env)
{
 struct bpf_insn *insn = env->prog->insnsi;
 int insn_cnt = env->prog->len;
 int i;

 for (i = 0; i < insn_cnt; i++, insn++)
  if (insn->code == (BPF_LD | BPF_IMM | BPF_DW))
   insn->src_reg = 0;
}




static int convert_ctx_accesses(struct verifier_env *env)
{
 struct bpf_insn *insn = env->prog->insnsi;
 int insn_cnt = env->prog->len;
 struct bpf_insn insn_buf[16];
 struct bpf_prog *new_prog;
 enum bpf_access_type type;
 int i;

 if (!env->prog->aux->ops->convert_ctx_access)
  return 0;

 for (i = 0; i < insn_cnt; i++, insn++) {
  u32 insn_delta, cnt;

  if (insn->code == (BPF_LDX | BPF_MEM | BPF_W))
   type = BPF_READ;
  else if (insn->code == (BPF_STX | BPF_MEM | BPF_W))
   type = BPF_WRITE;
  else
   continue;

  if (insn->imm != PTR_TO_CTX) {

   insn->imm = 0;
   continue;
  }

  cnt = env->prog->aux->ops->
   convert_ctx_access(type, insn->dst_reg, insn->src_reg,
        insn->off, insn_buf, env->prog);
  if (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {
   verbose("bpf verifier is misconfigured\n");
   return -EINVAL;
  }

  new_prog = bpf_patch_insn_single(env->prog, i, insn_buf, cnt);
  if (!new_prog)
   return -ENOMEM;

  insn_delta = cnt - 1;


  env->prog = new_prog;
  insn = new_prog->insnsi + i + insn_delta;

  insn_cnt += insn_delta;
  i += insn_delta;
 }

 return 0;
}

static void free_states(struct verifier_env *env)
{
 struct verifier_state_list *sl, *sln;
 int i;

 if (!env->explored_states)
  return;

 for (i = 0; i < env->prog->len; i++) {
  sl = env->explored_states[i];

  if (sl)
   while (sl != STATE_LIST_MARK) {
    sln = sl->next;
    kfree(sl);
    sl = sln;
   }
 }

 kfree(env->explored_states);
}

int bpf_check(struct bpf_prog **prog, union bpf_attr *attr)
{
 char __user *log_ubuf = NULL;
 struct verifier_env *env;
 int ret = -EINVAL;

 if ((*prog)->len <= 0 || (*prog)->len > BPF_MAXINSNS)
  return -E2BIG;




 env = kzalloc(sizeof(struct verifier_env), GFP_KERNEL);
 if (!env)
  return -ENOMEM;

 env->prog = *prog;


 mutex_lock(&bpf_verifier_lock);

 if (attr->log_level || attr->log_buf || attr->log_size) {



  log_level = attr->log_level;
  log_ubuf = (char __user *) (unsigned long) attr->log_buf;
  log_size = attr->log_size;
  log_len = 0;

  ret = -EINVAL;

  if (log_size < 128 || log_size > UINT_MAX >> 8 ||
      log_level == 0 || log_ubuf == NULL)
   goto free_env;

  ret = -ENOMEM;
  log_buf = vmalloc(log_size);
  if (!log_buf)
   goto free_env;
 } else {
  log_level = 0;
 }

 ret = replace_map_fd_with_map_ptr(env);
 if (ret < 0)
  goto skip_full_check;

 env->explored_states = kcalloc(env->prog->len,
           sizeof(struct verifier_state_list *),
           GFP_USER);
 ret = -ENOMEM;
 if (!env->explored_states)
  goto skip_full_check;

 ret = check_cfg(env);
 if (ret < 0)
  goto skip_full_check;

 env->allow_ptr_leaks = capable(CAP_SYS_ADMIN);

 ret = do_check(env);

skip_full_check:
 while (pop_stack(env, NULL) >= 0);
 free_states(env);

 if (ret == 0)

  ret = convert_ctx_accesses(env);

 if (log_level && log_len >= log_size - 1) {
  BUG_ON(log_len >= log_size);

  ret = -ENOSPC;

 }


 if (log_level && copy_to_user(log_ubuf, log_buf, log_len + 1) != 0) {
  ret = -EFAULT;
  goto free_log_buf;
 }

 if (ret == 0 && env->used_map_cnt) {

  env->prog->aux->used_maps = kmalloc_array(env->used_map_cnt,
         sizeof(env->used_maps[0]),
         GFP_KERNEL);

  if (!env->prog->aux->used_maps) {
   ret = -ENOMEM;
   goto free_log_buf;
  }

  memcpy(env->prog->aux->used_maps, env->used_maps,
         sizeof(env->used_maps[0]) * env->used_map_cnt);
  env->prog->aux->used_map_cnt = env->used_map_cnt;




  convert_pseudo_ld_imm64(env);
 }

free_log_buf:
 if (log_level)
  vfree(log_buf);
free_env:
 if (!env->prog->aux->used_maps)



  release_maps(env);
 *prog = env->prog;
 kfree(env);
 mutex_unlock(&bpf_verifier_lock);
 return ret;
}


static DEFINE_MUTEX(wakelocks_lock);

struct wakelock {
 char *name;
 struct rb_node node;
 struct wakeup_source ws;
 struct list_head lru;
};

static struct rb_root wakelocks_tree = RB_ROOT;

ssize_t pm_show_wakelocks(char *buf, bool show_active)
{
 struct rb_node *node;
 struct wakelock *wl;
 char *str = buf;
 char *end = buf + PAGE_SIZE;

 mutex_lock(&wakelocks_lock);

 for (node = rb_first(&wakelocks_tree); node; node = rb_next(node)) {
  wl = rb_entry(node, struct wakelock, node);
  if (wl->ws.active == show_active)
   str += scnprintf(str, end - str, "%s ", wl->name);
 }
 if (str > buf)
  str--;

 str += scnprintf(str, end - str, "\n");

 mutex_unlock(&wakelocks_lock);
 return (str - buf);
}

static unsigned int number_of_wakelocks;

static inline bool wakelocks_limit_exceeded(void)
{
 return number_of_wakelocks > CONFIG_PM_WAKELOCKS_LIMIT;
}

static inline void increment_wakelocks_number(void)
{
 number_of_wakelocks++;
}

static inline void decrement_wakelocks_number(void)
{
 number_of_wakelocks--;
}
static inline bool wakelocks_limit_exceeded(void) { return false; }
static inline void increment_wakelocks_number(void) {}
static inline void decrement_wakelocks_number(void) {}


static void __wakelocks_gc(struct work_struct *work);
static LIST_HEAD(wakelocks_lru_list);
static DECLARE_WORK(wakelock_work, __wakelocks_gc);
static unsigned int wakelocks_gc_count;

static inline void wakelocks_lru_add(struct wakelock *wl)
{
 list_add(&wl->lru, &wakelocks_lru_list);
}

static inline void wakelocks_lru_most_recent(struct wakelock *wl)
{
 list_move(&wl->lru, &wakelocks_lru_list);
}

static void __wakelocks_gc(struct work_struct *work)
{
 struct wakelock *wl, *aux;
 ktime_t now;

 mutex_lock(&wakelocks_lock);

 now = ktime_get();
 list_for_each_entry_safe_reverse(wl, aux, &wakelocks_lru_list, lru) {
  u64 idle_time_ns;
  bool active;

  spin_lock_irq(&wl->ws.lock);
  idle_time_ns = ktime_to_ns(ktime_sub(now, wl->ws.last_time));
  active = wl->ws.active;
  spin_unlock_irq(&wl->ws.lock);

  if (idle_time_ns < ((u64)WL_GC_TIME_SEC * NSEC_PER_SEC))
   break;

  if (!active) {
   wakeup_source_remove(&wl->ws);
   rb_erase(&wl->node, &wakelocks_tree);
   list_del(&wl->lru);
   kfree(wl->name);
   kfree(wl);
   decrement_wakelocks_number();
  }
 }
 wakelocks_gc_count = 0;

 mutex_unlock(&wakelocks_lock);
}

static void wakelocks_gc(void)
{
 if (++wakelocks_gc_count <= WL_GC_COUNT_MAX)
  return;

 schedule_work(&wakelock_work);
}
static inline void wakelocks_lru_add(struct wakelock *wl) {}
static inline void wakelocks_lru_most_recent(struct wakelock *wl) {}
static inline void wakelocks_gc(void) {}

static struct wakelock *wakelock_lookup_add(const char *name, size_t len,
         bool add_if_not_found)
{
 struct rb_node **node = &wakelocks_tree.rb_node;
 struct rb_node *parent = *node;
 struct wakelock *wl;

 while (*node) {
  int diff;

  parent = *node;
  wl = rb_entry(*node, struct wakelock, node);
  diff = strncmp(name, wl->name, len);
  if (diff == 0) {
   if (wl->name[len])
    diff = -1;
   else
    return wl;
  }
  if (diff < 0)
   node = &(*node)->rb_left;
  else
   node = &(*node)->rb_right;
 }
 if (!add_if_not_found)
  return ERR_PTR(-EINVAL);

 if (wakelocks_limit_exceeded())
  return ERR_PTR(-ENOSPC);


 wl = kzalloc(sizeof(*wl), GFP_KERNEL);
 if (!wl)
  return ERR_PTR(-ENOMEM);

 wl->name = kstrndup(name, len, GFP_KERNEL);
 if (!wl->name) {
  kfree(wl);
  return ERR_PTR(-ENOMEM);
 }
 wl->ws.name = wl->name;
 wakeup_source_add(&wl->ws);
 rb_link_node(&wl->node, parent, node);
 rb_insert_color(&wl->node, &wakelocks_tree);
 wakelocks_lru_add(wl);
 increment_wakelocks_number();
 return wl;
}

int pm_wake_lock(const char *buf)
{
 const char *str = buf;
 struct wakelock *wl;
 u64 timeout_ns = 0;
 size_t len;
 int ret = 0;

 if (!capable(CAP_BLOCK_SUSPEND))
  return -EPERM;

 while (*str && !isspace(*str))
  str++;

 len = str - buf;
 if (!len)
  return -EINVAL;

 if (*str && *str != '\n') {

  ret = kstrtou64(skip_spaces(str), 10, &timeout_ns);
  if (ret)
   return -EINVAL;
 }

 mutex_lock(&wakelocks_lock);

 wl = wakelock_lookup_add(buf, len, true);
 if (IS_ERR(wl)) {
  ret = PTR_ERR(wl);
  goto out;
 }
 if (timeout_ns) {
  u64 timeout_ms = timeout_ns + NSEC_PER_MSEC - 1;

  do_div(timeout_ms, NSEC_PER_MSEC);
  __pm_wakeup_event(&wl->ws, timeout_ms);
 } else {
  __pm_stay_awake(&wl->ws);
 }

 wakelocks_lru_most_recent(wl);

 out:
 mutex_unlock(&wakelocks_lock);
 return ret;
}

int pm_wake_unlock(const char *buf)
{
 struct wakelock *wl;
 size_t len;
 int ret = 0;

 if (!capable(CAP_BLOCK_SUSPEND))
  return -EPERM;

 len = strlen(buf);
 if (!len)
  return -EINVAL;

 if (buf[len-1] == '\n')
  len--;

 if (!len)
  return -EINVAL;

 mutex_lock(&wakelocks_lock);

 wl = wakelock_lookup_add(buf, len, false);
 if (IS_ERR(wl)) {
  ret = PTR_ERR(wl);
  goto out;
 }
 __pm_relax(&wl->ws);

 wakelocks_lru_most_recent(wl);
 wakelocks_gc();

 out:
 mutex_unlock(&wakelocks_lock);
 return ret;
}



static DEFINE_MUTEX(watchdog_proc_mutex);

static unsigned long __read_mostly watchdog_enabled = SOFT_WATCHDOG_ENABLED|NMI_WATCHDOG_ENABLED;
static unsigned long __read_mostly watchdog_enabled = SOFT_WATCHDOG_ENABLED;
int __read_mostly nmi_watchdog_enabled;
int __read_mostly soft_watchdog_enabled;
int __read_mostly watchdog_user_enabled;
int __read_mostly watchdog_thresh = 10;

int __read_mostly sysctl_softlockup_all_cpu_backtrace;
int __read_mostly sysctl_hardlockup_all_cpu_backtrace;
static struct cpumask watchdog_cpumask __read_mostly;
unsigned long *watchdog_cpumask_bits = cpumask_bits(&watchdog_cpumask);


 for_each_cpu_and((cpu), cpu_online_mask, &watchdog_cpumask)






static int __read_mostly watchdog_running;
static int __read_mostly watchdog_suspended;

static u64 __read_mostly sample_period;

static DEFINE_PER_CPU(unsigned long, watchdog_touch_ts);
static DEFINE_PER_CPU(struct task_struct *, softlockup_watchdog);
static DEFINE_PER_CPU(struct hrtimer, watchdog_hrtimer);
static DEFINE_PER_CPU(bool, softlockup_touch_sync);
static DEFINE_PER_CPU(bool, soft_watchdog_warn);
static DEFINE_PER_CPU(unsigned long, hrtimer_interrupts);
static DEFINE_PER_CPU(unsigned long, soft_lockup_hrtimer_cnt);
static DEFINE_PER_CPU(struct task_struct *, softlockup_task_ptr_saved);
static DEFINE_PER_CPU(bool, hard_watchdog_warn);
static DEFINE_PER_CPU(bool, watchdog_nmi_touch);
static DEFINE_PER_CPU(unsigned long, hrtimer_interrupts_saved);
static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
static unsigned long soft_lockup_nmi_warn;





unsigned int __read_mostly hardlockup_panic =
   CONFIG_BOOTPARAM_HARDLOCKUP_PANIC_VALUE;
static unsigned long hardlockup_allcpu_dumped;
void hardlockup_detector_disable(void)
{
 watchdog_enabled &= ~NMI_WATCHDOG_ENABLED;
}

static int __init hardlockup_panic_setup(char *str)
{
 if (!strncmp(str, "panic", 5))
  hardlockup_panic = 1;
 else if (!strncmp(str, "nopanic", 7))
  hardlockup_panic = 0;
 else if (!strncmp(str, "0", 1))
  watchdog_enabled &= ~NMI_WATCHDOG_ENABLED;
 else if (!strncmp(str, "1", 1))
  watchdog_enabled |= NMI_WATCHDOG_ENABLED;
 return 1;
}
__setup("nmi_watchdog=", hardlockup_panic_setup);

unsigned int __read_mostly softlockup_panic =
   CONFIG_BOOTPARAM_SOFTLOCKUP_PANIC_VALUE;

static int __init softlockup_panic_setup(char *str)
{
 softlockup_panic = simple_strtoul(str, NULL, 0);

 return 1;
}
__setup("softlockup_panic=", softlockup_panic_setup);

static int __init nowatchdog_setup(char *str)
{
 watchdog_enabled = 0;
 return 1;
}
__setup("nowatchdog", nowatchdog_setup);

static int __init nosoftlockup_setup(char *str)
{
 watchdog_enabled &= ~SOFT_WATCHDOG_ENABLED;
 return 1;
}
__setup("nosoftlockup", nosoftlockup_setup);

static int __init softlockup_all_cpu_backtrace_setup(char *str)
{
 sysctl_softlockup_all_cpu_backtrace =
  !!simple_strtol(str, NULL, 0);
 return 1;
}
__setup("softlockup_all_cpu_backtrace=", softlockup_all_cpu_backtrace_setup);
static int __init hardlockup_all_cpu_backtrace_setup(char *str)
{
 sysctl_hardlockup_all_cpu_backtrace =
  !!simple_strtol(str, NULL, 0);
 return 1;
}
__setup("hardlockup_all_cpu_backtrace=", hardlockup_all_cpu_backtrace_setup);
static int get_softlockup_thresh(void)
{
 return watchdog_thresh * 2;
}






static unsigned long get_timestamp(void)
{
 return running_clock() >> 30LL;
}

static void set_sample_period(void)
{







 sample_period = get_softlockup_thresh() * ((u64)NSEC_PER_SEC / 5);
}


static void __touch_watchdog(void)
{
 __this_cpu_write(watchdog_touch_ts, get_timestamp());
}
void touch_softlockup_watchdog_sched(void)
{




 raw_cpu_write(watchdog_touch_ts, 0);
}

void touch_softlockup_watchdog(void)
{
 touch_softlockup_watchdog_sched();
 wq_watchdog_touch(raw_smp_processor_id());
}
EXPORT_SYMBOL(touch_softlockup_watchdog);

void touch_all_softlockup_watchdogs(void)
{
 int cpu;






 for_each_watchdog_cpu(cpu)
  per_cpu(watchdog_touch_ts, cpu) = 0;
 wq_watchdog_touch(-1);
}

void touch_nmi_watchdog(void)
{







 raw_cpu_write(watchdog_nmi_touch, true);
 touch_softlockup_watchdog();
}
EXPORT_SYMBOL(touch_nmi_watchdog);


void touch_softlockup_watchdog_sync(void)
{
 __this_cpu_write(softlockup_touch_sync, true);
 __this_cpu_write(watchdog_touch_ts, 0);
}


static bool is_hardlockup(void)
{
 unsigned long hrint = __this_cpu_read(hrtimer_interrupts);

 if (__this_cpu_read(hrtimer_interrupts_saved) == hrint)
  return true;

 __this_cpu_write(hrtimer_interrupts_saved, hrint);
 return false;
}

static int is_softlockup(unsigned long touch_ts)
{
 unsigned long now = get_timestamp();

 if ((watchdog_enabled & SOFT_WATCHDOG_ENABLED) && watchdog_thresh){

  if (time_after(now, touch_ts + get_softlockup_thresh()))
   return now - touch_ts;
 }
 return 0;
}


static struct perf_event_attr wd_hw_attr = {
 .type = PERF_TYPE_HARDWARE,
 .config = PERF_COUNT_HW_CPU_CYCLES,
 .size = sizeof(struct perf_event_attr),
 .pinned = 1,
 .disabled = 1,
};


static void watchdog_overflow_callback(struct perf_event *event,
   struct perf_sample_data *data,
   struct pt_regs *regs)
{

 event->hw.interrupts = 0;

 if (__this_cpu_read(watchdog_nmi_touch) == true) {
  __this_cpu_write(watchdog_nmi_touch, false);
  return;
 }







 if (is_hardlockup()) {
  int this_cpu = smp_processor_id();
  struct pt_regs *regs = get_irq_regs();


  if (__this_cpu_read(hard_watchdog_warn) == true)
   return;

  pr_emerg("Watchdog detected hard LOCKUP on cpu %d", this_cpu);
  print_modules();
  print_irqtrace_events(current);
  if (regs)
   show_regs(regs);
  else
   dump_stack();





  if (sysctl_hardlockup_all_cpu_backtrace &&
    !test_and_set_bit(0, &hardlockup_allcpu_dumped))
   trigger_allbutself_cpu_backtrace();

  if (hardlockup_panic)
   nmi_panic(regs, "Hard LOCKUP");

  __this_cpu_write(hard_watchdog_warn, true);
  return;
 }

 __this_cpu_write(hard_watchdog_warn, false);
 return;
}

static void watchdog_interrupt_count(void)
{
 __this_cpu_inc(hrtimer_interrupts);
}

static int watchdog_nmi_enable(unsigned int cpu);
static void watchdog_nmi_disable(unsigned int cpu);

static int watchdog_enable_all_cpus(void);
static void watchdog_disable_all_cpus(void);


static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)
{
 unsigned long touch_ts = __this_cpu_read(watchdog_touch_ts);
 struct pt_regs *regs = get_irq_regs();
 int duration;
 int softlockup_all_cpu_backtrace = sysctl_softlockup_all_cpu_backtrace;


 watchdog_interrupt_count();


 wake_up_process(__this_cpu_read(softlockup_watchdog));


 hrtimer_forward_now(hrtimer, ns_to_ktime(sample_period));

 if (touch_ts == 0) {
  if (unlikely(__this_cpu_read(softlockup_touch_sync))) {




   __this_cpu_write(softlockup_touch_sync, false);
   sched_clock_tick();
  }


  kvm_check_and_clear_guest_paused();
  __touch_watchdog();
  return HRTIMER_RESTART;
 }







 duration = is_softlockup(touch_ts);
 if (unlikely(duration)) {





  if (kvm_check_and_clear_guest_paused())
   return HRTIMER_RESTART;


  if (__this_cpu_read(soft_watchdog_warn) == true) {
   if (__this_cpu_read(softlockup_task_ptr_saved) !=
       current) {
    __this_cpu_write(soft_watchdog_warn, false);
    __touch_watchdog();
   }
   return HRTIMER_RESTART;
  }

  if (softlockup_all_cpu_backtrace) {



   if (test_and_set_bit(0, &soft_lockup_nmi_warn)) {

    __this_cpu_write(soft_watchdog_warn, true);
    return HRTIMER_RESTART;
   }
  }

  pr_emerg("BUG: soft lockup - CPU#%d stuck for %us! [%s:%d]\n",
   smp_processor_id(), duration,
   current->comm, task_pid_nr(current));
  __this_cpu_write(softlockup_task_ptr_saved, current);
  print_modules();
  print_irqtrace_events(current);
  if (regs)
   show_regs(regs);
  else
   dump_stack();

  if (softlockup_all_cpu_backtrace) {



   trigger_allbutself_cpu_backtrace();

   clear_bit(0, &soft_lockup_nmi_warn);

   smp_mb__after_atomic();
  }

  add_taint(TAINT_SOFTLOCKUP, LOCKDEP_STILL_OK);
  if (softlockup_panic)
   panic("softlockup: hung tasks");
  __this_cpu_write(soft_watchdog_warn, true);
 } else
  __this_cpu_write(soft_watchdog_warn, false);

 return HRTIMER_RESTART;
}

static void watchdog_set_prio(unsigned int policy, unsigned int prio)
{
 struct sched_param param = { .sched_priority = prio };

 sched_setscheduler(current, policy, &param);
}

static void watchdog_enable(unsigned int cpu)
{
 struct hrtimer *hrtimer = raw_cpu_ptr(&watchdog_hrtimer);


 hrtimer_init(hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 hrtimer->function = watchdog_timer_fn;


 watchdog_nmi_enable(cpu);


 hrtimer_start(hrtimer, ns_to_ktime(sample_period),
        HRTIMER_MODE_REL_PINNED);


 watchdog_set_prio(SCHED_FIFO, MAX_RT_PRIO - 1);
 __touch_watchdog();
}

static void watchdog_disable(unsigned int cpu)
{
 struct hrtimer *hrtimer = raw_cpu_ptr(&watchdog_hrtimer);

 watchdog_set_prio(SCHED_NORMAL, 0);
 hrtimer_cancel(hrtimer);

 watchdog_nmi_disable(cpu);
}

static void watchdog_cleanup(unsigned int cpu, bool online)
{
 watchdog_disable(cpu);
}

static int watchdog_should_run(unsigned int cpu)
{
 return __this_cpu_read(hrtimer_interrupts) !=
  __this_cpu_read(soft_lockup_hrtimer_cnt);
}
static void watchdog(unsigned int cpu)
{
 __this_cpu_write(soft_lockup_hrtimer_cnt,
    __this_cpu_read(hrtimer_interrupts));
 __touch_watchdog();
 if (!(watchdog_enabled & NMI_WATCHDOG_ENABLED))
  watchdog_nmi_disable(cpu);
}






static unsigned long cpu0_err;

static int watchdog_nmi_enable(unsigned int cpu)
{
 struct perf_event_attr *wd_attr;
 struct perf_event *event = per_cpu(watchdog_ev, cpu);


 if (!(watchdog_enabled & NMI_WATCHDOG_ENABLED))
  goto out;


 if (event && event->state > PERF_EVENT_STATE_OFF)
  goto out;


 if (event != NULL)
  goto out_enable;

 wd_attr = &wd_hw_attr;
 wd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);


 event = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);


 if (cpu == 0 && IS_ERR(event))
  cpu0_err = PTR_ERR(event);

 if (!IS_ERR(event)) {

  if (cpu == 0 || cpu0_err)
   pr_info("enabled on all CPUs, permanently consumes one hw-PMU counter.\n");
  goto out_save;
 }
 smp_mb__before_atomic();
 clear_bit(NMI_WATCHDOG_ENABLED_BIT, &watchdog_enabled);
 smp_mb__after_atomic();


 if (cpu > 0 && (PTR_ERR(event) == cpu0_err))
  return PTR_ERR(event);


 if (PTR_ERR(event) == -EOPNOTSUPP)
  pr_info("disabled (cpu%i): not supported (no LAPIC?)\n", cpu);
 else if (PTR_ERR(event) == -ENOENT)
  pr_warn("disabled (cpu%i): hardware events not enabled\n",
    cpu);
 else
  pr_err("disabled (cpu%i): unable to create perf event: %ld\n",
   cpu, PTR_ERR(event));

 pr_info("Shutting down hard lockup detector on all cpus\n");

 return PTR_ERR(event);


out_save:
 per_cpu(watchdog_ev, cpu) = event;
out_enable:
 perf_event_enable(per_cpu(watchdog_ev, cpu));
out:
 return 0;
}

static void watchdog_nmi_disable(unsigned int cpu)
{
 struct perf_event *event = per_cpu(watchdog_ev, cpu);

 if (event) {
  perf_event_disable(event);
  per_cpu(watchdog_ev, cpu) = NULL;


  perf_event_release_kernel(event);
 }
 if (cpu == 0) {

  cpu0_err = 0;
 }
}

static int watchdog_nmi_enable(unsigned int cpu) { return 0; }
static void watchdog_nmi_disable(unsigned int cpu) { return; }

static struct smp_hotplug_thread watchdog_threads = {
 .store = &softlockup_watchdog,
 .thread_should_run = watchdog_should_run,
 .thread_fn = watchdog,
 .thread_comm = "watchdog/%u",
 .setup = watchdog_enable,
 .cleanup = watchdog_cleanup,
 .park = watchdog_disable,
 .unpark = watchdog_enable,
};
static int watchdog_park_threads(void)
{
 int cpu, ret = 0;

 for_each_watchdog_cpu(cpu) {
  ret = kthread_park(per_cpu(softlockup_watchdog, cpu));
  if (ret)
   break;
 }

 return ret;
}







static void watchdog_unpark_threads(void)
{
 int cpu;

 for_each_watchdog_cpu(cpu)
  kthread_unpark(per_cpu(softlockup_watchdog, cpu));
}




int lockup_detector_suspend(void)
{
 int ret = 0;

 get_online_cpus();
 mutex_lock(&watchdog_proc_mutex);







 if (watchdog_running && !watchdog_suspended)
  ret = watchdog_park_threads();

 if (ret == 0)
  watchdog_suspended++;
 else {
  watchdog_disable_all_cpus();
  pr_err("Failed to suspend lockup detectors, disabled\n");
  watchdog_enabled = 0;
 }

 mutex_unlock(&watchdog_proc_mutex);

 return ret;
}




void lockup_detector_resume(void)
{
 mutex_lock(&watchdog_proc_mutex);

 watchdog_suspended--;




 if (watchdog_running && !watchdog_suspended)
  watchdog_unpark_threads();

 mutex_unlock(&watchdog_proc_mutex);
 put_online_cpus();
}

static int update_watchdog_all_cpus(void)
{
 int ret;

 ret = watchdog_park_threads();
 if (ret)
  return ret;

 watchdog_unpark_threads();

 return 0;
}

static int watchdog_enable_all_cpus(void)
{
 int err = 0;

 if (!watchdog_running) {
  err = smpboot_register_percpu_thread_cpumask(&watchdog_threads,
            &watchdog_cpumask);
  if (err)
   pr_err("Failed to create watchdog threads, disabled\n");
  else
   watchdog_running = 1;
 } else {




  err = update_watchdog_all_cpus();

  if (err) {
   watchdog_disable_all_cpus();
   pr_err("Failed to update lockup detectors, disabled\n");
  }
 }

 if (err)
  watchdog_enabled = 0;

 return err;
}

static void watchdog_disable_all_cpus(void)
{
 if (watchdog_running) {
  watchdog_running = 0;
  smpboot_unregister_percpu_thread(&watchdog_threads);
 }
}





static int proc_watchdog_update(void)
{
 int err = 0;
 if (watchdog_enabled && watchdog_thresh)
  err = watchdog_enable_all_cpus();
 else
  watchdog_disable_all_cpus();

 return err;

}
static int proc_watchdog_common(int which, struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos)
{
 int err, old, new;
 int *watchdog_param = (int *)table->data;

 get_online_cpus();
 mutex_lock(&watchdog_proc_mutex);

 if (watchdog_suspended) {

  err = -EAGAIN;
  goto out;
 }






 if (!write) {
  *watchdog_param = (watchdog_enabled & which) != 0;
  err = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
 } else {
  err = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
  if (err)
   goto out;
  do {
   old = watchdog_enabled;




   if (*watchdog_param)
    new = old | which;
   else
    new = old & ~which;
  } while (cmpxchg(&watchdog_enabled, old, new) != old);
  if (old == new)
   goto out;

  err = proc_watchdog_update();
 }
out:
 mutex_unlock(&watchdog_proc_mutex);
 put_online_cpus();
 return err;
}




int proc_watchdog(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos)
{
 return proc_watchdog_common(NMI_WATCHDOG_ENABLED|SOFT_WATCHDOG_ENABLED,
        table, write, buffer, lenp, ppos);
}




int proc_nmi_watchdog(struct ctl_table *table, int write,
        void __user *buffer, size_t *lenp, loff_t *ppos)
{
 return proc_watchdog_common(NMI_WATCHDOG_ENABLED,
        table, write, buffer, lenp, ppos);
}




int proc_soft_watchdog(struct ctl_table *table, int write,
   void __user *buffer, size_t *lenp, loff_t *ppos)
{
 return proc_watchdog_common(SOFT_WATCHDOG_ENABLED,
        table, write, buffer, lenp, ppos);
}




int proc_watchdog_thresh(struct ctl_table *table, int write,
    void __user *buffer, size_t *lenp, loff_t *ppos)
{
 int err, old, new;

 get_online_cpus();
 mutex_lock(&watchdog_proc_mutex);

 if (watchdog_suspended) {

  err = -EAGAIN;
  goto out;
 }

 old = ACCESS_ONCE(watchdog_thresh);
 err = proc_dointvec_minmax(table, write, buffer, lenp, ppos);

 if (err || !write)
  goto out;




 new = ACCESS_ONCE(watchdog_thresh);
 if (old == new)
  goto out;

 set_sample_period();
 err = proc_watchdog_update();
 if (err) {
  watchdog_thresh = old;
  set_sample_period();
 }
out:
 mutex_unlock(&watchdog_proc_mutex);
 put_online_cpus();
 return err;
}







int proc_watchdog_cpumask(struct ctl_table *table, int write,
     void __user *buffer, size_t *lenp, loff_t *ppos)
{
 int err;

 get_online_cpus();
 mutex_lock(&watchdog_proc_mutex);

 if (watchdog_suspended) {

  err = -EAGAIN;
  goto out;
 }

 err = proc_do_large_bitmap(table, write, buffer, lenp, ppos);
 if (!err && write) {

  cpumask_and(&watchdog_cpumask, &watchdog_cpumask,
       cpu_possible_mask);

  if (watchdog_running) {





   if (smpboot_update_cpumask_percpu_thread(
        &watchdog_threads, &watchdog_cpumask) != 0)
    pr_err("cpumask update failed\n");
  }
 }
out:
 mutex_unlock(&watchdog_proc_mutex);
 put_online_cpus();
 return err;
}


void __init lockup_detector_init(void)
{
 set_sample_period();

 if (tick_nohz_full_enabled()) {
  pr_info("Disabling watchdog on nohz_full cores by default\n");
  cpumask_copy(&watchdog_cpumask, housekeeping_mask);
 } else
  cpumask_copy(&watchdog_cpumask, cpu_possible_mask);
 cpumask_copy(&watchdog_cpumask, cpu_possible_mask);

 if (watchdog_enabled)
  watchdog_enable_all_cpus();
}


enum {
 POOL_DISASSOCIATED = 1 << 2,


 WORKER_DIE = 1 << 1,
 WORKER_IDLE = 1 << 2,
 WORKER_PREP = 1 << 3,
 WORKER_CPU_INTENSIVE = 1 << 6,
 WORKER_UNBOUND = 1 << 7,
 WORKER_REBOUND = 1 << 8,

 WORKER_NOT_RUNNING = WORKER_PREP | WORKER_CPU_INTENSIVE |
      WORKER_UNBOUND | WORKER_REBOUND,

 NR_STD_WORKER_POOLS = 2,

 UNBOUND_POOL_HASH_ORDER = 6,
 BUSY_WORKER_HASH_ORDER = 6,

 MAX_IDLE_WORKERS_RATIO = 4,
 IDLE_WORKER_TIMEOUT = 300 * HZ,

 MAYDAY_INITIAL_TIMEOUT = HZ / 100 >= 2 ? HZ / 100 : 2,


 MAYDAY_INTERVAL = HZ / 10,
 CREATE_COOLDOWN = HZ,





 RESCUER_NICE_LEVEL = MIN_NICE,
 HIGHPRI_NICE_LEVEL = MIN_NICE,

 WQ_NAME_LEN = 24,
};
struct worker_pool {
 spinlock_t lock;
 int cpu;
 int node;
 int id;
 unsigned int flags;

 unsigned long watchdog_ts;

 struct list_head worklist;
 int nr_workers;


 int nr_idle;

 struct list_head idle_list;
 struct timer_list idle_timer;
 struct timer_list mayday_timer;


 DECLARE_HASHTABLE(busy_hash, BUSY_WORKER_HASH_ORDER);



 struct mutex manager_arb;
 struct worker *manager;
 struct mutex attach_mutex;
 struct list_head workers;
 struct completion *detach_completion;

 struct ida worker_ida;

 struct workqueue_attrs *attrs;
 struct hlist_node hash_node;
 int refcnt;






 atomic_t nr_running ____cacheline_aligned_in_smp;





 struct rcu_head rcu;
} ____cacheline_aligned_in_smp;







struct pool_workqueue {
 struct worker_pool *pool;
 struct workqueue_struct *wq;
 int work_color;
 int flush_color;
 int refcnt;
 int nr_in_flight[WORK_NR_COLORS];

 int nr_active;
 int max_active;
 struct list_head delayed_works;
 struct list_head pwqs_node;
 struct list_head mayday_node;







 struct work_struct unbound_release_work;
 struct rcu_head rcu;
} __aligned(1 << WORK_STRUCT_FLAG_BITS);




struct wq_flusher {
 struct list_head list;
 int flush_color;
 struct completion done;
};

struct wq_device;





struct workqueue_struct {
 struct list_head pwqs;
 struct list_head list;

 struct mutex mutex;
 int work_color;
 int flush_color;
 atomic_t nr_pwqs_to_flush;
 struct wq_flusher *first_flusher;
 struct list_head flusher_queue;
 struct list_head flusher_overflow;

 struct list_head maydays;
 struct worker *rescuer;

 int nr_drainers;
 int saved_max_active;

 struct workqueue_attrs *unbound_attrs;
 struct pool_workqueue *dfl_pwq;

 struct wq_device *wq_dev;
 struct lockdep_map lockdep_map;
 char name[WQ_NAME_LEN];






 struct rcu_head rcu;


 unsigned int flags ____cacheline_aligned;
 struct pool_workqueue __percpu *cpu_pwqs;
 struct pool_workqueue __rcu *numa_pwq_tbl[];
};

static struct kmem_cache *pwq_cache;

static cpumask_var_t *wq_numa_possible_cpumask;


static bool wq_disable_numa;
module_param_named(disable_numa, wq_disable_numa, bool, 0444);


static bool wq_power_efficient = IS_ENABLED(CONFIG_WQ_POWER_EFFICIENT_DEFAULT);
module_param_named(power_efficient, wq_power_efficient, bool, 0444);

static bool wq_numa_enabled;


static struct workqueue_attrs *wq_update_unbound_numa_attrs_buf;

static DEFINE_MUTEX(wq_pool_mutex);
static DEFINE_SPINLOCK(wq_mayday_lock);

static LIST_HEAD(workqueues);
static bool workqueue_freezing;


static cpumask_var_t wq_unbound_cpumask;


static DEFINE_PER_CPU(int, wq_rr_cpu_last);






static bool wq_debug_force_rr_cpu = true;
static bool wq_debug_force_rr_cpu = false;
module_param_named(debug_force_rr_cpu, wq_debug_force_rr_cpu, bool, 0644);


static DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS], cpu_worker_pools);

static DEFINE_IDR(worker_pool_idr);


static DEFINE_HASHTABLE(unbound_pool_hash, UNBOUND_POOL_HASH_ORDER);


static struct workqueue_attrs *unbound_std_wq_attrs[NR_STD_WORKER_POOLS];


static struct workqueue_attrs *ordered_wq_attrs[NR_STD_WORKER_POOLS];

struct workqueue_struct *system_wq __read_mostly;
EXPORT_SYMBOL(system_wq);
struct workqueue_struct *system_highpri_wq __read_mostly;
EXPORT_SYMBOL_GPL(system_highpri_wq);
struct workqueue_struct *system_long_wq __read_mostly;
EXPORT_SYMBOL_GPL(system_long_wq);
struct workqueue_struct *system_unbound_wq __read_mostly;
EXPORT_SYMBOL_GPL(system_unbound_wq);
struct workqueue_struct *system_freezable_wq __read_mostly;
EXPORT_SYMBOL_GPL(system_freezable_wq);
struct workqueue_struct *system_power_efficient_wq __read_mostly;
EXPORT_SYMBOL_GPL(system_power_efficient_wq);
struct workqueue_struct *system_freezable_power_efficient_wq __read_mostly;
EXPORT_SYMBOL_GPL(system_freezable_power_efficient_wq);

static int worker_thread(void *__worker);
static void workqueue_sysfs_unregister(struct workqueue_struct *wq);


 RCU_LOCKDEP_WARN(!rcu_read_lock_sched_held() && \
    !lockdep_is_held(&wq_pool_mutex), \
    "sched RCU or wq_pool_mutex should be held")

 RCU_LOCKDEP_WARN(!rcu_read_lock_sched_held() && \
    !lockdep_is_held(&wq->mutex), \
    "sched RCU or wq->mutex should be held")

 RCU_LOCKDEP_WARN(!rcu_read_lock_sched_held() && \
    !lockdep_is_held(&wq->mutex) && \
    !lockdep_is_held(&wq_pool_mutex), \
    "sched RCU, wq->mutex or wq_pool_mutex should be held")

 for ((pool) = &per_cpu(cpu_worker_pools, cpu)[0]; \
      (pool) < &per_cpu(cpu_worker_pools, cpu)[NR_STD_WORKER_POOLS]; \
      (pool)++)
 idr_for_each_entry(&worker_pool_idr, pool, pi) \
  if (({ assert_rcu_or_pool_mutex(); false; })) { } \
  else
 list_for_each_entry((worker), &(pool)->workers, node) \
  if (({ lockdep_assert_held(&pool->attach_mutex); false; })) { } \
  else
 list_for_each_entry_rcu((pwq), &(wq)->pwqs, pwqs_node) \
  if (({ assert_rcu_or_wq_mutex(wq); false; })) { } \
  else


static struct debug_obj_descr work_debug_descr;

static void *work_debug_hint(void *addr)
{
 return ((struct work_struct *) addr)->func;
}

static bool work_is_static_object(void *addr)
{
 struct work_struct *work = addr;

 return test_bit(WORK_STRUCT_STATIC_BIT, work_data_bits(work));
}





static bool work_fixup_init(void *addr, enum debug_obj_state state)
{
 struct work_struct *work = addr;

 switch (state) {
 case ODEBUG_STATE_ACTIVE:
  cancel_work_sync(work);
  debug_object_init(work, &work_debug_descr);
  return true;
 default:
  return false;
 }
}





static bool work_fixup_free(void *addr, enum debug_obj_state state)
{
 struct work_struct *work = addr;

 switch (state) {
 case ODEBUG_STATE_ACTIVE:
  cancel_work_sync(work);
  debug_object_free(work, &work_debug_descr);
  return true;
 default:
  return false;
 }
}

static struct debug_obj_descr work_debug_descr = {
 .name = "work_struct",
 .debug_hint = work_debug_hint,
 .is_static_object = work_is_static_object,
 .fixup_init = work_fixup_init,
 .fixup_free = work_fixup_free,
};

static inline void debug_work_activate(struct work_struct *work)
{
 debug_object_activate(work, &work_debug_descr);
}

static inline void debug_work_deactivate(struct work_struct *work)
{
 debug_object_deactivate(work, &work_debug_descr);
}

void __init_work(struct work_struct *work, int onstack)
{
 if (onstack)
  debug_object_init_on_stack(work, &work_debug_descr);
 else
  debug_object_init(work, &work_debug_descr);
}
EXPORT_SYMBOL_GPL(__init_work);

void destroy_work_on_stack(struct work_struct *work)
{
 debug_object_free(work, &work_debug_descr);
}
EXPORT_SYMBOL_GPL(destroy_work_on_stack);

void destroy_delayed_work_on_stack(struct delayed_work *work)
{
 destroy_timer_on_stack(&work->timer);
 debug_object_free(&work->work, &work_debug_descr);
}
EXPORT_SYMBOL_GPL(destroy_delayed_work_on_stack);

static inline void debug_work_activate(struct work_struct *work) { }
static inline void debug_work_deactivate(struct work_struct *work) { }
static int worker_pool_assign_id(struct worker_pool *pool)
{
 int ret;

 lockdep_assert_held(&wq_pool_mutex);

 ret = idr_alloc(&worker_pool_idr, pool, 0, WORK_OFFQ_POOL_NONE,
   GFP_KERNEL);
 if (ret >= 0) {
  pool->id = ret;
  return 0;
 }
 return ret;
}
static struct pool_workqueue *unbound_pwq_by_node(struct workqueue_struct *wq,
        int node)
{
 assert_rcu_or_wq_mutex_or_pool_mutex(wq);







 if (unlikely(node == NUMA_NO_NODE))
  return wq->dfl_pwq;

 return rcu_dereference_raw(wq->numa_pwq_tbl[node]);
}

static unsigned int work_color_to_flags(int color)
{
 return color << WORK_STRUCT_COLOR_SHIFT;
}

static int get_work_color(struct work_struct *work)
{
 return (*work_data_bits(work) >> WORK_STRUCT_COLOR_SHIFT) &
  ((1 << WORK_STRUCT_COLOR_BITS) - 1);
}

static int work_next_color(int color)
{
 return (color + 1) % WORK_NR_COLORS;
}
static inline void set_work_data(struct work_struct *work, unsigned long data,
     unsigned long flags)
{
 WARN_ON_ONCE(!work_pending(work));
 atomic_long_set(&work->data, data | flags | work_static(work));
}

static void set_work_pwq(struct work_struct *work, struct pool_workqueue *pwq,
    unsigned long extra_flags)
{
 set_work_data(work, (unsigned long)pwq,
        WORK_STRUCT_PENDING | WORK_STRUCT_PWQ | extra_flags);
}

static void set_work_pool_and_keep_pending(struct work_struct *work,
        int pool_id)
{
 set_work_data(work, (unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT,
        WORK_STRUCT_PENDING);
}

static void set_work_pool_and_clear_pending(struct work_struct *work,
         int pool_id)
{






 smp_wmb();
 set_work_data(work, (unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT, 0);
 smp_mb();
}

static void clear_work_data(struct work_struct *work)
{
 smp_wmb();
 set_work_data(work, WORK_STRUCT_NO_POOL, 0);
}

static struct pool_workqueue *get_work_pwq(struct work_struct *work)
{
 unsigned long data = atomic_long_read(&work->data);

 if (data & WORK_STRUCT_PWQ)
  return (void *)(data & WORK_STRUCT_WQ_DATA_MASK);
 else
  return NULL;
}
static struct worker_pool *get_work_pool(struct work_struct *work)
{
 unsigned long data = atomic_long_read(&work->data);
 int pool_id;

 assert_rcu_or_pool_mutex();

 if (data & WORK_STRUCT_PWQ)
  return ((struct pool_workqueue *)
   (data & WORK_STRUCT_WQ_DATA_MASK))->pool;

 pool_id = data >> WORK_OFFQ_POOL_SHIFT;
 if (pool_id == WORK_OFFQ_POOL_NONE)
  return NULL;

 return idr_find(&worker_pool_idr, pool_id);
}
static int get_work_pool_id(struct work_struct *work)
{
 unsigned long data = atomic_long_read(&work->data);

 if (data & WORK_STRUCT_PWQ)
  return ((struct pool_workqueue *)
   (data & WORK_STRUCT_WQ_DATA_MASK))->pool->id;

 return data >> WORK_OFFQ_POOL_SHIFT;
}

static void mark_work_canceling(struct work_struct *work)
{
 unsigned long pool_id = get_work_pool_id(work);

 pool_id <<= WORK_OFFQ_POOL_SHIFT;
 set_work_data(work, pool_id | WORK_OFFQ_CANCELING, WORK_STRUCT_PENDING);
}

static bool work_is_canceling(struct work_struct *work)
{
 unsigned long data = atomic_long_read(&work->data);

 return !(data & WORK_STRUCT_PWQ) && (data & WORK_OFFQ_CANCELING);
}







static bool __need_more_worker(struct worker_pool *pool)
{
 return !atomic_read(&pool->nr_running);
}
static bool need_more_worker(struct worker_pool *pool)
{
 return !list_empty(&pool->worklist) && __need_more_worker(pool);
}


static bool may_start_working(struct worker_pool *pool)
{
 return pool->nr_idle;
}


static bool keep_working(struct worker_pool *pool)
{
 return !list_empty(&pool->worklist) &&
  atomic_read(&pool->nr_running) <= 1;
}


static bool need_to_create_worker(struct worker_pool *pool)
{
 return need_more_worker(pool) && !may_start_working(pool);
}


static bool too_many_workers(struct worker_pool *pool)
{
 bool managing = mutex_is_locked(&pool->manager_arb);
 int nr_idle = pool->nr_idle + managing;
 int nr_busy = pool->nr_workers - nr_idle;

 return nr_idle > 2 && (nr_idle - 2) * MAX_IDLE_WORKERS_RATIO >= nr_busy;
}






static struct worker *first_idle_worker(struct worker_pool *pool)
{
 if (unlikely(list_empty(&pool->idle_list)))
  return NULL;

 return list_first_entry(&pool->idle_list, struct worker, entry);
}
static void wake_up_worker(struct worker_pool *pool)
{
 struct worker *worker = first_idle_worker(pool);

 if (likely(worker))
  wake_up_process(worker->task);
}
void wq_worker_waking_up(struct task_struct *task, int cpu)
{
 struct worker *worker = kthread_data(task);

 if (!(worker->flags & WORKER_NOT_RUNNING)) {
  WARN_ON_ONCE(worker->pool->cpu != cpu);
  atomic_inc(&worker->pool->nr_running);
 }
}
struct task_struct *wq_worker_sleeping(struct task_struct *task)
{
 struct worker *worker = kthread_data(task), *to_wakeup = NULL;
 struct worker_pool *pool;






 if (worker->flags & WORKER_NOT_RUNNING)
  return NULL;

 pool = worker->pool;


 if (WARN_ON_ONCE(pool->cpu != raw_smp_processor_id()))
  return NULL;
 if (atomic_dec_and_test(&pool->nr_running) &&
     !list_empty(&pool->worklist))
  to_wakeup = first_idle_worker(pool);
 return to_wakeup ? to_wakeup->task : NULL;
}
static inline void worker_set_flags(struct worker *worker, unsigned int flags)
{
 struct worker_pool *pool = worker->pool;

 WARN_ON_ONCE(worker->task != current);


 if ((flags & WORKER_NOT_RUNNING) &&
     !(worker->flags & WORKER_NOT_RUNNING)) {
  atomic_dec(&pool->nr_running);
 }

 worker->flags |= flags;
}
static inline void worker_clr_flags(struct worker *worker, unsigned int flags)
{
 struct worker_pool *pool = worker->pool;
 unsigned int oflags = worker->flags;

 WARN_ON_ONCE(worker->task != current);

 worker->flags &= ~flags;






 if ((flags & WORKER_NOT_RUNNING) && (oflags & WORKER_NOT_RUNNING))
  if (!(worker->flags & WORKER_NOT_RUNNING))
   atomic_inc(&pool->nr_running);
}
static struct worker *find_worker_executing_work(struct worker_pool *pool,
       struct work_struct *work)
{
 struct worker *worker;

 hash_for_each_possible(pool->busy_hash, worker, hentry,
          (unsigned long)work)
  if (worker->current_work == work &&
      worker->current_func == work->func)
   return worker;

 return NULL;
}
static void move_linked_works(struct work_struct *work, struct list_head *head,
         struct work_struct **nextp)
{
 struct work_struct *n;





 list_for_each_entry_safe_from(work, n, NULL, entry) {
  list_move_tail(&work->entry, head);
  if (!(*work_data_bits(work) & WORK_STRUCT_LINKED))
   break;
 }






 if (nextp)
  *nextp = n;
}
static void get_pwq(struct pool_workqueue *pwq)
{
 lockdep_assert_held(&pwq->pool->lock);
 WARN_ON_ONCE(pwq->refcnt <= 0);
 pwq->refcnt++;
}
static void put_pwq(struct pool_workqueue *pwq)
{
 lockdep_assert_held(&pwq->pool->lock);
 if (likely(--pwq->refcnt))
  return;
 if (WARN_ON_ONCE(!(pwq->wq->flags & WQ_UNBOUND)))
  return;
 schedule_work(&pwq->unbound_release_work);
}







static void put_pwq_unlocked(struct pool_workqueue *pwq)
{
 if (pwq) {




  spin_lock_irq(&pwq->pool->lock);
  put_pwq(pwq);
  spin_unlock_irq(&pwq->pool->lock);
 }
}

static void pwq_activate_delayed_work(struct work_struct *work)
{
 struct pool_workqueue *pwq = get_work_pwq(work);

 trace_workqueue_activate_work(work);
 if (list_empty(&pwq->pool->worklist))
  pwq->pool->watchdog_ts = jiffies;
 move_linked_works(work, &pwq->pool->worklist, NULL);
 __clear_bit(WORK_STRUCT_DELAYED_BIT, work_data_bits(work));
 pwq->nr_active++;
}

static void pwq_activate_first_delayed(struct pool_workqueue *pwq)
{
 struct work_struct *work = list_first_entry(&pwq->delayed_works,
          struct work_struct, entry);

 pwq_activate_delayed_work(work);
}
static void pwq_dec_nr_in_flight(struct pool_workqueue *pwq, int color)
{

 if (color == WORK_NO_COLOR)
  goto out_put;

 pwq->nr_in_flight[color]--;

 pwq->nr_active--;
 if (!list_empty(&pwq->delayed_works)) {

  if (pwq->nr_active < pwq->max_active)
   pwq_activate_first_delayed(pwq);
 }


 if (likely(pwq->flush_color != color))
  goto out_put;


 if (pwq->nr_in_flight[color])
  goto out_put;


 pwq->flush_color = -1;





 if (atomic_dec_and_test(&pwq->wq->nr_pwqs_to_flush))
  complete(&pwq->wq->first_flusher->done);
out_put:
 put_pwq(pwq);
}
static int try_to_grab_pending(struct work_struct *work, bool is_dwork,
          unsigned long *flags)
{
 struct worker_pool *pool;
 struct pool_workqueue *pwq;

 local_irq_save(*flags);


 if (is_dwork) {
  struct delayed_work *dwork = to_delayed_work(work);






  if (likely(del_timer(&dwork->timer)))
   return 1;
 }


 if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)))
  return 0;





 pool = get_work_pool(work);
 if (!pool)
  goto fail;

 spin_lock(&pool->lock);
 pwq = get_work_pwq(work);
 if (pwq && pwq->pool == pool) {
  debug_work_deactivate(work);
  if (*work_data_bits(work) & WORK_STRUCT_DELAYED)
   pwq_activate_delayed_work(work);

  list_del_init(&work->entry);
  pwq_dec_nr_in_flight(pwq, get_work_color(work));


  set_work_pool_and_keep_pending(work, pool->id);

  spin_unlock(&pool->lock);
  return 1;
 }
 spin_unlock(&pool->lock);
fail:
 local_irq_restore(*flags);
 if (work_is_canceling(work))
  return -ENOENT;
 cpu_relax();
 return -EAGAIN;
}
static void insert_work(struct pool_workqueue *pwq, struct work_struct *work,
   struct list_head *head, unsigned int extra_flags)
{
 struct worker_pool *pool = pwq->pool;


 set_work_pwq(work, pwq, extra_flags);
 list_add_tail(&work->entry, head);
 get_pwq(pwq);






 smp_mb();

 if (__need_more_worker(pool))
  wake_up_worker(pool);
}





static bool is_chained_work(struct workqueue_struct *wq)
{
 struct worker *worker;

 worker = current_wq_worker();




 return worker && worker->current_pwq->wq == wq;
}






static int wq_select_unbound_cpu(int cpu)
{
 static bool printed_dbg_warning;
 int new_cpu;

 if (likely(!wq_debug_force_rr_cpu)) {
  if (cpumask_test_cpu(cpu, wq_unbound_cpumask))
   return cpu;
 } else if (!printed_dbg_warning) {
  pr_warn("workqueue: round-robin CPU selection forced, expect performance impact\n");
  printed_dbg_warning = true;
 }

 if (cpumask_empty(wq_unbound_cpumask))
  return cpu;

 new_cpu = __this_cpu_read(wq_rr_cpu_last);
 new_cpu = cpumask_next_and(new_cpu, wq_unbound_cpumask, cpu_online_mask);
 if (unlikely(new_cpu >= nr_cpu_ids)) {
  new_cpu = cpumask_first_and(wq_unbound_cpumask, cpu_online_mask);
  if (unlikely(new_cpu >= nr_cpu_ids))
   return cpu;
 }
 __this_cpu_write(wq_rr_cpu_last, new_cpu);

 return new_cpu;
}

static void __queue_work(int cpu, struct workqueue_struct *wq,
    struct work_struct *work)
{
 struct pool_workqueue *pwq;
 struct worker_pool *last_pool;
 struct list_head *worklist;
 unsigned int work_flags;
 unsigned int req_cpu = cpu;







 WARN_ON_ONCE(!irqs_disabled());

 debug_work_activate(work);


 if (unlikely(wq->flags & __WQ_DRAINING) &&
     WARN_ON_ONCE(!is_chained_work(wq)))
  return;
retry:
 if (req_cpu == WORK_CPU_UNBOUND)
  cpu = wq_select_unbound_cpu(raw_smp_processor_id());


 if (!(wq->flags & WQ_UNBOUND))
  pwq = per_cpu_ptr(wq->cpu_pwqs, cpu);
 else
  pwq = unbound_pwq_by_node(wq, cpu_to_node(cpu));






 last_pool = get_work_pool(work);
 if (last_pool && last_pool != pwq->pool) {
  struct worker *worker;

  spin_lock(&last_pool->lock);

  worker = find_worker_executing_work(last_pool, work);

  if (worker && worker->current_pwq->wq == wq) {
   pwq = worker->current_pwq;
  } else {

   spin_unlock(&last_pool->lock);
   spin_lock(&pwq->pool->lock);
  }
 } else {
  spin_lock(&pwq->pool->lock);
 }
 if (unlikely(!pwq->refcnt)) {
  if (wq->flags & WQ_UNBOUND) {
   spin_unlock(&pwq->pool->lock);
   cpu_relax();
   goto retry;
  }

  WARN_ONCE(true, "workqueue: per-cpu pwq for %s on cpu%d has 0 refcnt",
     wq->name, cpu);
 }


 trace_workqueue_queue_work(req_cpu, pwq, work);

 if (WARN_ON(!list_empty(&work->entry))) {
  spin_unlock(&pwq->pool->lock);
  return;
 }

 pwq->nr_in_flight[pwq->work_color]++;
 work_flags = work_color_to_flags(pwq->work_color);

 if (likely(pwq->nr_active < pwq->max_active)) {
  trace_workqueue_activate_work(work);
  pwq->nr_active++;
  worklist = &pwq->pool->worklist;
  if (list_empty(worklist))
   pwq->pool->watchdog_ts = jiffies;
 } else {
  work_flags |= WORK_STRUCT_DELAYED;
  worklist = &pwq->delayed_works;
 }

 insert_work(pwq, work, worklist, work_flags);

 spin_unlock(&pwq->pool->lock);
}
bool queue_work_on(int cpu, struct workqueue_struct *wq,
     struct work_struct *work)
{
 bool ret = false;
 unsigned long flags;

 local_irq_save(flags);

 if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {
  __queue_work(cpu, wq, work);
  ret = true;
 }

 local_irq_restore(flags);
 return ret;
}
EXPORT_SYMBOL(queue_work_on);

void delayed_work_timer_fn(unsigned long __data)
{
 struct delayed_work *dwork = (struct delayed_work *)__data;


 __queue_work(dwork->cpu, dwork->wq, &dwork->work);
}
EXPORT_SYMBOL(delayed_work_timer_fn);

static void __queue_delayed_work(int cpu, struct workqueue_struct *wq,
    struct delayed_work *dwork, unsigned long delay)
{
 struct timer_list *timer = &dwork->timer;
 struct work_struct *work = &dwork->work;

 WARN_ON_ONCE(timer->function != delayed_work_timer_fn ||
       timer->data != (unsigned long)dwork);
 WARN_ON_ONCE(timer_pending(timer));
 WARN_ON_ONCE(!list_empty(&work->entry));







 if (!delay) {
  __queue_work(cpu, wq, &dwork->work);
  return;
 }

 timer_stats_timer_set_start_info(&dwork->timer);

 dwork->wq = wq;
 dwork->cpu = cpu;
 timer->expires = jiffies + delay;

 if (unlikely(cpu != WORK_CPU_UNBOUND))
  add_timer_on(timer, cpu);
 else
  add_timer(timer);
}
bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
      struct delayed_work *dwork, unsigned long delay)
{
 struct work_struct *work = &dwork->work;
 bool ret = false;
 unsigned long flags;


 local_irq_save(flags);

 if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {
  __queue_delayed_work(cpu, wq, dwork, delay);
  ret = true;
 }

 local_irq_restore(flags);
 return ret;
}
EXPORT_SYMBOL(queue_delayed_work_on);
bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,
    struct delayed_work *dwork, unsigned long delay)
{
 unsigned long flags;
 int ret;

 do {
  ret = try_to_grab_pending(&dwork->work, true, &flags);
 } while (unlikely(ret == -EAGAIN));

 if (likely(ret >= 0)) {
  __queue_delayed_work(cpu, wq, dwork, delay);
  local_irq_restore(flags);
 }


 return ret;
}
EXPORT_SYMBOL_GPL(mod_delayed_work_on);
static void worker_enter_idle(struct worker *worker)
{
 struct worker_pool *pool = worker->pool;

 if (WARN_ON_ONCE(worker->flags & WORKER_IDLE) ||
     WARN_ON_ONCE(!list_empty(&worker->entry) &&
    (worker->hentry.next || worker->hentry.pprev)))
  return;


 worker->flags |= WORKER_IDLE;
 pool->nr_idle++;
 worker->last_active = jiffies;


 list_add(&worker->entry, &pool->idle_list);

 if (too_many_workers(pool) && !timer_pending(&pool->idle_timer))
  mod_timer(&pool->idle_timer, jiffies + IDLE_WORKER_TIMEOUT);







 WARN_ON_ONCE(!(pool->flags & POOL_DISASSOCIATED) &&
       pool->nr_workers == pool->nr_idle &&
       atomic_read(&pool->nr_running));
}
static void worker_leave_idle(struct worker *worker)
{
 struct worker_pool *pool = worker->pool;

 if (WARN_ON_ONCE(!(worker->flags & WORKER_IDLE)))
  return;
 worker_clr_flags(worker, WORKER_IDLE);
 pool->nr_idle--;
 list_del_init(&worker->entry);
}

static struct worker *alloc_worker(int node)
{
 struct worker *worker;

 worker = kzalloc_node(sizeof(*worker), GFP_KERNEL, node);
 if (worker) {
  INIT_LIST_HEAD(&worker->entry);
  INIT_LIST_HEAD(&worker->scheduled);
  INIT_LIST_HEAD(&worker->node);

  worker->flags = WORKER_PREP;
 }
 return worker;
}
static void worker_attach_to_pool(struct worker *worker,
       struct worker_pool *pool)
{
 mutex_lock(&pool->attach_mutex);





 set_cpus_allowed_ptr(worker->task, pool->attrs->cpumask);






 if (pool->flags & POOL_DISASSOCIATED)
  worker->flags |= WORKER_UNBOUND;

 list_add_tail(&worker->node, &pool->workers);

 mutex_unlock(&pool->attach_mutex);
}
static void worker_detach_from_pool(struct worker *worker,
        struct worker_pool *pool)
{
 struct completion *detach_completion = NULL;

 mutex_lock(&pool->attach_mutex);
 list_del(&worker->node);
 if (list_empty(&pool->workers))
  detach_completion = pool->detach_completion;
 mutex_unlock(&pool->attach_mutex);


 worker->flags &= ~(WORKER_UNBOUND | WORKER_REBOUND);

 if (detach_completion)
  complete(detach_completion);
}
static struct worker *create_worker(struct worker_pool *pool)
{
 struct worker *worker = NULL;
 int id = -1;
 char id_buf[16];


 id = ida_simple_get(&pool->worker_ida, 0, 0, GFP_KERNEL);
 if (id < 0)
  goto fail;

 worker = alloc_worker(pool->node);
 if (!worker)
  goto fail;

 worker->pool = pool;
 worker->id = id;

 if (pool->cpu >= 0)
  snprintf(id_buf, sizeof(id_buf), "%d:%d%s", pool->cpu, id,
    pool->attrs->nice < 0 ? "H" : "");
 else
  snprintf(id_buf, sizeof(id_buf), "u%d:%d", pool->id, id);

 worker->task = kthread_create_on_node(worker_thread, worker, pool->node,
           "kworker/%s", id_buf);
 if (IS_ERR(worker->task))
  goto fail;

 set_user_nice(worker->task, pool->attrs->nice);
 kthread_bind_mask(worker->task, pool->attrs->cpumask);


 worker_attach_to_pool(worker, pool);


 spin_lock_irq(&pool->lock);
 worker->pool->nr_workers++;
 worker_enter_idle(worker);
 wake_up_process(worker->task);
 spin_unlock_irq(&pool->lock);

 return worker;

fail:
 if (id >= 0)
  ida_simple_remove(&pool->worker_ida, id);
 kfree(worker);
 return NULL;
}
static void destroy_worker(struct worker *worker)
{
 struct worker_pool *pool = worker->pool;

 lockdep_assert_held(&pool->lock);


 if (WARN_ON(worker->current_work) ||
     WARN_ON(!list_empty(&worker->scheduled)) ||
     WARN_ON(!(worker->flags & WORKER_IDLE)))
  return;

 pool->nr_workers--;
 pool->nr_idle--;

 list_del_init(&worker->entry);
 worker->flags |= WORKER_DIE;
 wake_up_process(worker->task);
}

static void idle_worker_timeout(unsigned long __pool)
{
 struct worker_pool *pool = (void *)__pool;

 spin_lock_irq(&pool->lock);

 while (too_many_workers(pool)) {
  struct worker *worker;
  unsigned long expires;


  worker = list_entry(pool->idle_list.prev, struct worker, entry);
  expires = worker->last_active + IDLE_WORKER_TIMEOUT;

  if (time_before(jiffies, expires)) {
   mod_timer(&pool->idle_timer, expires);
   break;
  }

  destroy_worker(worker);
 }

 spin_unlock_irq(&pool->lock);
}

static void send_mayday(struct work_struct *work)
{
 struct pool_workqueue *pwq = get_work_pwq(work);
 struct workqueue_struct *wq = pwq->wq;

 lockdep_assert_held(&wq_mayday_lock);

 if (!wq->rescuer)
  return;


 if (list_empty(&pwq->mayday_node)) {





  get_pwq(pwq);
  list_add_tail(&pwq->mayday_node, &wq->maydays);
  wake_up_process(wq->rescuer->task);
 }
}

static void pool_mayday_timeout(unsigned long __pool)
{
 struct worker_pool *pool = (void *)__pool;
 struct work_struct *work;

 spin_lock_irq(&pool->lock);
 spin_lock(&wq_mayday_lock);

 if (need_to_create_worker(pool)) {






  list_for_each_entry(work, &pool->worklist, entry)
   send_mayday(work);
 }

 spin_unlock(&wq_mayday_lock);
 spin_unlock_irq(&pool->lock);

 mod_timer(&pool->mayday_timer, jiffies + MAYDAY_INTERVAL);
}
static void maybe_create_worker(struct worker_pool *pool)
__releases(&pool->lock)
__acquires(&pool->lock)
{
restart:
 spin_unlock_irq(&pool->lock);


 mod_timer(&pool->mayday_timer, jiffies + MAYDAY_INITIAL_TIMEOUT);

 while (true) {
  if (create_worker(pool) || !need_to_create_worker(pool))
   break;

  schedule_timeout_interruptible(CREATE_COOLDOWN);

  if (!need_to_create_worker(pool))
   break;
 }

 del_timer_sync(&pool->mayday_timer);
 spin_lock_irq(&pool->lock);





 if (need_to_create_worker(pool))
  goto restart;
}
static bool manage_workers(struct worker *worker)
{
 struct worker_pool *pool = worker->pool;
 if (!mutex_trylock(&pool->manager_arb))
  return false;
 pool->manager = worker;

 maybe_create_worker(pool);

 pool->manager = NULL;
 mutex_unlock(&pool->manager_arb);
 return true;
}
static void process_one_work(struct worker *worker, struct work_struct *work)
__releases(&pool->lock)
__acquires(&pool->lock)
{
 struct pool_workqueue *pwq = get_work_pwq(work);
 struct worker_pool *pool = worker->pool;
 bool cpu_intensive = pwq->wq->flags & WQ_CPU_INTENSIVE;
 int work_color;
 struct worker *collision;







 struct lockdep_map lockdep_map;

 lockdep_copy_map(&lockdep_map, &work->lockdep_map);

 WARN_ON_ONCE(!(pool->flags & POOL_DISASSOCIATED) &&
       raw_smp_processor_id() != pool->cpu);







 collision = find_worker_executing_work(pool, work);
 if (unlikely(collision)) {
  move_linked_works(work, &collision->scheduled, NULL);
  return;
 }


 debug_work_deactivate(work);
 hash_add(pool->busy_hash, &worker->hentry, (unsigned long)work);
 worker->current_work = work;
 worker->current_func = work->func;
 worker->current_pwq = pwq;
 work_color = get_work_color(work);

 list_del_init(&work->entry);







 if (unlikely(cpu_intensive))
  worker_set_flags(worker, WORKER_CPU_INTENSIVE);
 if (need_more_worker(pool))
  wake_up_worker(pool);







 set_work_pool_and_clear_pending(work, pool->id);

 spin_unlock_irq(&pool->lock);

 lock_map_acquire_read(&pwq->wq->lockdep_map);
 lock_map_acquire(&lockdep_map);
 trace_workqueue_execute_start(work);
 worker->current_func(work);




 trace_workqueue_execute_end(work);
 lock_map_release(&lockdep_map);
 lock_map_release(&pwq->wq->lockdep_map);

 if (unlikely(in_atomic() || lockdep_depth(current) > 0)) {
  pr_err("BUG: workqueue leaked lock or atomic: %s/0x%08x/%d\n"
         "     last function: %pf\n",
         current->comm, preempt_count(), task_pid_nr(current),
         worker->current_func);
  debug_show_held_locks(current);
  dump_stack();
 }
 cond_resched_rcu_qs();

 spin_lock_irq(&pool->lock);


 if (unlikely(cpu_intensive))
  worker_clr_flags(worker, WORKER_CPU_INTENSIVE);


 hash_del(&worker->hentry);
 worker->current_work = NULL;
 worker->current_func = NULL;
 worker->current_pwq = NULL;
 worker->desc_valid = false;
 pwq_dec_nr_in_flight(pwq, work_color);
}
static void process_scheduled_works(struct worker *worker)
{
 while (!list_empty(&worker->scheduled)) {
  struct work_struct *work = list_first_entry(&worker->scheduled,
      struct work_struct, entry);
  process_one_work(worker, work);
 }
}
static int worker_thread(void *__worker)
{
 struct worker *worker = __worker;
 struct worker_pool *pool = worker->pool;


 worker->task->flags |= PF_WQ_WORKER;
woke_up:
 spin_lock_irq(&pool->lock);


 if (unlikely(worker->flags & WORKER_DIE)) {
  spin_unlock_irq(&pool->lock);
  WARN_ON_ONCE(!list_empty(&worker->entry));
  worker->task->flags &= ~PF_WQ_WORKER;

  set_task_comm(worker->task, "kworker/dying");
  ida_simple_remove(&pool->worker_ida, worker->id);
  worker_detach_from_pool(worker, pool);
  kfree(worker);
  return 0;
 }

 worker_leave_idle(worker);
recheck:

 if (!need_more_worker(pool))
  goto sleep;


 if (unlikely(!may_start_working(pool)) && manage_workers(worker))
  goto recheck;






 WARN_ON_ONCE(!list_empty(&worker->scheduled));
 worker_clr_flags(worker, WORKER_PREP | WORKER_REBOUND);

 do {
  struct work_struct *work =
   list_first_entry(&pool->worklist,
      struct work_struct, entry);

  pool->watchdog_ts = jiffies;

  if (likely(!(*work_data_bits(work) & WORK_STRUCT_LINKED))) {

   process_one_work(worker, work);
   if (unlikely(!list_empty(&worker->scheduled)))
    process_scheduled_works(worker);
  } else {
   move_linked_works(work, &worker->scheduled, NULL);
   process_scheduled_works(worker);
  }
 } while (keep_working(pool));

 worker_set_flags(worker, WORKER_PREP);
sleep:







 worker_enter_idle(worker);
 __set_current_state(TASK_INTERRUPTIBLE);
 spin_unlock_irq(&pool->lock);
 schedule();
 goto woke_up;
}
static int rescuer_thread(void *__rescuer)
{
 struct worker *rescuer = __rescuer;
 struct workqueue_struct *wq = rescuer->rescue_wq;
 struct list_head *scheduled = &rescuer->scheduled;
 bool should_stop;

 set_user_nice(current, RESCUER_NICE_LEVEL);





 rescuer->task->flags |= PF_WQ_WORKER;
repeat:
 set_current_state(TASK_INTERRUPTIBLE);
 should_stop = kthread_should_stop();


 spin_lock_irq(&wq_mayday_lock);

 while (!list_empty(&wq->maydays)) {
  struct pool_workqueue *pwq = list_first_entry(&wq->maydays,
     struct pool_workqueue, mayday_node);
  struct worker_pool *pool = pwq->pool;
  struct work_struct *work, *n;
  bool first = true;

  __set_current_state(TASK_RUNNING);
  list_del_init(&pwq->mayday_node);

  spin_unlock_irq(&wq_mayday_lock);

  worker_attach_to_pool(rescuer, pool);

  spin_lock_irq(&pool->lock);
  rescuer->pool = pool;





  WARN_ON_ONCE(!list_empty(scheduled));
  list_for_each_entry_safe(work, n, &pool->worklist, entry) {
   if (get_work_pwq(work) == pwq) {
    if (first)
     pool->watchdog_ts = jiffies;
    move_linked_works(work, scheduled, &n);
   }
   first = false;
  }

  if (!list_empty(scheduled)) {
   process_scheduled_works(rescuer);
   if (need_to_create_worker(pool)) {
    spin_lock(&wq_mayday_lock);
    get_pwq(pwq);
    list_move_tail(&pwq->mayday_node, &wq->maydays);
    spin_unlock(&wq_mayday_lock);
   }
  }





  put_pwq(pwq);






  if (need_more_worker(pool))
   wake_up_worker(pool);

  rescuer->pool = NULL;
  spin_unlock_irq(&pool->lock);

  worker_detach_from_pool(rescuer, pool);

  spin_lock_irq(&wq_mayday_lock);
 }

 spin_unlock_irq(&wq_mayday_lock);

 if (should_stop) {
  __set_current_state(TASK_RUNNING);
  rescuer->task->flags &= ~PF_WQ_WORKER;
  return 0;
 }


 WARN_ON_ONCE(!(rescuer->flags & WORKER_NOT_RUNNING));
 schedule();
 goto repeat;
}
static void check_flush_dependency(struct workqueue_struct *target_wq,
       struct work_struct *target_work)
{
 work_func_t target_func = target_work ? target_work->func : NULL;
 struct worker *worker;

 if (target_wq->flags & WQ_MEM_RECLAIM)
  return;

 worker = current_wq_worker();

 WARN_ONCE(current->flags & PF_MEMALLOC,
    "workqueue: PF_MEMALLOC task %d(%s) is flushing !WQ_MEM_RECLAIM %s:%pf",
    current->pid, current->comm, target_wq->name, target_func);
 WARN_ONCE(worker && ((worker->current_pwq->wq->flags &
         (WQ_MEM_RECLAIM | __WQ_LEGACY)) == WQ_MEM_RECLAIM),
    "workqueue: WQ_MEM_RECLAIM %s:%pf is flushing !WQ_MEM_RECLAIM %s:%pf",
    worker->current_pwq->wq->name, worker->current_func,
    target_wq->name, target_func);
}

struct wq_barrier {
 struct work_struct work;
 struct completion done;
 struct task_struct *task;
};

static void wq_barrier_func(struct work_struct *work)
{
 struct wq_barrier *barr = container_of(work, struct wq_barrier, work);
 complete(&barr->done);
}
static void insert_wq_barrier(struct pool_workqueue *pwq,
         struct wq_barrier *barr,
         struct work_struct *target, struct worker *worker)
{
 struct list_head *head;
 unsigned int linked = 0;







 INIT_WORK_ONSTACK(&barr->work, wq_barrier_func);
 __set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(&barr->work));
 init_completion(&barr->done);
 barr->task = current;





 if (worker)
  head = worker->scheduled.next;
 else {
  unsigned long *bits = work_data_bits(target);

  head = target->entry.next;

  linked = *bits & WORK_STRUCT_LINKED;
  __set_bit(WORK_STRUCT_LINKED_BIT, bits);
 }

 debug_work_activate(&barr->work);
 insert_work(pwq, &barr->work, head,
      work_color_to_flags(WORK_NO_COLOR) | linked);
}
static bool flush_workqueue_prep_pwqs(struct workqueue_struct *wq,
          int flush_color, int work_color)
{
 bool wait = false;
 struct pool_workqueue *pwq;

 if (flush_color >= 0) {
  WARN_ON_ONCE(atomic_read(&wq->nr_pwqs_to_flush));
  atomic_set(&wq->nr_pwqs_to_flush, 1);
 }

 for_each_pwq(pwq, wq) {
  struct worker_pool *pool = pwq->pool;

  spin_lock_irq(&pool->lock);

  if (flush_color >= 0) {
   WARN_ON_ONCE(pwq->flush_color != -1);

   if (pwq->nr_in_flight[flush_color]) {
    pwq->flush_color = flush_color;
    atomic_inc(&wq->nr_pwqs_to_flush);
    wait = true;
   }
  }

  if (work_color >= 0) {
   WARN_ON_ONCE(work_color != work_next_color(pwq->work_color));
   pwq->work_color = work_color;
  }

  spin_unlock_irq(&pool->lock);
 }

 if (flush_color >= 0 && atomic_dec_and_test(&wq->nr_pwqs_to_flush))
  complete(&wq->first_flusher->done);

 return wait;
}
void flush_workqueue(struct workqueue_struct *wq)
{
 struct wq_flusher this_flusher = {
  .list = LIST_HEAD_INIT(this_flusher.list),
  .flush_color = -1,
  .done = COMPLETION_INITIALIZER_ONSTACK(this_flusher.done),
 };
 int next_color;

 lock_map_acquire(&wq->lockdep_map);
 lock_map_release(&wq->lockdep_map);

 mutex_lock(&wq->mutex);




 next_color = work_next_color(wq->work_color);

 if (next_color != wq->flush_color) {





  WARN_ON_ONCE(!list_empty(&wq->flusher_overflow));
  this_flusher.flush_color = wq->work_color;
  wq->work_color = next_color;

  if (!wq->first_flusher) {

   WARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);

   wq->first_flusher = &this_flusher;

   if (!flush_workqueue_prep_pwqs(wq, wq->flush_color,
             wq->work_color)) {

    wq->flush_color = next_color;
    wq->first_flusher = NULL;
    goto out_unlock;
   }
  } else {

   WARN_ON_ONCE(wq->flush_color == this_flusher.flush_color);
   list_add_tail(&this_flusher.list, &wq->flusher_queue);
   flush_workqueue_prep_pwqs(wq, -1, wq->work_color);
  }
 } else {





  list_add_tail(&this_flusher.list, &wq->flusher_overflow);
 }

 check_flush_dependency(wq, NULL);

 mutex_unlock(&wq->mutex);

 wait_for_completion(&this_flusher.done);







 if (wq->first_flusher != &this_flusher)
  return;

 mutex_lock(&wq->mutex);


 if (wq->first_flusher != &this_flusher)
  goto out_unlock;

 wq->first_flusher = NULL;

 WARN_ON_ONCE(!list_empty(&this_flusher.list));
 WARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);

 while (true) {
  struct wq_flusher *next, *tmp;


  list_for_each_entry_safe(next, tmp, &wq->flusher_queue, list) {
   if (next->flush_color != wq->flush_color)
    break;
   list_del_init(&next->list);
   complete(&next->done);
  }

  WARN_ON_ONCE(!list_empty(&wq->flusher_overflow) &&
        wq->flush_color != work_next_color(wq->work_color));


  wq->flush_color = work_next_color(wq->flush_color);


  if (!list_empty(&wq->flusher_overflow)) {






   list_for_each_entry(tmp, &wq->flusher_overflow, list)
    tmp->flush_color = wq->work_color;

   wq->work_color = work_next_color(wq->work_color);

   list_splice_tail_init(&wq->flusher_overflow,
           &wq->flusher_queue);
   flush_workqueue_prep_pwqs(wq, -1, wq->work_color);
  }

  if (list_empty(&wq->flusher_queue)) {
   WARN_ON_ONCE(wq->flush_color != wq->work_color);
   break;
  }





  WARN_ON_ONCE(wq->flush_color == wq->work_color);
  WARN_ON_ONCE(wq->flush_color != next->flush_color);

  list_del_init(&next->list);
  wq->first_flusher = next;

  if (flush_workqueue_prep_pwqs(wq, wq->flush_color, -1))
   break;





  wq->first_flusher = NULL;
 }

out_unlock:
 mutex_unlock(&wq->mutex);
}
EXPORT_SYMBOL(flush_workqueue);
void drain_workqueue(struct workqueue_struct *wq)
{
 unsigned int flush_cnt = 0;
 struct pool_workqueue *pwq;






 mutex_lock(&wq->mutex);
 if (!wq->nr_drainers++)
  wq->flags |= __WQ_DRAINING;
 mutex_unlock(&wq->mutex);
reflush:
 flush_workqueue(wq);

 mutex_lock(&wq->mutex);

 for_each_pwq(pwq, wq) {
  bool drained;

  spin_lock_irq(&pwq->pool->lock);
  drained = !pwq->nr_active && list_empty(&pwq->delayed_works);
  spin_unlock_irq(&pwq->pool->lock);

  if (drained)
   continue;

  if (++flush_cnt == 10 ||
      (flush_cnt % 100 == 0 && flush_cnt <= 1000))
   pr_warn("workqueue %s: drain_workqueue() isn't complete after %u tries\n",
    wq->name, flush_cnt);

  mutex_unlock(&wq->mutex);
  goto reflush;
 }

 if (!--wq->nr_drainers)
  wq->flags &= ~__WQ_DRAINING;
 mutex_unlock(&wq->mutex);
}
EXPORT_SYMBOL_GPL(drain_workqueue);

static bool start_flush_work(struct work_struct *work, struct wq_barrier *barr)
{
 struct worker *worker = NULL;
 struct worker_pool *pool;
 struct pool_workqueue *pwq;

 might_sleep();

 local_irq_disable();
 pool = get_work_pool(work);
 if (!pool) {
  local_irq_enable();
  return false;
 }

 spin_lock(&pool->lock);

 pwq = get_work_pwq(work);
 if (pwq) {
  if (unlikely(pwq->pool != pool))
   goto already_gone;
 } else {
  worker = find_worker_executing_work(pool, work);
  if (!worker)
   goto already_gone;
  pwq = worker->current_pwq;
 }

 check_flush_dependency(pwq->wq, work);

 insert_wq_barrier(pwq, barr, work, worker);
 spin_unlock_irq(&pool->lock);







 if (pwq->wq->saved_max_active == 1 || pwq->wq->rescuer)
  lock_map_acquire(&pwq->wq->lockdep_map);
 else
  lock_map_acquire_read(&pwq->wq->lockdep_map);
 lock_map_release(&pwq->wq->lockdep_map);

 return true;
already_gone:
 spin_unlock_irq(&pool->lock);
 return false;
}
bool flush_work(struct work_struct *work)
{
 struct wq_barrier barr;

 lock_map_acquire(&work->lockdep_map);
 lock_map_release(&work->lockdep_map);

 if (start_flush_work(work, &barr)) {
  wait_for_completion(&barr.done);
  destroy_work_on_stack(&barr.work);
  return true;
 } else {
  return false;
 }
}
EXPORT_SYMBOL_GPL(flush_work);

struct cwt_wait {
 wait_queue_t wait;
 struct work_struct *work;
};

static int cwt_wakefn(wait_queue_t *wait, unsigned mode, int sync, void *key)
{
 struct cwt_wait *cwait = container_of(wait, struct cwt_wait, wait);

 if (cwait->work != key)
  return 0;
 return autoremove_wake_function(wait, mode, sync, key);
}

static bool __cancel_work_timer(struct work_struct *work, bool is_dwork)
{
 static DECLARE_WAIT_QUEUE_HEAD(cancel_waitq);
 unsigned long flags;
 int ret;

 do {
  ret = try_to_grab_pending(work, is_dwork, &flags);
  if (unlikely(ret == -ENOENT)) {
   struct cwt_wait cwait;

   init_wait(&cwait.wait);
   cwait.wait.func = cwt_wakefn;
   cwait.work = work;

   prepare_to_wait_exclusive(&cancel_waitq, &cwait.wait,
        TASK_UNINTERRUPTIBLE);
   if (work_is_canceling(work))
    schedule();
   finish_wait(&cancel_waitq, &cwait.wait);
  }
 } while (unlikely(ret < 0));


 mark_work_canceling(work);
 local_irq_restore(flags);

 flush_work(work);
 clear_work_data(work);






 smp_mb();
 if (waitqueue_active(&cancel_waitq))
  __wake_up(&cancel_waitq, TASK_NORMAL, 1, work);

 return ret;
}
bool cancel_work_sync(struct work_struct *work)
{
 return __cancel_work_timer(work, false);
}
EXPORT_SYMBOL_GPL(cancel_work_sync);
bool flush_delayed_work(struct delayed_work *dwork)
{
 local_irq_disable();
 if (del_timer_sync(&dwork->timer))
  __queue_work(dwork->cpu, dwork->wq, &dwork->work);
 local_irq_enable();
 return flush_work(&dwork->work);
}
EXPORT_SYMBOL(flush_delayed_work);
bool cancel_delayed_work(struct delayed_work *dwork)
{
 unsigned long flags;
 int ret;

 do {
  ret = try_to_grab_pending(&dwork->work, true, &flags);
 } while (unlikely(ret == -EAGAIN));

 if (unlikely(ret < 0))
  return false;

 set_work_pool_and_clear_pending(&dwork->work,
     get_work_pool_id(&dwork->work));
 local_irq_restore(flags);
 return ret;
}
EXPORT_SYMBOL(cancel_delayed_work);
bool cancel_delayed_work_sync(struct delayed_work *dwork)
{
 return __cancel_work_timer(&dwork->work, true);
}
EXPORT_SYMBOL(cancel_delayed_work_sync);
int schedule_on_each_cpu(work_func_t func)
{
 int cpu;
 struct work_struct __percpu *works;

 works = alloc_percpu(struct work_struct);
 if (!works)
  return -ENOMEM;

 get_online_cpus();

 for_each_online_cpu(cpu) {
  struct work_struct *work = per_cpu_ptr(works, cpu);

  INIT_WORK(work, func);
  schedule_work_on(cpu, work);
 }

 for_each_online_cpu(cpu)
  flush_work(per_cpu_ptr(works, cpu));

 put_online_cpus();
 free_percpu(works);
 return 0;
}
int execute_in_process_context(work_func_t fn, struct execute_work *ew)
{
 if (!in_interrupt()) {
  fn(&ew->work);
  return 0;
 }

 INIT_WORK(&ew->work, fn);
 schedule_work(&ew->work);

 return 1;
}
EXPORT_SYMBOL_GPL(execute_in_process_context);







void free_workqueue_attrs(struct workqueue_attrs *attrs)
{
 if (attrs) {
  free_cpumask_var(attrs->cpumask);
  kfree(attrs);
 }
}
struct workqueue_attrs *alloc_workqueue_attrs(gfp_t gfp_mask)
{
 struct workqueue_attrs *attrs;

 attrs = kzalloc(sizeof(*attrs), gfp_mask);
 if (!attrs)
  goto fail;
 if (!alloc_cpumask_var(&attrs->cpumask, gfp_mask))
  goto fail;

 cpumask_copy(attrs->cpumask, cpu_possible_mask);
 return attrs;
fail:
 free_workqueue_attrs(attrs);
 return NULL;
}

static void copy_workqueue_attrs(struct workqueue_attrs *to,
     const struct workqueue_attrs *from)
{
 to->nice = from->nice;
 cpumask_copy(to->cpumask, from->cpumask);





 to->no_numa = from->no_numa;
}


static u32 wqattrs_hash(const struct workqueue_attrs *attrs)
{
 u32 hash = 0;

 hash = jhash_1word(attrs->nice, hash);
 hash = jhash(cpumask_bits(attrs->cpumask),
       BITS_TO_LONGS(nr_cpumask_bits) * sizeof(long), hash);
 return hash;
}


static bool wqattrs_equal(const struct workqueue_attrs *a,
     const struct workqueue_attrs *b)
{
 if (a->nice != b->nice)
  return false;
 if (!cpumask_equal(a->cpumask, b->cpumask))
  return false;
 return true;
}
static int init_worker_pool(struct worker_pool *pool)
{
 spin_lock_init(&pool->lock);
 pool->id = -1;
 pool->cpu = -1;
 pool->node = NUMA_NO_NODE;
 pool->flags |= POOL_DISASSOCIATED;
 pool->watchdog_ts = jiffies;
 INIT_LIST_HEAD(&pool->worklist);
 INIT_LIST_HEAD(&pool->idle_list);
 hash_init(pool->busy_hash);

 init_timer_deferrable(&pool->idle_timer);
 pool->idle_timer.function = idle_worker_timeout;
 pool->idle_timer.data = (unsigned long)pool;

 setup_timer(&pool->mayday_timer, pool_mayday_timeout,
      (unsigned long)pool);

 mutex_init(&pool->manager_arb);
 mutex_init(&pool->attach_mutex);
 INIT_LIST_HEAD(&pool->workers);

 ida_init(&pool->worker_ida);
 INIT_HLIST_NODE(&pool->hash_node);
 pool->refcnt = 1;


 pool->attrs = alloc_workqueue_attrs(GFP_KERNEL);
 if (!pool->attrs)
  return -ENOMEM;
 return 0;
}

static void rcu_free_wq(struct rcu_head *rcu)
{
 struct workqueue_struct *wq =
  container_of(rcu, struct workqueue_struct, rcu);

 if (!(wq->flags & WQ_UNBOUND))
  free_percpu(wq->cpu_pwqs);
 else
  free_workqueue_attrs(wq->unbound_attrs);

 kfree(wq->rescuer);
 kfree(wq);
}

static void rcu_free_pool(struct rcu_head *rcu)
{
 struct worker_pool *pool = container_of(rcu, struct worker_pool, rcu);

 ida_destroy(&pool->worker_ida);
 free_workqueue_attrs(pool->attrs);
 kfree(pool);
}
static void put_unbound_pool(struct worker_pool *pool)
{
 DECLARE_COMPLETION_ONSTACK(detach_completion);
 struct worker *worker;

 lockdep_assert_held(&wq_pool_mutex);

 if (--pool->refcnt)
  return;


 if (WARN_ON(!(pool->cpu < 0)) ||
     WARN_ON(!list_empty(&pool->worklist)))
  return;


 if (pool->id >= 0)
  idr_remove(&worker_pool_idr, pool->id);
 hash_del(&pool->hash_node);






 mutex_lock(&pool->manager_arb);

 spin_lock_irq(&pool->lock);
 while ((worker = first_idle_worker(pool)))
  destroy_worker(worker);
 WARN_ON(pool->nr_workers || pool->nr_idle);
 spin_unlock_irq(&pool->lock);

 mutex_lock(&pool->attach_mutex);
 if (!list_empty(&pool->workers))
  pool->detach_completion = &detach_completion;
 mutex_unlock(&pool->attach_mutex);

 if (pool->detach_completion)
  wait_for_completion(pool->detach_completion);

 mutex_unlock(&pool->manager_arb);


 del_timer_sync(&pool->idle_timer);
 del_timer_sync(&pool->mayday_timer);


 call_rcu_sched(&pool->rcu, rcu_free_pool);
}
static struct worker_pool *get_unbound_pool(const struct workqueue_attrs *attrs)
{
 u32 hash = wqattrs_hash(attrs);
 struct worker_pool *pool;
 int node;
 int target_node = NUMA_NO_NODE;

 lockdep_assert_held(&wq_pool_mutex);


 hash_for_each_possible(unbound_pool_hash, pool, hash_node, hash) {
  if (wqattrs_equal(pool->attrs, attrs)) {
   pool->refcnt++;
   return pool;
  }
 }


 if (wq_numa_enabled) {
  for_each_node(node) {
   if (cpumask_subset(attrs->cpumask,
        wq_numa_possible_cpumask[node])) {
    target_node = node;
    break;
   }
  }
 }


 pool = kzalloc_node(sizeof(*pool), GFP_KERNEL, target_node);
 if (!pool || init_worker_pool(pool) < 0)
  goto fail;

 lockdep_set_subclass(&pool->lock, 1);
 copy_workqueue_attrs(pool->attrs, attrs);
 pool->node = target_node;





 pool->attrs->no_numa = false;

 if (worker_pool_assign_id(pool) < 0)
  goto fail;


 if (!create_worker(pool))
  goto fail;


 hash_add(unbound_pool_hash, &pool->hash_node, hash);

 return pool;
fail:
 if (pool)
  put_unbound_pool(pool);
 return NULL;
}

static void rcu_free_pwq(struct rcu_head *rcu)
{
 kmem_cache_free(pwq_cache,
   container_of(rcu, struct pool_workqueue, rcu));
}





static void pwq_unbound_release_workfn(struct work_struct *work)
{
 struct pool_workqueue *pwq = container_of(work, struct pool_workqueue,
        unbound_release_work);
 struct workqueue_struct *wq = pwq->wq;
 struct worker_pool *pool = pwq->pool;
 bool is_last;

 if (WARN_ON_ONCE(!(wq->flags & WQ_UNBOUND)))
  return;

 mutex_lock(&wq->mutex);
 list_del_rcu(&pwq->pwqs_node);
 is_last = list_empty(&wq->pwqs);
 mutex_unlock(&wq->mutex);

 mutex_lock(&wq_pool_mutex);
 put_unbound_pool(pool);
 mutex_unlock(&wq_pool_mutex);

 call_rcu_sched(&pwq->rcu, rcu_free_pwq);





 if (is_last)
  call_rcu_sched(&wq->rcu, rcu_free_wq);
}
static void pwq_adjust_max_active(struct pool_workqueue *pwq)
{
 struct workqueue_struct *wq = pwq->wq;
 bool freezable = wq->flags & WQ_FREEZABLE;


 lockdep_assert_held(&wq->mutex);


 if (!freezable && pwq->max_active == wq->saved_max_active)
  return;

 spin_lock_irq(&pwq->pool->lock);






 if (!freezable || !workqueue_freezing) {
  pwq->max_active = wq->saved_max_active;

  while (!list_empty(&pwq->delayed_works) &&
         pwq->nr_active < pwq->max_active)
   pwq_activate_first_delayed(pwq);





  wake_up_worker(pwq->pool);
 } else {
  pwq->max_active = 0;
 }

 spin_unlock_irq(&pwq->pool->lock);
}


static void init_pwq(struct pool_workqueue *pwq, struct workqueue_struct *wq,
       struct worker_pool *pool)
{
 BUG_ON((unsigned long)pwq & WORK_STRUCT_FLAG_MASK);

 memset(pwq, 0, sizeof(*pwq));

 pwq->pool = pool;
 pwq->wq = wq;
 pwq->flush_color = -1;
 pwq->refcnt = 1;
 INIT_LIST_HEAD(&pwq->delayed_works);
 INIT_LIST_HEAD(&pwq->pwqs_node);
 INIT_LIST_HEAD(&pwq->mayday_node);
 INIT_WORK(&pwq->unbound_release_work, pwq_unbound_release_workfn);
}


static void link_pwq(struct pool_workqueue *pwq)
{
 struct workqueue_struct *wq = pwq->wq;

 lockdep_assert_held(&wq->mutex);


 if (!list_empty(&pwq->pwqs_node))
  return;


 pwq->work_color = wq->work_color;


 pwq_adjust_max_active(pwq);


 list_add_rcu(&pwq->pwqs_node, &wq->pwqs);
}


static struct pool_workqueue *alloc_unbound_pwq(struct workqueue_struct *wq,
     const struct workqueue_attrs *attrs)
{
 struct worker_pool *pool;
 struct pool_workqueue *pwq;

 lockdep_assert_held(&wq_pool_mutex);

 pool = get_unbound_pool(attrs);
 if (!pool)
  return NULL;

 pwq = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL, pool->node);
 if (!pwq) {
  put_unbound_pool(pool);
  return NULL;
 }

 init_pwq(pwq, wq, pool);
 return pwq;
}
static bool wq_calc_node_cpumask(const struct workqueue_attrs *attrs, int node,
     int cpu_going_down, cpumask_t *cpumask)
{
 if (!wq_numa_enabled || attrs->no_numa)
  goto use_dfl;


 cpumask_and(cpumask, cpumask_of_node(node), attrs->cpumask);
 if (cpu_going_down >= 0)
  cpumask_clear_cpu(cpu_going_down, cpumask);

 if (cpumask_empty(cpumask))
  goto use_dfl;


 cpumask_and(cpumask, attrs->cpumask, wq_numa_possible_cpumask[node]);
 return !cpumask_equal(cpumask, attrs->cpumask);

use_dfl:
 cpumask_copy(cpumask, attrs->cpumask);
 return false;
}


static struct pool_workqueue *numa_pwq_tbl_install(struct workqueue_struct *wq,
         int node,
         struct pool_workqueue *pwq)
{
 struct pool_workqueue *old_pwq;

 lockdep_assert_held(&wq_pool_mutex);
 lockdep_assert_held(&wq->mutex);


 link_pwq(pwq);

 old_pwq = rcu_access_pointer(wq->numa_pwq_tbl[node]);
 rcu_assign_pointer(wq->numa_pwq_tbl[node], pwq);
 return old_pwq;
}


struct apply_wqattrs_ctx {
 struct workqueue_struct *wq;
 struct workqueue_attrs *attrs;
 struct list_head list;
 struct pool_workqueue *dfl_pwq;
 struct pool_workqueue *pwq_tbl[];
};


static void apply_wqattrs_cleanup(struct apply_wqattrs_ctx *ctx)
{
 if (ctx) {
  int node;

  for_each_node(node)
   put_pwq_unlocked(ctx->pwq_tbl[node]);
  put_pwq_unlocked(ctx->dfl_pwq);

  free_workqueue_attrs(ctx->attrs);

  kfree(ctx);
 }
}


static struct apply_wqattrs_ctx *
apply_wqattrs_prepare(struct workqueue_struct *wq,
        const struct workqueue_attrs *attrs)
{
 struct apply_wqattrs_ctx *ctx;
 struct workqueue_attrs *new_attrs, *tmp_attrs;
 int node;

 lockdep_assert_held(&wq_pool_mutex);

 ctx = kzalloc(sizeof(*ctx) + nr_node_ids * sizeof(ctx->pwq_tbl[0]),
        GFP_KERNEL);

 new_attrs = alloc_workqueue_attrs(GFP_KERNEL);
 tmp_attrs = alloc_workqueue_attrs(GFP_KERNEL);
 if (!ctx || !new_attrs || !tmp_attrs)
  goto out_free;






 copy_workqueue_attrs(new_attrs, attrs);
 cpumask_and(new_attrs->cpumask, new_attrs->cpumask, wq_unbound_cpumask);
 if (unlikely(cpumask_empty(new_attrs->cpumask)))
  cpumask_copy(new_attrs->cpumask, wq_unbound_cpumask);






 copy_workqueue_attrs(tmp_attrs, new_attrs);






 ctx->dfl_pwq = alloc_unbound_pwq(wq, new_attrs);
 if (!ctx->dfl_pwq)
  goto out_free;

 for_each_node(node) {
  if (wq_calc_node_cpumask(new_attrs, node, -1, tmp_attrs->cpumask)) {
   ctx->pwq_tbl[node] = alloc_unbound_pwq(wq, tmp_attrs);
   if (!ctx->pwq_tbl[node])
    goto out_free;
  } else {
   ctx->dfl_pwq->refcnt++;
   ctx->pwq_tbl[node] = ctx->dfl_pwq;
  }
 }


 copy_workqueue_attrs(new_attrs, attrs);
 cpumask_and(new_attrs->cpumask, new_attrs->cpumask, cpu_possible_mask);
 ctx->attrs = new_attrs;

 ctx->wq = wq;
 free_workqueue_attrs(tmp_attrs);
 return ctx;

out_free:
 free_workqueue_attrs(tmp_attrs);
 free_workqueue_attrs(new_attrs);
 apply_wqattrs_cleanup(ctx);
 return NULL;
}


static void apply_wqattrs_commit(struct apply_wqattrs_ctx *ctx)
{
 int node;


 mutex_lock(&ctx->wq->mutex);

 copy_workqueue_attrs(ctx->wq->unbound_attrs, ctx->attrs);


 for_each_node(node)
  ctx->pwq_tbl[node] = numa_pwq_tbl_install(ctx->wq, node,
         ctx->pwq_tbl[node]);


 link_pwq(ctx->dfl_pwq);
 swap(ctx->wq->dfl_pwq, ctx->dfl_pwq);

 mutex_unlock(&ctx->wq->mutex);
}

static void apply_wqattrs_lock(void)
{

 get_online_cpus();
 mutex_lock(&wq_pool_mutex);
}

static void apply_wqattrs_unlock(void)
{
 mutex_unlock(&wq_pool_mutex);
 put_online_cpus();
}

static int apply_workqueue_attrs_locked(struct workqueue_struct *wq,
     const struct workqueue_attrs *attrs)
{
 struct apply_wqattrs_ctx *ctx;


 if (WARN_ON(!(wq->flags & WQ_UNBOUND)))
  return -EINVAL;


 if (WARN_ON((wq->flags & __WQ_ORDERED) && !list_empty(&wq->pwqs)))
  return -EINVAL;

 ctx = apply_wqattrs_prepare(wq, attrs);
 if (!ctx)
  return -ENOMEM;


 apply_wqattrs_commit(ctx);
 apply_wqattrs_cleanup(ctx);

 return 0;
}
int apply_workqueue_attrs(struct workqueue_struct *wq,
     const struct workqueue_attrs *attrs)
{
 int ret;

 apply_wqattrs_lock();
 ret = apply_workqueue_attrs_locked(wq, attrs);
 apply_wqattrs_unlock();

 return ret;
}
static void wq_update_unbound_numa(struct workqueue_struct *wq, int cpu,
       bool online)
{
 int node = cpu_to_node(cpu);
 int cpu_off = online ? -1 : cpu;
 struct pool_workqueue *old_pwq = NULL, *pwq;
 struct workqueue_attrs *target_attrs;
 cpumask_t *cpumask;

 lockdep_assert_held(&wq_pool_mutex);

 if (!wq_numa_enabled || !(wq->flags & WQ_UNBOUND) ||
     wq->unbound_attrs->no_numa)
  return;






 target_attrs = wq_update_unbound_numa_attrs_buf;
 cpumask = target_attrs->cpumask;

 copy_workqueue_attrs(target_attrs, wq->unbound_attrs);
 pwq = unbound_pwq_by_node(wq, node);







 if (wq_calc_node_cpumask(wq->dfl_pwq->pool->attrs, node, cpu_off, cpumask)) {
  if (cpumask_equal(cpumask, pwq->pool->attrs->cpumask))
   return;
 } else {
  goto use_dfl_pwq;
 }


 pwq = alloc_unbound_pwq(wq, target_attrs);
 if (!pwq) {
  pr_warn("workqueue: allocation failed while updating NUMA affinity of \"%s\"\n",
   wq->name);
  goto use_dfl_pwq;
 }


 mutex_lock(&wq->mutex);
 old_pwq = numa_pwq_tbl_install(wq, node, pwq);
 goto out_unlock;

use_dfl_pwq:
 mutex_lock(&wq->mutex);
 spin_lock_irq(&wq->dfl_pwq->pool->lock);
 get_pwq(wq->dfl_pwq);
 spin_unlock_irq(&wq->dfl_pwq->pool->lock);
 old_pwq = numa_pwq_tbl_install(wq, node, wq->dfl_pwq);
out_unlock:
 mutex_unlock(&wq->mutex);
 put_pwq_unlocked(old_pwq);
}

static int alloc_and_link_pwqs(struct workqueue_struct *wq)
{
 bool highpri = wq->flags & WQ_HIGHPRI;
 int cpu, ret;

 if (!(wq->flags & WQ_UNBOUND)) {
  wq->cpu_pwqs = alloc_percpu(struct pool_workqueue);
  if (!wq->cpu_pwqs)
   return -ENOMEM;

  for_each_possible_cpu(cpu) {
   struct pool_workqueue *pwq =
    per_cpu_ptr(wq->cpu_pwqs, cpu);
   struct worker_pool *cpu_pools =
    per_cpu(cpu_worker_pools, cpu);

   init_pwq(pwq, wq, &cpu_pools[highpri]);

   mutex_lock(&wq->mutex);
   link_pwq(pwq);
   mutex_unlock(&wq->mutex);
  }
  return 0;
 } else if (wq->flags & __WQ_ORDERED) {
  ret = apply_workqueue_attrs(wq, ordered_wq_attrs[highpri]);

  WARN(!ret && (wq->pwqs.next != &wq->dfl_pwq->pwqs_node ||
         wq->pwqs.prev != &wq->dfl_pwq->pwqs_node),
       "ordering guarantee broken for workqueue %s\n", wq->name);
  return ret;
 } else {
  return apply_workqueue_attrs(wq, unbound_std_wq_attrs[highpri]);
 }
}

static int wq_clamp_max_active(int max_active, unsigned int flags,
          const char *name)
{
 int lim = flags & WQ_UNBOUND ? WQ_UNBOUND_MAX_ACTIVE : WQ_MAX_ACTIVE;

 if (max_active < 1 || max_active > lim)
  pr_warn("workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\n",
   max_active, name, 1, lim);

 return clamp_val(max_active, 1, lim);
}

struct workqueue_struct *__alloc_workqueue_key(const char *fmt,
            unsigned int flags,
            int max_active,
            struct lock_class_key *key,
            const char *lock_name, ...)
{
 size_t tbl_size = 0;
 va_list args;
 struct workqueue_struct *wq;
 struct pool_workqueue *pwq;


 if ((flags & WQ_POWER_EFFICIENT) && wq_power_efficient)
  flags |= WQ_UNBOUND;


 if (flags & WQ_UNBOUND)
  tbl_size = nr_node_ids * sizeof(wq->numa_pwq_tbl[0]);

 wq = kzalloc(sizeof(*wq) + tbl_size, GFP_KERNEL);
 if (!wq)
  return NULL;

 if (flags & WQ_UNBOUND) {
  wq->unbound_attrs = alloc_workqueue_attrs(GFP_KERNEL);
  if (!wq->unbound_attrs)
   goto err_free_wq;
 }

 va_start(args, lock_name);
 vsnprintf(wq->name, sizeof(wq->name), fmt, args);
 va_end(args);

 max_active = max_active ?: WQ_DFL_ACTIVE;
 max_active = wq_clamp_max_active(max_active, flags, wq->name);


 wq->flags = flags;
 wq->saved_max_active = max_active;
 mutex_init(&wq->mutex);
 atomic_set(&wq->nr_pwqs_to_flush, 0);
 INIT_LIST_HEAD(&wq->pwqs);
 INIT_LIST_HEAD(&wq->flusher_queue);
 INIT_LIST_HEAD(&wq->flusher_overflow);
 INIT_LIST_HEAD(&wq->maydays);

 lockdep_init_map(&wq->lockdep_map, lock_name, key, 0);
 INIT_LIST_HEAD(&wq->list);

 if (alloc_and_link_pwqs(wq) < 0)
  goto err_free_wq;





 if (flags & WQ_MEM_RECLAIM) {
  struct worker *rescuer;

  rescuer = alloc_worker(NUMA_NO_NODE);
  if (!rescuer)
   goto err_destroy;

  rescuer->rescue_wq = wq;
  rescuer->task = kthread_create(rescuer_thread, rescuer, "%s",
            wq->name);
  if (IS_ERR(rescuer->task)) {
   kfree(rescuer);
   goto err_destroy;
  }

  wq->rescuer = rescuer;
  kthread_bind_mask(rescuer->task, cpu_possible_mask);
  wake_up_process(rescuer->task);
 }

 if ((wq->flags & WQ_SYSFS) && workqueue_sysfs_register(wq))
  goto err_destroy;






 mutex_lock(&wq_pool_mutex);

 mutex_lock(&wq->mutex);
 for_each_pwq(pwq, wq)
  pwq_adjust_max_active(pwq);
 mutex_unlock(&wq->mutex);

 list_add_tail_rcu(&wq->list, &workqueues);

 mutex_unlock(&wq_pool_mutex);

 return wq;

err_free_wq:
 free_workqueue_attrs(wq->unbound_attrs);
 kfree(wq);
 return NULL;
err_destroy:
 destroy_workqueue(wq);
 return NULL;
}
EXPORT_SYMBOL_GPL(__alloc_workqueue_key);







void destroy_workqueue(struct workqueue_struct *wq)
{
 struct pool_workqueue *pwq;
 int node;


 drain_workqueue(wq);


 mutex_lock(&wq->mutex);
 for_each_pwq(pwq, wq) {
  int i;

  for (i = 0; i < WORK_NR_COLORS; i++) {
   if (WARN_ON(pwq->nr_in_flight[i])) {
    mutex_unlock(&wq->mutex);
    return;
   }
  }

  if (WARN_ON((pwq != wq->dfl_pwq) && (pwq->refcnt > 1)) ||
      WARN_ON(pwq->nr_active) ||
      WARN_ON(!list_empty(&pwq->delayed_works))) {
   mutex_unlock(&wq->mutex);
   return;
  }
 }
 mutex_unlock(&wq->mutex);





 mutex_lock(&wq_pool_mutex);
 list_del_rcu(&wq->list);
 mutex_unlock(&wq_pool_mutex);

 workqueue_sysfs_unregister(wq);

 if (wq->rescuer)
  kthread_stop(wq->rescuer->task);

 if (!(wq->flags & WQ_UNBOUND)) {




  call_rcu_sched(&wq->rcu, rcu_free_wq);
 } else {





  for_each_node(node) {
   pwq = rcu_access_pointer(wq->numa_pwq_tbl[node]);
   RCU_INIT_POINTER(wq->numa_pwq_tbl[node], NULL);
   put_pwq_unlocked(pwq);
  }





  pwq = wq->dfl_pwq;
  wq->dfl_pwq = NULL;
  put_pwq_unlocked(pwq);
 }
}
EXPORT_SYMBOL_GPL(destroy_workqueue);
void workqueue_set_max_active(struct workqueue_struct *wq, int max_active)
{
 struct pool_workqueue *pwq;


 if (WARN_ON(wq->flags & __WQ_ORDERED))
  return;

 max_active = wq_clamp_max_active(max_active, wq->flags, wq->name);

 mutex_lock(&wq->mutex);

 wq->saved_max_active = max_active;

 for_each_pwq(pwq, wq)
  pwq_adjust_max_active(pwq);

 mutex_unlock(&wq->mutex);
}
EXPORT_SYMBOL_GPL(workqueue_set_max_active);
bool current_is_workqueue_rescuer(void)
{
 struct worker *worker = current_wq_worker();

 return worker && worker->rescue_wq;
}
bool workqueue_congested(int cpu, struct workqueue_struct *wq)
{
 struct pool_workqueue *pwq;
 bool ret;

 rcu_read_lock_sched();

 if (cpu == WORK_CPU_UNBOUND)
  cpu = smp_processor_id();

 if (!(wq->flags & WQ_UNBOUND))
  pwq = per_cpu_ptr(wq->cpu_pwqs, cpu);
 else
  pwq = unbound_pwq_by_node(wq, cpu_to_node(cpu));

 ret = !list_empty(&pwq->delayed_works);
 rcu_read_unlock_sched();

 return ret;
}
EXPORT_SYMBOL_GPL(workqueue_congested);
unsigned int work_busy(struct work_struct *work)
{
 struct worker_pool *pool;
 unsigned long flags;
 unsigned int ret = 0;

 if (work_pending(work))
  ret |= WORK_BUSY_PENDING;

 local_irq_save(flags);
 pool = get_work_pool(work);
 if (pool) {
  spin_lock(&pool->lock);
  if (find_worker_executing_work(pool, work))
   ret |= WORK_BUSY_RUNNING;
  spin_unlock(&pool->lock);
 }
 local_irq_restore(flags);

 return ret;
}
EXPORT_SYMBOL_GPL(work_busy);
void set_worker_desc(const char *fmt, ...)
{
 struct worker *worker = current_wq_worker();
 va_list args;

 if (worker) {
  va_start(args, fmt);
  vsnprintf(worker->desc, sizeof(worker->desc), fmt, args);
  va_end(args);
  worker->desc_valid = true;
 }
}
void print_worker_info(const char *log_lvl, struct task_struct *task)
{
 work_func_t *fn = NULL;
 char name[WQ_NAME_LEN] = { };
 char desc[WORKER_DESC_LEN] = { };
 struct pool_workqueue *pwq = NULL;
 struct workqueue_struct *wq = NULL;
 bool desc_valid = false;
 struct worker *worker;

 if (!(task->flags & PF_WQ_WORKER))
  return;





 worker = probe_kthread_data(task);





 probe_kernel_read(&fn, &worker->current_func, sizeof(fn));
 probe_kernel_read(&pwq, &worker->current_pwq, sizeof(pwq));
 probe_kernel_read(&wq, &pwq->wq, sizeof(wq));
 probe_kernel_read(name, wq->name, sizeof(name) - 1);


 probe_kernel_read(&desc_valid, &worker->desc_valid, sizeof(desc_valid));
 if (desc_valid)
  probe_kernel_read(desc, worker->desc, sizeof(desc) - 1);

 if (fn || name[0] || desc[0]) {
  printk("%sWorkqueue: %s %pf", log_lvl, name, fn);
  if (desc[0])
   pr_cont(" (%s)", desc);
  pr_cont("\n");
 }
}

static void pr_cont_pool_info(struct worker_pool *pool)
{
 pr_cont(" cpus=%*pbl", nr_cpumask_bits, pool->attrs->cpumask);
 if (pool->node != NUMA_NO_NODE)
  pr_cont(" node=%d", pool->node);
 pr_cont(" flags=0x%x nice=%d", pool->flags, pool->attrs->nice);
}

static void pr_cont_work(bool comma, struct work_struct *work)
{
 if (work->func == wq_barrier_func) {
  struct wq_barrier *barr;

  barr = container_of(work, struct wq_barrier, work);

  pr_cont("%s BAR(%d)", comma ? "," : "",
   task_pid_nr(barr->task));
 } else {
  pr_cont("%s %pf", comma ? "," : "", work->func);
 }
}

static void show_pwq(struct pool_workqueue *pwq)
{
 struct worker_pool *pool = pwq->pool;
 struct work_struct *work;
 struct worker *worker;
 bool has_in_flight = false, has_pending = false;
 int bkt;

 pr_info("  pwq %d:", pool->id);
 pr_cont_pool_info(pool);

 pr_cont(" active=%d/%d%s\n", pwq->nr_active, pwq->max_active,
  !list_empty(&pwq->mayday_node) ? " MAYDAY" : "");

 hash_for_each(pool->busy_hash, bkt, worker, hentry) {
  if (worker->current_pwq == pwq) {
   has_in_flight = true;
   break;
  }
 }
 if (has_in_flight) {
  bool comma = false;

  pr_info("    in-flight:");
  hash_for_each(pool->busy_hash, bkt, worker, hentry) {
   if (worker->current_pwq != pwq)
    continue;

   pr_cont("%s %d%s:%pf", comma ? "," : "",
    task_pid_nr(worker->task),
    worker == pwq->wq->rescuer ? "(RESCUER)" : "",
    worker->current_func);
   list_for_each_entry(work, &worker->scheduled, entry)
    pr_cont_work(false, work);
   comma = true;
  }
  pr_cont("\n");
 }

 list_for_each_entry(work, &pool->worklist, entry) {
  if (get_work_pwq(work) == pwq) {
   has_pending = true;
   break;
  }
 }
 if (has_pending) {
  bool comma = false;

  pr_info("    pending:");
  list_for_each_entry(work, &pool->worklist, entry) {
   if (get_work_pwq(work) != pwq)
    continue;

   pr_cont_work(comma, work);
   comma = !(*work_data_bits(work) & WORK_STRUCT_LINKED);
  }
  pr_cont("\n");
 }

 if (!list_empty(&pwq->delayed_works)) {
  bool comma = false;

  pr_info("    delayed:");
  list_for_each_entry(work, &pwq->delayed_works, entry) {
   pr_cont_work(comma, work);
   comma = !(*work_data_bits(work) & WORK_STRUCT_LINKED);
  }
  pr_cont("\n");
 }
}







void show_workqueue_state(void)
{
 struct workqueue_struct *wq;
 struct worker_pool *pool;
 unsigned long flags;
 int pi;

 rcu_read_lock_sched();

 pr_info("Showing busy workqueues and worker pools:\n");

 list_for_each_entry_rcu(wq, &workqueues, list) {
  struct pool_workqueue *pwq;
  bool idle = true;

  for_each_pwq(pwq, wq) {
   if (pwq->nr_active || !list_empty(&pwq->delayed_works)) {
    idle = false;
    break;
   }
  }
  if (idle)
   continue;

  pr_info("workqueue %s: flags=0x%x\n", wq->name, wq->flags);

  for_each_pwq(pwq, wq) {
   spin_lock_irqsave(&pwq->pool->lock, flags);
   if (pwq->nr_active || !list_empty(&pwq->delayed_works))
    show_pwq(pwq);
   spin_unlock_irqrestore(&pwq->pool->lock, flags);
  }
 }

 for_each_pool(pool, pi) {
  struct worker *worker;
  bool first = true;

  spin_lock_irqsave(&pool->lock, flags);
  if (pool->nr_workers == pool->nr_idle)
   goto next_pool;

  pr_info("pool %d:", pool->id);
  pr_cont_pool_info(pool);
  pr_cont(" hung=%us workers=%d",
   jiffies_to_msecs(jiffies - pool->watchdog_ts) / 1000,
   pool->nr_workers);
  if (pool->manager)
   pr_cont(" manager: %d",
    task_pid_nr(pool->manager->task));
  list_for_each_entry(worker, &pool->idle_list, entry) {
   pr_cont(" %s%d", first ? "idle: " : "",
    task_pid_nr(worker->task));
   first = false;
  }
  pr_cont("\n");
 next_pool:
  spin_unlock_irqrestore(&pool->lock, flags);
 }

 rcu_read_unlock_sched();
}
static void wq_unbind_fn(struct work_struct *work)
{
 int cpu = smp_processor_id();
 struct worker_pool *pool;
 struct worker *worker;

 for_each_cpu_worker_pool(pool, cpu) {
  mutex_lock(&pool->attach_mutex);
  spin_lock_irq(&pool->lock);
  for_each_pool_worker(worker, pool)
   worker->flags |= WORKER_UNBOUND;

  pool->flags |= POOL_DISASSOCIATED;

  spin_unlock_irq(&pool->lock);
  mutex_unlock(&pool->attach_mutex);







  schedule();
  atomic_set(&pool->nr_running, 0);






  spin_lock_irq(&pool->lock);
  wake_up_worker(pool);
  spin_unlock_irq(&pool->lock);
 }
}







static void rebind_workers(struct worker_pool *pool)
{
 struct worker *worker;

 lockdep_assert_held(&pool->attach_mutex);
 for_each_pool_worker(worker, pool)
  WARN_ON_ONCE(set_cpus_allowed_ptr(worker->task,
        pool->attrs->cpumask) < 0);

 spin_lock_irq(&pool->lock);






 if (!(pool->flags & POOL_DISASSOCIATED)) {
  spin_unlock_irq(&pool->lock);
  return;
 }

 pool->flags &= ~POOL_DISASSOCIATED;

 for_each_pool_worker(worker, pool) {
  unsigned int worker_flags = worker->flags;
  if (worker_flags & WORKER_IDLE)
   wake_up_process(worker->task);
  WARN_ON_ONCE(!(worker_flags & WORKER_UNBOUND));
  worker_flags |= WORKER_REBOUND;
  worker_flags &= ~WORKER_UNBOUND;
  ACCESS_ONCE(worker->flags) = worker_flags;
 }

 spin_unlock_irq(&pool->lock);
}
static void restore_unbound_workers_cpumask(struct worker_pool *pool, int cpu)
{
 static cpumask_t cpumask;
 struct worker *worker;

 lockdep_assert_held(&pool->attach_mutex);


 if (!cpumask_test_cpu(cpu, pool->attrs->cpumask))
  return;


 cpumask_and(&cpumask, pool->attrs->cpumask, cpu_online_mask);
 if (cpumask_weight(&cpumask) != 1)
  return;


 for_each_pool_worker(worker, pool)
  WARN_ON_ONCE(set_cpus_allowed_ptr(worker->task,
        pool->attrs->cpumask) < 0);
}





static int workqueue_cpu_up_callback(struct notifier_block *nfb,
            unsigned long action,
            void *hcpu)
{
 int cpu = (unsigned long)hcpu;
 struct worker_pool *pool;
 struct workqueue_struct *wq;
 int pi;

 switch (action & ~CPU_TASKS_FROZEN) {
 case CPU_UP_PREPARE:
  for_each_cpu_worker_pool(pool, cpu) {
   if (pool->nr_workers)
    continue;
   if (!create_worker(pool))
    return NOTIFY_BAD;
  }
  break;

 case CPU_DOWN_FAILED:
 case CPU_ONLINE:
  mutex_lock(&wq_pool_mutex);

  for_each_pool(pool, pi) {
   mutex_lock(&pool->attach_mutex);

   if (pool->cpu == cpu)
    rebind_workers(pool);
   else if (pool->cpu < 0)
    restore_unbound_workers_cpumask(pool, cpu);

   mutex_unlock(&pool->attach_mutex);
  }


  list_for_each_entry(wq, &workqueues, list)
   wq_update_unbound_numa(wq, cpu, true);

  mutex_unlock(&wq_pool_mutex);
  break;
 }
 return NOTIFY_OK;
}





static int workqueue_cpu_down_callback(struct notifier_block *nfb,
       unsigned long action,
       void *hcpu)
{
 int cpu = (unsigned long)hcpu;
 struct work_struct unbind_work;
 struct workqueue_struct *wq;

 switch (action & ~CPU_TASKS_FROZEN) {
 case CPU_DOWN_PREPARE:

  INIT_WORK_ONSTACK(&unbind_work, wq_unbind_fn);
  queue_work_on(cpu, system_highpri_wq, &unbind_work);


  mutex_lock(&wq_pool_mutex);
  list_for_each_entry(wq, &workqueues, list)
   wq_update_unbound_numa(wq, cpu, false);
  mutex_unlock(&wq_pool_mutex);


  flush_work(&unbind_work);
  destroy_work_on_stack(&unbind_work);
  break;
 }
 return NOTIFY_OK;
}


struct work_for_cpu {
 struct work_struct work;
 long (*fn)(void *);
 void *arg;
 long ret;
};

static void work_for_cpu_fn(struct work_struct *work)
{
 struct work_for_cpu *wfc = container_of(work, struct work_for_cpu, work);

 wfc->ret = wfc->fn(wfc->arg);
}
long work_on_cpu(int cpu, long (*fn)(void *), void *arg)
{
 struct work_for_cpu wfc = { .fn = fn, .arg = arg };

 INIT_WORK_ONSTACK(&wfc.work, work_for_cpu_fn);
 schedule_work_on(cpu, &wfc.work);
 flush_work(&wfc.work);
 destroy_work_on_stack(&wfc.work);
 return wfc.ret;
}
EXPORT_SYMBOL_GPL(work_on_cpu);

void freeze_workqueues_begin(void)
{
 struct workqueue_struct *wq;
 struct pool_workqueue *pwq;

 mutex_lock(&wq_pool_mutex);

 WARN_ON_ONCE(workqueue_freezing);
 workqueue_freezing = true;

 list_for_each_entry(wq, &workqueues, list) {
  mutex_lock(&wq->mutex);
  for_each_pwq(pwq, wq)
   pwq_adjust_max_active(pwq);
  mutex_unlock(&wq->mutex);
 }

 mutex_unlock(&wq_pool_mutex);
}
bool freeze_workqueues_busy(void)
{
 bool busy = false;
 struct workqueue_struct *wq;
 struct pool_workqueue *pwq;

 mutex_lock(&wq_pool_mutex);

 WARN_ON_ONCE(!workqueue_freezing);

 list_for_each_entry(wq, &workqueues, list) {
  if (!(wq->flags & WQ_FREEZABLE))
   continue;




  rcu_read_lock_sched();
  for_each_pwq(pwq, wq) {
   WARN_ON_ONCE(pwq->nr_active < 0);
   if (pwq->nr_active) {
    busy = true;
    rcu_read_unlock_sched();
    goto out_unlock;
   }
  }
  rcu_read_unlock_sched();
 }
out_unlock:
 mutex_unlock(&wq_pool_mutex);
 return busy;
}
void thaw_workqueues(void)
{
 struct workqueue_struct *wq;
 struct pool_workqueue *pwq;

 mutex_lock(&wq_pool_mutex);

 if (!workqueue_freezing)
  goto out_unlock;

 workqueue_freezing = false;


 list_for_each_entry(wq, &workqueues, list) {
  mutex_lock(&wq->mutex);
  for_each_pwq(pwq, wq)
   pwq_adjust_max_active(pwq);
  mutex_unlock(&wq->mutex);
 }

out_unlock:
 mutex_unlock(&wq_pool_mutex);
}

static int workqueue_apply_unbound_cpumask(void)
{
 LIST_HEAD(ctxs);
 int ret = 0;
 struct workqueue_struct *wq;
 struct apply_wqattrs_ctx *ctx, *n;

 lockdep_assert_held(&wq_pool_mutex);

 list_for_each_entry(wq, &workqueues, list) {
  if (!(wq->flags & WQ_UNBOUND))
   continue;

  if (wq->flags & __WQ_ORDERED)
   continue;

  ctx = apply_wqattrs_prepare(wq, wq->unbound_attrs);
  if (!ctx) {
   ret = -ENOMEM;
   break;
  }

  list_add_tail(&ctx->list, &ctxs);
 }

 list_for_each_entry_safe(ctx, n, &ctxs, list) {
  if (!ret)
   apply_wqattrs_commit(ctx);
  apply_wqattrs_cleanup(ctx);
 }

 return ret;
}
int workqueue_set_unbound_cpumask(cpumask_var_t cpumask)
{
 int ret = -EINVAL;
 cpumask_var_t saved_cpumask;

 if (!zalloc_cpumask_var(&saved_cpumask, GFP_KERNEL))
  return -ENOMEM;

 cpumask_and(cpumask, cpumask, cpu_possible_mask);
 if (!cpumask_empty(cpumask)) {
  apply_wqattrs_lock();


  cpumask_copy(saved_cpumask, wq_unbound_cpumask);


  cpumask_copy(wq_unbound_cpumask, cpumask);
  ret = workqueue_apply_unbound_cpumask();


  if (ret < 0)
   cpumask_copy(wq_unbound_cpumask, saved_cpumask);

  apply_wqattrs_unlock();
 }

 free_cpumask_var(saved_cpumask);
 return ret;
}

struct wq_device {
 struct workqueue_struct *wq;
 struct device dev;
};

static struct workqueue_struct *dev_to_wq(struct device *dev)
{
 struct wq_device *wq_dev = container_of(dev, struct wq_device, dev);

 return wq_dev->wq;
}

static ssize_t per_cpu_show(struct device *dev, struct device_attribute *attr,
       char *buf)
{
 struct workqueue_struct *wq = dev_to_wq(dev);

 return scnprintf(buf, PAGE_SIZE, "%d\n", (bool)!(wq->flags & WQ_UNBOUND));
}
static DEVICE_ATTR_RO(per_cpu);

static ssize_t max_active_show(struct device *dev,
          struct device_attribute *attr, char *buf)
{
 struct workqueue_struct *wq = dev_to_wq(dev);

 return scnprintf(buf, PAGE_SIZE, "%d\n", wq->saved_max_active);
}

static ssize_t max_active_store(struct device *dev,
    struct device_attribute *attr, const char *buf,
    size_t count)
{
 struct workqueue_struct *wq = dev_to_wq(dev);
 int val;

 if (sscanf(buf, "%d", &val) != 1 || val <= 0)
  return -EINVAL;

 workqueue_set_max_active(wq, val);
 return count;
}
static DEVICE_ATTR_RW(max_active);

static struct attribute *wq_sysfs_attrs[] = {
 &dev_attr_per_cpu.attr,
 &dev_attr_max_active.attr,
 NULL,
};
ATTRIBUTE_GROUPS(wq_sysfs);

static ssize_t wq_pool_ids_show(struct device *dev,
    struct device_attribute *attr, char *buf)
{
 struct workqueue_struct *wq = dev_to_wq(dev);
 const char *delim = "";
 int node, written = 0;

 rcu_read_lock_sched();
 for_each_node(node) {
  written += scnprintf(buf + written, PAGE_SIZE - written,
         "%s%d:%d", delim, node,
         unbound_pwq_by_node(wq, node)->pool->id);
  delim = " ";
 }
 written += scnprintf(buf + written, PAGE_SIZE - written, "\n");
 rcu_read_unlock_sched();

 return written;
}

static ssize_t wq_nice_show(struct device *dev, struct device_attribute *attr,
       char *buf)
{
 struct workqueue_struct *wq = dev_to_wq(dev);
 int written;

 mutex_lock(&wq->mutex);
 written = scnprintf(buf, PAGE_SIZE, "%d\n", wq->unbound_attrs->nice);
 mutex_unlock(&wq->mutex);

 return written;
}


static struct workqueue_attrs *wq_sysfs_prep_attrs(struct workqueue_struct *wq)
{
 struct workqueue_attrs *attrs;

 lockdep_assert_held(&wq_pool_mutex);

 attrs = alloc_workqueue_attrs(GFP_KERNEL);
 if (!attrs)
  return NULL;

 copy_workqueue_attrs(attrs, wq->unbound_attrs);
 return attrs;
}

static ssize_t wq_nice_store(struct device *dev, struct device_attribute *attr,
        const char *buf, size_t count)
{
 struct workqueue_struct *wq = dev_to_wq(dev);
 struct workqueue_attrs *attrs;
 int ret = -ENOMEM;

 apply_wqattrs_lock();

 attrs = wq_sysfs_prep_attrs(wq);
 if (!attrs)
  goto out_unlock;

 if (sscanf(buf, "%d", &attrs->nice) == 1 &&
     attrs->nice >= MIN_NICE && attrs->nice <= MAX_NICE)
  ret = apply_workqueue_attrs_locked(wq, attrs);
 else
  ret = -EINVAL;

out_unlock:
 apply_wqattrs_unlock();
 free_workqueue_attrs(attrs);
 return ret ?: count;
}

static ssize_t wq_cpumask_show(struct device *dev,
          struct device_attribute *attr, char *buf)
{
 struct workqueue_struct *wq = dev_to_wq(dev);
 int written;

 mutex_lock(&wq->mutex);
 written = scnprintf(buf, PAGE_SIZE, "%*pb\n",
       cpumask_pr_args(wq->unbound_attrs->cpumask));
 mutex_unlock(&wq->mutex);
 return written;
}

static ssize_t wq_cpumask_store(struct device *dev,
    struct device_attribute *attr,
    const char *buf, size_t count)
{
 struct workqueue_struct *wq = dev_to_wq(dev);
 struct workqueue_attrs *attrs;
 int ret = -ENOMEM;

 apply_wqattrs_lock();

 attrs = wq_sysfs_prep_attrs(wq);
 if (!attrs)
  goto out_unlock;

 ret = cpumask_parse(buf, attrs->cpumask);
 if (!ret)
  ret = apply_workqueue_attrs_locked(wq, attrs);

out_unlock:
 apply_wqattrs_unlock();
 free_workqueue_attrs(attrs);
 return ret ?: count;
}

static ssize_t wq_numa_show(struct device *dev, struct device_attribute *attr,
       char *buf)
{
 struct workqueue_struct *wq = dev_to_wq(dev);
 int written;

 mutex_lock(&wq->mutex);
 written = scnprintf(buf, PAGE_SIZE, "%d\n",
       !wq->unbound_attrs->no_numa);
 mutex_unlock(&wq->mutex);

 return written;
}

static ssize_t wq_numa_store(struct device *dev, struct device_attribute *attr,
        const char *buf, size_t count)
{
 struct workqueue_struct *wq = dev_to_wq(dev);
 struct workqueue_attrs *attrs;
 int v, ret = -ENOMEM;

 apply_wqattrs_lock();

 attrs = wq_sysfs_prep_attrs(wq);
 if (!attrs)
  goto out_unlock;

 ret = -EINVAL;
 if (sscanf(buf, "%d", &v) == 1) {
  attrs->no_numa = !v;
  ret = apply_workqueue_attrs_locked(wq, attrs);
 }

out_unlock:
 apply_wqattrs_unlock();
 free_workqueue_attrs(attrs);
 return ret ?: count;
}

static struct device_attribute wq_sysfs_unbound_attrs[] = {
 __ATTR(pool_ids, 0444, wq_pool_ids_show, NULL),
 __ATTR(nice, 0644, wq_nice_show, wq_nice_store),
 __ATTR(cpumask, 0644, wq_cpumask_show, wq_cpumask_store),
 __ATTR(numa, 0644, wq_numa_show, wq_numa_store),
 __ATTR_NULL,
};

static struct bus_type wq_subsys = {
 .name = "workqueue",
 .dev_groups = wq_sysfs_groups,
};

static ssize_t wq_unbound_cpumask_show(struct device *dev,
  struct device_attribute *attr, char *buf)
{
 int written;

 mutex_lock(&wq_pool_mutex);
 written = scnprintf(buf, PAGE_SIZE, "%*pb\n",
       cpumask_pr_args(wq_unbound_cpumask));
 mutex_unlock(&wq_pool_mutex);

 return written;
}

static ssize_t wq_unbound_cpumask_store(struct device *dev,
  struct device_attribute *attr, const char *buf, size_t count)
{
 cpumask_var_t cpumask;
 int ret;

 if (!zalloc_cpumask_var(&cpumask, GFP_KERNEL))
  return -ENOMEM;

 ret = cpumask_parse(buf, cpumask);
 if (!ret)
  ret = workqueue_set_unbound_cpumask(cpumask);

 free_cpumask_var(cpumask);
 return ret ? ret : count;
}

static struct device_attribute wq_sysfs_cpumask_attr =
 __ATTR(cpumask, 0644, wq_unbound_cpumask_show,
        wq_unbound_cpumask_store);

static int __init wq_sysfs_init(void)
{
 int err;

 err = subsys_virtual_register(&wq_subsys, NULL);
 if (err)
  return err;

 return device_create_file(wq_subsys.dev_root, &wq_sysfs_cpumask_attr);
}
core_initcall(wq_sysfs_init);

static void wq_device_release(struct device *dev)
{
 struct wq_device *wq_dev = container_of(dev, struct wq_device, dev);

 kfree(wq_dev);
}
int workqueue_sysfs_register(struct workqueue_struct *wq)
{
 struct wq_device *wq_dev;
 int ret;






 if (WARN_ON(wq->flags & __WQ_ORDERED))
  return -EINVAL;

 wq->wq_dev = wq_dev = kzalloc(sizeof(*wq_dev), GFP_KERNEL);
 if (!wq_dev)
  return -ENOMEM;

 wq_dev->wq = wq;
 wq_dev->dev.bus = &wq_subsys;
 wq_dev->dev.release = wq_device_release;
 dev_set_name(&wq_dev->dev, "%s", wq->name);





 dev_set_uevent_suppress(&wq_dev->dev, true);

 ret = device_register(&wq_dev->dev);
 if (ret) {
  kfree(wq_dev);
  wq->wq_dev = NULL;
  return ret;
 }

 if (wq->flags & WQ_UNBOUND) {
  struct device_attribute *attr;

  for (attr = wq_sysfs_unbound_attrs; attr->attr.name; attr++) {
   ret = device_create_file(&wq_dev->dev, attr);
   if (ret) {
    device_unregister(&wq_dev->dev);
    wq->wq_dev = NULL;
    return ret;
   }
  }
 }

 dev_set_uevent_suppress(&wq_dev->dev, false);
 kobject_uevent(&wq_dev->dev.kobj, KOBJ_ADD);
 return 0;
}







static void workqueue_sysfs_unregister(struct workqueue_struct *wq)
{
 struct wq_device *wq_dev = wq->wq_dev;

 if (!wq->wq_dev)
  return;

 wq->wq_dev = NULL;
 device_unregister(&wq_dev->dev);
}
static void workqueue_sysfs_unregister(struct workqueue_struct *wq) { }

static void wq_watchdog_timer_fn(unsigned long data);

static unsigned long wq_watchdog_thresh = 30;
static struct timer_list wq_watchdog_timer =
 TIMER_DEFERRED_INITIALIZER(wq_watchdog_timer_fn, 0, 0);

static unsigned long wq_watchdog_touched = INITIAL_JIFFIES;
static DEFINE_PER_CPU(unsigned long, wq_watchdog_touched_cpu) = INITIAL_JIFFIES;

static void wq_watchdog_reset_touched(void)
{
 int cpu;

 wq_watchdog_touched = jiffies;
 for_each_possible_cpu(cpu)
  per_cpu(wq_watchdog_touched_cpu, cpu) = jiffies;
}

static void wq_watchdog_timer_fn(unsigned long data)
{
 unsigned long thresh = READ_ONCE(wq_watchdog_thresh) * HZ;
 bool lockup_detected = false;
 struct worker_pool *pool;
 int pi;

 if (!thresh)
  return;

 rcu_read_lock();

 for_each_pool(pool, pi) {
  unsigned long pool_ts, touched, ts;

  if (list_empty(&pool->worklist))
   continue;


  pool_ts = READ_ONCE(pool->watchdog_ts);
  touched = READ_ONCE(wq_watchdog_touched);

  if (time_after(pool_ts, touched))
   ts = pool_ts;
  else
   ts = touched;

  if (pool->cpu >= 0) {
   unsigned long cpu_touched =
    READ_ONCE(per_cpu(wq_watchdog_touched_cpu,
        pool->cpu));
   if (time_after(cpu_touched, ts))
    ts = cpu_touched;
  }


  if (time_after(jiffies, ts + thresh)) {
   lockup_detected = true;
   pr_emerg("BUG: workqueue lockup - pool");
   pr_cont_pool_info(pool);
   pr_cont(" stuck for %us!\n",
    jiffies_to_msecs(jiffies - pool_ts) / 1000);
  }
 }

 rcu_read_unlock();

 if (lockup_detected)
  show_workqueue_state();

 wq_watchdog_reset_touched();
 mod_timer(&wq_watchdog_timer, jiffies + thresh);
}

void wq_watchdog_touch(int cpu)
{
 if (cpu >= 0)
  per_cpu(wq_watchdog_touched_cpu, cpu) = jiffies;
 else
  wq_watchdog_touched = jiffies;
}

static void wq_watchdog_set_thresh(unsigned long thresh)
{
 wq_watchdog_thresh = 0;
 del_timer_sync(&wq_watchdog_timer);

 if (thresh) {
  wq_watchdog_thresh = thresh;
  wq_watchdog_reset_touched();
  mod_timer(&wq_watchdog_timer, jiffies + thresh * HZ);
 }
}

static int wq_watchdog_param_set_thresh(const char *val,
     const struct kernel_param *kp)
{
 unsigned long thresh;
 int ret;

 ret = kstrtoul(val, 0, &thresh);
 if (ret)
  return ret;

 if (system_wq)
  wq_watchdog_set_thresh(thresh);
 else
  wq_watchdog_thresh = thresh;

 return 0;
}

static const struct kernel_param_ops wq_watchdog_thresh_ops = {
 .set = wq_watchdog_param_set_thresh,
 .get = param_get_ulong,
};

module_param_cb(watchdog_thresh, &wq_watchdog_thresh_ops, &wq_watchdog_thresh,
  0644);

static void wq_watchdog_init(void)
{
 wq_watchdog_set_thresh(wq_watchdog_thresh);
}


static inline void wq_watchdog_init(void) { }


static void __init wq_numa_init(void)
{
 cpumask_var_t *tbl;
 int node, cpu;

 if (num_possible_nodes() <= 1)
  return;

 if (wq_disable_numa) {
  pr_info("workqueue: NUMA affinity support disabled\n");
  return;
 }

 wq_update_unbound_numa_attrs_buf = alloc_workqueue_attrs(GFP_KERNEL);
 BUG_ON(!wq_update_unbound_numa_attrs_buf);






 tbl = kzalloc(nr_node_ids * sizeof(tbl[0]), GFP_KERNEL);
 BUG_ON(!tbl);

 for_each_node(node)
  BUG_ON(!zalloc_cpumask_var_node(&tbl[node], GFP_KERNEL,
    node_online(node) ? node : NUMA_NO_NODE));

 for_each_possible_cpu(cpu) {
  node = cpu_to_node(cpu);
  if (WARN_ON(node == NUMA_NO_NODE)) {
   pr_warn("workqueue: NUMA node mapping not available for cpu%d, disabling NUMA support\n", cpu);

   return;
  }
  cpumask_set_cpu(cpu, tbl[node]);
 }

 wq_numa_possible_cpumask = tbl;
 wq_numa_enabled = true;
}

static int __init init_workqueues(void)
{
 int std_nice[NR_STD_WORKER_POOLS] = { 0, HIGHPRI_NICE_LEVEL };
 int i, cpu;

 WARN_ON(__alignof__(struct pool_workqueue) < __alignof__(long long));

 BUG_ON(!alloc_cpumask_var(&wq_unbound_cpumask, GFP_KERNEL));
 cpumask_copy(wq_unbound_cpumask, cpu_possible_mask);

 pwq_cache = KMEM_CACHE(pool_workqueue, SLAB_PANIC);

 cpu_notifier(workqueue_cpu_up_callback, CPU_PRI_WORKQUEUE_UP);
 hotcpu_notifier(workqueue_cpu_down_callback, CPU_PRI_WORKQUEUE_DOWN);

 wq_numa_init();


 for_each_possible_cpu(cpu) {
  struct worker_pool *pool;

  i = 0;
  for_each_cpu_worker_pool(pool, cpu) {
   BUG_ON(init_worker_pool(pool));
   pool->cpu = cpu;
   cpumask_copy(pool->attrs->cpumask, cpumask_of(cpu));
   pool->attrs->nice = std_nice[i++];
   pool->node = cpu_to_node(cpu);


   mutex_lock(&wq_pool_mutex);
   BUG_ON(worker_pool_assign_id(pool));
   mutex_unlock(&wq_pool_mutex);
  }
 }


 for_each_online_cpu(cpu) {
  struct worker_pool *pool;

  for_each_cpu_worker_pool(pool, cpu) {
   pool->flags &= ~POOL_DISASSOCIATED;
   BUG_ON(!create_worker(pool));
  }
 }


 for (i = 0; i < NR_STD_WORKER_POOLS; i++) {
  struct workqueue_attrs *attrs;

  BUG_ON(!(attrs = alloc_workqueue_attrs(GFP_KERNEL)));
  attrs->nice = std_nice[i];
  unbound_std_wq_attrs[i] = attrs;






  BUG_ON(!(attrs = alloc_workqueue_attrs(GFP_KERNEL)));
  attrs->nice = std_nice[i];
  attrs->no_numa = true;
  ordered_wq_attrs[i] = attrs;
 }

 system_wq = alloc_workqueue("events", 0, 0);
 system_highpri_wq = alloc_workqueue("events_highpri", WQ_HIGHPRI, 0);
 system_long_wq = alloc_workqueue("events_long", 0, 0);
 system_unbound_wq = alloc_workqueue("events_unbound", WQ_UNBOUND,
         WQ_UNBOUND_MAX_ACTIVE);
 system_freezable_wq = alloc_workqueue("events_freezable",
           WQ_FREEZABLE, 0);
 system_power_efficient_wq = alloc_workqueue("events_power_efficient",
           WQ_POWER_EFFICIENT, 0);
 system_freezable_power_efficient_wq = alloc_workqueue("events_freezable_power_efficient",
           WQ_FREEZABLE | WQ_POWER_EFFICIENT,
           0);
 BUG_ON(!system_wq || !system_highpri_wq || !system_long_wq ||
        !system_unbound_wq || !system_freezable_wq ||
        !system_power_efficient_wq ||
        !system_freezable_power_efficient_wq);

 wq_watchdog_init();

 return 0;
}
early_initcall(init_workqueues);
